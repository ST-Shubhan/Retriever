{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88857f4a-241e-4958-8280-ed52c56953ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import openai\n",
    "from langchain.llms import openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_transformers import (\n",
    "    EmbeddingsRedundantFilter,\n",
    "    EmbeddingsClusteringFilter,\n",
    ")\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader , PdfWriter, PdfMerger\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeece5e9-9bc1-41f9-90d8-3756747406bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model Loaded..........\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "print(\"Embedding Model Loaded..........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f9d4d5-09c0-44ce-858a-60bcad8ae9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection already exists\n",
      "-0.0003512999992381083\n"
     ]
    }
   ],
   "source": [
    "def load_chunk_persist_pdf() -> Chroma:\n",
    "    start = timeit.timeit()\n",
    "    pdf_folder_path = \"./data\"\n",
    "    documents = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "    text_splitter = CharacterTextSplitter(separator = \"\\n\\n\",chunk_size=1000, chunk_overlap=10)\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    client = chromadb.Client()\n",
    "    if client.list_collections():\n",
    "        consent_collection = client.create_collection(\"consent_collection\")\n",
    "    else:\n",
    "        print(\"Collection already exists\")\n",
    "    \n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunked_documents,\n",
    "        embedding=hf,\n",
    "        persist_directory=\"store/chroma/hugging_face/charactertext\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    end = timeit.timeit()\n",
    "    print(start - end)\n",
    "    return vectordb,chunked_documents\n",
    "\n",
    "_,c_docs = load_chunk_persist_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d2e9da-a0f1-4e82-b9d6-5adb422d2be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bee1d093-c60b-4401-9e81-3767a409c46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A Resource -efficient FIR Filter Design Based on an \\nRAG Improve d Algorithm  \\n \\nMengwei Hu  \\nNational Demonstration Center for \\nExperimental Physics and Education  \\nSchool of Physics and Technology, \\nWuhan University  \\nWuhan, China  \\nhumengv@whu.edu.cn  Xianyang Jiang*  \\nNational Demonstration Center for \\nExperimental Physics and Education  \\nSchool of Physics and Technology, \\nWuhan University  \\nWuhan, China  \\njiang@whu. edu.cn  Zhengxiong Li  \\nNational Demonstration Center for \\nExperimental Physics and Education  \\nHongyi Honor College of Wuhan \\nUniversity  \\nWuhan, China  \\nli_zhengxiong@whu.edu.cn  \\n \\nAbstract ‚ÄîIn modern digital filter chip design, efficient \\nresource u tilization is a hot topic. Due to linear phase \\ncharacteristics of FIR filters, a pulsed fully parallel structure \\ncan be applied to attack the problem . In order to further reduce \\nhardware resource consumption especially caused by \\nmultiplication function, an improved RAG algorithm is \\nproposed. Filters with different orders and for different \\nalgorithms are compared, and the experimental results show \\nthat the improved RAG algorithm is excellent in terms of logic \\nresource utilization, resource allocation, running  speed, and \\npower consumption under  different application scenarios. The \\nproposed algorithm invokes a better circuit structure for FIR \\nfilter , it gives full play to resource allocation strategy and \\nreduces logic resource consumption. The proposed circuit i s \\nfaster and more stable, and suitable for a variety of complex \\napplication scenarios.  \\nKeywords ‚ÄîFIR filter, pulsed fully parallel structure, \\nimproved RAG algorithm, resource allocation strategy.  \\nI. INTRODUCTION  \\nFIR filters have a wide range of applications in  \\ncommunication, audio processing, image processing, and \\nother fields. With blowout type increase ment  of portable \\ndevices and Internet of Things, there is an increasing demand \\nfor low power consumption and small size in this field. FIR \\nfilter design also ev olves in this tide to meet requirements of \\nembedded systems. Efficient FIR filter design methods have \\nbeen continuously explored to reduce computational cost and \\nproduct cost, which may involve new optimization algorithms, \\napproximation techniques, and res ource allocation strategies.  \\nFIR filters have an important linear phase property, which \\nallows us to exploit the symmetry of coefficients to build both \\nserial and parallel structures. Compared to IIR filters, FIR \\nfilters have many significant advantages su ch as bounded \\ninput and output stability, phase linearity, and low coefficient \\nsensitivity, which makes it more suitable under many \\napplications [1]. However, FIR filters involve a large number \\nof arithmetic operations , which limits their processing speed \\n[2]. In order to overcome this limitation, a fully parallel \\nstructure can be exploited which allows a single filtering \\noperation to perform multiple multiplications simultaneously \\nto improve performance. Compared to fully parallel FIR \\nfilters, adoption of improved Reduced Adder Graph (RAG) \\nalgorithm [3] can significantly reduce hardware consumption \\nby exploiting redundancy between coefficients [4]. Our work focuses on how to efficiently implement FIR filters with fixed \\ncoefficients. In the proposed design, fully parallel structure \\nand RAG algorithm characteristics have been utilized to \\neffectively reduce hardware cost. Meanwhile, a better \\nresource allocation strategy is taken to further improve FIR \\nfilter implementation.  \\nII. PULSED F ULLY PARALLEL FIR  FILTERS  \\nFIR filters differ from IIR filters in that their impulse \\nresponse can be expressed in terms of a finite number of \\nsampled values and can be described by a difference equation \\n(1), where N is the number of filters tap coefficients and x(n) \\nis the input time s eries [5].  \\n   ùë¶(ùëõ)=‚àë‚Ñé(ùëò)ùë•(ùëõ‚àíùëò)ùëÅ‚àí1\\nùëò=0       (\\uf031) \\nIn order to implement an efficient specific circuit for FIR, \\nthe \"pulsation\" structure, originally proposed by H.T. Kung, \\nrepresents a parallel pipelined approach for high -speed signal \\nprocessing and data processing. This architecture is known for \\na number  of advantages such as modularity, regularity, local \\nlinking, and high degree of pipelining. In pulsation \\narchitecture, Process Element (PE) constitutes a \\nmultiprocessor system and these PEs work together in a \\nsynchronized manner so that this architecture offers \\nsignificant performance in processing large -scale data. The \\nhardware structure of a pulsation FIR filter is shown in Fig. 1, \\nand N PEs are required to accomplish one such operation.  \\n \\nFig. 1  Hardware structure of pulsation FIR filter  \\nAs for an FIR fil ter with symmetric coefficients, its linear \\nphase property can be further utilized to reduce the number of \\nPEs by pre -addition. For example, a hardware structure of \\neven -symmetric filter is shown in Fig. 2, and it is clear that the \\nnumber of PEs is reduced  to N/2 for the same N tap \\ncoefficients.   \\nZ -1 Z -1 Z -1Z -1\\nout\\n‚Ä¶‚Ä¶\\nPE1din\\nPE2 PE3 PEn0h(0) h(1) h(2) h(n-1)', metadata={'source': './data\\\\A Resource-efficient FIR Filter Design Based on an.pdf', 'page': 0}),\n",
       " Document(page_content='Fig. 2  Structure of pulsation FIR filter with even symmetry of coefficients  \\nIII. FILTER IMPLEMENTATION OF RAG  IMPROVEMENT \\nALGORITHM  \\nA. RAG Improved Algorithm  \\nPulsation filter designs are usually based on PEs that have \\nidentical coefficients. Since such coefficients are constant and \\nshift operations can be easily implemented in hardware, \\nmultiplication can be replaced using shift, add, and subtract \\noperations to reduc e resource consumption of multiplier. The \\nconcept of u sing an addition tree in a multiplier was first \\nintroduced by Bull, who advocated the implementation of \\nmultiplication by constructing a structural graph consisting of \\nsimple addition and shift operations, A.G. Dempster and M.D. \\nMacleod proposed RAG algori thm [ 6], the core idea of this \\nalgorithm is to use an equivalent structure to convert all \\ncoefficients to bases, and at the same time, introduce \\nsubtraction structure to make all intermediate values positive, \\nwhich greatly simplifies the structure of addit ion tree. The \\nredundancy relationship between coefficients is also utilized \\nto reduce logical depth in order to cut down the total resource \\nconsumption [ 7,8]. \\nIn addition, the RAG algorithm introduces a concept \\ncalled adder depth \"cost\". Despite obvious ad vantages of \\nRAG algorithm, its disadvantage lies in the need to calculate \\ncost value for each coefficient. For smaller coefficients, cost \\nvalue can be obtained directly by looking up the table, but \\nwhen coefficients are larger, cost value calculation becom es \\nmore difficult [1 1]. \\nIn order to address shortcomings of the RAG algorithm in \\ncircuit design, an improved algorithm is proposed, which \\ncombines the advantages of pulsation structure fully parallel \\nFIR filter and the RAG algorithm structure FIR filter.  \\nLet \"coeff\" be all filter coefficients to be realized, \"coeff -\\nr\" is the set of smaller coefficients, \"coeff -s\" is the set of larger \\ncoefficients. \"cost -n\" (n=1,2,3,4) is the set of coefficients with \\ndifferent adder depths, \"cost -o\" is the set of coefficient s with \\nother adder depths, and the improved algorithm [ 9-12] are as \\nfollows:  \\n‚ö´ Take the absolute values of all coefficients and store the \\nresults in \"coeff\" set;  \\n‚ö´ Remove duplicate coefficients and coefficients with \\nvalue 2n, and the number of remaining coefficients is \\ndenoted as N;  \\n‚ö´ The smaller coefficients are deposited into set \"coeff -r\", and the number of coefficients deposited is N/2 or (N -\\n1)/2; \\n‚ö´ Deposit the remaining larger coefficients into set \\n\"coeff -s\"; \\n‚ö´ Divide the even numbers in the \" coeff -r\" set by 2n to \\nobtain the base;  \\n‚ö´ Look up the table to get the depth of adder \\ncorresponding to each base number, store these \\ncoefficients in \"cost -n\" set, and store the coefficients \\nthat cannot be categorized by the table in \"cost -o\" set;  \\n‚ö´ Realize coef ficients in \"cost -1\" set;  \\n‚ö´ Check the sum/difference of coefficients in all realized \\ncost sets, realize the coefficients in higher cost sets by \\nthe sum/difference of coefficients and the realized \\ncoefficients, and finally realize the coefficients in \\n\"cost -o\" set; \\n‚ö´ Realize the coefficients in \"coeff -s\" set according to \\nthe hardware structure of pulsation FIR filter with \\nsymmetric coefficients.  \\nB. Implementation Example  \\nTaking a 64th order filter as an example, Fs=250KHz \\nand Fc=20KHz, the filter coefficients after  quantization and \\nrounding [1 3] are shown in Table 1, and due to the symmetry \\nof the coefficients of the FIR filters, only coefficients with \\n0‚â§n‚â§31 need to be di scussed here.  \\nTABLE I.  FILTER COEFFICIENTS , H(N) = H(63-N), 32‚â§N‚â§63 \\nh(0)=219  h(1)=137  h(2)=162  h(3)=174  \\nh(4)=168  h(5)=137  h(6)=79  h(7)= -9 \\nh(8)= -127 h(9)= -269 h(10)= -428 h(11)= -592 \\nh(12)= -747 h(13)= -875 h(14)= -957 h(15)= -972 \\nh(16)= -903 h(17)= -733 h(18)= -450 h(19)= -49 \\nh(20)=470  h(21)=1100  h(22)=1825  h(23)=2622  \\nh(24)=3462  h(25)=4311  h(26)=5134  h(27)=5891  \\nh(28)=6548  h(29)=7072  h(30)=7437  h(31)=7624  \\nFirstly, all the coefficients in the above table are taken as \\nabsolute values, and duplicates  and numbers divisible by 2n \\nare removed, and then the coefficients are divided into the two \\nparts, one part of the coefficients is smaller, which is easy to \\noptimize using the RAG improvement algorithm, and the \\ncoefficients are stored into the \"coeff -r\" set = [9, 49, 79, 127, \\n137, 1 62, 168, 174, 219, 269, 428, 450, 470, 592, 733], and \\none part is larger, and the corresponding coefficients can be \\nstored according to the pulsation structure and the symmetry \\nof the coefficients. 162, 168, 174, 219, 269, 428, 450, 470, \\n592, 733], part s of the coefficients are larger, according to the \\npulsation structure and the symmetry of the coefficients, the \\ncorresponding input signals can be pre -added or subtracted, \\nand then multiplied by the filter coefficients, which are \\ndeposited in the \"coeff -s\" collection. The coefficients are \\nstored in the set of \"coeff -s\" = [747, 875, 957, 972, 903, 1100, \\n1825, 2622, 3462, 4311, 5134, 5891, 6548, 7072, 7437, 7624].  \\nCheck the adder depth table to categorize the coefficients \\nand store the coefficients in the corre sponding sets:  \\n\"cost -1\" set = [9, 127];   \\nout‚Ä¶‚Ä¶\\nPE1din\\nPE2 PE3 PEn0h(0) h(1) h(2) h(n-1)Z -1Z -1\\nZ -1 Z -1Z -1 Z -1Z -1 ‚Ä¶‚Ä¶', metadata={'source': './data\\\\A Resource-efficient FIR Filter Design Based on an.pdf', 'page': 1}),\n",
       " Document(page_content='\"cost -2\" set = [49, 79, 137, 162, 168];  \\n\"cost -3\" set = [174, 219];  \\n\"cost -o\" set = [269, 428, 450, 470, 592, 733].  \\nFirst realize the coefficients of cost -1: \\n {x9 = xin<<3 + xin,   \\nx127 = xin<<7 ‚Äì xin, \\nContinue to reali ze the cost -2 factor : \\n \\n{    x49 = x9<<4 + x9 + xin<<2,        \\nx79 = x127 ‚Äì x49 + xin,                           \\nx137 = x127 + x9 + xin,                         \\nx162 = x137 + x9 + xin<<4,             \\nx168 = x162 + xin<<2 + xin<<1, \\nThe cost -3 factor is then reali zed: \\n {x174 = x127 + x49 ‚Äì xin<<1,\\nx219 = x168 + x49 ‚Äì xin<<1, \\nThe flexible use of multiplexing makes it possible to use \\nonly two layers of adder depth for the coefficients of cost -3 as \\nwell, and then gradually implement the other coefficients at \\nthe end. The final result of the RAG improvement algorithm \\ndesign is as follows:  \\n {x9 = xin<<3 + xin,   \\nx127 = xin<<7 ‚Äì xin, \\n \\n{    x49 = x9<<4 + x9 + xin<<2,        \\nx79 = x127 ‚Äì x49 + xin,                           \\nx137 = x127 + x9 + xin,                         \\nx162 = x137 + x9 + xin<<4,             \\nx168 = x162 + xin<<2 + xin<<1, \\n {x174 = x127 + x49 ‚Äì xin<<1,\\nx219 = x168 + x49 ‚Äì xin<<1, \\n {x450 = x428 + x9<<1 + xin<<2,\\nx470 = x450 + x9<<1 + xin<<1,\\nx592 = x450 + x269 ‚Äì x127,                 \\nx733 = x592 + x137 + xin<<2,       \\nComparing to the pre -optimization design, which uses a \\ntotal of  28 adders, performs 7 shift operations and keeps the \\nadder depth at 2 and below, saves more than half of the total \\nnumber of adders and a large number of shift operations, while \\nalso reducing the adder depth, compared to the unimproved \\nalgorithm.  \\nIV. COMPARI SON OF HARDWARE SYNTHESIS RESULTS  \\nThe consumption of FPGA hardware resources can be \\nmeasured by FPGA LUT resources, FF register resources, and \\nDSP resources, and the hardware performance can be \\nmeasured by power consumption and device junction \\ntemperature [9,14]. In our design , we adopt a  Virtex -7 series \\nxc7vx485tffg1157 -1 FPGA, and implement and compare \\npulsed fully parallel structure, traditional RAG algorithmic \\nstructure, and RAG improved algorithmic structure according \\nto these measuements . The realizat ion results are shown in \\nTable 2 and Figure 3.  \\nTABLE II.  COMPARISON OF 64TH ORDER FILTER HARDWARE  Resource \\nperformance \\nindicators  different algorithmic structures  \\nPulsed Fully \\nParallel  RAG \\nalgorithm  RAG Improved \\nAlgorithm  \\nLUT  574 4956 934 \\nFF 1286 528 904 \\nDSP 4 0 2 \\nPower( W) 32.8 234.7 38.6 \\nTem(‚ÑÉ) 70.8 125.0 79 \\n \\n \\nFig. 3  Comprehensive comparison of 64th order filter hardware  \\nFrom the results, we can see that the pulsation structure \\nfully parallel filter uses the most DSP and FF and the least \\nLUT resources, while the conventional RAG algorithmic \\nstructure filter does not use DSP and consumes the least FF \\nresources, but it is cl ear that the RAG algorithmic structure \\nuses too much LUT resources compared  to the other two \\nstructures to the extent that the power and junction \\ntemperatures are too high to be used for practical applications. \\nThe RAG improved algorithmic structure filter reduces the \\nDSP usage by half and the FF resources by about 29.7% \\ncompared  to the pulsed fully parallel structure, while the LUT \\nresources are reduced by about 81.2% compared  to the RAG \\nalgorithmic structure while achieving low power consumption \\nand junction temperature.  \\nTable 3 and 4 show the hardware synthesis comparison of \\nthe 8th and 32nd order filters, respectively, to validate the \\nhardware synthesis results for different order filters.  \\nTABLE III.  COMPARISON OF 8TH ORDER FILTER HARDWARE  \\nResource \\nperformance \\nindicators  different algorithmic structures  \\nPulsed Fully \\nParallel  RAG \\nalgorithm  RAG Improved \\nAlgorithm  \\nLUT  141 212 185 \\nFF 203 120 222 \\nDSP 4 0 2 \\nPower( W) 41.762  33.673  36.75  \\nTem(‚ÑÉ) 83.4 72.1 76.4 \\nIt can be seen that the traditional RAG algorithm structure \\nhas the best integrated performance when the filter order is 8, \\nthe RAG improved algorithm structure has excellent \\nperformance, and the puls ed fully parallel structure has the \\nworst integrated performance, and there is little difference in \\nthe integrated performance of the three except for the DSP \\nresource consumption.', metadata={'source': './data\\\\A Resource-efficient FIR Filter Design Based on an.pdf', 'page': 2}),\n",
       " Document(page_content='TABLE IV.  COMPARISON OF 32ND ORDER FILTER HARDWARE  \\nResource \\nperformance \\nindicators  different algorithmic structures  \\nPulsed Fully \\nParallel  RAG \\nalgorithm  RAG Improved \\nAlgorithm  \\nLUT  358 695 555 \\nFF 679 287 538 \\nDSP 4 0 2 \\nPower( W) 21.34  24.52  19.75  \\nTem(‚ÑÉ) 54.8 59.3 52.6 \\n While the puls ed fully parallel structure consumes the \\nmost FF and DSP resources and the RAG algorithm structure \\nconsumes the most LUT resources when the filter order is 32, \\nthe LUT, FF, and DSP resource consumption of the RAG \\nimproved algorithm structure filter are all in the middle \\nbetween the puls ed fully parallel structure and the traditional \\nRAG algorithm filter, which are more balanced in logic \\ndistribution, and the power consumption and junction \\ntemperature are the lowest  among the three [1 5]. \\n Comprehensive comparison concluded that the puls ed \\nfully parallel structure is more suitable for higher order filters, \\nthe traditional RAG algorithm structure is  more  suitable for \\nlow order filters, and regardless of the order  numbe r, the RAG \\nimproved algorithm structure filters have excellent \\nperformance, effectively take advantage of the resource \\nallocation strategy,  and achieve low power consumption, \\nlow junction temperature , and optimal resource consumption \\nfor 32-order and 64 -order filters . Specifically, the 64th order \\nfilter reduces DSP usage and balances logic resource \\nconsumption, and stabilizes power consumption and junction \\ntemperature  as well. The proposed design  is very suitable for \\napplication s in the case of many coefficients and high \\ncomplexity.  \\nV. CONCLUSION  \\nFIR filters exists in more and more application scenarios, \\nand practical requirements are becoming more and more \\nindividualized, so novel design solutions must b e \\ncontinuously explored to meet these requirements under \\nvarious scenarios. The algorithmic structure described \\nprovides a new design scheme that is suitable for most \\napplication scenarios and is well suited when the number of \\nfilter orders is large or the  coefficients are large. FIR filter \\ndesign for improved RAG algorithm gives full play to \\nresource allocation strategy by using shift and add operations \\ninstead of a direct multiplier structure. Combining with \\npulsation fully parallel structure, both operat ion speed and \\nresource utilization efficiency are enhanced . Comparative \\nsimulation experiments demonstrate that the  improved RAG \\nalgorithm and filter structure save a large amount of logic \\nresources, meet low -power requirements, and decrease  \\ncontradiction between speed and resource -consumption as \\nwell.  \\nACKNOWLEDGMENT  \\nThis work was supported in part by National Science \\nFoundation of China under Grant 61072135, 81971702, the \\nFundamental Research Funds for the Central Universities \\nunder Grant 2042017gf0075, 20 42019gf0072, and Natural \\nScience Foundation of Hubei Province under Grant \\n2017CFB721.  REFERENCES  \\n[1] Abhijit Chandra, Sudipta Chattopadhyay,  Design of hardware efficient \\nFIR filter: A review of the state -of-the-art approaches,  Engineering \\nScience and Technolog y, an International Journal,  Volume 19, Issue 1,  \\n2016,  Pages 212 -226, ISSN 2215 -0986.  \\n[2] A. Pathan, A. H. Chandio and R. Aziz, \"An Optimization in \\nConventional Shift &Add Multiplier for Area -Efficient \\nImplementation on FPGA,\" 2022 International Conference on \\nEmerging Technologies in Electronics, Computing and \\nCommunication (ICETECC), Jamshoro, Sindh, Pakistan, 2022, pp. 1 -\\n6, doi: 10.1109/ICETECC56662.2022.10069099.  \\n[3] A. G. Dempster and M. D. Macleod, \"Use of minimum -adder \\nmultiplier blocks in FIR digital filters ,\" in IEEE Transactions on \\nCircuits and Systems II: Analog and Digital Signal Processing, vol. 42, \\nno. 9, pp. 569 -577, Sept. 1995, doi: 10.1109/82.466647.  \\n[4] U. Meyer -Baese, J. Chen, C. H. Chang and A. G. Dempster, \"A \\nComparison of Pipelined RAG -n and DA FPGA -based Multiplierless \\nFilters,\" APCCAS 2006 - 2006 IEEE Asia Pacific Conference on \\nCircuits and Systems, Singapore, 2006, pp. 1555 -1558, doi: \\n10.1109/APCCAS.2006.342540.  \\n[5] U. Meyer -Baese, \"Digital Signal Processing with Field Programmable \\nGate Arrays,\" T allahassee, USA: Springer Berlin, Heidelberg, Sept. \\n2007, doi: 10.1007/978 -3-540-72613 -5, ISSN 1860 -4862.  \\n[6] O. Gustafsson, \"A Difference Based Adder Graph Heuristic for \\nMultiple Constant Multiplication Problems,\" 2007 IEEE International \\nSymposium on Circuits  and Systems, New Orleans, LA, USA, 2007, \\npp. 1097 -1100, doi: 10.1109/ISCAS.2007.378201.   \\n[7] M. Potkonjak, M. B. Srivastava and A. P. Chandrakasan, \"Multiple \\nconstant multiplications: efficient and versatile framework and \\nalgorithms for exploring common subex pression elimination,\" in IEEE \\nTransactions on Computer -Aided Design of Integrated Circuits and \\nSystems, vol. 15, no. 2, pp. 151 -165, Feb. 1996, doi: \\n10.1109/43.486662.  \\n[8] A. G. Dempster and M. D. Macleod, \"Comments on \"Minimum \\nnumber of adders for implementi ng a multiplier and its application to \\nthe design of multiplierless digital filters\",\" in IEEE Transactions on \\nCircuits and Systems II: Analog and Digital Signal Processing, vol. 45, \\nno. 2, pp. 242 -243, Feb. 1998, doi: 10.1109/82.661661.  \\n[9] S. Mirzaei, A. Hos angadi and R. Kastner, \"FPGA Implementation of \\nHigh Speed FIR Filters Using Add and Shift Method,\" 2006 \\nInternational Conference on Computer Design, San Jose, CA, USA, \\n2006, pp. 308 -313, doi: 10.1109/ICCD.2006.4380833.  \\n[10] Jeong -Ho Han and I. -C. Park, \"Digita l filter synthesis considering \\nmultiple adder graphs for a coefficient,\" 2008 IEEE International \\nConference on Computer Design, Lake Tahoe, CA, USA, 2008, pp. \\n315-320, doi: 10.1109/ICCD.2008.4751879.  \\n[11] A. Abbaszadeh, A. Azerbaijan and K. D. Sadeghipour, \"A n ew \\nhardware efficient reconfigurable fir filter architecture suitable for \\nFPGA applications,\" 2011 17th International Conference on Digital \\nSignal Processing (DSP), Corfu, Greece, 2011, pp. 1 -4, doi: \\n10.1109/ICDSP.2011.6004958.  \\n[12] H V Kumaraswamy, AmruthKaran th P, Samarth Athreyas and Akash \\nBharadwaj B R, \"  Comparative Analysis of Different Area-efficient Fir \\nFilter Structures for Symmetric Convolutions ,\" International Journal of \\nElectrical and Electronic Engineering & Telecommunications, Vol. 3, \\nNo. 2, pp. 50 -55, April 2014.  \\n[13] Y. J. Yu and Y. C. Lim, \"Design of Linear Phase FIR Filters in \\nSubexpression Space Using Mixed Integer Linear Programming,\" in \\nIEEE Transactions on Circuits and Systems I: Regular Papers, vol. 54, \\nno. 10, pp. 2330 -2338, Oct. 2007, doi: 10. 1109/TCSI.2007.904599.  \\n[14] LI Ying,LU Weijun,YU Dunshan et al. A resource optimization \\nalgorithm for implementing FIR digital filters on FPGA[J]. Journal of \\nPeking University (Natural Science Edition),2009,45(02):222 -\\n226.doi:10.13209/j.0479 -8023.2009.034.  \\n[15] Nakka Sivaraju and S Suman, \"  Area Power and Delay Efficiency \\nEvaluation of Truncated and Modified Wallace Fir Filters ,\" \\nInternational Journal of Electrical and Electronic Engineering & \\nTelecommunications, Vol. 4, No. 4, pp. 43 -52, October 2015.', metadata={'source': './data\\\\A Resource-efficient FIR Filter Design Based on an.pdf', 'page': 3}),\n",
       " Document(page_content='A Survey on Retrieval-Augmented Text Generation\\nHuayang Li‚ô•,‚àóYixuan Su‚ô†,‚àóDeng Cai‚ô¶,‚àóYan Wang‚ô£,‚àóLemao Liu‚ô£,‚àó\\n‚ô•Nara Institute of Science and Technology‚ô†University of Cambridge\\n‚ô¶The Chinese University of Hong Kong‚ô£Tencent AI Lab\\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\\nthisisjcykcd@gmail.com, brandenwang@tencent.com\\nlemaoliu@gmail.com\\nAbstract\\nRecently, retrieval-augmented text generation\\nattracted increasing attention of the compu-\\ntational linguistics community. Compared\\nwith conventional generation models, retrieval-\\naugmented text generation has remarkable ad-\\nvantages and particularly has achieved state-of-\\nthe-art performance in many NLP tasks. This\\npaper aims to conduct a survey about retrieval-\\naugmented text generation. It Ô¨Årstly highlights\\nthe generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable ap-\\nproaches according to different tasks including\\ndialogue response generation, machine trans-\\nlation, and other generation tasks. Finally, it\\npoints out some promising directions on top of\\nrecent methods to facilitate future research.\\n1 Introduction\\nRetrieval-augmented text generation, as a new\\ntext generation paradigm that fuses emerging deep\\nlearning technology and traditional retrieval tech-\\nnology, has achieved state-of-the-art (SOTA) per-\\nformance in many NLP tasks and attracted the at-\\ntention of the computational linguistics community\\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\\n2021). Compared with generation-based counter-\\npart, this new paradigm has some remarkable ad-\\nvantages: 1) The knowledge is not necessary to be\\nimplicitly stored in model parameters, but is explic-\\nitly acquired in a plug-and-play manner, leading\\nto great scalibility; 2) Instead of generating from\\nscratch, the paradigm generating text from some re-\\ntrieved human-written reference, which potentially\\nalleviates the difÔ¨Åculty of text generation.\\nThis paper aims to review many representative\\napproaches for retrieval-augmented text generation\\ntasks including dialogue response generation (We-\\nston et al., 2018), machine translation (Gu et al.,\\n2018) and others (Hashimoto et al., 2018). We\\n‚àóAll authors contributed equally.Ô¨Årstly present the generic paradigm of retrieval-\\naugmented generation as well as three key com-\\nponents under this paradigm, which are retrieval\\nsources, retrieval metrics and generation models.\\nThen, we introduce notable methods about\\nretrieval-augmented generation, which are orga-\\nnized with respect to different tasks. SpeciÔ¨Åcally,\\non the dialogue response generation task, exem-\\nplar/template retrieval as an intermediate step has\\nbeen shown beneÔ¨Åcial to informative response gen-\\neration (Weston et al., 2018; Wu et al., 2019; Cai\\net al., 2019a,b). In addition, there has been growing\\ninterest in knowledge-grounded generation explor-\\ning different forms of knowledge such as knowl-\\nedge bases and external documents (Dinan et al.,\\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\\n2021). On the machine translation task, we summa-\\nrize the early work on how the retrieved sentences\\n(called translation memory) are used to improve\\nstatistical machine translation (SMT) (Koehn et al.,\\n2003) models (Simard and Isabelle, 2009; Koehn\\nand Senellart, 2010) and in particular, we inten-\\nsively highlight several popular methods to inte-\\ngrating translation memory to NMT models (Gu\\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\\nHe et al., 2021). We also review the applications\\nof retrieval-augmented generation in other genera-\\ntion tasks such as abstractive summarization (Peng\\net al., 2019), code generation (Hashimoto et al.,\\n2018), paraphrase (Kazemnejad et al., 2020; Su\\net al., 2021b), and knowledge-intensive generation\\n(Lewis et al., 2020b). Finally, we also point out\\nsome promising directions on retrieval-augmented\\ngeneration to push forward the future research.\\n2 Retrieval-Augmented Paradigm\\nIn this section, we Ô¨Årst give a general formulation\\nof retrieval-augmented text generation. Then, we\\ndiscuss three major components of the retrieval-\\naugmented generation paradigm, including the re-arXiv:2202.01110v2  [cs.CL]  13 Feb 2022', metadata={'source': './data\\\\a survey.pdf', 'page': 0}),\n",
       " Document(page_content='Input\\nSources (Sec. 2.2):Training CorpusExternal DataUnsupervised DataMetrics(Sec. 2.3):Sparse-vector RetrievalDense-vector RetrievalTask-specific RetrievalRetrieval MemoryGeneration ModelSec. 4: Machine TranslationSec. 5: Other TasksData AugmentationAttention MechanismSkeleton & TemplatesInformation RetrievalTasks:Sec. 3: Dialogue GenerationModels (Sec 2.4):OutputFigure 1: The overview of this survey.\\ntrieval source, retrieval metric and integration meth-\\nods.\\n2.1 Formulation\\nMost text generation tasks can be formulated as a\\nmapping from input sequence xto output sequence\\ny:y=f(x). For instance, xandycould be the\\ndialogue history and the corresponding response\\nfor dialogue response generation, the text in the\\nsource language and the translation in the target\\nlanguage for machine translation, and so on.\\nRecently, some researchers suggest to endow\\nmodels the capability to access external memory\\nvia some information retrieval techniques, so that\\nthey can acquire more information in the generation\\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\\net al., 2019b). The retrieval-augmented generation\\ncan be further formulated as:\\ny=f(x,z) (1)\\nwhere z={‚ü®xr,yr‚ü©}is a set of relevant instances\\nretrieved from the original training set or external\\ndatasets. The main idea of this paradigm is that yr\\nmay beneÔ¨Åt the response generation, if xr(oryr)\\nis similar (or relevant) to the input x. It is worth\\nnoting that xr=‚àÖwhen unsupervised retrieval\\nsources are used. In general, the retrieval mem-\\nory can be retrieved from three kinds of sources:\\nthe training corpus, external datasets in the same\\nformat with the training corpus, and large-scale\\nunsupervised corpus (¬ß2.2). Metrics that evaluate\\nthe relevance between text are varied as well, in\\n¬ß2.3 we divided them into three categories: sparse-\\nvector retrieval, dense-vector retrieval, and training-\\nbased retrieval. Finally, how to integrate the re-\\ntrieval memory to the generation model is also sig-\\nniÔ¨Åcant, we also introduce some popular integra-\\ntion approaches in ¬ß2.4.2.2 Retrieval Sources\\nTraining Corpus Most previous studies search\\nthe external memory from its training corpus (Song\\net al., 2016; Gu et al., 2018; Weston et al., 2018).\\nIn the inference time, retrieved examples with high\\nrelevant scores could be regarded as extra refer-\\nences and reduce model‚Äôs uncertainty in generation.\\nThe main motivation of those works is to to store\\nknowledge not only in the model parameters but\\nalso in an explicit and accessible form, making the\\nmodel be able to re-access it during inference.\\nExternal Data Some researchers also propose to\\nretrieval relevant samples from external datasets\\n(Su et al., 2021c; Xiao et al., 2021). In these stud-\\nies, the retrieval pool is different with the training\\ncorpus, which can further provide additional infor-\\nmation that are not contained in the training corpus.\\nThis is especially beneÔ¨Åcial for applications such\\nas domain adaptation and knowledge update. For\\nexample, Khandelwal et al. (2020a); Zheng et al.\\n(2021a) employ the in-domain dataset as the exter-\\nnal memory to achieve fast domain adaptation for\\nmachine translation.\\nUnsupervised Data One limitation for previous\\ntwo sources is that the datasets have to be super-\\nvised datasets consisting of aligned input-output\\npairs. For machine translation, Cai et al. (2021) pro-\\npose a cross-lingual retriever to directly retrieve tar-\\nget sentence from unsupervised corpus (i.e., mono-\\nlingual corpus in the target language). The main\\nidea is aligning source-side sentences and the corre-\\nsponding target-side translations in a dense vector\\nspace, i.e., aligning xandyrwhen xris absent.\\nAs a result, the retriever directly connects the dots\\nbetween the source-side input and target-side trans-\\nlations, enabling monolingual data in the target', metadata={'source': './data\\\\a survey.pdf', 'page': 1}),\n",
       " Document(page_content='language to be used alone as memories.\\n2.3 Retrieval Metrics\\nSparse-vector Retrieval Given an input se-\\nquence xand a retrieval corpus, retrieval model\\naims to retrieve a set of relevant examples z=\\n{‚ü®xr,yr‚ü©}from the corpus. When a supervised\\ncorpus is used,{‚ü®xr,yr‚ü©}is retrieved by measur-\\ning the similarity between xandxr. For simi-\\nlarity measurement, sparse-vector retrieval meth-\\nods such as TF-IDF and BM25 (Robertson and\\nZaragoza, 2009) are widely used. They match key-\\nwords efÔ¨Åciently with an inverted index.\\nDense-vector Retrieval However, these meth-\\nods prefer examples with similar surfaces, and may\\nfail to retrieve examples that are only semantically\\nrelevant. To alleviate above problem, some stud-\\nies (Cao and Xiong, 2018) attempt to retrieve in\\ndense-vector space instead of the lexical overlap.\\nRecent work (Lee et al., 2019) makes use of pre-\\ntrained language models, which encodes the text to\\nlow-dimensional dense vectors via BERT-based en-\\ncoders. The retrieval score are computed via inner\\nproducts between vectors.\\nTask-speciÔ¨Åc Retrieval Similarity-based re-\\ntrieval is based on a simple heuristic. That is, the\\nmore xrresembles with x, the more likely xr\\nandyrwill help the generation. However, the\\nmost similar one by universal textual similarity\\ndoes not necessarily serve the best for downstream\\nmodels. Ideally, the retrieval metric would be\\nlearned from the data in a task-dependent way: we\\nwish to consider a memory only if it can indeed\\nboost the quality of Ô¨Ånal generation. To this end,\\nCai et al. (2021) propose to unify the memory\\nretriever and its downstream generation model\\ninto a learnable whole. Such memory retrieval is\\nend-to-end optimized for task-speciÔ¨Åc objectives.\\n2.4 Integration\\nData Augmentation There are several ways to\\nintegrate the retrieved external memory in gener-\\nation. One straightforward way is data augmen-\\ntation , which constructs some augmented inputs\\nby concatenating spans from {‚ü®xr,yr‚ü©}with the\\noriginal input x. By training on the augmented\\ninputs, a generation model implicitly leans how\\nto integrate the retrieved information. Despite the\\nsimplicity, this kind of methods works efÔ¨Åciently\\nin lots of tasks (Song et al., 2016; Weston et al.,\\n2018; Bulte and Tezcan, 2019).Attention Mechanisms Another integration\\nmethod is based on attention mechanisms\\n(Bahdanau et al., 2014). The main idea of this\\nfashion is adopting additional encoders (in various\\narchitectures) to encode retrieved target sentences,\\nand integrate them through attention (Cao and\\nXiong, 2018; Gu et al., 2018; Bapna and Firat,\\n2019). Since the attention mechanism is becoming\\n(Bahdanau et al., 2014; Vaswani et al., 2017) a\\nkey module in lots of NLP models, integrating\\nretrieved memory through attention becomes a\\nvery nature and efÔ¨Åcient way.\\nSkeleton Extraction In the previous two meth-\\nods, the downstream generation model learns how\\nto Ô¨Ålter out irrelevant or even harmful informa-\\ntion from the retrieved examples implicitly. There\\nalso exist some works that try to explicitly extract\\nuseful information, i.e., skeleton extraction , from\\nthe retrieved memory (Cai et al., 2019a; Wu et al.,\\n2019; Cai et al., 2019b). For example, one skeleton\\nshould be a part of a whole utterance with irrelevant\\ncontent masked, and the generation model only in-\\ntegrate this skeleton in the generation process.\\n3 Dialogue Response Generation\\nBackground Dialogue systems can be grouped\\ninto two categories: chit-chat systems and task-\\noriented systems. While task-oriented dialogue\\nsystems are designed to accomplish speciÔ¨Åc user\\ntasks such as air tickets booking, chit-chat dialogue\\nsystems aim at giving a meaningful and Ô¨Çuent re-\\nsponse for any dialogue history in the open domain.\\nDialogue response generation in chit-chat dialogue\\nsystem is challenging partly due to the diversity\\nof possible responses to a single dialogue history\\n(i.e., the one-to-many problem). The dialogue his-\\ntory alone cannot decide a meaningful and speciÔ¨Åc\\nresponse. Also, external knowledge that is not\\npresent in the dialogue history are often necessary\\nfor avoiding safe but boring responses. We focus\\non recent efforts tackling the challenges to develop\\nchit-chat dialogue systems.\\nMost modern chit-chat dialogue systems can\\nbe categorized into two classes, namely, retrieval-\\nbased models and generation-based models. The\\nretrieval-based models (Ji et al., 2014; Hu et al.,\\n2014) directly copy an existing response from cu-\\nrated dialogue corpora (i.e., the retrieval pool)\\nwhen receiving a response request. The retrieved\\nresponses are often informative and grammatical\\nas they are collected from real-world conversa-', metadata={'source': './data\\\\a survey.pdf', 'page': 2}),\n",
       " Document(page_content='tions and possibly post-edited by a human. How-\\never, such systems perform poorly when a given\\ndialogue history is substantially different from\\nthose in the retrieval pool. On the other hand,\\nthe generation-based models (Shang et al., 2015;\\nVinyals and Le, 2015; Li et al., 2016a) generate\\na new utterance from scratch. Those generation-\\nbased models have better generalization capacity\\nwhen handling unseen dialogue contexts. Never-\\ntheless, the generated utterances are inclined to be\\ndull and non-informative (e.g., ‚ÄúI don‚Äôt know‚Äù, ‚ÄúI\\nthink so‚Äù, ‚ÄúMe too‚Äù etc.) (Li et al., 2016a).\\nShallow Integration As discussed, retrieval-\\nbased models may give informative but inappro-\\npriate responses while generation-based models\\noften do the opposite. It is desirable to combine the\\nbest of both worlds. Early work (Qiu et al., 2017)\\nattempts to re-rank the output from both models.\\nFor a deep integration, Song et al. (2016) and Yang\\net al. (2019) extend the standard SEQ2SEQencoder-\\ndecoder model (Bahdanau et al., 2014) with an ex-\\ntra encoder for encoding the retrieval result. The\\noutput of the extra encoder, along with the output\\nfrom the original encoder for dialogue history, is\\nused to feed the decoder. Weston et al. (2018) use\\na single encoder that takes the concatenation of\\nthe original dialogue history and the retrieved as\\ninput. Wu et al. (2019) note that the retrieved infor-\\nmation should be used in awareness of the context\\ndifference, and further proposed to construct an\\nedit vector by explicitly encoding the lexical differ-\\nences between the input dialogue history and the\\nretrieved dialogue history. Pandey et al. (2018) fur-\\nther propose to weight different training instances\\nby context similarity.\\nDeep Integration To prevent the inÔ¨Çow of er-\\nroneous information, Cai et al. (2019a) propose\\na general framework that Ô¨Årst extracts a skeleton\\nfrom the retrieved response and then generates the\\nresponse based on the extracted skeleton. This\\nframework is also adopted for stylistic response\\ngeneration (Su et al., 2021c). Gupta et al. (2021)\\nsuggest to use the semantic structure of an exem-\\nplar response, instead of the tokens of the exem-\\nplar response, to guide generation. Despite their\\ndifferences, a common issue is that the genera-\\ntion model easily learns to ignore the retrieved re-\\nsponse entirely and collapses to a vanilla seq2seq\\nmodel. This happens with improper training in-\\nstances. Due to the one-to-many nature, it hap-pens frequently that a retrieved response (extracted\\nskeleton) is suitable for responding to the query,\\nbut inconsistent with the current target response.\\nEarlier studies (Weston et al., 2018; Wu et al.,\\n2019; Cai et al., 2019a) alleviate the above prob-\\nlems by putting hard constraints on the data (e.g.,\\ndiscarding data with low similarity of the retrieved\\nresponse and the target response), which, however,\\ngreatly reduces the amount of usable data. Cai\\net al. (2019b) employ a random mechanism for\\ngenerating the skeletons used for training, which\\nextract skeletons from the corresponding responses\\nwith some deliberate disturbance. Paranjape et al.\\n(2021) propose to model the retriever after the pos-\\nterior distribution of retrieval given the input and\\nthe target output and train it jointly with the stan-\\ndard retriever and the generator by maximizing the\\nevidence lower bound (ELBo) in expectation over\\nretrieval.\\nKnowledge-Enhanced Generation The afore-\\nmentioned work demonstrates that retrieval-based\\ndialogue systems can be used for building bet-\\nter generation-based models. In general, this is\\ndone by conditioning the generation on some re-\\ntrieved responses. More traditionally, to infuse\\nthe response with external knowledge, the retrieval\\npool is not necessarily a dialogue corpus. In fact,\\nknowledge-grounded dialogue response generation\\nexploring different forms of knowledge such as\\nknowledge bases and external documents (Dinan\\net al., 2018; Zhou et al., 2018; Lian et al., 2019;\\nLi et al., 2019; Qin et al., 2019; Wu et al., 2021;\\nZhang et al., 2021; Komeili et al., 2021) has been\\nactively explored.\\nLimitations We note that there are three major\\nlimitations in existing work for dialogue response\\ngeneration. First, current methods only use one\\nretrieved response for generation. It can be more\\nbeneÔ¨Åcial to combine multiple retrieval responses.\\nHowever, this can be difÔ¨Åcult due to the one-to-\\nmany nature of dialogue response generation. Sec-\\nond, current methods use universal relevance score\\nfor retrieval. It can be more effective if we can\\nuse more customized retrieval metric especially\\nfor controlled dialogue response generation (e.g.,\\npersona, emotion, etc). Third, the retrieval pool\\nof existing methods is limited to dialogue corpora\\n(context-response pairs) or documents. It might\\nbe useful to enlarge the retrieval pool by including\\nmore corpora in other domains or in other modali-', metadata={'source': './data\\\\a survey.pdf', 'page': 3}),\n",
       " Document(page_content='ties. As discussed, there leaves plenty of possible\\ndirections to explore in the future.\\n4 Machine Translation\\nRetrieval augmented translation originates from hu-\\nman translation scenarios (Somers, 2003). When\\ntranslating ÀÜyfrom an input source sentence x, a hu-\\nman translator typically involves a search engine to\\nretrieve similar sentences {‚ü®xr,yr‚ü©}from a bilin-\\ngual database. Such a technique called translation\\nmemory is helpful to improve the translation qual-\\nity and efÔ¨Åciency for human translators (Dillon\\nand Fraser, 2006). As the development of ma-\\nchine translation techniques, there is a surge of\\ninterests in improving machine translation models\\nwith translation memory. In the rest of this section,\\nwe will review translation memory for both statisti-\\ncal machine translation (SMT) and neural machine\\ntranslation (NMT).\\n4.1 Translation Memory in SMT\\nGenerally, SMT includes three key components in\\na pipeline manner such as phrase table extraction,\\nparameter tuning and decoding (Koehn et al., 2003;\\nChiang, 2007). As a result, many efforts have been\\nmade to make use of translation memory (TM) on\\ntop of each component.\\nConstrained Decoding with TM Constrained\\ndecoding is the most straightforward way to in-\\ntegrating TM into SMT (Smith and Clark, 2009;\\nKoehn and Senellart, 2010; Zhechev and Van Gen-\\nabith, 2010; Ma et al., 2011). Its basic idea is\\nto reuse the useful segments in yrwhile trans-\\nlate other segments by SMT. SpeciÔ¨Åcally, the ap-\\nproach consists of three steps: 1) identify the un-\\nmatched segments in both xrandxthrough the\\nedit-distance algorithm; 2) identify the unmatched\\nsegments in yr, each of which is aligned to one\\nunmatched segment in xrby a word alignment\\nalgorithm; 3) decode each unmatched segment in\\nxby SMT and then use the result to replace its\\ncorresponding unmatched segment in yr. Li et al.\\n(2016b) further extend this approach from sentence\\nlevel to phrase level. The advantage in constrained\\ndecoding is that it does not require to change the\\ntranslation model (including phrase table and pa-\\nrameters) and can be applied in a plug-and-play\\nway. This approach is successful when xis highly\\nsimilar to xr; otherwise its performance is de-\\ngraded largely, because it explicitly isolates TMmatching and SMT decoding and reuses the results\\ninxror not in a deterministic way.\\nPhrase Table Aggregation with TM There are\\nalso notable efforts to augment the phrase table\\nfor SMT by extracting translation rules from the\\nretrieved bilingual sentences {‚ü®xr,yr‚ü©}. Then\\nthey re-tune the parameters for the SMT model\\nwhich makes use of translation knowledge from\\n{‚ü®xr,yr‚ü©}in a implicit way when translating x.\\nFor example, Bi√ßici and Dymetman (2008); Simard\\nand Isabelle (2009) directly combine the extracted\\ntranslation rules into the phrase table in a shallow\\ncombination way. They introduce an additional fea-\\nture to indicate that whether translation rule is from\\n{‚ü®xr,yr‚ü©}or not and then train all feature weights\\nwith MERT (Och, 2003). One characteristic of\\nthese work is that a translation rule extracted from\\n{‚ü®xr,yr‚ü©}which can not exactly match any seg-\\nments in xis useless even if it may contain some\\nuseful words in its target side. To remedy this ob-\\nservation, Wang et al. (2013, 2014) resort to a deep\\ncombination way to using the extracted translation\\nrules. For each rule in the phrase table, it designs\\na generative model to reward the rules which are\\nsimilar to those extracted from {‚ü®xr,yr‚ü©}. Then\\nthis generative model is used as a feature in the log-\\nlinear based SMT model whose weight is tuned\\ntogether with other features by MERT. In addition,\\nLi et al. (2014) employ a similar way to reward\\nthe rules but it relies on a discriminative model\\nwhich is easy to integrate potential features from\\n{‚ü®xr,yr‚ü©}.\\nParameter Tuning with TM Unlike the above\\ntwo research lines, Liu et al. (2012, 2014) make use\\nof translation memory only in tuning parameters.\\nTo be speciÔ¨Åc, when translating an input sentence\\nx, they Ô¨Årstly retrieve many similar bilingual sen-\\ntences{‚ü®xr,yr‚ü©}, and then tune the parameters on\\ntop of the retrieved sentences as well as a given de-\\nvelopment dataset in a sentence-wise manner, i.e.,\\nit performs an independent tuning for each input\\nsentence. To improve the efÔ¨Åciency of each tuning\\nstep, it propose a local update on top of {‚ü®xr,yr‚ü©}\\nfrom a baseline model.\\nDespite the successes of translation memory in\\nSMT, there are still some limitations for the above\\nthree kinds of methods. Firstly, all these methods\\nemploy fuzzy score for retrieval which is highly de-\\npendent on word matching and thus can not recall\\nsuch examples which are similar in word seman-', metadata={'source': './data\\\\a survey.pdf', 'page': 4}),\n",
       " Document(page_content='tics but different in surface form. Secondly, these\\nmethods integrate the retrieved examples into a\\nmodule of SMT in the ways which can not make\\nfull use of the knowledge in retrieved examples.\\nFor example, the integration ways in the Ô¨Årst two\\nkinds (constrained decoding and phrase table ag-\\ngregation) are heuristic and not optimized towards\\ntranslation quality; the parameter tuning method\\nÔ¨Åne-tunes few parameters for log-linear based SMT\\nwhich are not enough to preserve sufÔ¨Åcient knowl-\\nedge from retrieved examples. Thirdly, since SMT\\nperforms in a pipeline manner, it is intractable to\\njointly optimize retrieval metrics as well as SMT\\nmodels. Consequently, all these methods adopt an\\noff-the-shelf metric for retrieval, leading to sub-\\noptimal performance.\\n4.2 Translation Memory in NMT\\nTranslation memory has been widely explored in\\nNeural Machine Translation (NMT). Depending\\non when retrieval is involved, we can categorize\\nprevious works into two classes: 1) an NMT model\\nleans how to cooperate with the retrieval model in\\nthe training phase; 2) an NMT model is only aware\\nof the retrieved data in the inference phase.\\nInference Phase The key point of literature in\\nthis line is to reward some target words based on\\nwords in yrin the inference process. Thus, a de-\\ncision can be made based on both the distribution\\nof generation model and the additional reward of\\nretrieval model. Some previous works propose to\\nreward target words based on the sentence-level\\nsimilarity between xandxr, and the word align-\\nment between xrandyr. Given the input sentence\\nx, Zhang et al. (2018) try to assign target words\\ninÀÜywith higher rewards, when they appear in yr\\nand the aligned source words are in both xrand\\nx. He et al. (2019) follow a similar framework\\nand consider the position information of those tar-\\nget words when rewarding. Those works reward\\nthe target words in an explicit way, however, the\\none-sentence-one-model approach (Li et al., 2016c;\\nTurchi et al., 2017) propose to reward target word\\nimplicitly. For each testing input x, their approach\\nwill Ô¨Årst Ô¨Ånetune the translation model on retrieved\\nmemory{‚ü®xr,yr‚ü©}and then translate x.\\nOthers try to reward target words based on token-\\nlevel similarity score. Most works in this line are\\nbased on the dense retriever (Khandelwal et al.,\\n2020a), e.g., faiss. Khandelwal et al. (2020a) build\\na key-value datastore, where key h(xr,yr\\n<t)is thehidden state at each time step when translating yr\\nfromxr, and value is its golden-truth target word\\nyr\\nt. Therefore, in the inference time, they can use\\ntheh(x,ÀÜy<t)as query and reward target words\\nwith similar hidden representations in the datas-\\ntore. Although this method achieves signiÔ¨Åcant\\nperformance gain, one drawback of it is the high la-\\ntency. To address this issue, Meng et al. (2021) use\\nsome heuristics, e.g., pre-Ô¨Åltering, to avoid search-\\ning on the entire datastore. The reward score of\\nprevious works is got from some non-parametric\\napproaches, however, Zheng et al. (2021a) propose\\na light-weight network to learn the reward score.\\nSince dense retrieval has the potential of cross-\\nlingual retrieval, Zheng et al. (2021b) use a similar\\napproach to achieve unsupervised domain adapta-\\ntion, where a main change is to create the datastore\\nbased on synthetic sources sentence and the real\\ntarget sentences.\\nTraining Phase Different from those model-\\nagnostic approaches, previous works in this line\\naim to train the generation model to learn how\\nto cooperate with the retrieval model. It is also\\nworth noting that most works in this line adopt\\nthe sentence-level retrieval, when integrating the\\nretrieval information in the training process. To\\nachieve its goal, Bulte and Tezcan (2019) and\\nHossain et al. (2020) propose a data augmenta-\\ntion method to integrate the retrieved information,\\nwhere xis concatenated with yrbefore feeding\\ninto the model . Following the data augmentation\\napproach, Xu et al. (2020) propose more matching\\nmethods to determine including which retrieved\\nexample in the source is better.\\nThere also exist some works that propose new\\narchitectures to integrate the retrieval information.\\nUnder the RNN-based framework, Cao and Xiong\\n(2018) and Gu et al. (2018) use the gating and at-\\ntention mechanism to incorporate the retrieved tar-\\nget sentences. When Transformer (Vaswani et al.,\\n2017) becomes the backbone of NMT, some works\\nalso use additional transformer encoders to en-\\ncode retrieved target sentences, and integrate them\\nthrough attention mechanism (Bapna and Firat,\\n2019; Cao et al., 2019). Xia et al. (2019) repre-\\nsent the retrieved target sentences in a different\\ndata structure, i.e., a graph structure, and integrate\\nit through attention mechanism. He et al. (2021)\\npropose a light-weight method to encode the re-\\ntrieved target sentences and leverage the alignment\\ninformation to Ô¨Ålter out irrelevant information. Dif-', metadata={'source': './data\\\\a survey.pdf', 'page': 5}),\n",
       " Document(page_content='ferent from previous works that rely on bilingual\\nmemories, Cai et al. (2021) propose a framework\\nthat can retrieve the most similar target sentence in\\na monolingual dataset, using a source sentence as\\nquery.\\nLimitations In the section of SMT, we have\\nshowed some limitations of the retrieval augmented\\napproaches. There also exist some limitations in\\nthe line of NMT. First, the information used for\\nderiving reward scores is limited. The similarity\\nbetween an input and retrieved examples is the\\nprimary feature to derive reward scores. How-\\never, some information, e.g., frequencies of words\\nand context, may also be beneÔ¨Åcial for integrating\\nthe translation memory. Second, it remains to be\\nan open question that when should we use the re-\\ntrieved information and when not. In the inference\\nphase, approaches tend to integrate the translation\\nmemory excessively, e.g., at each time step, which\\nnot only reduces the translation efÔ¨Åciency but may\\nalso dampen the Ô¨Çuency of generated results.\\n5 Other Tasks\\nIn addition to dialogue system and machine trans-\\nlation, retrieval-augmented generation techniques\\nhave shown to be beneÔ¨Åcial in many other tasks. In\\nthe following, we highlight several key tasks that\\napply retrieval-augmented generation approaches.1\\nLanguage Modelling It has been shown that\\nproperly leveraging information from retrieval\\nmemory could improve the performance of large\\npre-trained language model. To build a more accu-\\nrate language model, Khandelwal et al. (2020b) pro-\\npose to incorporate a soft memory module into the\\nsystem. SpeciÔ¨Åcally, an index is built by caching\\nthe hidden states of the training corpus. Then, the\\nlanguage model accesses the index via k-NN search\\nand displays a greatly improved performance. As\\nanother example, Guu et al. (2020) propose a new\\nparadigm that applies retrieval-augmented tech-\\nnique into the pre-training of generative language\\nmodel. During learning, they train a neural se-\\nlector that dynamically samples a relevant text to\\nguide the reconstruction of a corrupted input se-\\nquence. In this way, the pre-trained model deliv-\\ners better results by explicitly grounding on the\\nretrieval memory. Lewis et al. (2020a) combine\\nlanguage model pre-training with a paraphrasing\\n1Here, we focus on tasks other than question answering.\\nWe refer readers interested in QA to Chen and Yih (2020).approach. During learning, an input sequence to\\nthe model is Ô¨Årst corrupted. In the meantime, a set\\nof multi-lingual texts are retrieved based on which\\nthe model learns to reconstruct the original input\\nsequence. Recently, Borgeaud et al. (2021) pro-\\npose RETRO , a large pre-trained language model\\nenhanced with retrieved documents, and obtained\\ncomparable performances with GPT-3 using 25 √ó\\nfewer parameters.\\nSummarization Text summarization is another\\nresearch area that beneÔ¨Åts from retrieval-\\naugmented text generation. Peng et al. (2019)\\npropose an adaptive decoding framework which\\nÔ¨Årst retrieves an exemplar document given the\\nsource document. Then, the summarization of the\\nsource document is derived through an adaptive\\ngeneration process based on the retrieved template.\\nDifferent from Peng et al. (2019), Cao et al.\\n(2018) and Hossain et al. (2020) introduce an\\nintermediate re-ranking stage into the generation\\npipeline. SpeciÔ¨Åcally, before generating the\\ndocument summary, the retrieval documents are\\nÔ¨Årst re-ranked based on their similarity scores\\nwith respect to the source document. Then, the\\ndocument summarization is produced by re-writing\\nthe selected templates.\\nParaphrase Generation To address the lack of\\nquality as well as diversity in the generation of para-\\nphrases, Kazemnejad et al. (2020) propose a gen-\\neration framework which Ô¨Årst retrieves a sentence\\nthat is similar to input sentence. Then, based on\\nthe retrieved sentence, a neural editor produces the\\nresulting paraphrased sentence. Chen et al. (2019)\\ninvestigate a different aspect of paraphrasing, i.e.\\nhow to control the linguistic syntax displayed in\\nthe generated text. To achieve this goal, Chen et al.\\n(2019) propose to Ô¨Årst extract a sentential exem-\\nplar that serves as the syntax template. A neural\\nmodel then generates the paraphrase with desired\\nlinguistic syntax following the retrieved exemplar.\\nText Style Transfer To improve the quality of\\ngenerated text, Li et al. (2018) propose a retrieval-\\naugmented framework which Ô¨Årst retrieves texts\\nthat are similar to the input based on lexical-level\\nsimilarity. Then, the retrieved tokens that are irrel-\\nevant to the source are deleted, and the output is\\nderived from the edited template. Xiao et al. (2021)\\nalso adopte this framework by incorporating re-\\ntrieval information from two sources (i.e. sparse\\nand dense memories) and obtained an improved', metadata={'source': './data\\\\a survey.pdf', 'page': 6}),\n",
       " Document(page_content='model performance.\\nData-to-Text Generation Recently, retrieval-\\naugmented generation has been adapted to the task\\nof data-to-text generation. To bridge the gap be-\\ntween the structured data and natural language\\ntext, Su et al. (2021a) propose a novel retrieval-\\naugmented framework. SpeciÔ¨Åcally, given the\\nsource data, a set of candidate texts are Ô¨Årst re-\\ntrieved from a large unlabelled corpus. Then, a\\nneural selector is applied to measure the similari-\\nties between the source data and candidate texts,\\nand extract a set of more Ô¨Åne-grained prototypes\\nfrom the candidates. Lastly, a generation model\\ntakes the prototypes as input to produce the text\\nthat describes the given structured data.\\nWhile retrieval-augmented generation has been\\nwidely explored in the NLP community, we sug-\\ngest that future research could extend this approach\\nto tasks that involve data from multiple modali-\\nties. For instance, with recent advancements in\\nimage-text retrieval (Jia et al., 2021; Radford et al.,\\n2021), the structural gap between images and texts\\nis largely bridged. Some early studies (Zhang et al.,\\n2020) have shown that information retrieved from\\nimages could improve the performance of neural\\nmachine translation model. Naturally, such meth-\\nods could be extended to other multi-modal tasks,\\nsuch as image captioning (Karpathy and Li, 2015).\\nA similar idea could also be applied to tasks be-\\nyond images, such as speech-to-text transcription\\n(Gales and Young, 2007).\\n6 Future Directions\\nDespite the current success of retrieval augmented\\ntext generation, there is still a long way to go as\\ndiscussed in previous sections. We highlight some\\ndirections to facilitate the future research as fol-\\nlows:\\nRetrieval Sensitivity The performance of re-\\ntrieval augmented text generation is very sensitive\\nto the retrieval quality, i.e., the similarity between\\nthe query and the retrieved examples. Currently, re-\\ntrieval augmented text generation models perform\\nwell when the retrieved examples are very simi-\\nlar to the query. However, they are even worse\\nthan the generation models without retrieval when\\nthe retrieval examples are less similar. Therefore,\\nit would be important to exploit new methods to\\naddress such an issue on similarity.Retrieval EfÔ¨Åciency Generally, if one enlarges\\nthe retrieval memory to some extent, it would be\\npossible to retrieve an example which is very simi-\\nlar to the query.Unfortunately, the downside is that\\nthe overall inference for the retrieval augmented\\ngeneration models is less efÔ¨Åcient due the consid-\\nerable retrieval overhead. In this sense, it is urgent\\nto consider some methods to trade off the retrieval\\nmemory size and retrieval efÔ¨Åciency, for example,\\ndata compression for the retrieval memory.\\nLocal vs. Global Optimization Theoretically, it\\nseems promising to jointly learn retrieval metrics\\nand generation models. However, in practice, there\\nis an essential gap about the retrieval metric be-\\ntween the training and inference phrases. In the\\ntraining phase, the loss is locally back-propagated\\nto only a few retrieved examples while in the infer-\\nence phase the metric is globally conducted among\\nall examples in the memory. It would be interesting\\nto narrow such a gap when learning a better metric\\nfor generation tasks.\\nMulti-Modalities With recent advancement in\\nimage-text retrieval, directly associating images\\nwith relevant text becomes possible. This urges\\nresearchers to investigate the possibility of retrieval-\\nbased text generation in tasks that involve data from\\ndifferent modalities. One typical task is image\\ncaptioning. Beyond images, other tasks like speech-\\nto-text transcription could potentially beneÔ¨Åt from\\nretrieval-based generation methods as well.\\nDiverse & Controllable Retrieval Most of the\\nexisting approaches adopt a universal metric for\\nretrieval, such as lexical similarities of sentences.\\nFuture work should explore how to use customized\\nmetrics for retrieval. This can be beneÔ¨Åcial for\\nmore controlled text generation. For example, in-\\nstances with emotions and styles may be more de-\\nsirable in the personalized dialogue generation, par-\\nallel data that contains speciÔ¨Åc terminologies is\\nmore helpful in machine translation, and so on. On\\nthe other hand, using a universal metric for retrieval\\nmay lead to the lack of diversity of the retrieval re-\\nsults. Collecting a diverse set of retrieval results\\ncan improve the coverage of useful information.\\nThus, considering multiple different metrics for re-\\ntrieval may lead to generation with higher quality\\nin the future.', metadata={'source': './data\\\\a survey.pdf', 'page': 7}),\n",
       " Document(page_content='7 Conclusion\\nIn this paper, we surveyed recent approaches for\\nretrieval-augmented text generation. We reviewed\\nand summarized the development of different com-\\nponents of retrieval-augmented text generation in-\\ncluding retrieval metrics, retrieval sources, and in-\\ntegration paradigms. We gave in-depth discussions\\nwhen retrieval-augmented text generation comes to\\ndifferent applications including dialogue response\\ngeneration, machine translation, and other genera-\\ntion tasks. We also pointed out some future direc-\\ntions for retrieval-augmented text generation.\\nReferences\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\\ngio. 2014. Neural machine translation by jointly\\nlearning to align and translate. arXiv preprint\\narXiv:1409.0473 .\\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\\nadaptation for neural machine translation. In Pro-\\nceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Vol-\\nume 1 (Long and Short Papers) , pages 1921‚Äì1931.\\nErgun Bi√ßici and Marc Dymetman. 2008. Dynamic\\ntranslation memory: Using statistical machine trans-\\nlation to improve translation memory fuzzy matches.\\nInInternational Conference on Intelligent Text Pro-\\ncessing and Computational Linguistics , pages 454‚Äì\\n465. Springer.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\nGeorge van den Driessche, Jean-Baptiste Lespiau,\\nBogdan Damoc, Aidan Clark, Diego de Las Casas,\\nAurelia Guy, Jacob Menick, Roman Ring, Tom Hen-\\nnigan, Saffron Huang, Loren Maggiore, Chris Jones,\\nAlbin Cassirer, Andy Brock, Michela Paganini, Ge-\\noffrey Irving, Oriol Vinyals, Simon Osindero, Karen\\nSimonyan, Jack W. Rae, Erich Elsen, and Laurent\\nSifre. 2021. Improving language models by retriev-\\ning from trillions of tokens. CoRR , abs/2112.04426.\\nBram Bulte and Arda Tezcan. 2019. Neural fuzzy re-\\npair: Integrating fuzzy matches into neural machine\\ntranslation. In Proceedings of the 57th Annual Meet-\\ning of the Association for Computational Linguistics ,\\npages 1800‚Äì1809.\\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xi-\\naojiang Liu, Wai Lam, and Shuming Shi. 2019a.\\nSkeleton-to-response: Dialogue generation guided\\nby retrieval memory. In Proceedings of the 2019\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short\\nPapers) , pages 1219‚Äì1228.Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiao-\\njiang Liu, and Shuming Shi. 2019b. Retrieval-\\nguided dialogue response generation via a matching-\\nto-generation framework. In Proceedings of the\\n2019 Conference on Empirical Methods in Natu-\\nral Language Processing and the 9th International\\nJoint Conference on Natural Language Processing\\n(EMNLP-IJCNLP) , pages 1866‚Äì1875.\\nDeng Cai, Yan Wang, Huayang Li, Wai Lam, and\\nLemao Liu. 2021. Neural machine translation with\\nmonolingual translation memory. In Proceedings of\\nthe 59th Annual Meeting of the Association for Com-\\nputational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing\\n(Volume 1: Long Papers) , pages 7307‚Äì7318, Online.\\nAssociation for Computational Linguistics.\\nQian Cao, Shaohui Kuang, and Deyi Xiong. 2019.\\nLearning to reuse translations: Guiding neural ma-\\nchine translation with examples. arXiv preprint\\narXiv:1911.10732 .\\nQian Cao and Deyi Xiong. 2018. Encoding gated\\ntranslation memory into neural machine translation.\\nInProceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n3042‚Äì3047.\\nZiqiang Cao, Wenjie Li, Sujian Li, and Furu Wei.\\n2018. Retrieve, rerank and rewrite: Soft template\\nbased neural summarization. In Proceedings of the\\n56th Annual Meeting of the Association for Com-\\nputational Linguistics, ACL 2018, Melbourne, Aus-\\ntralia, July 15-20, 2018, Volume 1: Long Papers ,\\npages 152‚Äì161. Association for Computational Lin-\\nguistics.\\nDanqi Chen and Wen-tau Yih. 2020. Open-domain\\nquestion answering. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics: Tutorial Abstracts , pages 34‚Äì37, On-\\nline. Association for Computational Linguistics.\\nMingda Chen, Qingming Tang, Sam Wiseman, and\\nKevin Gimpel. 2019. Controllable paraphrase gen-\\neration with a syntactic exemplar. In Proceedings of\\nthe 57th Conference of the Association for Compu-\\ntational Linguistics, ACL 2019, Florence, Italy, July\\n28- August 2, 2019, Volume 1: Long Papers , pages\\n5972‚Äì5984. Association for Computational Linguis-\\ntics.\\nDavid Chiang. 2007. Hierarchical phrase-based trans-\\nlation. computational linguistics , 33(2):201‚Äì228.\\nSarah Dillon and Janet Fraser. 2006. Translators and\\ntm: An investigation of translators‚Äô perceptions of\\ntranslation memory adoption. Machine Translation ,\\n20(2):67‚Äì79.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\\nFan, Michael Auli, and Jason Weston. 2018. Wizard\\nof wikipedia: Knowledge-powered conversational\\nagents. arXiv preprint arXiv:1811.01241 .', metadata={'source': './data\\\\a survey.pdf', 'page': 8}),\n",
       " Document(page_content='Mark J. F. Gales and Steve J. Young. 2007. The applica-\\ntion of hidden markov models in speech recognition.\\nFound. Trends Signal Process. , 1(3):195‚Äì304.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-\\ntor OK Li. 2018. Search engine guided neural ma-\\nchine translation. In Proceedings of the AAAI Con-\\nference on ArtiÔ¨Åcial Intelligence , volume 32.\\nPrakhar Gupta, Jeffrey Bigham, Yulia Tsvetkov, and\\nAmy Pavel. 2021. Controlling dialogue generation\\nwith semantic exemplars. In Proceedings of the\\n2021 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies , pages 3018‚Äì3029, On-\\nline. Association for Computational Linguistics.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\\naugmented language model pre-training. CoRR ,\\nabs/2002.08909.\\nTatsunori B Hashimoto, Kelvin Guu, Yonatan Oren,\\nand Percy S Liang. 2018. A retrieve-and-edit frame-\\nwork for predicting structured outputs. In Advances\\nin Neural Information Processing Systems , pages\\n10052‚Äì10062.\\nQiuxiang He, Guoping Huang, Qu Cui, Li Li, and\\nLemao Liu. 2021. Fast and accurate neural machine\\ntranslation with translation memory. In Proceed-\\nings of the 59th Annual Meeting of the Association\\nfor Computational Linguistics and the 11th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing (Volume 1: Long Papers) , pages 3170‚Äì3180.\\nQiuxiang He, Guoping Huang, Lemao Liu, and Li Li.\\n2019. Word position aware translation memory for\\nneural machine translation. In CCF International\\nConference on Natural Language Processing and\\nChinese Computing , pages 367‚Äì379. Springer.\\nNabil Hossain, Marjan Ghazvininejad, and Luke Zettle-\\nmoyer. 2020. Simple and effective retrieve-edit-\\nrerank text generation. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computa-\\ntional Linguistics , pages 2532‚Äì2538.\\nBaotian Hu, Zhengdong Lu, Hang Li, and Qingcai\\nChen. 2014. Convolutional neural network architec-\\ntures for matching natural language sentences. In\\nNIPS , pages 2042‚Äì2050.\\nZongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An\\ninformation retrieval approach to short text conver-\\nsation. arXiv preprint arXiv:1408.6988 .\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\\nParekh, Hieu Pham, Quoc V . Le, Yun-Hsuan Sung,\\nZhen Li, and Tom Duerig. 2021. Scaling up visual\\nand vision-language representation learning with\\nnoisy text supervision. In Proceedings of the 38th In-\\nternational Conference on Machine Learning, ICML\\n2021, 18-24 July 2021, Virtual Event , volume 139 of\\nProceedings of Machine Learning Research , pages\\n4904‚Äì4916. PMLR.Andrej Karpathy and Fei-Fei Li. 2015. Deep visual-\\nsemantic alignments for generating image descrip-\\ntions. In IEEE Conference on Computer Vision and\\nPattern Recognition, CVPR 2015, Boston, MA, USA,\\nJune 7-12, 2015 , pages 3128‚Äì3137. IEEE Computer\\nSociety.\\nAmirhossein Kazemnejad, Mohammadreza Salehi, and\\nMahdieh Soleymani Baghshah. 2020. Paraphrase\\ngeneration by learning how to edit from samples. In\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics , pages 6010‚Äì\\n6021, Online. Association for Computational Lin-\\nguistics.\\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2020a. Near-\\nest neighbor machine translation. arXiv preprint\\narXiv:2010.00710 .\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2020b. Generaliza-\\ntion through memorization: Nearest neighbor lan-\\nguage models. In 8th International Conference on\\nLearning Representations, ICLR 2020, Addis Ababa,\\nEthiopia, April 26-30, 2020 . OpenReview.net.\\nPhilipp Koehn, Franz J. Och, and Daniel Marcu. 2003.\\nStatistical phrase-based translation. In Proceedings\\nof the 2003 Human Language Technology Confer-\\nence of the North American Chapter of the Associa-\\ntion for Computational Linguistics , pages 127‚Äì133.\\nPhilipp Koehn and Jean Senellart. 2010. Convergence\\nof translation memory and statistical machine trans-\\nlation. In Proceedings of AMTA Workshop on MT\\nResearch and the Translation Industry , pages 21‚Äì31.\\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\\n2021. Internet-augmented dialogue generation.\\narXiv preprint arXiv:2107.07566 .\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised\\nopen domain question answering. arXiv preprint\\narXiv:1906.00300 .\\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\\n2020a. Pre-training via paraphrasing. In Advances\\nin Neural Information Processing Systems 33: An-\\nnual Conference on Neural Information Processing\\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\\nvirtual .\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\nt√§schel, et al. 2020b. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. arXiv\\npreprint arXiv:2005.11401 .\\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\\nand Bill Dolan. 2016a. A diversity-promoting ob-\\njective function for neural conversation models. In\\nNAACL , pages 110‚Äì119.', metadata={'source': './data\\\\a survey.pdf', 'page': 9}),\n",
       " Document(page_content='Juncen Li, Robin Jia, He He, and Percy Liang. 2018.\\nDelete, retrieve, generate: a simple approach to sen-\\ntiment and style transfer. In Proceedings of the 2018\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, NAACL-HLT 2018, New\\nOrleans, Louisiana, USA, June 1-6, 2018, Volume\\n1 (Long Papers) , pages 1865‚Äì1874. Association for\\nComputational Linguistics.\\nLiangyou Li, Andy Way, and Qun Liu. 2014. A\\ndiscriminative framework of integrating translation\\nmemory features into smt. In Proceedings of the\\n11th Conference of the Association for Machine\\nTranslation in the Americas , volume 1, pages 249‚Äì\\n260.\\nLiangyou Li, Andy Way, and Qun Liu. 2016b. Phrase-\\nlevel combination of smt and tm using constrained\\nword lattice. Association for Computational Lin-\\nguistics (ACL).\\nXiaoqing Li, Jiajun Zhang, and Chengqing Zong.\\n2016c. One sentence one model for neural machine\\ntranslation. arXiv preprint arXiv:1609.06490 .\\nZekang Li, Cheng Niu, Fandong Meng, Yang Feng,\\nQian Li, and Jie Zhou. 2019. Incremental trans-\\nformer with deliberation decoder for document\\ngrounded conversations. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computa-\\ntional Linguistics , pages 12‚Äì21.\\nRongzhong Lian, Min Xie, Fan Wang, Jinhua Peng,\\nand Hua Wu. 2019. Learning to select knowledge\\nfor response generation in dialog systems. arXiv\\npreprint arXiv:1902.04911 .\\nLemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao,\\nMo Yu, and Conghui Zhu. 2012. Locally training\\nthe log-linear model for smt. In Proceedings of the\\n2012 Joint Conference on Empirical Methods in Nat-\\nural Language Processing and Computational Natu-\\nral Language Learning , pages 402‚Äì411.\\nLemao Liu, Tiejun Zhao, Taro Watanabe, Hailong Cao,\\nand Conghui Zhu. 2014. Discriminative training for\\nlog-linear based smt: Global or local methods. ACM\\nTransactions on Asian Language Information Pro-\\ncessing (TALIP) , 13(4):1‚Äì25.\\nYanjun Ma, Yifan He, Andy Way, and Josef van Gen-\\nabith. 2011. Consistent translation using discrim-\\ninative learning-a translation memory-inspired ap-\\nproach. In Proceedings of the 49th Annual Meet-\\ning of the Association for Computational Linguistics:\\nHuman Language Technologies , pages 1239‚Äì1248.\\nYuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xi-\\naofei Sun, Tianwei Zhang, and Jiwei Li. 2021.\\nFast nearest neighbor machine translation. arXiv\\npreprint arXiv:2105.14528 .\\nFranz Josef Och. 2003. Minimum error rate training in\\nstatistical machine translation. In Proceedings of the41st Annual Meeting of the Association for Compu-\\ntational Linguistics , pages 160‚Äì167, Sapporo, Japan.\\nAssociation for Computational Linguistics.\\nGaurav Pandey, Danish Contractor, Vineet Kumar, and\\nSachindra Joshi. 2018. Exemplar encoder-decoder\\nfor neural conversation generation. In ACL, pages\\n1329‚Äì1338.\\nAshwin Paranjape, Omar Khattab, Christopher Potts,\\nMatei Zaharia, and Christopher D Manning. 2021.\\nHindsight: Posterior-guided training of retrievers for\\nimproved open-ended generation. arXiv preprint\\narXiv:2110.07752 .\\nHao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan\\nDhingra, and Das Dipanjan. 2019. Text generation\\nwith exemplar-based adaptive decoding. In Proceed-\\nings of the Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies .\\nLianhui Qin, Michel Galley, Chris Brockett, Xiaodong\\nLiu, Xiang Gao, William B Dolan, Yejin Choi, and\\nJianfeng Gao. 2019. Conversing by reading: Con-\\ntentful neural conversation with on-demand machine\\nreading. In Proceedings of the 57th Annual Meet-\\ning of the Association for Computational Linguistics ,\\npages 5427‚Äì5436.\\nMinghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan\\nChen, Weipeng Zhao, Haiqing Chen, Jun Huang,\\nand Wei Chu. 2017. Alime chat: A sequence to se-\\nquence and rerank based chatbot engine. In ACL,\\npages 498‚Äì503.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\\ning transferable visual models from natural lan-\\nguage supervision. In Proceedings of the 38th In-\\nternational Conference on Machine Learning, ICML\\n2021, 18-24 July 2021, Virtual Event , volume 139 of\\nProceedings of Machine Learning Research , pages\\n8748‚Äì8763. PMLR.\\nStephen Robertson and Hugo Zaragoza. 2009. The\\nprobabilistic relevance framework: BM25 and be-\\nyond . Now Publishers Inc.\\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-\\nral responding machine for short-text conversation.\\nInACL, pages 1577‚Äì1586.\\nMichel Simard and Pierre Isabelle. 2009. Phrase-based\\nmachine translation in a computer-assisted transla-\\ntion environment. Proceedings of the Twelfth Ma-\\nchine Translation Summit (MT Summit XII) , pages\\n120‚Äì127.\\nJames Smith and Stephen Clark. 2009. Ebmt for smt:\\na new ebmt-smt hybrid. In Proceedings of the 3rd\\nInternational Workshop on Example-Based Machine\\nTranslation , pages 3‚Äì10. Citeseer.', metadata={'source': './data\\\\a survey.pdf', 'page': 10}),\n",
       " Document(page_content='Harold Somers. 2003. Translation memory systems.\\nBenjamins Translation Library , 35:31‚Äì48.\\nYiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and\\nMing Zhang. 2016. Two are better than one: An en-\\nsemble of retrieval-and generation-based dialog sys-\\ntems. arXiv preprint arXiv:1610.07149 .\\nYixuan Su, Zaiqiao Meng, Simon Baker, and Nigel Col-\\nlier. 2021a. Few-shot table-to-text generation with\\nprototype memory. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2021, Vir-\\ntual Event / Punta Cana, Dominican Republic, 16-\\n20 November, 2021 , pages 910‚Äì917. Association for\\nComputational Linguistics.\\nYixuan Su, David Vandyke, Simon Baker, Yan Wang,\\nand Nigel Collier. 2021b. Keep the primary, rewrite\\nthe secondary: A two-stage approach for paraphrase\\ngeneration. In Findings of the Association for Com-\\nputational Linguistics: ACL-IJCNLP 2021 , pages\\n560‚Äì569, Online. Association for Computational\\nLinguistics.\\nYixuan Su, Yan Wang, Deng Cai, Simon Baker, Anna\\nKorhonen, and Nigel Collier. 2021c. PROTOTYPE-\\nTO-STYLE: dialogue generation with style-aware\\nediting on retrieval memory. IEEE ACM Trans. Au-\\ndio Speech Lang. Process. , 29:2152‚Äì2161.\\nMarco Turchi, Matteo Negri, M Farajian, and Marcello\\nFederico. 2017. Continuous learning from human\\npost-edits for neural machine translation.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in neural information pro-\\ncessing systems , pages 5998‚Äì6008.\\nOriol Vinyals and Quoc Le. 2015. A neural conversa-\\ntional model. In ICML (Deep Learning Workshop) .\\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2013.\\nIntegrating translation memory into phrase-based\\nmachine translation during decoding. In Proceed-\\nings of the 51st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Pa-\\npers) , pages 11‚Äì21.\\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\\nDynamically integrating cross-domain translation\\nmemory into phrase-based machine translation dur-\\ning decoding. In Proceedings of COLING 2014,\\nthe 25th International Conference on Computational\\nLinguistics: Technical Papers , pages 398‚Äì408.\\nJason Weston, Emily Dinan, and Alexander Miller.\\n2018. Retrieve and reÔ¨Åne: Improved sequence gen-\\neration models for dialogue. In Proceedings of the\\n2018 EMNLP Workshop SCAI: The 2nd Interna-\\ntional Workshop on Search-Oriented Conversational\\nAI, pages 87‚Äì92.Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\\njun Li, and Ming Zhou. 2019. Response generation\\nby context-aware prototype editing. In Proceedings\\nof the AAAI Conference on ArtiÔ¨Åcial Intelligence ,\\nvolume 33, pages 7281‚Äì7288.\\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\\nJianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\\net al. 2021. A controllable model of grounded re-\\nsponse generation. In Proceedings of the AAAI Con-\\nference on ArtiÔ¨Åcial Intelligence , volume 35, pages\\n14085‚Äì14093.\\nMengzhou Xia, Guoping Huang, Lemao Liu, and\\nShuming Shi. 2019. Graph based translation mem-\\nory for neural machine translation. In Proceedings\\nof the AAAI Conference on ArtiÔ¨Åcial Intelligence ,\\nvolume 33, pages 7297‚Äì7304.\\nFei Xiao, Liang Pang, Yanyan Lan, Yan Wang, Huawei\\nShen, and Xueqi Cheng. 2021. Transductive learn-\\ning for unsupervised text style transfer. In Proceed-\\nings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing, EMNLP 2021, Vir-\\ntual Event / Punta Cana, Dominican Republic, 7-11\\nNovember, 2021 , pages 2510‚Äì2521. Association for\\nComputational Linguistics.\\nJitao Xu, Josep M Crego, and Jean Senellart. 2020.\\nBoosting neural machine translation with similar\\ntranslations. In Proceedings of the 58th Annual\\nMeeting of the Association for Computational Lin-\\nguistics , pages 1580‚Äì1590.\\nLiu Yang, Junjie Hu, Minghui Qiu, Chen Qu, Jian-\\nfeng Gao, W Bruce Croft, Xiaodong Liu, Yelong\\nShen, and Jingjing Liu. 2019. A hybrid retrieval-\\ngeneration neural conversation model. In Proceed-\\nings of the 28th ACM international conference on in-\\nformation and knowledge management , pages 1341‚Äì\\n1350.\\nJingyi Zhang, Masao Utiyama, Eiichiro Sumita, Gra-\\nham Neubig, and Satoshi Nakamura. 2018. Guiding\\nneural machine translation with retrieved translation\\npieces. In Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technolo-\\ngies, Volume 1 (Long Papers) , pages 1325‚Äì1335.\\nYizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris\\nBrockett, Michel Galley, Jianfeng Gao, and Bill\\nDolan. 2021. Joint retrieval and generation train-\\ning for grounded text generation. arXiv preprint\\narXiv:2105.06597 .\\nZhuosheng Zhang, Kehai Chen, Rui Wang, Masao\\nUtiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao.\\n2020. Neural machine translation with universal\\nvisual representation. In 8th International Confer-\\nence on Learning Representations, ICLR 2020, Ad-\\ndis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-\\nview.net.', metadata={'source': './data\\\\a survey.pdf', 'page': 11}),\n",
       " Document(page_content='Ventsislav Zhechev and Josef Van Genabith. 2010.\\nSeeding statistical machine translation with trans-\\nlation memory output through tree-based structural\\nalignment. In Proceedings of the 4th Workshop\\non Syntax and Structure in Statistical Translation ,\\npages 43‚Äì51.\\nXin Zheng, Zhirui Zhang, Junliang Guo, Shujian\\nHuang, Boxing Chen, Weihua Luo, and Jiajun Chen.\\n2021a. Adaptive nearest neighbor machine transla-\\ntion. arXiv preprint arXiv:2105.13022 .\\nXin Zheng, Zhirui Zhang, Shujian Huang, Boxing\\nChen, Jun Xie, Weihua Luo, and Jiajun Chen. 2021b.\\nNon-parametric unsupervised domain adaptation for\\nneural machine translation. In Findings of the As-\\nsociation for Computational Linguistics: EMNLP\\n2021 , pages 4234‚Äì4241.\\nKangyan Zhou, Shrimai Prabhumoye, and Alan W\\nBlack. 2018. A dataset for document grounded con-\\nversations. arXiv preprint arXiv:1809.07358 .', metadata={'source': './data\\\\a survey.pdf', 'page': 12}),\n",
       " Document(page_content='Benchmarking Large Language Models in Retrieval-Augmented Generation\\nJiawei Chen1,3, Hongyu Lin1,*, Xianpei Han1,2,*, Le Sun1,2\\n1Chinese Information Processing Laboratory2State Key Laboratory of Computer Science\\nInstitute of Software, Chinese Academy of Sciences, Beijing, China\\n3University of Chinese Academy of Sciences, Beijing, China\\n{jiawei2020,hongyu,xianpei,sunle }@iscas.ac.cn\\nAbstract\\nRetrieval-Augmented Generation (RAG) is a promising ap-\\nproach for mitigating the hallucination of large language\\nmodels (LLMs). However, existing research lacks rigorous\\nevaluation of the impact of retrieval-augmented generation\\non different large language models, which make it challeng-\\ning to identify the potential bottlenecks in the capabilities\\nof RAG for different LLMs. In this paper, we systemati-\\ncally investigate the impact of Retrieval-Augmented Gener-\\nation on large language models. We analyze the performance\\nof different large language models in 4 fundamental abili-\\nties required for RAG, including noise robustness, negative\\nrejection, information integration, and counterfactual robust-\\nness. To this end, we establish Retrieval-Augmented Genera-\\ntion Benchmark (RGB), a new corpus for RAG evaluation in\\nboth English and Chinese. RGB divides the instances within\\nthe benchmark into 4 separate testbeds based on the afore-\\nmentioned fundamental abilities required to resolve the case.\\nThen we evaluate 6 representative LLMs on RGB to diag-\\nnose the challenges of current LLMs when applying RAG.\\nEvaluation reveals that while LLMs exhibit a certain degree\\nof noise robustness, they still struggle significantly in terms of\\nnegative rejection, information integration, and dealing with\\nfalse information. The aforementioned assessment outcomes\\nindicate that there is still a considerable journey ahead to ef-\\nfectively apply RAG to LLMs.\\nIntroduction\\nRecently, there have been impressive advancements in large\\nlanguage models (LLMs) like ChatGPT (OpenAI 2022),\\nLLaMA-2 (Touvron et al. 2023), and ChatGLM (THUDM\\n2023a). Although these models have shown remarkable gen-\\neral abilities (Bang et al. 2023; Guo et al. 2023), they still\\nsuffer severely from challenges including factual halluci-\\nnation (Cao et al. 2020; Raunak, Menezes, and Junczys-\\nDowmunt 2021; Ji et al. 2023), knowledge out-dating (He,\\nZhang, and Roth 2022), and the lack of domain-specific ex-\\npertise (Li et al. 2023c; Shen et al. 2023).\\nIncorporating external knowledge via information re-\\ntrieval, i.e., Retrieval-Augmented Generation (RAG), has\\nbeen regarded as a promising way to resolve the above chal-\\nlenges. (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.\\n2022; Izacard et al. 2022). With the help of external knowl-\\n*Corresponding authors.\\nNoise Robustness Negative Rejection\\nWho was awarded the 2022 Nobel prize in \\nliterature?\\nThe Nobel Prize in Literature for 2022 is \\nawarded to the French author Annie Ernaux, \\n‚Äúfor the courage and clinical acuity ‚Ä¶\\nThe Nobel Prize in Literature for 2021 is \\nawarded to the novelist Abdulrazak Gurnah, \\nborn in Zanzibar and active in ‚Ä¶\\nAnnie ErnauxQuestion\\nExternal documents contain noises\\nRetrieval Augmented \\nGenerationWho was awarded the 2022 Nobel prize in \\nliterature?\\nThe Nobel Prize in Literature for 2021 is \\nawarded to the novelist Abdulrazak Gurnah, \\nborn in Zanzibar and active in ‚Ä¶\\nThe 2020 Nobel Laureate in Literature, \\npoet Louise Gl√ºck, has written both poetry \\nand essays about poetry. Since her‚Ä¶\\nI can not answer the question because of the \\ninsufficient information in documentsQuestion\\nExternal documents are all noises\\nInformation Integration\\nWhen were the ChatGPT app for iOS and \\nChatGPT apilaunched?\\nOn May 18th, 2023, OpenAI introduced its \\nown ChatGPT app for iOS‚Ä¶\\nThat changed on March 1, when OpenAI \\nannounced the release of API access to \\nChatGPT and Whisper,‚Ä¶\\nMay 18 and March 1 .Question\\nExternal documents contain all answers\\nRetrieval Augmented \\nGenerationCounterfactual Robustness\\nWhich city hosted the Olympic games in \\n2004?\\nThe 2004 Olympic Games returned home to \\nNew York , birthplace of the ‚Ä¶ \\nAfter leading all voting rounds, New York\\neasily defeated Rome in the fifth and \\nfinal vote ‚Ä¶\\nThere are factual errors in the provided \\ndocuments. The answer should be Athens . Question\\nCounterfactual external documents\\nRetrieval Augmented \\nGenerationRetrieval Augmented \\nGenerationFigure 1: Illustration of 4 kinds of abilities required for\\nretrieval-augmented generation of LLMs.\\nedge, LLMs can generate more accurate and reliable re-\\nsponses. The most common method is to use a search engine\\nas a retriever such as New Bing. Due to the vast amount of\\ninformation available on the Internet, using a search engine\\ncan provide more real-time information.\\nHowever, Retrieval-Augmented Generation brings not\\nonly positive effects to LLMs (Liu, Zhang, and Liang 2023;\\nMaynez et al. 2020). On one hand, there is a significant\\namount of noise information even fake news in the content\\navailable on the Internet, which poses challenges for search\\nengines in accurately retrieving desirable knowledge. On the\\nother hand, LLMs suffer from unreliable generation chal-\\nlenge. LLMs can be misled by incorrect information con-\\ntained in the context (Bian et al. 2023) and also suffer from\\nhallucination during the generation (Adlakha et al. 2023),\\nresulting in generating content that goes beyond external in-\\nformation. These challenges result in LLMs being unable toarXiv:2309.01431v1  [cs.CL]  4 Sep 2023', metadata={'source': './data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'page': 0}),\n",
       " Document(page_content='consistently generate reliable and accurate responses. Un-\\nfortunately, currently there lacks of comprehensive under-\\nstanding on how these factors can influence RAG, and how\\ncould each model survives from these drawbacks and im-\\nprovement their performance via information retrieval. As a\\nresult, there is a pressing need for a comprehensive evalua-\\ntion of LLMs on their ability to effectively utilize retrieved\\ninformation, as well as their ability to withstand the various\\ndrawbacks present in information retrieval.\\nTo this end, this paper conducts a comprehensive evalua-\\ntion of RAG for current LLMs. Specifically, we create a new\\nRetrieval-Augmented Generation Benchmark, namely RGB,\\nin both English and Chinese. In order to ensure that the in-\\nternal knowledge of LLMs does not introduce bias into the\\nevaluation results, RGB chooses to aggregate the latest news\\ninformation and constructs queries based on the news infor-\\nmation. Then, based on these queries, we use Search API to\\nfetch relevant documents and select most relevant snippets\\nfrom the content as external retrieved documents. Finally,\\nbased on different compositions of query and document-set\\npairs, we expand the corpus and divided it into 4 testbeds to\\nevaluate the following basic abilities of LLMs according to\\nthe common challenges in RAG, as shown in Figure 1:\\n‚Ä¢Noise Robustness , which means a model can extract use-\\nful information from noisy documents. In this paper, we\\ndefine noisy documents as those that are relevant to the\\nquestion but do not contain any information of the an-\\nswer. For the instance in Figure 1, the noisy documents\\nrelated to the question ‚ÄúWho was awarded the 2022 No-\\nbel Prize in Literature‚Äù include reports about the 2021\\nNobel Prize in Literature. To this end, the testbed for\\nnoise robustness contains instances whose external doc-\\numents contain a certain number of noisy documents\\nbased on the desired noise ratio.\\n‚Ä¢Negative Rejection , which means that a model should\\nreject to answer the question when the required knowl-\\nedge is not present in any retrieved document. The\\ntestbed for negative rejection contains instances whose\\nexternal documents are only with noisy documents.\\nLLMs are expected to indicate ‚Äúinsufficient information‚Äù\\nor other rejection signals.\\n‚Ä¢Information Integration , which evaluates whether the\\nmodel can answer complex questions that require inte-\\ngrating information from multiple documents. For the\\ninstance in Figure 1, for the question ‚ÄúWhen were the\\nChatGPT app for iOS and ChatGPT api launched?‚Äù, the\\nLLMs are expected to provide information on the launch\\ndates for both the ChatGPT app on iOS and the ChatGPT\\nAPI. The testbed for information integration contains in-\\nstances that can only be answered using multiple external\\ndocuments.\\n‚Ä¢Counterfactual Robustness , which evaluates whether\\nthe model can identify risks of known factual errors in the\\nretrieved documents when the LLMs are given warnings\\nabout potential risks in the retrieved information through\\ninstruction. The testbed for counterfactual robustness in-\\ncludes instances that can be answered directly by the\\nLLMs, but the external documents contain factual errors.Based on RGB, we conduct evaluation on 6 state-of-\\nthe-art large language models including ChatGPT (Ope-\\nnAI 2022), ChatGLM-6B (THUDM 2023a), ChatGLM2-\\n6B (THUDM 2023b), Vicuna-7b (Chiang et al. 2023),\\nQwen-7B-Chat (QwenLM 2023), BELLE-7B (Yunjie Ji\\n2023). We found that even though RAG can improve the re-\\nsponse accuracy of LLMs, they still suffer from the above-\\nmentioned challenges significantly. Specifically, we found\\nthat even though LLMs demonstrate some level of noise ro-\\nbustness, they tend to confuse similar information and fre-\\nquently generate inaccurate answers when relevant informa-\\ntion exists. For example, when faced with a question about\\nthe 2022 Nobel Prize in Literature, if there are noisy docu-\\nments about the 2021 Nobel Prize in Literature in external\\ndocuments, LLMs may become confused and provide inac-\\ncurate answers. Besides, LLMs frequently fail to reject an-\\nswering and generate incorrect answers when none of the\\nexternal documents contain relevant information. Further-\\nmore, LLMs lack the ability to summarize from multiple\\ndocuments, and therefore if multiple documents are needed\\nto answer a question, LLMs often fail to provide accurate\\nanswer. Finally, we found that even when the LLMs contain\\nthe required knowledge and are given warnings about po-\\ntential risks in the retrieved information through instruction,\\nthey still tend to trust and prioritize the retrieved information\\nover their own existing knowledge. The experimental results\\nmentioned above highlight the need for further resolution of\\nimportant issues in the existing RAG method. Therefore, it\\nis crucial to exercise caution and carefully design its usage.\\nGenerally speaking, the contributions of this paper are1:\\n‚Ä¢ We proposed to evaluate four capabilities for retrieval-\\naugmented generation of LLMs and created the\\nRetrieval-Augmented Generation Benchmark in both En-\\nglish and Chinese. To best of our knowledge, it is the first\\nbenchmark designed to assess these four capabilities for\\nretrieval-augmented generation of LLMs.\\n‚Ä¢ We evaluated the existing LLMs using RGB and found\\nthe limitations of them in the four different abilities.\\n‚Ä¢ We analyzed the responses of LLMs in RGB and identi-\\nfied their current shortcomings as well as suggested di-\\nrections for improvement.\\nRelated work\\nRetrieval-augmented models The knowledge stored in\\nlarge language models is commonly out-of-date (He, Zhang,\\nand Roth 2022) and they also sometimes generate hallu-\\ncination (Cao et al. 2020; Raunak, Menezes, and Junczys-\\nDowmunt 2021; Ji et al. 2023) i.e., they may generate ir-\\nrelevant or factually incorrect contents. By using external\\nknowledge as guidance, retrieval-augmented models can\\ngenerate more accurate and reliable responses (Guu et al.\\n2020; Lewis et al. 2020; Borgeaud et al. 2022; Izacard\\net al. 2022; Shi et al. 2023; Ren et al. 2023). Retrieval-\\naugmented models have achieved remarkable results in var-\\nious tasks such as open-domain QA (Izacard and Grave\\n1We will release the code and RGB of this paper in https:\\n//github.com/chen700564/RGB.', metadata={'source': './data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'page': 1}),\n",
       " Document(page_content='News CollectionThe 2022 Nobel Prize for Physiology and Medicine was awarded on \\nMonday to Swedish scientist Svante P√§ √§ bo for sequencing the \\ngenome of the Neanderthal.Data adjustment \\nand filtering by \\nHuman{\\n‚ÄúQuestion‚Äù: ‚ÄúWho was awarded the 2022 \\nNobel Prize for Physiology and Medicine ?‚Äù,\\n‚ÄúAnswer‚Äù: [\\'Svante P√§ √§ bo \\',\\'Svante Paabo ‚Äô]\\n}\\nData generation by \\nChatGPTRetrieve using \\nsearch engineRerank by dense \\nretrieval model\\nWe simulate the process of a user querying and obtaining \\ninformation. Suppose the user retrieves a current event news, \\nspeculate the event that the user is concerned about and the \\nquestion that he/she may want to know, and generate the key \\ninformation corresponding to the answer to the question. ‚Ä¶\\n‚Ä¶\\nNews: The 2022 Nobel Prize for Physiology and Medicine was ‚Ä¶Related event: 2022 Nobel Prize for Physiology and Medicine\\nQuestion: Who was awarded the 2022 Nobel Prize for Physiology \\nand Medicine?\\nKey information: Svante P√§√§bo and Svante Paabo\\ngpt-3.5-turbo apiQuery: Who was awarded the 2022 Nobel Prize for Physiology and \\nMedicine?‚Äù,{\"link\": \"https://www.nobelprize.org/prizes/medicine/\", \"title\": \\n\"The Nobel Prize in Physiology or Medicine 2022\", \"snippet\": \"The \\nNobel Assembly...\"}, ...\\nGoogle Search APIChun2ChunkWho was awarded the 2022 Nobel \\nPrize for Physiology and Medicine?‚Äù,Dense retrieval modelTop1 Chunk Top30 Chunk Top2 Chunk ‚Ä¶‚Ä¶\\n‚Ä¶‚Ä¶Figure 2: The process of data generation. Firstly, we use\\nmodels to extract (event, question, answer) from news ar-\\nticles. Next, we utilize search engines to retrieve relevant\\nweb pages. Finally, a dense retrieval model is employed to\\nre-rank the content of these web pages.\\n2021; Trivedi et al. 2023; Li et al. 2023a), dialogue (Cai\\net al. 2019a,b; Peng et al. 2023), domain-specific ques-\\ntion answering (Cui et al. 2023) and code generation (Zhou\\net al. 2023b). Recently, with the development of large mod-\\nels, a series of retrieval-enhanced tools and products have\\ngained widespread attention, such as ChatGPT retrieval plu-\\ngin, Langchain, New Bing, etc. However, in real-world sce-\\nnarios, the retrieved text inevitably contains noise. There-\\nfore, in this paper we conducted a systematic evaluation and\\nanalysis of retrieval-augmented generation in LLMs.\\nEvaluation of LLMs Evaluating LLMs has received sig-\\nnificant attention due to their remarkable general capabil-\\nity (Chang et al. 2023). It enables us to gain a deeper under-\\nstanding of the specific abilities and limitations of LLMs,\\nwhile also providing valuable guidance for future research.\\nIn the past, benchmarks such as GLUE (Wang et al. 2019b)\\nand SuperCLUE (Wang et al. 2019a) primarily focused on\\nevaluating NLP tasks, particularly in natural language un-\\nderstanding. However, these evaluations often fail to fully\\ncapture the capabilities of LLMs. MMLU (Hendrycks et al.\\n2021) was then proposed to measure the knowledge acquired\\nby language models when pre-training. Recently, with the\\ndevelopment of LLMs, a series of general evaluation bench-\\nmarks have emerged, such as AGIEval (Zhong et al. 2023),\\nC-Eval (Huang et al. 2023), AlpacaEval (Li et al. 2023b),OpenLLM Leaderboard (Edward Beeching 2023), etc. In\\naddition to general abilities, there are also specific bench-\\nmarks that focus on evaluating the capabilities of models.\\nFor example, CValues (Xu et al. 2023a) focuses on the safety\\nand responsibility of LLMs, M3Exam (Zhang et al. 2023)\\nfocuses on human exam and ToolBench (Qin et al. 2023)\\nevaluates how well LLMs use external tools. Recently, Ad-\\nlakha et al. (2023) evaluate the RAG of LLMs in exist QA\\ndataset. Different from their work, we focus on 4 required\\nabilities of RAG and create Retrieval-Augmented Genera-\\ntion Benchmark to evaluate the LLMs.\\nRetrieval-Augmented Generation Benchmark\\nIn this section, we first introduce the specific retrieval-\\naugmented generation abilities we aim to evaluate in LLMs.\\nNext, we outline the process of constructing the RAG bench-\\nmark for evaluation. Lastly, we present the evaluation met-\\nrics.\\nRequired abilities of RAG\\nExternal knowledge is the key to resolving the problems\\nof LLMs such as hallucination and outdated knowledge,\\nwhich can make LLMs generate more accurate and reliable\\nresponses through retrieval-augmented generation (RAG).\\nHowever, LLMs cannot always response as expected with\\nRAG. For one thing, there are numerous irrelevant docu-\\nments and false information on the Internet. Incorporating\\nthese external documents into LLMs could have a detrimen-\\ntal effect. For anthoer, LLMs suffer from the unreliable gen-\\neration challenge. The generation of LLMs is often unpre-\\ndictable, and we cannot guarantee that they will utilize the\\nuseful information entailed in the external documents. Ad-\\nditionally, LLMs can easily be misled by incorrect infor-\\nmation in the document. To this end, we build Retrieval-\\nAugmented Generation Benchmark (RGB) to evaluate the\\nretrieval-augmented generation of LLMs, and we concern\\nabout 4 specific abilities:\\nNoise Robustness is the robustness of LLMs in noisy\\ndocuments. As retrievers are not perfect, the external knowl-\\nedge they retrieve often contains a significant amount of\\nnoise, i.e., documents which are relevant to the question but\\ndo not contain any information about the answer. To effec-\\ntively answer user questions, LLMs must be able to extract\\nthe necessary information from documents despite there are\\nnoisy documents.\\nNegative Rejection is a measure of whether LLMs can\\ndecline to answer a question when none of the contexts pro-\\nvide useful information. In real-world situations, the search\\nengine often fails to retrieve documents containing the an-\\nswers. In these cases, it is important for the model to have\\nthe capability to reject recognition and avoid generating mis-\\nleading content.\\nInformation Integration is a capacity to integrate an-\\nswers from multiple documents. In many cases, the an-\\nswer to a question may be contained in multiple documents.\\nFor example, for the question ‚ÄùWho are the champions of\\nthe U.S. Open 2022 men‚Äôs and women‚Äôs singles?‚Äù , the two\\nchampions may be mentioned in different documents. In or-', metadata={'source': './data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'page': 2}),\n",
       " Document(page_content=\"der to provide better answers to complex questions, it is nec-\\nessary for LLMs to have the ability to integrate information.\\nCounterfactual Robustness refers to a capacity to han-\\ndle errors in external knowledge. In the real world, there is\\nan abundance of false information on the internet. Please\\nnote that we only evaluate the situation that LLMs are given\\nwarnings about potential risks in the retrieved information\\nthrough instruction.\\nIn real-world scenarios, it is not possible to obtain per-\\nfect documents with all the necessary external knowledge.\\nTherefore, evaluating these four abilities of the model be-\\ncomes essential in order to measure the RAG of LLMs.\\nData construction\\nInspired by previous benchmarks for LLMs, RGB utilizes\\na question-answering format for evaluation. We evaluate the\\nLLMs by judging the retrieval-augmented responses of them\\nto the questions. To simulate real-world scenarios, we con-\\nstruct question and answer data using actual news articles.\\nDue to the abundance of knowledge contained within the\\nLLMs there is a potential for bias when measuring the first\\nthree abilities. To mitigate this, the instances of RGB are\\nconstructed by latest news articles. Additionally, we retrieve\\nexternal documents from Internet through search engines.\\nFinally, we expand the corpus and divided it into 4 testbeds\\nto evaluate the above basic abilities of LLMs. The overall\\nprocedure of our data construction is illustrated in Figure 2.\\nQA instances generation. We first collect latest news ar-\\nticles and use prompts to have ChatGPT generate events,\\nquestions, and answers for each articles. For example, as\\nshown in the Figure 2, for a report about ‚ÄúThe 2022 Nobel\\nPrize‚Äù, ChatGPT will generate corresponding event, ques-\\ntion and provide key information for answering it. By gen-\\nerating events, the model is able to preliminarily filter out\\nnews articles that do not contain any events. After genera-\\ntion, we manually check the answer and filter out data that\\nis difficult to retrieve through search engines.\\nRetrieve using search engine. For each query, we use\\nGoogle‚Äôs API to fetch 10 relevant web pages and extract\\ncorresponding snippets of text from them. Simultaneously,\\nwe read these web pages and convert their textual content\\ninto text chunks with a maximum length of 300 tokens. Us-\\ning an existing dense retrieval model2, we select the top-30\\ntext chunks that match the query most effectively. These re-\\ntrieved text chunks, along with the snippets provided by the\\nsearch API, will serve as our external documents. These doc-\\numents will be divided into positive documents and negative\\ndocuments based on whether they contain the answer.\\nTestbeds construction for each ability. We expand the\\ncorpus and divided it into 4 testbeds to evaluate the above\\nbasic abilities of LLMs. To evaluate the noise robustness,\\nwe sample varying numbers of negative documents ac-\\ncording to the desired ratio of noises. For negative rejec-\\ntion, all the external documents are sampled from negative\\ndocuments. For the information integration ability, we fur-\\n2https://huggingface.co/sentence-transformers/all-mpnet-base-\\nv2 for English; https://huggingface.co/moka-ai/m3e-base for\\nChinese.\\nSystem instruction\\nYou are an accurate and reliable AI assistant that can \\nanswer questions with the help of external documents. \\nPlease note that external documents may contain noisy \\nor factually incorrect information. If the information in \\nthe document contains the correct answer, you will give \\nan accurate answer. If the information in the document \\ndoes not contain the answer, you will generate ‚ÄôI can not \\nanswer the question because of the insufficient \\ninformation in documents.‚Äò If there are inconsistencies \\nwith the facts in some of the documents, please generate \\nthe response 'There are factual errors in the provided \\ndocuments.' and provide the correct answer.\\nUser input Instruction\\nDocument: \\\\n{DOCS} \\\\n\\\\nQuestion: \\\\n{QUERY}System instruction\\n‰Ω†ÊòØ‰∏Ä‰∏™ÂáÜÁ°ÆÂíåÂèØÈù†ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂä©ÊâãÔºå\\nËÉΩÂ§üÂÄüÂä©Â§ñÈÉ®ÊñáÊ°£ÂõûÁ≠îÈóÆÈ¢òÔºåËØ∑Ê≥®ÊÑè\\nÂ§ñÈÉ®ÊñáÊ°£ÂèØËÉΩÂ≠òÂú®Âô™Â£∞‰∫ãÂÆûÊÄßÈîôËØØ„ÄÇ\\nÂ¶ÇÊûúÊñáÊ°£‰∏≠ÁöÑ‰ø°ÊÅØÂåÖÂê´‰∫ÜÊ≠£Á°ÆÁ≠îÊ°àÔºå\\n‰Ω†Â∞ÜËøõË°åÂáÜÁ°ÆÁöÑÂõûÁ≠î„ÄÇÂ¶ÇÊûúÊñáÊ°£‰∏≠ÁöÑ\\n‰ø°ÊÅØ‰∏çÂåÖÂê´Á≠îÊ°àÔºå‰Ω†Â∞ÜÁîüÊàê‚ÄúÊñáÊ°£‰ø°\\nÊÅØ‰∏çË∂≥ÔºåÂõ†Ê≠§ÊàëÊó†Ê≥ïÂü∫‰∫éÊèê‰æõÁöÑÊñáÊ°£\\nÂõûÁ≠îËØ•ÈóÆÈ¢ò„ÄÇ‚ÄùÂ¶ÇÊûúÈÉ®ÂàÜÊñáÊ°£‰∏≠Â≠òÂú®\\n‰∏é‰∫ãÂÆû‰∏ç‰∏ÄËá¥ÁöÑÈîôËØØÔºåËØ∑ÂÖàÁîüÊàê‚ÄúÊèê\\n‰æõÊñáÊ°£ÁöÑÊñáÊ°£Â≠òÂú®‰∫ãÂÆûÊÄßÈîôËØØ„ÄÇ‚ÄùÔºå\\nÂπ∂ÁîüÊàêÊ≠£Á°ÆÁ≠îÊ°à„ÄÇ\\nUser input Instruction\\nÊñáÊ°£Ôºö\\\\n{DOCS} \\\\n\\\\nÈóÆÈ¢òÔºö\\\\n{QUERY}English ChineseFigure 3: The instructions used in our experiments, which\\ninclude a system instruction followed by a user input instruc-\\ntion. The ‚Äú {DOCS }‚Äù and ‚Äú {QUERY }‚Äù will be replaced by\\nthe external documents and the question.\\nther construct data based on the above generated questions.\\nThis involves expanding or rewriting these questions so that\\ntheir answers encompass multiple aspects. For example, the\\nquestion ‚ÄúWho won the MVP of Super Bowl 2023?‚Äù can\\nbe rewrite as ‚ÄúWho won the MVPs of Super Bowl 2022\\nand 2023?‚Äù. Consequently, answering such questions re-\\nquires utilizing information from various documents. Dif-\\nferent from the first three abilities, the data of counterfactual\\nrobustness is constructed solely based on the internal knowl-\\nedge of the model. Based on the aforementioned generated\\nquestions mentioned above, we adopt ChatGPT to automat-\\nically generate its known knowledge. Specifically, we use\\nprompts to allow the model to generate both questions and\\nanswers that are already known. For example, based on the\\nquestion ‚ÄúWho was awarded the 2022 Nobel Prize for Phys-\\niology and Medicine?‚Äù, the model will generate the known\\nquestion ‚ÄúWho was awarded the 2021 Nobel Prize in Lit-\\nerature?‚Äù and answer ‚Äú Abdulrazak Gurnah ‚Äù. We then man-\\nually verified the generated answers, and retrieve relevant\\ndocuments as described above. In order to make documents\\ncontain factual errors, we manually modify the answers and\\nreplace the corresponding parts in the document.\\nFinally, we collect totally 600 base questions in RGB,\\nand 200 additional questions for the information integration\\nability and 200 additional questions for counterfactual ro-\\nbustness ability. Half of the instances are in English, and the\\nother half are in Chinese.\\nEvaluation metrics\\nThe core of this benchmark is to evaluate whether LLMs can\\nutilize the provided external documents to acquire knowl-\\nedge and generate reasonable answers. We evaluate the re-\\nsponses of LLMs in order to measure above-mentioned four\\nabilities of them.\\nAccuracy is used to measure noise robustness and infor-\\nmation integration We employ an exact matching approach\\nwhere if the generated text contains an exact match to the\\nanswer, it is considered as a correct answer.\\nRejection rate is used to measure negative rejection.\\nWhen only noisy documents are provided, LLMs should\\noutput the specific content ‚Äì ‚ÄùI can not answer the question\\nbecause of the insufficient information in documents.‚Äù (We\", metadata={'source': './data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'page': 3}),\n",
       " Document(page_content='English Chinese\\nNoise Ratio 0 0.2 0.4 0.6 0.8 0 0.2 0.4 0.6 0.8\\nChatGPT (OpenAI 2022) 96.33 94.67 94.00 90.00 76.00 95.67 94.67 91.00 87.67 70.67\\nChatGLM-6B (THUDM 2023a) 93.67 90.67 89.33 84.67 70.67 94.33 90.67 89.00 82.33 69.00\\nChatGLM2-6B (THUDM 2023b) 91.33 89.67 83.00 77.33 57.33 86.67 82.33 76.67 72.33 54.00\\nVicuna-7B-v1.3 (Chiang et al. 2023) 87.67 83.33 86.00 82.33 60.33 85.67 82.67 77.00 69.33 49.67\\nQwen-7B-Chat (QwenLM 2023) 94.33 91.67 91.00 87.67 73.67 94.00 92.33 88.00 84.33 68.67\\nBELLE-7B-2M (Yunjie Ji 2023) 83.33 81.00 79.00 71.33 64.67 92.00 88.67 85.33 78.33 67.68\\nTable 1: The experimental result of noise robustness measured by accuracy (%) under different noise ratios. We can see that the\\nincreasing noise rate poses a challenge for RAG in LLMs.\\nLong-distance information. Evidence uncertainty. Concept confusion.\\nQuestion Who did Iga Swiatek defeat to win the Qatar Open 2022? What is the name of Apple‚Äôs headset? What was Tesla‚Äôs revenue in Q1 2022?\\nAnswer Anett Kontaveit Vision Pro 18.76 billion\\nDocumentsPositive document\\nIn February, Swiatek entered into the Qatar Open ...\\nIn the final, she won ... Anett Kontaveit ...\\nNegative document\\nThis time, she defeated Ons Jabeur 6-2, 7-6(5) to win\\nthe 2022 US Open, ...Positive document\\nApple (AAPL.O) on Monday unveiled a costly\\naugmented-reality headset called the Vision Pro ...\\nNegative document\\n... is what Gurman believes will be called\\nApple Reality Pro. ...Positive document\\nTesla, Inc. (TSLA) reported Q1 FY 2022 earnings results\\n... detailed revenues of $ 18.76 billion ...\\nNegative document\\n...first-quarter earnings for 2022 ...\\n...Automotive revenue reached $16.86 billion...\\nResponsesIga Swiatek defeated Ons Jabeur in the second round\\nof the Qatar Open 2022 to win the tournament.According to the document, the name of Apple‚Äôs\\nheadset is Apple Reality Pro.According to the financial results provided in the article,\\nTesla‚Äôs revenue in Q1 2022 was $16.86 billion.\\nTable 2: Error cases of noise robustness, and only one positive document and one negative document are shown. The responses\\nare generated by ChatGLM2-6B. The blue text indicates the matching parts between the document and the question or answer,\\nwhile the red text highlights the non-matching parts.\\nuse instructions to inform the model.). If the model gener-\\nates this content, it indicates a successful rejection.\\nError detection rate measures whether the model can\\ndetect the factual errors in the documents for counterfactual\\nrobustness. When the provided documents contain factual\\nerrors, the model should output the specific content ‚Äì ‚ÄùThere\\nare factual errors in the provided documents.‚Äù (We use in-\\nstructions to inform the model.). If the model generates this\\ncontent, it indicates that the model has detected erroneous\\ninformation in the document.\\nError correction rate measures whether the model can\\nprovide the correct answer after identifying errors for coun-\\nterfactual robustness. The model is asked to generate the cor-\\nrect answer after identifying the factual errors. If the model\\ngenerates the correct answer, it indicates that the model is\\ncapable of correcting errors in the document.\\nThe formula for the metrics is\\nACC =#tt\\n#nums(1)\\nwhere #tt is the number of correct responses and #nums is\\nthe number of instances to be evaluated.\\nConsidering that the model may not fully adhere to in-\\nstructions, for rejection rate and error detection rate, we\\nalso use ChatGPT to conduct additional evaluation of the\\nanswers. Specifically, we assess the model‚Äôs responses by\\nusing instructions and demonstrations to determine if they\\ncan reflect information that is not present in the document or\\nidentify any factual errors.Experiments\\nIn this section, we evaluate the performance of various\\nLLMs, analyze and discuss the results, summarizing the\\nmain challenges that existing LLMs encounter when using\\nexternal knowledge.\\nSettings\\nTask formats. Due to contextual limitations, we provide 5\\nexternal documents for each question. In our experiments\\non noise robustness, we evaluate scenarios with noise ra-\\ntios ranging from 0 to 0.8. To comprehensively evaluate the\\noverall capabilities, we have adopted a unified instruction\\nfor each language, as shown in Figure 3. The experiments\\nwere conducted using an NVIDIA GeForce RTX 3090.\\nModels We conduct evaluation on 6 state-of-the-art large\\nlanguage models which can generate both English and\\nChinese including ChatGPT3, ChatGPT (OpenAI 2022),\\nChatGLM-6B (THUDM 2023a), ChatGLM2-6B (THUDM\\n2023b), Vicuna-7b-v1.3 (Chiang et al. 2023), Qwen-7B-\\nChat (QwenLM 2023), BELLE-7B-2M (Yunjie Ji 2023).\\nResults on Noise Robustness\\nWe evaluated the accuracy based on the different noise ratios\\nin external documents, and the results are shown in Table 1.\\nWe can see that:\\n(1) RAG can effect improve the responses of LLMs.\\nLLMs have shown strong performance even in the presence\\nof noise, indicating that RAG is a promising way for LLMs\\nto generate accurate and reliable responses.\\n3We use gpt-3.5-turbo api in the experiments.', metadata={'source': './data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'page': 4}),\n",
       " Document(page_content='(2) The increasing noise rate poses a challenge for\\nRAG in LLMs. Specifically, when the noise ratio exceeds\\n80%, the accuracy of the model decreases significantly. For\\nexample, the performance of ChatGPT has decreased from\\n96.33% to 76.00%, while the performance of ChatGLM2-\\n6B has decreased from 91.33% to 57.33%.\\nError Analysis. To better comprehend the negative im-\\npact of noise on model generation, we examined the incor-\\nrect answers and found that these errors typically originate\\nfrom three reasons, as shown in Table 2.\\n(1) Long-distance information. When dealing with an\\nexternal document, the model often faces difficulty in iden-\\ntifying the correct answer when the information related to\\nthe question is distant from the information related to the an-\\nswer. This scenario is quite common as longer texts are fre-\\nquently encountered on the internet. In such cases, it is typ-\\nical for the question‚Äôs information to be initially presented\\nat the start of the document and subsequently referred to us-\\ning pronouns. In Table 2, the question information (‚ÄùQatar\\nOpen 2022‚Äù) is only mentioned once at the beginning and is\\nfar from where the answer text ‚ÄùAnett Kontaveit‚Äù appears.\\nThis situation may cause LLMs to depend on information\\nfrom other documents and create false impressions, i.e., hal-\\nlucination.\\n(2) Evidence uncertainty. Before highly anticipated\\nevents, like the release of new Apple products or the an-\\nnouncement of the Oscars, there is often a significant\\namount of speculative information circulating on the inter-\\nnet. Although the relevant documents explicitly state that\\nit is uncertain or speculative content, they can still impact\\non the retrieval-augmented generation of LLMs. In Table 2,\\nwhen the noise ratio increases, the content of erroneous\\ndocuments is all about some people‚Äôs predictions about the\\nname of the headset (‚ÄúApple Reality Pro‚Äù). Even if there is a\\ncorrect answer (‚ÄúVision Pro‚Äù) in the relevant documents, the\\nmodel can still be misled by uncertain evidences.\\n(3) Concept confusion. The concepts in external docu-\\nments may be similar to, but different from, the concepts in\\nthe question. This can cause confusion for LLMs and make\\nLLMs generate incorrect answers. In Table 2, the model an-\\nswer focuses on the concept ‚Äúautomotive revenue‚Äù in the\\ndocument rather than ‚Äúrevenue‚Äù in the question.\\nBased on the analysis above, we have identified certain\\nlimitations in LLMs regarding retrieval-augmented genera-\\ntion. To effectively handle the vast amount of noise present\\non the internet, further detailed enhancements are required\\nfor the model such as long documents modeling and precise\\nconcept comprehension.\\nResults on Negative Rejection testbed\\nWe evaluated the rejection rate when only noise documents\\nwere provided. The results are shown in Table 3. In addi-\\ntion to evaluating the rejection rate through exact matching\\n(Rej in Table 3), we also utilize ChatGPT to determine if\\nthe responses from the LLMs contain any rejection informa-\\ntion (Rej‚àóin Table 3). We can see that: Negative Rejection\\nposes a challenge for RAG in LLMs. The highest rejection\\nrates for LLMs in English and Chinese were only 45% andLanguages English Chinese\\nRej Rej‚àóRej Rej‚àó\\nChatGPT 24.67 45.00 5.33 43.33\\nChatGLM-6B 9.00 25.00 6.33 17.00\\nChatGLM2-6B 10.33 41.33 6.33 36.33\\nVicuna-7B-v1.3 17.00 33.33 3.37 24.67\\nQwen-7B-Chat 31.00 35.67 8.67 25.33\\nBELLE-7B-2M 5.67 32.33 5.33 13.67\\nTable 3: The result of negative rejection. Rej means the re-\\njection rate (%) and Rej‚àómeans the rejection rate evaluated\\nby ChatGPT. We can see that negative rejection poses a chal-\\nlenge for RAG in LLMs.\\nQuestion Answer Response\\nwho will direct\\nIrredeemable film?Jeymes\\nSamuelThe information provided in the document\\nis that Fox has attached ‚ÄùThe Big Short‚Äù\\nwriter-director Adam McKay to its movie\\nadaptation of ‚ÄùIrredeemable‚Äù from Boom!\\nComics. Therefore, the answer is Adam McKay.\\nWhich country won the\\nmost medals at the\\n2022 Winter Olympics?NorwayBased on the information provided in the\\ndocument, the country that won the most medals\\nat the 2022 Winter Olympics is Germany.\\nGermany has won a total of 31 medals, including 14\\ngold medals, 14 bronze medals, and 31 silver medals.\\nTable 4: Error cases of negative rejection generated by\\nChatGLM2-6B. The red text highlights the error answers.\\n43.33%, respectively. This suggests that LLMs can be easily\\nmisled by noisy documents, leading to incorrect answers.\\nIn addition, through comparing Rej and Rej‚àó, we found\\nthat LLMs fail to strictly follow instructions, and they often\\ngenerate unpredictable responses, which make it hard to use\\nthem as state triggers (such as for recognizing rejection).\\nWe conduct case studies in Table 4. The first error is\\nbecause of Evidence uncertainty . Although the document\\nonly mentions contact with ‚ÄúAdam McKay‚Äù and does not\\nexplicitly state that he is the director of the movie, the\\nmodel still concludes that he holds this role. The first er-\\nror is because of Concept confusion . The information pro-\\nvided in the answer pertains to ‚Äúthe 2018 Winter Olympics‚Äù\\ninstead of ‚Äúthe 2022 Olympics‚Äù mentioned in the question.\\nRetrieval-augmented generation poses a greater challenge of\\nnegative rejection compared to answer directly as it presents\\nrelevant documents that could potentially mislead the LLMs\\nand result in incorrect responses. In future developments, it\\nwill be crucial for LLMs to enhance their ability to accu-\\nrately match questions with the appropriate documents.\\nResults on Information Integration testbed\\nWe evaluated the accuracy based on the different noise ratios\\nin external documents, and the results are shown in Table 5.\\nWhen comparing the model to Table 1, we observed that\\nit has a weak information integration ability, which in turn\\naffects its noise robustness. We can see that:\\n(1) Information integration poses a challenge for RAG\\nin LLMs. Even without noise, the highest accuracy of LLMs\\ncan only reach 60% and 67% for English and Chinese,\\nrespectively. After adding noise, the highest accuracy de-', metadata={'source': './data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'page': 5}),\n",
       " Document(page_content='English Chinese\\nNoise Ratio 0 0.2 0.4 0 0.2 0.4\\nChatGPT 55 51 34 63 58 47\\nChatGLM-6B 45 36 35 60 53 52\\nChatGLM2-6B 34 32 21 44 43 32\\nVicuna-7B-v1.3 60 53 43 43 36 25\\nQwen-7B-Chat 55 50 37 67 56 55\\nBELLE-7B-2M 40 34 24 49 41 38\\nTable 5: The experimental result of information integration\\nmeasured by accuracy (%) under different noise ratios. We\\ncan see that information integration poses a challenge for\\nRAG in LLMs.\\nQuestion Answer Response Errors\\nWhat groupings are France and\\nGermany in World Cup 2022?Group D\\nGroup EFrance and Germany are in\\nGroup D of the 2022 FIFA\\nWorld Cup.Merging\\nError\\nWho were the MVP of Super\\nBowl 2022 and 2023?Cooper Kupp\\nPatrick\\nMahomesAccording to the provided\\ndocument, the MVP of Super\\nBowl LVI was Cooper Kupp\\nof the Los Angeles Rams.Ignoring\\nError\\nWhat films won the 2022 and\\n2023 Academy Awards for\\nBest Picture?CODA\\nEverything\\nEverywhere\\nAll at OnceThe film CODA won the\\naward for Best Picture at the\\n95th Academy Awards\\nceremony held on 2023.Misalignment\\nError\\nTable 6: Error cases of information integration, the re-\\nsponses are generated by ChatGLM2-6B. The blue and red\\ntexts represent the answers to two sub-questions.\\ncreases to 43% and 55%. These results suggest that LLMs\\nstruggle with integrating information effectively and are not\\nwell-suited for directly answering complex questions.\\n(2) Complex questions are more challenging for RAG\\nwith noisy documents. Performance decline becomes sig-\\nnificant when the noise ratio is 0.4, but for simple problems,\\na significant decline occurs only at a noise ratio of 0.8. This\\nindicates that complex problems are more vulnerable to in-\\nterference from noise. We speculate that this is because solv-\\ning complex problems requires integrating information from\\nmultiple documents, and this information can be considered\\nas noise to each other, making it harder for the model to ex-\\ntract relevant information from the documents.\\nError Analysis. We conducted an error analysis on\\nChatGLM2-6B (noise ratio is 0). Apart from the similar er-\\nrors founded in the noise robustness experiment (38% of the\\ntotal), there are also three types of unique errors. We have\\npresented these cases in Table 6.\\n(1) Merging Error (28% of the total). The model some-\\ntimes merges the answers of the two sub-questions, resulting\\nin an error. It mistakenly uses the answer from one question\\nto address both two questions. At this point, the model will\\ndisregard any documents related to one sub-question. For\\nexample, in Table 6, it incorrectly states that Group D is the\\nWorld Cup group for both France and Germany, while in fact\\nGermany is actually assigned to Group E.\\n(2) Ignoring Error (28% of the total). Sometimes, the\\nmodel may ignore one of the sub-questions and only answer\\nthe other. This error occurs when the model lacks a complete\\nunderstanding of the problem and fails to recognize that it\\nconsists of multiple sub-problems. As a result, the modelAcc Acc doc ED ED‚àóCR\\nChatGPT-zh 91 17 1 3 33.33\\nQwen-7B-Chat-zh 77 12 5 4 25.00\\nChatGPT-en 89 9 8 7 57.14\\nTable 7: The result of counterfactual robustness. ACC is the\\naccuracy (%) of LLMs without external documents. ACC doc\\nis the accuracy (%) of LLMs with counterfactual documents.\\nED and ED‚àóare error detection rates evaluated by exact\\nmatching and ChatGPT, respectively. CR is the error cor-\\nrection rate.\\nonly considers relevant documents for one sub-problem in\\norder to generate an answer, disregarding the question posed\\nby another sub-problem. For example, in Table 6, the model\\nonly provides the answer for the MVP of Super Bowl 2022\\nand does not consider 2023.\\n(3) Misalignment Error (6% of the total). Sometimes,\\nthe model incorrectly identifies the documents for one sub-\\nquestion as the documents for another sub-question, leading\\nto misaligned answers. For example, in Table 6, the third an-\\nswer has two errors: an ignoring error and a misalignment er-\\nror. Firstly, the model only mentioned the Best Picture of the\\n2023 (95th) Academy Awards, completely disregarding the\\n2022 awards. Additionally, it incorrectly stated that ‚ÄúCODA‚Äù\\nis the Best Picture of 2023 when it was actually awarded as\\nthe Best Picture in 2022.\\nThe errors mentioned above are primarily caused by the\\nlimited understanding of complex questions, which hinders\\nthe ability to effectively utilize information from different\\nsub-problems. The key lies in improving the model‚Äôs rea-\\nsoning capability. One possible solution is to use a chain-of-\\nthought approach to break down complex problems (Zhou\\net al. 2023a; Xu et al. 2023b; Drozdov et al. 2023). How-\\never, these methods slow down the inference speed and can-\\nnot provide timely responses.\\nResults on Counterfactual Robustness testbed\\nIn order to ensure that LLMs possess relevant knowledge,\\nwe assess their performance by directly asking them ques-\\ntions. We only consider LLMs that have an accuracy rate of\\nover 70%. The results are shown in Table 7. We present the\\nfollowing metrics: accuracy without any documents, accu-\\nracy with counterfactual documents, error detection rates,\\nand error correction rates. We can see that It is hard for\\nLLMs to identify and correct factual errors in the docu-\\nments. This suggests that the model can be easily misled by\\ndocuments containing incorrect facts.\\nIt is important to note that retrieval-augmented generation\\nis not designed to automatically address factual errors within\\na given context, as this contradicts the underlying assump-\\ntion that the model lacks knowledge and relies on retrieved\\ndocuments for additional information. However, this issue is\\ncrucial in practical applications due to the abundance of fake\\nnews on the internet. Existing LLMs do not have a safeguard\\nto handle inaccurate responses caused by misinformation. In\\nfact, they heavily depend on the information they retrieve.\\nEven when LLMs contain the internal knowledge about the', metadata={'source': './data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'page': 6}),\n",
       " Document(page_content='questions, they often trust false information that is retrieved.\\nThis presents significant a challenge for the future develop-\\nment of RAG in LLMs.\\nConclusion\\nIn this paper, we evaluated four abilities of retrieval-\\naugmented generation in LLMs: noise robustness, nega-\\ntive rejection, information integration, and counterfactual\\nrobustness. To conduct the evaluation, we built Retrieval-\\nAugmented Generation Benchmark (RGB). The instances of\\nRGB are generated from latest news articles and the external\\ndocuments obtained from search engines. The experimental\\nresults suggest that current LLMs have limitations in the 4\\nabilities. This indicates that there is still a significant amount\\nof work needed to effectively apply RAG to LLMs. To en-\\nsure accurate and reliable responses from LLMs, it is crucial\\nto exercise caution and carefully design for RAG.\\nReferences\\nAdlakha, V .; BehnamGhader, P.; Lu, X. H.; Meade, N.; and\\nReddy, S. 2023. Evaluating Correctness and Faithfulness\\nof Instruction-Following Models for Question Answering.\\narXiv:2307.16877.\\nBang, Y .; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie,\\nB.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; Do, Q. V .; Xu,\\nY .; and Fung, P. 2023. A Multitask, Multilingual, Multi-\\nmodal Evaluation of ChatGPT on Reasoning, Hallucination,\\nand Interactivity. arXiv:2302.04023.\\nBian, N.; Liu, P.; Han, X.; Lin, H.; Lu, Y .; He, B.; and\\nSun, L. 2023. A Drop of Ink Makes a Million Think: The\\nSpread of False Information in Large Language Models.\\narXiv:2305.04812.\\nBorgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Ruther-\\nford, E.; Millican, K.; van den Driessche, G.; Lespiau, J.-B.;\\nDamoc, B.; Clark, A.; de Las Casas, D.; Guy, A.; Menick, J.;\\nRing, R.; Hennigan, T.; Huang, S.; Maggiore, L.; Jones, C.;\\nCassirer, A.; Brock, A.; Paganini, M.; Irving, G.; Vinyals,\\nO.; Osindero, S.; Simonyan, K.; Rae, J. W.; Elsen, E.; and\\nSifre, L. 2022. Improving language models by retrieving\\nfrom trillions of tokens. arXiv:2112.04426.\\nCai, D.; Wang, Y .; Bi, W.; Tu, Z.; Liu, X.; Lam, W.; and\\nShi, S. 2019a. Skeleton-to-Response: Dialogue Genera-\\ntion Guided by Retrieval Memory. In Proceedings of the\\n2019 Conference of the North American Chapter of the As-\\nsociation for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) , 1219‚Äì\\n1228. Minneapolis, Minnesota: Association for Computa-\\ntional Linguistics.\\nCai, D.; Wang, Y .; Bi, W.; Tu, Z.; Liu, X.; and Shi, S.\\n2019b. Retrieval-guided Dialogue Response Generation via\\na Matching-to-Generation Framework. In Proceedings of\\nthe 2019 Conference on Empirical Methods in Natural Lan-\\nguage Processing and the 9th International Joint Confer-\\nence on Natural Language Processing (EMNLP-IJCNLP) ,\\n1866‚Äì1875. Hong Kong, China: Association for Computa-\\ntional Linguistics.Cao, M.; Dong, Y .; Wu, J.; and Cheung, J. C. K. 2020. Fac-\\ntual Error Correction for Abstractive Summarization Mod-\\nels. In Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP) , 6251‚Äì\\n6258. Online: Association for Computational Linguistics.\\nChang, Y .; Wang, X.; Wang, J.; Wu, Y .; Yang, L.; Zhu,\\nK.; Chen, H.; Yi, X.; Wang, C.; Wang, Y .; Ye, W.;\\nZhang, Y .; Chang, Y .; Yu, P. S.; Yang, Q.; and Xie, X.\\n2023. A Survey on Evaluation of Large Language Models.\\narXiv:2307.03109.\\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,\\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\\nImpressing GPT-4 with 90%* ChatGPT Quality.\\nCui, J.; Li, Z.; Yan, Y .; Chen, B.; and Yuan, L. 2023. Chat-\\nLaw: Open-Source Legal Large Language Model with Inte-\\ngrated External Knowledge Bases. arXiv:2306.16092.\\nDrozdov, A.; Sch ¬®arli, N.; Aky ¬®urek, E.; Scales, N.; Song,\\nX.; Chen, X.; Bousquet, O.; and Zhou, D. 2023. Compo-\\nsitional Semantic Parsing with Large Language Models. In\\nThe Eleventh International Conference on Learning Repre-\\nsentations .\\nEdward Beeching, N. H. S. H. N. L. N. R. O. S. L. T.\\nT. W., Cl ¬¥ementine Fourrier. 2023. Open LLM Leader-\\nboard. https://huggingface.co/spaces/HuggingFaceH4/\\nopen llmleaderboard.\\nGuo, B.; Zhang, X.; Wang, Z.; Jiang, M.; Nie, J.; Ding, Y .;\\nYue, J.; and Wu, Y . 2023. How Close is ChatGPT to Hu-\\nman Experts? Comparison Corpus, Evaluation, and Detec-\\ntion. arXiv:2301.07597.\\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.-W.\\n2020. REALM: Retrieval-Augmented Language Model Pre-\\nTraining. In Proceedings of the 37th International Confer-\\nence on Machine Learning , ICML‚Äô20. JMLR.org.\\nHe, H.; Zhang, H.; and Roth, D. 2022. Rethinking\\nwith Retrieval: Faithful Large Language Model Inference.\\narXiv:2301.00303.\\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Mul-\\ntitask Language Understanding. In International Conference\\non Learning Representations .\\nHuang, Y .; Bai, Y .; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.;\\nLiu, J.; Lv, C.; Zhang, Y .; Lei, J.; Fu, Y .; Sun, M.; and He,\\nJ. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese\\nEvaluation Suite for Foundation Models. arXiv preprint\\narXiv:2305.08322 .\\nIzacard, G.; and Grave, E. 2021. Leveraging Passage Re-\\ntrieval with Generative Models for Open Domain Ques-\\ntion Answering. In Proceedings of the 16th Conference of\\nthe European Chapter of the Association for Computational\\nLinguistics: Main Volume , 874‚Äì880. Online: Association for\\nComputational Linguistics.\\nIzacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni,\\nF.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and\\nGrave, E. 2022. Atlas: Few-shot Learning with Retrieval\\nAugmented Language Models. arXiv:2208.03299.', metadata={'source': './data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'page': 7}),\n",
       " Document(page_content='Ji, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii, E.;\\nBang, Y . J.; Madotto, A.; and Fung, P. 2023. Survey of Hal-\\nlucination in Natural Language Generation. ACM Comput.\\nSurv. , 55(12).\\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V .;\\nGoyal, N.; K ¬®uttler, H.; Lewis, M.; Yih, W.-t.; Rockt ¬®aschel,\\nT.; Riedel, S.; and Kiela, D. 2020. Retrieval-Augmented\\nGeneration for Knowledge-Intensive NLP Tasks. In Pro-\\nceedings of the 34th International Conference on Neural\\nInformation Processing Systems , NIPS‚Äô20. Red Hook, NY ,\\nUSA: Curran Associates Inc. ISBN 9781713829546.\\nLi, D.; Rawat, A. S.; Zaheer, M.; Wang, X.; Lukasik, M.;\\nVeit, A.; Yu, F.; and Kumar, S. 2023a. Large Language\\nModels with Controllable Working Memory. In Findings of\\nthe Association for Computational Linguistics: ACL 2023 ,\\n1774‚Äì1793. Toronto, Canada: Association for Computa-\\ntional Linguistics.\\nLi, X.; Zhang, T.; Dubois, Y .; Taori, R.; Gulrajani, I.;\\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023b. Al-\\npacaEval: An Automatic Evaluator of Instruction-following\\nModels. https://github.com/tatsu-lab/alpaca eval.\\nLi, X.; Zhu, X.; Ma, Z.; Liu, X.; and Shah, S. 2023c. Are\\nChatGPT and GPT-4 General-Purpose Solvers for Financial\\nText Analytics? An Examination on Several Typical Tasks.\\narXiv:2305.05862.\\nLiu, N. F.; Zhang, T.; and Liang, P. 2023. Evaluating Verifi-\\nability in Generative Search Engines. arXiv:2304.09848.\\nMaynez, J.; Narayan, S.; Bohnet, B.; and McDonald, R.\\n2020. On Faithfulness and Factuality in Abstractive Sum-\\nmarization. In Proceedings of the 58th Annual Meeting of\\nthe Association for Computational Linguistics , 1906‚Äì1919.\\nOnline: Association for Computational Linguistics.\\nOpenAI. 2022. Chatgpt: Optimizing language models for\\ndialogue. https://openai.com/blog/chatgpt.\\nPeng, B.; Galley, M.; He, P.; Cheng, H.; Xie, Y .; Hu, Y .;\\nHuang, Q.; Liden, L.; Yu, Z.; Chen, W.; and Gao, J. 2023.\\nCheck Your Facts and Try Again: Improving Large Lan-\\nguage Models with External Knowledge and Automated\\nFeedback. arXiv:2302.12813.\\nQin, Y .; Liang, S.; Ye, Y .; Zhu, K.; Yan, L.; Lu, Y .; Lin, Y .;\\nCong, X.; Tang, X.; Qian, B.; Zhao, S.; Tian, R.; Xie, R.;\\nZhou, J.; Gerstein, M.; Li, D.; Liu, Z.; and Sun, M. 2023.\\nToolLLM: Facilitating Large Language Models to Master\\n16000+ Real-world APIs. arXiv:2307.16789.\\nQwenLM. 2023. Qwen-7B. https://github.com/QwenLM/\\nQwen-7B.\\nRaunak, V .; Menezes, A.; and Junczys-Dowmunt, M. 2021.\\nThe Curious Case of Hallucinations in Neural Machine\\nTranslation. In Proceedings of the 2021 Conference of the\\nNorth American Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies , 1172‚Äì\\n1183. Online: Association for Computational Linguistics.\\nRen, R.; Wang, Y .; Qu, Y .; Zhao, W. X.; Liu, J.; Tian, H.;\\nWu, H.; Wen, J.-R.; and Wang, H. 2023. Investigating the\\nFactual Knowledge Boundary of Large Language Models\\nwith Retrieval Augmentation. arXiv:2307.11019.Shen, X.; Chen, Z.; Backes, M.; and Zhang, Y . 2023. In\\nChatGPT We Trust? Measuring and Characterizing the Re-\\nliability of ChatGPT. arXiv:2304.08979.\\nShi, W.; Min, S.; Yasunaga, M.; Seo, M.; James, R.;\\nLewis, M.; Zettlemoyer, L.; and tau Yih, W. 2023. RE-\\nPLUG: Retrieval-Augmented Black-Box Language Models.\\narXiv:2301.12652.\\nTHUDM. 2023a. ChatGLM-6B. https://github.com/\\nTHUDM/ChatGLM-6B.\\nTHUDM. 2023b. ChatGLM2-6B. https://github.com/\\nTHUDM/ChatGLM2-6B.\\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\\nS.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucu-\\nrull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.;\\nGao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hosseini,\\nS.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa, M.;\\nKloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.;\\nLavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\\nX.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poul-\\nton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.;\\nSilva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang,\\nB.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.;\\nZarov, I.; Zhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Ro-\\ndriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023.\\nLlama 2: Open Foundation and Fine-Tuned Chat Models.\\narXiv:2307.09288.\\nTrivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal,\\nA. 2023. Interleaving Retrieval with Chain-of-Thought Rea-\\nsoning for Knowledge-Intensive Multi-Step Questions. In\\nProceedings of the 61st Annual Meeting of the Associa-\\ntion for Computational Linguistics (Volume 1: Long Papers) ,\\n10014‚Äì10037. Toronto, Canada: Association for Computa-\\ntional Linguistics.\\nWang, A.; Pruksachatkun, Y .; Nangia, N.; Singh, A.;\\nMichael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019a. Su-\\nperGLUE: A Stickier Benchmark for General-Purpose Lan-\\nguage Understanding Systems . Red Hook, NY , USA: Curran\\nAssociates Inc.\\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\\nBowman, S. R. 2019b. GLUE: A Multi-Task Benchmark\\nand Analysis Platform for Natural Language Understanding.\\nInInternational Conference on Learning Representations .\\nXu, G.; Liu, J.; Yan, M.; Xu, H.; Si, J.; Zhou, Z.; Yi, P.;\\nGao, X.; Sang, J.; Zhang, R.; Zhang, J.; Peng, C.; Huang, F.;\\nand Zhou, J. 2023a. CValues: Measuring the Values of Chi-\\nnese Large Language Models from Safety to Responsibility.\\narXiv:2307.09705.\\nXu, S.; Pang, L.; Shen, H.; Cheng, X.; and Chua, T.-\\nS. 2023b. Search-in-the-Chain: Towards Accurate, Credi-\\nble and Traceable Large Language Models for Knowledge-\\nintensive Tasks. arXiv:2304.14732.\\nYunjie Ji, Y . G. Y . P. Q. N. B. M. X. L., Yong Deng. 2023.\\nBELLE: Bloom-Enhanced Large Language model Engine.\\nhttps://github.com/LianjiaTech/BELLE.', metadata={'source': './data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'page': 8}),\n",
       " Document(page_content='Zhang, W.; Aljunied, S. M.; Gao, C.; Chia, Y . K.; and Bing,\\nL. 2023. M3Exam: A Multilingual, Multimodal, Multilevel\\nBenchmark for Examining Large Language Models.\\nZhong, W.; Cui, R.; Guo, Y .; Liang, Y .; Lu, S.; Wang,\\nY .; Saied, A.; Chen, W.; and Duan, N. 2023. AGIEval:\\nA Human-Centric Benchmark for Evaluating Foundation\\nModels. arXiv:2304.06364.\\nZhou, D.; Sch ¬®arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q. V .; and\\nChi, E. H. 2023a. Least-to-Most Prompting Enables Com-\\nplex Reasoning in Large Language Models. In The Eleventh\\nInternational Conference on Learning Representations .\\nZhou, S.; Alon, U.; Xu, F. F.; Jiang, Z.; and Neubig, G.\\n2023b. DocPrompting: Generating Code by Retrieving the\\nDocs. In The Eleventh International Conference on Learn-\\ning Representations .', metadata={'source': './data\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'page': 9}),\n",
       " Document(page_content='Generation-Augmented Retrieval for Open-Domain Question Answering\\nYuning Mao1‚àó, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\\nJianfeng Gao3, Jiawei Han1, Weizhu Chen2\\n1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\\n1{yuningm2, hanj}@illinois.edu\\n2,3{penhe, xiaodl, yeshe, jfgao,wzchen }@microsoft.com\\nAbstract\\nWe propose Generation-Augmented Retrieval\\n(GAR) for answering open-domain questions,\\nwhich augments a query through text genera-\\ntion of heuristically discovered relevant con-\\ntexts without external resources as supervi-\\nsion. We demonstrate that the generated con-\\ntexts substantially enrich the semantics of the\\nqueries and G ARwith sparse representations\\n(BM25) achieves comparable or better per-\\nformance than state-of-the-art dense retrieval\\nmethods such as DPR (Karpukhin et al., 2020).\\nWe show that generating diverse contexts for a\\nquery is beneÔ¨Åcial as fusing their results con-\\nsistently yields better retrieval accuracy. More-\\nover, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciÔ¨Åed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which Ô¨Årst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it\\nis infeasible to examine every piece of information\\nin the entire document collection ( e.g., millions\\nof Wikipedia passages) and the retrieval accuracy\\nbounds the performance of the (extractive) reader.\\n‚àóWork was done during internship at Microsoft Azure AI.\\n1Our code and retrieval results are available at https:\\n//github.com/morningmoni/GAR .Early OpenQA systems (Chen et al., 2017)\\nuse classic retrieval methods such as TF-IDF and\\nBM25 with sparse representations. Sparse methods\\nare lightweight and efÔ¨Åcient, but unable to per-\\nform semantic matching and fail to retrieve rele-\\nvant passages without lexical overlap. More re-\\ncently, methods based on dense representations\\n(Guu et al., 2020; Karpukhin et al., 2020) learn to\\nembed queries and passages into a latent vector\\nspace, in which text similarity beyond lexical over-\\nlap can be measured. Dense retrieval methods can\\nretrieve semantically relevant but lexically differ-\\nent passages and often achieve better performance\\nthan sparse methods. However, the dense mod-\\nels are more computationally expensive and suffer\\nfrom information loss as they condense the entire\\ntext sequence into a Ô¨Åxed-size vector that does not\\nguarantee exact matching (Luan et al., 2020).\\nThere have been some recent studies on query re-\\nformulation with text generation for other retrieval\\ntasks, which, for example, rewrite the queries to\\ncontext-independent (Yu et al., 2020; Lin et al.,\\n2020; Vakulenko et al., 2020) or well-formed (Liu\\net al., 2019) ones. However, these methods re-\\nquire either task-speciÔ¨Åc data ( e.g., conversational\\ncontexts, ill-formed queries) or external resources\\nsuch as paraphrase data (Zaiem and Sadat, 2019;\\nWang et al., 2020) that cannot or do not trans-\\nfer well to OpenQA. Also, some rely on time-\\nconsuming training process like reinforcement\\nlearning (RL) (Nogueira and Cho, 2017; Liu et al.,\\n2019; Wang et al., 2020) that is not efÔ¨Åcient enough\\nfor OpenQA (more discussions in Sec. 2).\\nIn this paper, we propose Generation-\\nAugmented Retrieval ( GAR), which augments\\na query through text generation of a pre-trained\\nlanguage model (PLM). Different from prior\\nstudies that reformulate queries, GARdoes not\\nrequire external resources or downstream feedback\\nvia RL as supervision, because it does not rewrite\\nthe query but expands it with heuristically discov-arXiv:2009.08553v4  [cs.CL]  6 Aug 2021', metadata={'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'page': 0}),\n",
       " Document(page_content='ered relevant contexts, which are fetched from\\nPLMs and provide richer background information\\n(Table 2). For example, by prompting a PLM\\nto generate the title of a relevant passage given\\na query and appending the generated title to the\\nquery, it becomes easier to retrieve that relevant\\npassage. Intuitively, the generated contexts\\nexplicitly express the search intent not presented\\nin the original query. As a result, GARwith\\nsparse representations achieves comparable or\\neven better performance than state-of-the-art\\napproaches (Karpukhin et al., 2020; Guu et al.,\\n2020) with dense representations of the original\\nqueries, while being more lightweight and efÔ¨Åcient\\nin terms of both training and inference (including\\nthe cost of the generation model) (Sec. 6.4).\\nSpeciÔ¨Åcally, we expand the query (question) by\\nadding relevant contexts as follows. We conduct\\nseq2seq learning with the question as the input\\nand various freely accessible in-domain contexts as\\nthe output such as the answer, the sentence where\\nthe answer belongs to , and the title of a passage\\nthat contains the answer . We then append the gen-\\nerated contexts to the question as the generation-\\naugmented query for retrieval. We demonstrate\\nthat using multiple contexts from diverse gener-\\nation targets is beneÔ¨Åcial as fusing the retrieval\\nresults of different generation-augmented queries\\nconsistently yields better retrieval accuracy.\\nWe conduct extensive experiments on the Nat-\\nural Questions (NQ) (Kwiatkowski et al., 2019)\\nand TriviaQA (Trivia) (Joshi et al., 2017) datasets.\\nThe results reveal four major advantages of GAR:\\n(1)GAR, combined with BM25, achieves signif-\\nicant gains over the same BM25 model that uses\\nthe original queries or existing unsupervised query\\nexpansion (QE) methods. (2) GARwith sparse rep-\\nresentations (BM25) achieves comparable or even\\nbetter performance than the current state-of-the-art\\nretrieval methods, such as DPR (Karpukhin et al.,\\n2020), that use dense representations. (3) Since\\nGARuses sparse representations to measure lexical\\noverlap2, it is complementary to dense representa-\\ntions: by fusing the retrieval results of GARand\\nDPR (denoted as GAR+), we obtain consistently\\nbetter performance than either method used individ-\\nually. (4) GARoutperforms DPR in the end-to-end\\nQA performance (EM) when the same extractive\\nreader is used: EM=41.8 (43.8 for GAR+) on NQ\\n2Strictly speaking, GARwith sparse representations han-\\ndles semantics before retrieval by enriching the queries, while\\nmaintaining the advantage of exact matching.and 62.7 on Trivia, creating new state-of-the-art re-\\nsults for extractive OpenQA. GARalso outperforms\\nother retrieval methods under the generative setup\\nwhen the same generative reader is used: EM=38.1\\n(45.3 for G AR+) on NQ and 62.2 on Trivia.\\nContributions . (1) We propose Generation-\\nAugmented Retrieval ( GAR), which augments\\nqueries with heuristically discovered relevant con-\\ntexts through text generation without external su-\\npervision or time-consuming downstream feedback.\\n(2) We show that using generation-augmented\\nqueries achieves signiÔ¨Åcantly better retrieval and\\nQA results than using the original queries or ex-\\nisting unsupervised QE methods. (3) We show\\nthatGAR, combined with a simple BM25 model,\\nachieves new state-of-the-art performance on two\\nbenchmark datasets in extractive OpenQA and com-\\npetitive results in the generative setting.\\n2 Related Work\\nConventional Query Expansion .GARshares\\nsome merits with query expansion (QE) meth-\\nods based on pseudo relevance feedback (Rocchio,\\n1971; Abdul-Jaleel et al., 2004; Lv and Zhai, 2010)\\nin that they both expand the queries with relevant\\ncontexts (terms) without the use of external super-\\nvision. GARis superior as it expands the queries\\nwith knowledge stored in the PLMs rather than\\nthe retrieved passages and its expanded terms are\\nlearned through text generation.\\nRecent Query Reformulation . There are recent\\nor concurrent studies (Nogueira and Cho, 2017;\\nZaiem and Sadat, 2019; Yu et al., 2020; Vaku-\\nlenko et al., 2020; Lin et al., 2020) that reformu-\\nlate queries with generation models for other re-\\ntrieval tasks. However, these studies are not eas-\\nily applicable or efÔ¨Åcient enough for OpenQA be-\\ncause: (1) They require external resources such as\\nparaphrase data (Zaiem and Sadat, 2019), search\\nsessions (Yu et al., 2020), or conversational con-\\ntexts (Lin et al., 2020; Vakulenko et al., 2020)\\nto form the reformulated queries, which are not\\navailable or showed inferior domain-transfer per-\\nformance in OpenQA (Zaiem and Sadat, 2019);\\n(2) They involve time-consuming training process\\nsuch as RL. For example, Nogueira and Cho (2017)\\nreported a training time of 8 to 10 days as it uses\\nretrieval performance in the reward function and\\nconducts retrieval at each iteration. In contrast,\\nGARuses freely accessible in-domain contexts like\\npassage titles as the generation targets and standard', metadata={'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'page': 1}),\n",
       " Document(page_content='seq2seq learning, which, despite its simplicity, is\\nnot only more efÔ¨Åcient but effective for OpenQA.\\nRetrieval for OpenQA . Existing sparse retrieval\\nmethods for OpenQA (Chen et al., 2017) solely rely\\non the information of the questions. GARextends\\nto contexts relevant to the questions by extracting\\ninformation inside PLMs and helps sparse meth-\\nods achieve comparable or better performance than\\ndense methods (Guu et al., 2020; Karpukhin et al.,\\n2020), while enjoying the simplicity and efÔ¨Åciency\\nof sparse representations. GARcan also be used\\nwith dense representations to seek for even better\\nperformance, which we leave as future work.\\nGenerative QA . Generative QA generates answers\\nthrough seq2seq learning instead of extracting an-\\nswer spans. Recent studies on generative OpenQA\\n(Lewis et al., 2020a; Min et al., 2020; Izacard and\\nGrave, 2020) are orthogonal to GARin that they\\nfocus on improving the reading stage and directly\\nreuse DPR (Karpukhin et al., 2020) as the retriever.\\nUnlike generative QA, the goal of GARis not to\\ngenerate perfect answers to the questions but perti-\\nnent contexts that are helpful for retrieval. Another\\nline in generative QA learns to generate answers\\nwithout relevant passages as the evidence but solely\\nthe question itself using PLMs (Roberts et al., 2020;\\nBrown et al., 2020). GARfurther conÔ¨Årms that one\\ncan extract factual knowledge from PLMs, which\\nis not limited to the answers as in prior studies but\\nalso other relevant contexts.\\n3 Generation-Augmented Retrieval\\n3.1 Task Formulation\\nOpenQA aims to answer factoid questions with-\\nout pre-speciÔ¨Åed domains. We assume that a large\\ncollection of documents C(i.e., Wikipedia) are\\ngiven as the resource to answer the questions and\\na retriever-reader architecture is used to tackle the\\ntask, where the retriever retrieves a small subset\\nof the documents D‚äÇCand the reader reads the\\ndocuments Dto extract (or generate) an answer.\\nOur goal is to improve the effectiveness and efÔ¨Å-\\nciency of the retriever and consequently improve\\nthe performance of the reader.\\n3.2 Generation of Query Contexts\\nInGAR, queries are augmented with various heuris-\\ntically discovered relevant contexts in order to re-\\ntrieve more relevant passages in terms of both quan-\\ntity and quality. For the task of OpenQA where the\\nquery is a question, we take the following threefreely accessible contexts as the generation targets.\\nWe show in Sec. 6.2 that having multiple gener-\\nation targets is helpful in that fusing their results\\nconsistently brings better retrieval accuracy.\\nContext 1: The default target (answer) . The de-\\nfault target is the label in the task of interest, which\\nis the answer in OpenQA. The answer to the ques-\\ntion is apparently useful for the retrieval of relevant\\npassages that contain the answer itself. As shown\\nin previous work (Roberts et al., 2020; Brown et al.,\\n2020), PLMs are able to answer certain questions\\nsolely by taking the questions as input ( i.e., closed-\\nbook QA). Instead of using the generated answers\\ndirectly as in closed-book QA, GARtreats them\\nas contexts of the question for retrieval. The ad-\\nvantage is that even if the generated answers are\\npartially correct (or even incorrect), they may still\\nbeneÔ¨Åt retrieval as long as they are relevant to the\\npassages that contain the correct answers ( e.g., co-\\noccur with the correct answers).\\nContext 2: Sentence containing the default tar-\\nget. The sentence in a passage that contains the\\nanswer is used as another generation target. Sim-\\nilar to using answers as the generation target, the\\ngenerated sentences are still beneÔ¨Åcial for retriev-\\ning relevant passages even if they do not contain\\nthe answers, as their semantics is highly related to\\nthe questions/answers (examples in Sec. 6.1). One\\ncan take the relevant sentences in the ground-truth\\npassages (if any) or those in the positive passages\\nof a retriever as the reference, depending on the\\ntrade-off between reference quality and diversity.\\nContext 3: Title of passage containing the de-\\nfault target . One can also use the titles of rele-\\nvant passages as the generation target if available.\\nSpeciÔ¨Åcally, we retrieve Wikipedia passages using\\nBM25 with the question as the query, and take the\\npage titles of positive passages that contain the an-\\nswers as the generation target. We observe that\\nthe page titles of positive passages are often entity\\nnames of interest, and sometimes (but not always)\\nthe answers to the questions. Intuitively, if GAR\\nlearns which Wikipedia pages the question is re-\\nlated to, the queries augmented by the generated\\ntitles would naturally have a better chance of re-\\ntrieving those relevant passages.\\nWhile it is likely that some of the generated\\nquery contexts involve unfaithful or nonfactual in-\\nformation due to hallucination in text generation\\n(Mao et al., 2020) and introduce noise during re-\\ntrieval, they are beneÔ¨Åcial rather than harmful over-', metadata={'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'page': 2}),\n",
       " Document(page_content='all, as our experiments show that GARimprove\\nboth retrieval and QA performance over BM25 sig-\\nniÔ¨Åcantly. Also, since we generate 3 different (com-\\nplementary) query contexts and fuse their retrieval\\nresults, the distraction of hallucinated content is\\nfurther alleviated.\\n3.3 Retrieval with Generation-Augmented\\nQueries\\nAfter generating the contexts of a query, we append\\nthem to the query to form a generation-augmented\\nquery .3We observe that conducting retrieval with\\nthe generated contexts ( e.g., answers) alone as\\nqueries instead of concatenation is ineffective be-\\ncause (1) some of the generated answers are rather\\nirrelevant, and (2) a query consisting of the correct\\nanswer alone (without the question) may retrieve\\nfalse positive passages with unrelated contexts that\\nhappen to contain the answer. Such low-quality\\npassages may lead to potential issues in the follow-\\ning passage reading stage.\\nIf there are multiple query contexts, we conduct\\nretrieval using queries with different generated con-\\ntexts separately and then fuse their results. The per-\\nformance of one-time retrieval with all the contexts\\nappended is slightly but not signiÔ¨Åcantly worse.\\nFor simplicity, we fuse the retrieval results in a\\nstraightforward way: an equal number of passages\\nare taken from the top-retrieved passages of each\\nsource. One may also use weighted or more so-\\nphisticated fusion strategies such as reciprocal rank\\nfusion (Cormack et al., 2009), the results of which\\nare slightly better according to our experiments.4\\nNext, one can use any off-the-shelf retriever for\\npassage retrieval. Here, we use a simple BM25\\nmodel to demonstrate that GARwith sparse repre-\\nsentations can already achieve comparable or better\\nperformance than state-of-the-art dense methods\\nwhile being more lightweight and efÔ¨Åcient (includ-\\ning the cost of the generation model), closing the\\ngap between sparse and dense retrieval methods.\\n4 OpenQA with G AR\\nTo further verify the effectiveness of GAR, we\\nequip it with both extractive and generative read-\\ners for end-to-end QA evaluation. We follow the\\n3One may create a title Ô¨Åeld during document indexing\\nand conduct multi-Ô¨Åeld retrieval but here we append the titles\\nto the questions as other query contexts for generalizability.\\n4We use the fusion tools at https://github.com/\\njoaopalotti/trectools .reader design of the major baselines for a fair com-\\nparison, while virtually any existing QA reader can\\nbe used with G AR.\\n4.1 Extractive Reader\\nFor the extractive setup, we largely follow the de-\\nsign of the extractive reader in DPR (Karpukhin\\net al., 2020). Let D= [d1, d2, ..., d k]denote the list\\nof retrieved passages with passage relevance scores\\nD. LetSi= [s1, s2, ..., s N]denote the top Ntext\\nspans in passage diranked by span relevance scores\\nSi. BrieÔ¨Çy, the DPR reader uses BERT-base (De-\\nvlin et al., 2019) for representation learning, where\\nit estimates the passage relevance score Dkfor\\neach retrieved passage dkbased on the [CLS] to-\\nkens of all retrieved passages D, and assigns span\\nrelevance scores Sifor each candidate span based\\non the representations of its start and end tokens.\\nFinally, the span with the highest span relevance\\nscore from the passage with the highest passage rel-\\nevance score is chosen as the answer. We refer the\\nreaders to Karpukhin et al. (2020) for more details.\\nPassage-level Span Voting . Many extractive QA\\nmethods (Chen et al., 2017; Min et al., 2019b; Guu\\net al., 2020; Karpukhin et al., 2020) measure the\\nprobability of span extraction in different retrieved\\npassages independently, despite that their collec-\\ntive signals may provide more evidence in deter-\\nmining the correct answer. We propose a simple\\nyet effective passage-level span voting mechanism,\\nwhich aggregates the predictions of the spans in\\nthe same surface form from different retrieved pas-\\nsages. Intuitively, if a text span is considered as the\\nanswer multiple times in different passages, it is\\nmore likely to be the correct answer. SpeciÔ¨Åcally,\\nGARcalculates a normalized score p(Si[j])for the\\nj-th span in passage diduring inference as follows:\\np(Si[j]) = softmax (D)[i]√ósoftmax (Si)[j].GAR\\nthen aggregates the scores of the spans with the\\nsame surface string among all the retrieved pas-\\nsages as the collective passage-level score.5\\n4.2 Generative Reader\\nFor the generative setup, we use a seq2seq frame-\\nwork where the input is the concatenation of the\\nquestion and top-retrieved passages and the target\\noutput is the desired answer. Such generative read-\\ners are adopted in recent methods such as SpanSe-\\n5We Ô¨Ånd that the number of spans used for normalization\\nin each passage does not have signiÔ¨Åcant impact on the Ô¨Ånal\\nperformance (we take N= 5) and using the raw or normalized\\nstrings for aggregation also perform similarly.', metadata={'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'page': 3}),\n",
       " Document(page_content='qGen (Min et al., 2020) and Longformer (Belt-\\nagy et al., 2020). SpeciÔ¨Åcally, we use BART-large\\n(Lewis et al., 2019) as the generative reader, which\\nconcatenates the question and top-retrieved pas-\\nsages up to its length limit (1,024 tokens, 7.8 pas-\\nsages on average). Generative GARis directly com-\\nparable with SpanSeqGen (Min et al., 2020) that\\nuses the retrieval results of DPR but not comparable\\nwith Fusion-in-Decoder (FID) (Izacard and Grave,\\n2020) since it encodes 100 passages rather than\\n1,024 tokens and involves more model parameters.\\n5 Experiment Setup\\n5.1 Datasets\\nWe conduct experiments on the open-domain ver-\\nsion of two popular QA benchmarks: Natural Ques-\\ntions (NQ) (Kwiatkowski et al., 2019) and Trivi-\\naQA (Trivia) (Joshi et al., 2017). The statistics of\\nthe datasets are listed in Table 1.\\nDataset Train / Val / Test Q-len A-len #-A\\nNQ 79,168 / 8,757 / 3,610 12.5 5.2 1.2\\nTrivia 78,785 / 8,837 / 11,313 20.2 5.5 13.7\\nTable 1: Dataset statistics that show the number of sam-\\nples per data split, the average question (answer) length,\\nand the number of answers for each question.\\n5.2 Evaluation Metrics\\nFollowing prior studies (Karpukhin et al., 2020),\\nwe use top-k retrieval accuracy to evaluate the per-\\nformance of the retriever and the Exact Match (EM)\\nscore to measure the performance of the reader.\\nTop-k retrieval accuracy is deÔ¨Åned as the pro-\\nportion of questions for which the top-k retrieved\\npassages contain at least one answer span, which\\nis an upper bound of how many questions are ‚Äúan-\\nswerable‚Äù by an extractive reader.\\nExact Match (EM) is the proportion of the pre-\\ndicted answer spans being exactly the same as (one\\nof) the ground-truth answer(s), after string normal-\\nization such as article and punctuation removal.\\n5.3 Compared Methods\\nFor passage retrieval, we mainly compare with\\nBM25 and DPR, which represent the most used\\nstate-of-the-art methods of sparse and dense re-\\ntrieval for OpenQA, respectively. For query ex-\\npansion, we re-emphasize that GARis the Ô¨Årst QE\\napproach designed for OpenQA and most of the\\nrecent approaches are not applicable or efÔ¨Åcientenough for OpenQA since they have task-speciÔ¨Åc\\nobjectives, require external supervision that was\\nshown to transfer poorly to OpenQA, or take many\\ndays to train (Sec. 2). We thus compare with a clas-\\nsic unsupervised QE method RM3 (Abdul-Jaleel\\net al., 2004) that does not need external resources\\nfor a fair comparison. For passage reading, we\\ncompare with both extractive (Min et al., 2019a;\\nAsai et al., 2019; Lee et al., 2019; Min et al., 2019b;\\nGuu et al., 2020; Karpukhin et al., 2020) and gen-\\nerative (Brown et al., 2020; Roberts et al., 2020;\\nMin et al., 2020; Lewis et al., 2020a; Izacard and\\nGrave, 2020) methods when equipping GARwith\\nthe corresponding reader.\\n5.4 Implementation Details\\nRetriever . We use Anserini (Yang et al., 2017) for\\ntext retrieval of BM25 and GARwith its default\\nparameters. We conduct grid search for the QE\\nbaseline RM3 (Abdul-Jaleel et al., 2004).\\nGenerator . We use BART-large (Lewis et al.,\\n2019) to generate query contexts in GAR. When\\nthere are multiple desired targets (such as multi-\\nple answers or titles), we concatenate them with\\n[SEP] tokens as the reference and remove the [SEP]\\ntokens in the generation-augmented queries. For\\nTrivia, in particular, we use the value Ô¨Åeld as the\\ngeneration target of answer and observe better per-\\nformance. We take the checkpoint with the best\\nROUGE-1 F1 score on the validation set, while\\nobserving that the retrieval accuracy of GARis rel-\\natively stable to the checkpoint selection since we\\ndo not directly use the generated contexts but treat\\nthem as augmentation of queries for retrieval.\\nReader . Extractive GARuses the reader of DPR\\nwith largely the same hyperparameters, which is\\ninitialized with BERT-base (Devlin et al., 2019)\\nand takes 100 (500) retrieved passages during train-\\ning (inference). Generative GARconcatenates the\\nquestion and top-10 retrieved passages, and takes\\nat most 1,024 tokens as input. Greedy decoding is\\nadopted for all generation models, which appears to\\nperform similarly to (more expensive) beam search.\\n6 Experiment Results\\nWe evaluate the effectiveness of GARin three\\nstages: generation of query contexts (Sec. 6.1),\\nretrieval of relevant passages (Sec. 6.2), and pas-\\nsage reading for OpenQA (Sec. 6.3). Ablation\\nstudies are mostly shown on the NQ dataset to un-\\nderstand the drawbacks of GARsince it achieves', metadata={'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'page': 4}),\n",
       " Document(page_content='Question : when did bat out of hell get released?\\nAnswer :September 1977 {September 1977}\\nSentence : Bat Out of Hell is the second studio album and the major - label debut by American rock singer Meat\\nLoaf ... released in September 1977 on Cleveland International / Epic Records.\\n{The album was released in September 1977 on Cleveland International / Epic Records. }\\nTitle :Bat Out of Hell {Bat Out of Hell}\\nQuestion : who sings does he love me with reba?\\nAnswer :Brooks & Dunn {Linda Davis}\\nSentence :Linda Kaye Davis ( born November 26, 1962 ) is an American country music singer.\\n{‚Äú Does He Love You ‚Äù is a song written by Sandy Knox and Billy Stritch, and recorded as a duet by American\\ncountry music artists Reba McEntire and Linda Davis. }\\nTitle :Does He Love Me [SEP] Does He Love Me (Reba McEntire song) [SEP] I Do (Reba McEntire album)\\n{Linda Davis [SEP] Greatest Hits V olume Two (Reba McEntire album) [SEP] Does He Love You }\\nQuestion : what is the name of wonder womans mother?\\nAnswer :Mother Magda {Queen Hippolyta}\\nSentence : In the Amazonian myths, she is the daughter of the Amazon queen Sifrat and the male dwarf Shuri,\\nand is the mother of Wonder Woman. {Wonder Woman‚Äôs origin story relates that she was sculpted from clay\\nby her mother Queen Hippolyta and given life by Aphrodite. }\\nTitle :Wonder Woman [SEP] Diana Prince [SEP] Wonder Woman (2011 TV pilot)\\n{Wonder Woman [SEP] Orana (comics) [SEP] Wonder Woman (TV series) }\\nTable 2: Examples of generated query contexts .Relevant andirrelevant contexts are shown in green and\\nred. Ground-truth references are shown in the {braces}. The issue of generating wrong answers is alleviated by\\ngenerating other contexts highly related to the question/answer.\\nbetter performance on Trivia.\\n6.1 Query Context Generation\\nAutomatic Evaluation . To evaluate the quality\\nof the generated query contexts, we Ô¨Årst measure\\ntheir lexical overlap with the ground-truth query\\ncontexts. As suggested by the nontrivial ROUGE\\nscores in Table 3, GARdoes learn to generate\\nmeaningful query contexts that could help the re-\\ntrieval stage. We next measure the lexical overlap\\nbetween the query and the ground-truth passage.\\nThe ROUGE-1/2/L F1 scores between the original\\nquery and ground-truth passage are 6.00/2.36/5.01,\\nand those for the generation-augmented query are\\n7.05/2.84/5.62 (answer), 13.21/6.99/10.27 (sen-\\ntence), 7.13/2.85/5.76 (title) on NQ, respectively.\\nSuch results further demonstrate that the generated\\nquery contexts signiÔ¨Åcantly increase the word over-\\nlap between the queries and the positive passages,\\nand thus are likely to improve retrieval results.6\\nCase Studies . In Table 2, we show several ex-\\namples of the generated query contexts and their\\nground-truth references. In the Ô¨Årst example, the\\ncorrect album release date appears in both the gen-\\nerated answer and the generated sentence, and the\\ngenerated title is the same as the Wikipedia page\\n6We use F1 instead of recall to avoid the unfair favor of\\n(longer) generation-augmented query.Context ROUGE-1 ROUGE-2 ROUGE-L\\nAnswer 33.51 20.54 33.30\\nSentence 37.14 24.71 33.91\\nTitle 43.20 32.11 39.67\\nTable 3: ROUGE F1 scores of the generated query\\ncontexts on the validation set of the NQ dataset.\\ntitle of the album. In the last two examples, the\\ngenerated answers are wrong but fortunately, the\\ngenerated sentences contain the correct answer and\\n(or) other relevant information and the generated\\ntitles are highly related to the question as well,\\nwhich shows that different query contexts are com-\\nplementary to each other and the noise during query\\ncontext generation is thus reduced.\\n6.2 Generation-Augmented Retrieval\\nComparison w. the state-of-the-art . We next\\nevaluate the effectiveness of GARfor retrieval.\\nIn Table 4, we show the top-k retrieval accuracy\\nof BM25, BM25 with query expansion (+RM3)\\n(Abdul-Jaleel et al., 2004), DPR (Karpukhin et al.,\\n2020), G AR, and G AR+(GAR+DPR).\\nOn the NQ dataset, while BM25 clearly under-\\nperforms DPR regardless of the number of retrieved\\npassages, the gap between GARand DPR is signiÔ¨Å-\\ncantly smaller and negligible when k‚â•100. When\\nk‚â•500,GARis slightly better than DPR despite', metadata={'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'page': 5}),\n",
       " Document(page_content='MethodNQ Trivia\\nTop-5 Top-20 Top-100 Top-500 Top-1000 Top-5 Top-20 Top-100 Top-500 Top-1000\\nBM25 (ours) 43.6 62.9 78.1 85.5 87.8 67.7 77.3 83.9 87.9 88.9\\nBM25 +RM3 44.6 64.2 79.6 86.8 88.9 67.0 77.1 83.8 87.7 88.9\\nDPR 68.3 80.1 86.1 90.3 91.2 72.7 80.2 84.8 - -\\nGAR 60.9 74.4 85.3 90.3 91.7 73.1 80.4 85.7 88.9 89.7\\nGAR+70.7 81.6 88.9 92.0 93.2 76.0 82.1 86.6 - -\\nTable 4: Top-k retrieval accuracy on the test sets . The baselines are evaluated by ourselves and better than\\nreported in Karpukhin et al. (2020). G ARhelps BM25 to achieve comparable or better performance than DPR.\\nBest and second best methods are bold and underlined , respectively.\\nthat it simply uses BM25 for retrieval. In con-\\ntrast, the classic QE method RM3, while showing\\nmarginal improvement over the vanilla BM25, does\\nnot achieve comparable performance with GARor\\nDPR. By fusing the results of GARand DPR in\\nthe same way as described in Sec. 3.3, we further\\nobtain consistently higher performance than both\\nmethods, with top-100 accuracy 88.9% and top-\\n1000 accuracy 93.2%.\\nOn the Trivia dataset, the results are even more\\nencouraging ‚Äì GARachieves consistently better\\nretrieval accuracy than DPR when k‚â•5. On\\nthe other hand, the difference between BM25 and\\nBM25 +RM3 is negligible, which suggests that\\nnaively considering top-ranked passages as relevant\\n(i.e., pseudo relevance feedback) for QE does not\\nalways work for OpenQA. Results on more cutoffs\\nofkcan be found in App. A.\\nEffectiveness of diverse query contexts . In\\nFig. 1, we show the performance of GARwhen\\ndifferent query contexts are used to augment the\\nqueries. Although the individual performance\\nwhen using each query context is somewhat similar,\\nfusing their retrieved passages consistently leads\\nto better performance, conÔ¨Årming that different\\ngeneration-augmented queries are complementary\\nto each other (recall examples in Table 2).\\nPerformance breakdown by question type . In\\nTable 5, we show the top-100 accuracy of the com-\\npared retrieval methods per question type on the\\nNQ test set. Again, GARoutperforms BM25 on all\\ntypes of questions signiÔ¨Åcantly and GAR+achieves\\nthe best performance across the board, which fur-\\nther veriÔ¨Åes the effectiveness of G AR.\\n6.3 Passage Reading with G AR\\nComparison w. the state-of-the-art . We show\\nthe comparison of end-to-end QA performance of\\nextractive and generative methods in Table 6. Ex-\\ntractive GARachieves state-of-the-art performance\\n1 5 10 20 50 100 200 300 500 1000\\nk: # of retrieved passages30405060708090Top-k Accuracy (%)\\nAnswer+Sentence+Title\\nAnswer+Sentence\\nAnswer+Title\\nAnswer\\nTitle\\nSentenceFigure 1: Top-k retrieval accuracy on the test\\nset of NQ when fusing retrieval results of different\\ngeneration-augmented queries.\\nType Percentage BM25 DPR G AR GAR+\\nWho 37.5% 82.1 88.0 87.5 90.8\\nWhen 19.0% 73.1 86.9 83.8 88.6\\nWhat 15.0% 76.5 82.6 81.5 86.0\\nWhere 10.9% 77.4 89.1 87.0 90.8\\nOther 9.1% 79.3 78.1 81.8 84.2\\nHow 5.0% 78.2 83.8 83.2 85.5\\nWhich 3.3% 89.0 90.7 94.1 94.9\\nWhy 0.3% 90.0 90.0 90.0 90.0\\nTable 5: Top-100 retrieval accuracy breakdown of\\nquestion type on NQ . Best and second best methods\\nin each category are bold and underlined , respectively.\\namong extractive methods on both NQ and Trivia\\ndatasets, despite that it is more lightweight and\\ncomputationally efÔ¨Åcient. Generative GARoutper-\\nforms most of the generative methods on Trivia but\\ndoes not perform as well on NQ, which is some-\\nwhat expected and consistent with the performance\\nat the retrieval stage, as the generative reader only\\ntakes a few passages as input and GARdoes not\\noutperform dense retrieval methods on NQ when k\\nis very small. However, combining GARwith DPR\\nachieves signiÔ¨Åcantly better performance than both', metadata={'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'page': 6}),\n",
       " Document(page_content='Method NQ TriviaExtractiveHard EM (Min et al., 2019a) 28.1 50.9 -\\nPath Retriever (Asai et al., 2019) 32.6 - -\\nORQA (Lee et al., 2019) 33.3 45.0 -\\nGraph Retriever (Min et al., 2019b) 34.5 56.0 -\\nREALM (Guu et al., 2020) 40.4 - -\\nDPR (Karpukhin et al., 2020) 41.5 57.9 -\\nBM25 (ours) 37.7 60.1 -\\nGAR 41.8 62.7 74.8\\nGAR+43.8 - -GenerativeGPT-3 (Brown et al., 2020) 29.9 - 71.2\\nT5 (Roberts et al., 2020) 36.6 60.5 -\\nSpanSeqGen (Min et al., 2020) 42.2 - -\\nRAG (Lewis et al., 2020a) 44.5 56.1 68.0\\nFID (Izacard and Grave, 2020) 51.4 67.6 80.1\\nBM25 (ours) 35.3 58.6 -\\nGAR 38.1 62.2 -\\nGAR+45.3 - -\\nTable 6: End-to-end comparison with the state-of-\\nthe-art methods in EM . For Trivia, the left column\\ndenotes the open-domain test set and the right is the\\nhidden Wikipedia test set on the public leaderboard.\\nmethods or baselines that use DPR as input such as\\nSpanSeqGen (Min et al., 2020) and RAG (Lewis\\net al., 2020a). Also, GARoutperforms BM25 sig-\\nniÔ¨Åcantly under both extractive and generative se-\\ntups, which again shows the effectiveness of the\\ngenerated query contexts, even if they are heuristi-\\ncally discovered without any external supervision.\\nThe best performing generative method FID\\n(Izacard and Grave, 2020) is not directly compara-\\nble as it takes more (100) passages as input. As an\\nindirect comparison, GARperforms better than FID\\nwhen FID encodes 10 passages (cf. Fig. 2 in Izac-\\nard and Grave (2020)). Moreover, since FID relies\\non the retrieval results of DPR as well, we believe\\nthat it is a low-hanging fruit to replace its input\\nwith GARorGAR+and further boost the perfor-\\nmance.7We also observe that, perhaps surprisingly,\\nextractive BM25 performs reasonably well, espe-\\ncially on the Trivia dataset, outperforming many\\nrecent state-of-the-art methods.8Generative BM25\\nalso performs competitively in our experiments.\\nModel Generalizability . Recent studies (Lewis\\net al., 2020b) show that there are signiÔ¨Åcant ques-\\ntion and answer overlaps between the training and\\ntest sets of popular OpenQA datasets. SpeciÔ¨Åcally,\\n60% to 70% test-time answers also appear in the\\n7This claim is later veriÔ¨Åed by the best systems in the\\nNeurIPS 2020 EfÔ¨ÅcientQA competition (Min et al., 2021).\\n8We Ô¨Ånd that taking 500 passages during reader inference\\ninstead of 100 as in Karpukhin et al. (2020) improves the\\nperformance of BM25 but not DPR.training set and roughly 30% test-set questions\\nhave a near-duplicate paraphrase in the training\\nset. Such observations suggest that many questions\\nmight have been answered by simple question or\\nanswer memorization. To further examine model\\ngeneralizability, we study the per-category perfor-\\nmance of different methods using the annotations\\nin Lewis et al. (2020b).\\nMethod TotalQuestion\\nOverlapAnswer\\nOverlap\\nOnlyNo\\nOverlap\\nDPR 41.3 69.4 34.6 19.3\\nGAR+(E) 43.8 66.7 38.1 23.9\\nBART 26.5 67.6 10.2 0.8\\nRAG 44.5 70.7 34.9 24.8\\nGAR+(G) 45.3 67.9 38.1 27.0\\nTable 7: EM scores with question-answer overlap\\ncategory breakdown on NQ. (E) and (G) denote ex-\\ntractive and generative readers, respectively. Results of\\nbaseline methods are taken from Lewis et al. (2020b).\\nThe observations on Trivia are similar and omitted.\\nAs listed in Table 7, for the No Overlap category,\\nGAR+(E) outperforms DPR on the extractive setup\\nandGAR+(G) outperforms RAG on the generative\\nsetup, which indicates that better end-to-end model\\ngeneralizability can be achieved by adding GAR\\nfor retrieval. GAR+also achieves the best EM un-\\nder the Answer Overlap Only category. In addition,\\nwe observe that a closed-book BART model that\\nonly takes the question as input performs much\\nworse than additionally taking top-retrieved pas-\\nsages, i.e.,GAR+(G), especially on the questions\\nthat require generalizability. Notably, all methods\\nperform signiÔ¨Åcantly better on the Question Over-\\nlapcategory, which suggests that the high Total\\nEM is mostly contributed by question memoriza-\\ntion. That said, GAR+appears to be less dependent\\non question memorization given its lower EM for\\nthis category.9\\n6.4 EfÔ¨Åciency of G AR\\nGARis efÔ¨Åcient and scalable since it uses sparse\\nrepresentations for retrieval and does not in-\\nvolve time-consuming training process such as\\nRL (Nogueira and Cho, 2017; Liu et al., 2019).\\nThe only overhead of GARis on the generation of\\nquery contexts and the retrieval with generation-\\n9The same ablation study is also conducted on the retrieval\\nstage and similar results are observed. More detailed discus-\\nsions can be found in App. A.', metadata={'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'page': 7}),\n",
       " Document(page_content='Training Indexing Retrieval\\nDPR 24h w. 8 GPUs 17.3h w. 8 GPUs 30 min w. 1 GPU\\nGAR 3‚àº6h w. 1 GPU 0.5h w. 35 CPUs 5 min w. 35 CPUs\\nTable 8: Comparison of computational cost between\\nDPR and G ARat different stages. The training time\\nof G ARis for one generation target but different gener-\\nators can be trained in parallel.\\naugmented (thus longer) queries, whose computa-\\ntional complexity is signiÔ¨Åcantly lower than other\\nmethods with comparable retrieval accuracy.\\nWe use Nvidia V100 GPUs and Intel Xeon Plat-\\ninum 8168 CPUs in our experiments. As listed in\\nTable 8, the training time of GARis 3 to 6 hours\\non 1 GPU depending on the generation target. As\\na comparison, REALM (Guu et al., 2020) uses\\n64 TPUs to train for 200k steps during pre-training\\nalone and DPR (Karpukhin et al., 2020) takes about\\n24 hours to train with 8 GPUs. To build the indices\\nof Wikipedia passages, GARonly takes around 30\\nmin with 35 CPUs, while DPR takes 8.8 hours\\non 8 GPUs to generate dense representations and\\nanother 8.5 hours to build the FAISS index (John-\\nson et al., 2017). For retrieval, GARtakes about\\n1 min to generate one query context with 1 GPU,\\n1 min to retrieve 1,000 passages for the NQ test\\nset with answer/title-augmented queries and 2 min\\nwith sentence-augmented queries using 35 CPUs.\\nIn contrast, DPR takes about 30 min on 1 GPU.\\n7 Conclusion\\nIn this work, we propose Generation-Augmented\\nRetrieval and demonstrate that the relevant contexts\\ngenerated by PLMs without external supervision\\ncan signiÔ¨Åcantly enrich query semantics and im-\\nprove retrieval accuracy. Remarkably, GARwith\\nsparse representations performs similarly or better\\nthan state-of-the-art methods based on the dense\\nrepresentations of the original queries. GARcan\\nalso be easily combined with dense representa-\\ntions to produce even better results. Furthermore,\\nGARachieves state-of-the-art end-to-end perfor-\\nmance on extractive OpenQA and competitive per-\\nformance under the generative setup.\\n8 Future Extensions\\nPotential improvements . There is still much\\nspace to explore and improve for GARin future\\nwork. For query context generation, one can ex-\\nplore multi-task learning to further reduce computa-\\ntional cost and examine whether different contextscan mutually enhance each other when generated\\nby the same generator. One may also sample multi-\\nple contexts instead of greedy decoding to enrich a\\nquery. For retrieval, one can adopt more advanced\\nfusion techniques based on both the ranking and\\nscore of the passages. As the generator and re-\\ntriever are largely independent now, it is also inter-\\nesting to study how to jointly or iteratively optimize\\ngeneration and retrieval such that the generator is\\naware of the retriever and generates query contexts\\nmore beneÔ¨Åcial for the retrieval stage. Last but not\\nleast, it is very likely that better results can be ob-\\ntained by more extensive hyper-parameter tuning.\\nApplicability to other tasks . Beyond OpenQA,\\nGARalso has great potentials for other tasks that\\ninvolve text matching such as conversation utter-\\nance selection (Lowe et al., 2015; Dinan et al.,\\n2020) or information retrieval (Nguyen et al., 2016;\\nCraswell et al., 2020). The default generation tar-\\nget is always available for supervised tasks. For\\nexample, for conversation utterance selection one\\ncan use the reference utterance as the default target\\nand then match the concatenation of the conversa-\\ntion history and the generated utterance with the\\nprovided utterance candidates. For article search,\\nthe default target could be (part of) the ground-truth\\narticle itself. Other generation targets are more task-\\nspeciÔ¨Åc and can be designed as long as they can\\nbe fetched from the latent knowledge inside PLMs\\nand are helpful for further text retrieval (matching).\\nNote that by augmenting (expanding) the queries\\nwith heuristically discovered relevant contexts ex-\\ntracted from PLMs instead of reformulating them,\\nGARbypasses the need for external supervision to\\nform the original-reformulated query pairs.\\nAcknowledgments\\nWe thank Vladimir Karpukhin, Sewon Min, Gau-\\ntier Izacard, Wenda Qiu, Revanth Reddy, and Hao\\nCheng for helpful discussions. We thank the anony-\\nmous reviewers for valuable comments.\\nReferences\\nNasreen Abdul-Jaleel, James Allan, W Bruce Croft,\\nFernando Diaz, Leah Larkey, Xiaoyan Li, Mark D\\nSmucker, and Courtney Wade. 2004. Umass at trec\\n2004: Novelty and hard. Computer Science Depart-\\nment Faculty Publication Series , page 189.\\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\\nRichard Socher, and Caiming Xiong. 2019. Learn-\\ning to retrieve reasoning paths over wikipedia', metadata={'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'page': 8}),\n",
       " Document(page_content='graph for question answering. arXiv preprint\\narXiv:1911.10470 .\\nIz Beltagy, Matthew E Peters, and Arman Cohan.\\n2020. Longformer: The long-document transformer.\\narXiv preprint arXiv:2004.05150 .\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. arXiv preprint arXiv:2005.14165 .\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 1870‚Äì\\n1879, Vancouver, Canada. Association for Computa-\\ntional Linguistics.\\nGordon V Cormack, Charles LA Clarke, and Stefan\\nBuettcher. 2009. Reciprocal rank fusion outper-\\nforms condorcet and individual rank learning meth-\\nods. In Proceedings of the 32nd international ACM\\nSIGIR conference on Research and development in\\ninformation retrieval , pages 758‚Äì759.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\\nCampos, and Ellen M V oorhees. 2020. Overview\\nof the trec 2019 deep learning track. arXiv preprint\\narXiv:2003.07820 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) ,\\npages 4171‚Äì4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\\nAlexander Miller, Kurt Shuster, Jack Urbanek,\\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\\nLowe, et al. 2020. The second conversational in-\\ntelligence challenge (convai2). In The NeurIPS‚Äô18\\nCompetition , pages 187‚Äì208. Springer.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training. arXiv\\npreprint arXiv:2002.08909 .\\nGautier Izacard and Edouard Grave. 2020. Lever-\\naging passage retrieval with generative models for\\nopen domain question answering. arXiv preprint\\narXiv:2007.01282 .\\nJeff Johnson, Matthijs Douze, and Herv ¬¥e J¬¥egou. 2017.\\nBillion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 .Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale dis-\\ntantly supervised challenge dataset for reading com-\\nprehension. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 1601‚Äì1611, Van-\\ncouver, Canada. Association for Computational Lin-\\nguistics.\\nVladimir Karpukhin, Barlas O Àòguz, Sewon Min, Ledell\\nWu, Sergey Edunov, Danqi Chen, and Wen-\\ntau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. arXiv preprint\\narXiv:2004.04906 .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nÔ¨Åeld, Michael Collins, Ankur Parikh, Chris Al-\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\nNatural questions: A benchmark for question an-\\nswering research. Transactions of the Association\\nfor Computational Linguistics , 7:452‚Äì466.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 6086‚Äì6096, Florence,\\nItaly. Association for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\\nBart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and\\ncomprehension. arXiv preprint arXiv:1910.13461 .\\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K ¬®uttler, Mike Lewis, Wen-tau Yih, Tim\\nRockt ¬®aschel, et al. 2020a. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. arXiv\\npreprint arXiv:2005.11401 .\\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\\n2020b. Question and answer test-train overlap in\\nopen-domain question answering datasets. arXiv\\npreprint arXiv:2008.02637 .\\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\\nNogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\\nJimmy Lin. 2020. Query reformulation using query\\nhistory for passage retrieval in conversational search.\\narXiv preprint arXiv:2005.02230 .\\nYe Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\\nPhilip S Yu. 2019. Generative question reÔ¨Ånement\\nwith deep reinforcement learning in retrieval-based\\nqa system. In Proceedings of the 28th ACM Inter-\\nnational Conference on Information and Knowledge\\nManagement , pages 1643‚Äì1652.', metadata={'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'page': 9}),\n",
       " Document(page_content='Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle\\nPineau. 2015. The ubuntu dialogue corpus: A large\\ndataset for research in unstructured multi-turn dia-\\nlogue systems. arXiv preprint arXiv:1506.08909 .\\nYi Luan, Jacob Eisenstein, Kristina Toutanova, and\\nMichael Collins. 2020. Sparse, dense, and at-\\ntentional representations for text retrieval. arXiv\\npreprint arXiv:2005.00181 .\\nYuanhua Lv and ChengXiang Zhai. 2010. Positional\\nrelevance model for pseudo-relevance feedback. In\\nProceedings of the 33rd international ACM SIGIR\\nconference on Research and development in infor-\\nmation retrieval , pages 579‚Äì586.\\nYuning Mao, Xiang Ren, Heng Ji, and Jiawei Han.\\n2020. Constrained abstractive summarization: Pre-\\nserving factual consistency with constrained genera-\\ntion. arXiv preprint arXiv:2010.12723 .\\nSewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi\\nChen, Eunsol Choi, Michael Collins, Kelvin Guu,\\nHannaneh Hajishirzi, Kenton Lee, Jennimaria Palo-\\nmaki, et al. 2021. Neurips 2020 efÔ¨Åcientqa compe-\\ntition: Systems, analyses and lessons learned. arXiv\\npreprint arXiv:2101.00133 .\\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2019a. A discrete hard EM ap-\\nproach for weakly supervised question answering.\\nInProceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the\\n9th International Joint Conference on Natural Lan-\\nguage Processing (EMNLP-IJCNLP) , pages 2851‚Äì\\n2864, Hong Kong, China. Association for Computa-\\ntional Linguistics.\\nSewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\\nnaneh Hajishirzi. 2019b. Knowledge guided text re-\\ntrieval and reading for open domain question answer-\\ning. arXiv preprint arXiv:1911.03868 .\\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2020. Ambigqa: Answering\\nambiguous open-domain questions. arXiv preprint\\narXiv:2004.10645 .\\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\\n2016. Ms marco: A human-generated machine read-\\ning comprehension dataset.\\nRodrigo Nogueira and Kyunghyun Cho. 2017. Task-\\noriented query reformulation with reinforcement\\nlearning. In Proceedings of the 2017 Conference on\\nEmpirical Methods in Natural Language Processing ,\\npages 574‚Äì583, Copenhagen, Denmark. Association\\nfor Computational Linguistics.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the pa-\\nrameters of a language model? arXiv preprint\\narXiv:2002.08910 .Joseph Rocchio. 1971. Relevance feedback in in-\\nformation retrieval. The Smart retrieval system-\\nexperiments in automatic document processing ,\\npages 313‚Äì323.\\nSvitlana Vakulenko, Shayne Longpre, Zhucheng Tu,\\nand Raviteja Anantha. 2020. Question rewriting for\\nconversational question answering. arXiv preprint\\narXiv:2004.14652 .\\nXiao Wang, Craig Macdonald, and Iadh Ounis. 2020.\\nDeep reinforced query reformulation for informa-\\ntion retrieval. arXiv preprint arXiv:2007.07987 .\\nPeilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:\\nEnabling the use of lucene for information retrieval\\nresearch. In Proceedings of the 40th International\\nACM SIGIR Conference on Research and Develop-\\nment in Information Retrieval , pages 1253‚Äì1256.\\nShi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong,\\nPaul Bennett, Jianfeng Gao, and Zhiyuan Liu. 2020.\\nFew-shot generative conversational query rewriting.\\narXiv preprint arXiv:2006.05009 .\\nSalah Zaiem and Fatiha Sadat. 2019. Sequence to se-\\nquence learning for query expansion. In Proceed-\\nings of the AAAI Conference on ArtiÔ¨Åcial Intelli-\\ngence, Student Abstract Track , volume 33, pages\\n10075‚Äì10076.', metadata={'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'page': 10}),\n",
       " Document(page_content='A More Analysis of Retrieval\\nPerformance\\nWe show the detailed results of top-k retrieval accu-\\nracy of the compared methods in Figs. 2 and 3.\\nGARperforms comparably or better than DPR\\nwhen k‚â•100on NQ and k‚â•5on Trivia.\\n1 5 10 20 50 100 200 300 500 1000\\nk: # of retrieved passages2030405060708090Top-k Accuracy (%)\\nGAR +DPR\\nDPR\\nGAR\\nBM25 +RM3\\nBM25\\nFigure 2: Top-k retrieval accuracy of sparse and\\ndense methods on the test set of NQ. GARimproves\\nBM25 and achieves comparable or better performance\\nthan DPR when k‚â•100.\\n1 5 10 20 50 100\\nk: # of retrieved passages5055606570758085Top-k Accuracy (%)\\nGAR +DPR\\nDPR\\nGAR\\nBM25 +RM3\\nBM25\\nFigure 3: Top-k retrieval accuracy on the Trivia test\\nset.GARachieves better results than DPR when k‚â•5.\\nWe show in Table 9 the retrieval accuracy break-\\ndown using the question-answer overlap categories.\\nThe most signiÔ¨Åcant gap between BM25 and other\\nmethods is on the Question Overlap category,\\nwhich coincides with the fact that BM25 is un-\\nable to conduct question paraphrasing (semantic\\nmatching). GARhelps BM25 to bridge the gap by\\nproviding the query contexts and even outperform\\nDPR in this category. Moreover, GARconsistently\\nimproves over BM25 on other categories and GAR+\\noutperforms DPR as well.Method TotalQuestion\\nOverlapAnswer\\nOverlap\\nOnlyNo\\nOverlap\\nBM25 78.8 81.2 85.1 70.6\\nDPR 86.1 93.2 89.5 76.8\\nGAR 85.3 94.1 87.9 73.7\\nGAR+88.9 96.3 91.7 79.8\\nTable 9: Top-100 retrieval accuracy by question-\\nanswer overlap categories on the NQ test set.', metadata={'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf', 'page': 11}),\n",
       " Document(page_content='In-Context Retrieval-Augmented Language Models\\nOri Ram‚àóYoav Levine‚àóItay Dalmedigos Dor Muhlgay\\nAmnon Shashua Kevin Leyton-Brown Yoav Shoham\\nAI21 Labs\\n{orir,yoavl,itayd,dorm,amnons,kevinlb,yoavs}@ai21.com\\nAbstract\\nRetrieval-Augmented Language Modeling\\n(RALM) methods, which condition a lan-\\nguage model (LM) on relevant documents\\nfrom a grounding corpus during generation,\\nwere shown to significantly improve lan-\\nguage modeling performance. In addition,\\nthey can mitigate the problem of factually\\ninaccurate text generation and provide natu-\\nral source attribution mechanism. Existing\\nRALM approaches focus on modifying the\\nLM architecture in order to facilitate the in-\\ncorporation of external information, signifi-\\ncantly complicating deployment. This paper\\nconsiders a simple alternative, which we dub\\nIn-Context RALM : leaving the LM architec-\\nture unchanged and prepending grounding\\ndocuments to the input, without any further\\ntraining of the LM . We show that In-Context\\nRALM that builds on off-the-shelf general\\npurpose retrievers provides surprisingly large\\nLM gains across model sizes and diverse cor-\\npora. We also demonstrate that the document\\nretrieval and ranking mechanism can be spe-\\ncialized to the RALM setting to further boost\\nperformance. We conclude that In-Context\\nRALM has considerable potential to increase\\nthe prevalence of LM grounding, particularly\\nin settings where a pretrained LM must be\\nused without modification or even via API\\naccess.1\\n1 Introduction\\nRecent advances in language modeling (LM) have\\ndramatically increased the usefulness of machine-\\ngenerated text across a wide range of use-cases\\nand domains (Brown et al., 2020). However, the\\nmainstream paradigm of generating text with LMs\\nbears inherent limitations in access to external\\nknowledge. First, LMs are not coupled with any\\n‚àóEqual contribution.\\n1Our code is available at https://github.com/\\nAI21Labs/in-context-ralm\\nPerplexity\\n10.015.020.025.030.0\\nGPT-2 345M (M) GPT-2 1.5B (XL)No Retrieval In-Context RALM (BM25)\\nIn-Context RALM (Predictive Reranking)Figure 1: Our framework, dubbed In-Context\\nRALM , provides large language modeling gains on\\nthe test set of WikiText-103, without modifying the\\nLM. Adapting the use of a BM25 retriever (Robert-\\nson and Zaragoza, 2009) to the LM task (¬ß5) yields\\nsignificant gains, and choosing the grounding doc-\\numents via our new class of Predictive Rerankers\\n(¬ß6) provides a further boost. See Table 1 for the\\nfull results on five diverse corpora.\\nsource attribution, and must be trained in order\\nto incorporate up-to-date information that was not\\nseen during training. More importantly, they tend\\nto produce factual inaccuracies and errors (Lin\\net al., 2022; Maynez et al., 2020; Huang et al.,\\n2020). This problem is present in any LM gen-\\neration scenario, and is exacerbated when gener-\\nation is made in uncommon domains or private\\ndata. A promising approach for addressing the\\nabove is Retrieval-Augmented Language Modeling\\n(RALM), grounding the LM during generation by\\nconditioning on relevant documents retrieved from\\nan external knowledge source. RALM systems in-\\nclude two high level components: (i) document se-\\nlection , selecting the set of documents upon which\\nto condition; and (ii) document reading , determin-\\ning how to incorporate the selected documents into\\nthe LM generation process.\\nLeading RALM systems introduced recentlyarXiv:2302.00083v3  [cs.CL]  1 Aug 2023', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 0}),\n",
       " Document(page_content='Language \\nModel World Cup 2022 was the \\nlast with 32 teams, \\nbefore the increase to Retriever FIFA World Cup 2026 will \\nexpand to 48 teams. \\nWorld Cup 2022 was the \\nlast with 32 teams, before \\nthe increase to 48 in the 2026 \\ntournament. \\nFigure 2: An example of In-Context RALM : we simply prepend the retrieved document before the input prefix.\\ntend to be focused on altering the language model\\narchitecture (Khandelwal et al., 2020; Borgeaud\\net al., 2022; Zhong et al., 2022; Levine et al., 2022c;\\nLi et al., 2022). Notably, Borgeaud et al. (2022) in-\\ntroduced RETRO, featuring document reading via\\nnontrivial modifications that require further train-\\ning to the LM architecture, while using an off-the-\\nshelf frozen BERT retriever for document selec-\\ntion. Although the paper‚Äôs experimental findings\\nshowed impressive performance gains, the need for\\nchanges in architecture and dedicated retraining\\nhas hindered the wide adoption of such models.\\nIn this paper, we show that a very simple doc-\\nument reading mechanism can have a large im-\\npact, and that substantial gains can also be made\\nby adapting the document selection mechanism to\\nthe task of language modeling. Thus, we show that\\nmany of the benefits of RALM can be achieved\\nwhile working with off-the-shelf LMs, even via\\nAPI access. Specifically, we consider a simple but\\npowerful RALM framework, dubbed In-Context\\nRALM (presented in Section 3), which employs a\\nzero-effort document reading mechanism: we sim-\\nply prepend the selected documents to the LM‚Äôs\\ninput text (Figure 2).\\nSection 4 describes our experimental setup. To\\nshow the wide applicability of our framework, we\\nperformed LM experiments on a suite of five di-\\nverse corpora: WikiText-103 (Merity et al., 2016),\\nRealNews (Zellers et al., 2019), and three datasets\\nfrom The Pile (Gao et al., 2021): ArXiv, Stack\\nExchange and FreeLaw. We use open-source LMs\\nranging from 110M to 66B parameters (from the\\nGPT-2, GPT-Neo, OPT and LLaMA model fami-\\nlies).\\nIn Section 5 we evaluate the application of off-\\nthe-shelf retrievers to our framework. In this\\nminimal-effort setting, we found that In-Context\\nRALM led to LM performance gains equivalent to\\nincreasing the LM‚Äôs number of parameters by 2‚Äì\\n3√óacross all of the text corpora we examined. In\\nSection 6 we investigate methods for adapting doc-ument ranking to the LM task, a relatively under-\\nexplored RALM degree of freedom. Our adapta-\\ntion methods range from using a small LM to per-\\nform zero-shot ranking of the retrieved documents,\\nup to training a dedicated bidirectional reranker\\nby employing self-supervision from the LM signal .\\nThese methods lead to further gains in the LM task\\ncorresponding to an additional size increase of 2√ó\\nin the LM architecture. As a concrete example of\\nthe gains, a 345M parameter GPT-2 enhanced by\\nIn-Context RALM outperforms a 762M parame-\\nter GPT-2 when employing an off-the-shelf BM25\\nretriever (Robertson and Zaragoza, 2009), and out-\\nperforms a 1.5B parameter GPT-2 when employing\\nour trained LM-oriented reranker (see Figure 1).\\nFor large model sizes, our method is even more\\neffective: In-Context RALM with an off-the-shelf\\nretriever improved the performance of a 6.7B pa-\\nrameter OPT model to match that of a 66B param-\\neter parameter OPT model (see Figure 4).\\nIn Section 7 we demonstrate the applicability\\nof In-Context RALM to downstream open-domain\\nquestions answering (ODQA) tasks.\\nIn a concurrent work, Shi et al. (2023) also sug-\\ngest to augment off-the-shelf LMs with retrieved\\ntexts by prepending them to the input. Their re-\\nsults are based on training a dedicated retriever for\\nlanguage modeling. In contrast, we focus on the\\ngains achievable in using off-the-shelf retrievers\\nfor this task. We show strong gains of this simpler\\nsetting by investigating: (1) which off-the-shelf\\nretriever is best suited for language modeling, (2)\\nthe frequency of retrieval operations, and (3) the\\noptimal query length. In addition, we boost the off-\\nthe-shelf retrieval performance by introducing two\\nreranking methods that demonstrate further gains\\nin perplexity.\\nWe believe that In-Context RALM can play two\\nimportant roles in making RALM systems more\\npowerful and more prevalent. First, given its simple\\nreading mechanism, In-Context RALM can serve\\nas a clean probe for developing document retrieval', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 1}),\n",
       " Document(page_content='methods that are specialized for the LM task. These\\nin turn can be used to improve both In-Context\\nRALM and other more elaborate RALM methods\\nthat currently leverage general purpose retrievers.\\nSecond, due to its compatibility with off-the-shelf\\nLMs, In-Context RALM can help drive wider de-\\nployment of RALM systems.\\n2 Related Work\\nRALM approaches can be roughly divided into two\\nfamilies of models: (i) nearest-neighbor language\\nmodels (also called kNN-LM), and (ii) retrieve\\nand read models . Our work belongs to the second\\nfamily, but is distinct in that it involves no further\\ntraining of the LM.\\nNearest Neighbor Language Models ThekNN-\\nLM approach was first introduced in Khandel-\\nwal et al. (2020). The authors suggest a simple\\ninference-time model that interpolates between two\\nnext-token distributions: one induced by the LM\\nitself, and one induced by the kneighbors from the\\nretrieval corpus that are closest to the query token in\\nthe LM embedding space. Zhong et al. (2022) sug-\\ngest a framework for training these models. While\\nthey showed significant gains from kNN-LM, the\\napproach requires storing the representations for\\neach token in the corpus , an expensive requirement\\neven for a small corpus like Wikipedia. Although\\nnumerous approaches have been suggested for al-\\nleviating this issue (He et al., 2021; Alon et al.,\\n2022), scaling any of them to large corpora remains\\nan open challenge.\\nRetrieve and Read Models This family of\\nRALMs creates a clear division between document\\nselection anddocument reading components. All\\nprior work involves training the LM. We begin by\\ndescribing works that use this approach for tack-\\nling downstream tasks, and then mention works ori-\\nented towards RALM. Lewis et al. (2020) and Izac-\\nard and Grave (2021) fine tuned encoder‚Äìdecoder\\narchitectures for downstream knowledge-intensive\\ntasks. Izacard et al. (2022b) explored different\\nways of pretraining such models, while Levine\\net al. (2022c) pretrained an autoregressive LM on\\nclusters of nearest neighbors in sentence embed-\\nding space. Levine et al. (2022a) showed competi-\\ntive open domain question-answering performance\\nby prompt-tuning a frozen LM as a reader. Guu\\net al. (2020) pretrained REALM, a retrieval aug-\\nmented bidirectional, masked LM, later fine-tunedfor open-domain question answering. The work\\nclosest to this paper‚Äîwith a focus on the language\\nmodeling task‚Äîis RETRO (Borgeaud et al., 2022),\\nwhich modifies an autoregressive LM to attend to\\nrelevant documents via chunked cross-attention,\\nthus introducing new parameters to the model. Our\\nIn-Context RALM differs from prior work in this\\nfamily of models in two key aspects:\\n‚Ä¢We use off-the-shelf LMs for document read-\\ningwithout any further training of the LM .\\n‚Ä¢We focus on how to choose documents for\\nimproved LM performance .\\n3 Our Framework\\n3.1 In-Context RALM\\nLanguage models define probability distributions\\nover sequences of tokens. Given such a sequence\\nx1, ..., x n, the standard way to model its probabil-\\nity is via next-token prediction: p(x1, ..., x n) =Qn\\ni=1p(xi|x<i), where x<i:=x1, ..., x i‚àí1is the\\nsequence of tokens preceding xi, also referred to\\nas its prefix . This autoregressive model is usu-\\nally implemented via a learned transformer net-\\nwork (Vaswani et al., 2017) parameterized by the\\nset of parameters Œ∏:\\np(x1, ..., x n) =nY\\ni=1pŒ∏(xi|x<i), (1)\\nwhere the conditional probabilities are modeled\\nby employing a causal self-attention mask (Rad-\\nford et al., 2018). Notably, leading LMs such\\nas GPT-2 (Radford et al., 2019), GPT-3 (Brown\\net al., 2020), OPT (Zhang et al., 2022) or Jurassic-\\n1 (Lieber et al., 2021) follow this simple parame-\\nterization.\\nRetrieval augmented language models (RALMs)\\nadd an operation that retrieves one or more docu-\\nments from an external corpus C, and condition the\\nabove LM predictions on these documents. Specifi-\\ncally, for predicting xi, the retrieval operation from\\nCdepends on its prefix: RC(x<i), so the most\\ngeneral RALM decomposition is: p(x1, ..., x n) =Qn\\ni=1p(xi|x<i,RC(x<i)). In order to condition\\nthe LM generation on the retrieved document, pre-\\nvious RALM approaches used specialized architec-\\ntures or algorithms (see ¬ß2). Inspired by the suc-\\ncess of In-Context Learning (Brown et al., 2020;\\nDong et al., 2023), In-Context RALM refers to the\\nfollowing specific, simple method of concatenating', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 2}),\n",
       " Document(page_content='the retrieved documents2within the Transformer‚Äôs\\ninput prior to the prefix (see Figure 2), which does\\nnot involve altering the LM weights Œ∏:\\np(x1, ..., x n) =\\nnY\\ni=1pŒ∏(xi|[RC(x<i);x<i]),(2)\\nwhere [a;b]denotes the concatenation of strings a\\nandb.\\nSince common Transformer-based LM imple-\\nmentations support limited length input sequences,\\nwhen the concatenation of the document and the\\ninput sequence exceed this limit we remove to-\\nkens from the beginning of xuntil the overall input\\nlength equals that allowed by the model. Because\\nour retrieved documents are passages of limited\\nlength, we always have enough context left from x\\n(see ¬ß4.3).\\n3.2 RALM Design Choices\\nWe detail below two practical design choices often\\nmade in RALM systems. In ¬ß5, we investigate the\\neffect of these in the setting of In-Context RALM.\\nRetrieval Stride While in the above formulation\\na retrieval operation can occur at each generation\\nstep, we might want to perform retrieval only once\\nevery s >1tokens due to the cost of calling the\\nretriever, and the need to replace the documents in\\nthe LM prefix during generation. We refer to sas\\ntheretrieval stride . This gives rise to the follow-\\ning In-Context RALM formulation (which reduces\\nback to Eq. (2) for s= 1):\\np(x1, ..., x n) =\\nns‚àí1Y\\nj=0sY\\ni=1pŒ∏\\x00\\nxs¬∑j+i|\\x02\\nRC(x‚â§s¬∑j);x<(s¬∑j+i)\\x03\\x01\\n,\\n(3)\\nwhere ns=n/sis the number of retrieval strides.\\nNotably, in this framework the runtime costs of\\neach retrieval operation is composed of (a) apply-\\ning the retriever itself, and (b) recomputing the\\nembeddings of the prefix. In ¬ß5.2 we show that us-\\ning smaller retrieval strides, i.e., retrieving as often\\nas possible, is superior to using larger ones (though\\nIn-Context RALM with larger strides already pro-\\nvides large gains over vanilla LM). Thus, choosing\\nthe retrieval stride is ultimately a tradeoff between\\nruntime and performance.\\n2We always use a single document , but it is conceptually\\nsimple to support multiple documents as well.Retrieval Query Length While the retrieval\\nquery above in principle depends on all prefix to-\\nkensx‚â§s¬∑j, the information at the very end of the\\nprefix is typically the most relevant to the generated\\ntokens. If the retrieval query is too long then this in-\\nformation can be diluted. To avoid this, we restrict\\nthe retrieval query at stride jto the last ‚Ñìtokens\\nof the prefix, i.e., we use qs,‚Ñì\\nj:=xs¬∑j‚àí‚Ñì+1, ..., x s¬∑j.\\nWe refer to ‚Ñìas the retrieval query length . Note that\\nprior RALM work couples the retrieval stride sand\\nthe retrieval query length ‚Ñì(Borgeaud et al., 2022).\\nIn ¬ß5, we show that enforcing s=‚Ñìdegrades LM\\nperformance. Integrating these hyper-parameters\\ninto the In-Context RALM formulation gives\\np(x1, ..., x n) =\\nns‚àí1Y\\nj=0sY\\ni=1pŒ∏\\x10\\nxs¬∑j+i|h\\nRC(qs,‚Ñì\\nj);x<(s¬∑j+i)i\\x11\\n.\\n(4)\\n4 Experimental Details\\nWe now describe our experimental setup, including\\nall models we use and their implementation details.\\n4.1 Datasets\\nWe evaluated the effectiveness of In-Context\\nRALM across five diverse language modeling\\ndatasets and two common open-domain question\\nanswering datasets.\\nLanguage Modeling The first LM dataset is\\nWikiText-103 (Merity et al., 2016), which has been\\nextensively used to evaluate RALMs (Khandelwal\\net al., 2020; He et al., 2021; Borgeaud et al., 2022;\\nAlon et al., 2022; Zhong et al., 2022). Second, we\\nchose three datasets spanning diverse subjects from\\nThe Pile (Gao et al., 2021): ArXiv ,Stack Exchange\\nandFreeLaw . Finally, we also investigated Real-\\nNews (Zellers et al., 2019), since The Pile lacks a\\ncorpus focused only on news (which is by nature a\\nknowledge-intensive domain).\\nOpen-Domain Question Answering In order\\nto evaluate In-Context RALM on downstream\\ntasks as well, we use the Natural Questions (NQ;\\nKwiatkowski et al. 2019) and TriviaQA (Joshi et al.,\\n2017) open-domain question answering datasets.\\n4.2 Models\\nLanguage Models We performed our experi-\\nments using the four models of GPT-2 (110M‚Äì\\n1.5B; Radford et al. 2019), three models of GPT-\\nNeo and GPT-J (1.3B‚Äì6B; Black et al. 2021; Wang', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 3}),\n",
       " Document(page_content='and Komatsuzaki 2021), eight models of OPT\\n(125M‚Äì66B; Zhang et al. 2022) and three mod-\\nels of LLaMA (7B‚Äì33B; Touvron et al. 2023). All\\nmodels are open source and publicly available.3\\nWe elected to study these particular models for\\nthe following reasons. The first four (GPT-2) mod-\\nels were trained on WebText (Radford et al., 2019),\\nwith Wikipedia documents excluded from their\\ntraining datasets. We were thus able to evaluate our\\nmethod‚Äôs ‚Äúzero-shot‚Äù performance when retrieving\\nfrom a novel corpus (for WikiText-103). The rest of\\nthe models brought two further benefits. First, they\\nallowed us to investigate how our methods scale\\nto models larger than GPT-2. Second, the fact that\\nWikipedia was part of their training data allowed us\\nto investigate the usefulness of In-Context RALM\\nfor corpora seen during training. The helpfulness\\nof such retrieval has been demonstrated for previ-\\nous RALM methods (Khandelwal et al., 2020) and\\nhas also been justified theoretically by Levine et al.\\n(2022c).\\nWe ran all models with a maximum sequence\\nlength of 1,024, even though GPT-Neo, OPT and\\nLLaMA models support a sequence length of\\n2,048.4\\nRetrievers We experimented with both sparse\\n(word-based) and dense (neural) retrievers. We\\nused BM25 (Robertson and Zaragoza, 2009) as our\\nsparse model. For dense models, we experimented\\nwith (i) a frozen BERT-base (Devlin et al., 2019)\\nfollowed by mean pooling, similar to Borgeaud\\net al. (2022); and (ii) the Contriever (Izacard et al.,\\n2022a) and Spider (Ram et al., 2022) models,\\nwhich are dense retrievers that were trained in un-\\nsupervised manners.\\nReranking When training rerankers (Sec-\\ntion 6.2), we initialized from RoBERTa-base (Liu\\net al., 2019).\\n4.3 Implementation Details\\nWe implemented our code base using the Trans-\\nformers library (Wolf et al., 2020). We based\\nour dense retrieval code on the DPR repository\\n(Karpukhin et al., 2020).\\n3All models are available for use use via https://\\nhuggingface.co/\\n4In preliminary experiments, we observed similar improve-\\nments from In-Context RALM when using a sequence length\\nof 2,048. We used a sequence length of 1,024 in order to\\nfacilitate a direct comparison between all models.\\nPerplexity\\n10203040\\nGPT-2 117M (S) GPT-2 1.5B (XL)No Retrieval BERT Contriever Spider BM25Figure 3: The performance of four off-the-shelf\\nretrievers used for In-Context RALM on the de-\\nvelopment set of WikiText-103. All RALMs are\\nrun with s= 4(i.e., retrieval is applied every four\\ntokens). For each RALM, we report the result of\\nthe best query length ‚Ñì(see Figures 6, 9, 10).\\nRetrieval Corpora For WikiText-103 and\\nODQA datasets, we used the Wikipedia corpus\\nfrom Dec. 20, 2018, standardized by Karpukhin\\net al. (2020) using the preprocessing from Chen\\net al. (2017). To avoid contamination, we found\\nand removed all 120 articles of the development\\nand test set of WikiText-103 from the corpus. For\\nthe remaining datasets, we used their training\\ndata as the retrieval corpus. Similar to Karpukhin\\net al. (2020), our retrieval corpora consist of\\nnon-overlapping passages of 100 words (which\\ntranslate to less than 150 tokens for the vast\\nmajority of passages). Thus, we truncate our\\nretrieved passages at 256 tokens when input to the\\nmodels, but they are usually much smaller.\\nRetrieval For sparse retrieval, we used the Py-\\nserini library (Lin et al., 2021). For dense retrieval,\\nwe applied exact search using FAISS (Johnson\\net al., 2021).\\n5 The Effectiveness of In-Context RALM\\nwith Off-the-Shelf Retrievers\\nWe now empirically show that despite its simple\\ndocument reading mechanism, In-Context RALM\\nleads to substantial LM gains across our diverse\\nevaluation suite. We begin in this section by inves-\\ntigating the effectiveness of off-the-shelf retrievers\\nfor In-Context RALM; we go on in ¬ß6 to show\\nthat further LM gains can be made by tailoring\\ndocument ranking functions to the LM task.\\nThe experiments in this section provided us\\nwith a recommended configuration for applying In-', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 4}),\n",
       " Document(page_content='Model Retrieval RerankingWikiText-103 RealNews ArXiv Stack Exch. FreeLaw\\nword ppl token ppl token ppl token ppl token ppl\\nGPT-2 S‚Äì ‚Äì 37.5 21.3 12.0 12.8 13.0\\nBM25 ¬ß5 ‚Äì 29.6 16.1 10.9 11.3 9.6\\nBM25 Zero-shot ¬ß6.1 28.6 15.5 10.1 10.6 8.8\\nBM25 Predictive ¬ß6.2 26.8 ‚Äì ‚Äì ‚Äì ‚Äì\\nGPT-2 M‚Äì ‚Äì 26.3 15.7 9.3 8.8 9.6\\nBM25 ¬ß5 ‚Äì 21.5 12.4 8.6 8.1 7.4\\nBM25 Zero-shot ¬ß6.1 20.8 12.0 8.0 7.7 6.9\\nBM25 Predictive ¬ß6.2 19.7 ‚Äì ‚Äì ‚Äì ‚Äì\\nGPT-2 L‚Äì ‚Äì 22.0 13.6 8.4 8.5 8.7\\nBM25 ¬ß5 ‚Äì 18.1 10.9 7.8 7.8 6.8\\nBM25 Zero-shot ¬ß6.1 17.6 10.6 7.3 7.4 6.4\\nBM25 Predictive ¬ß6.2 16.6 ‚Äì ‚Äì ‚Äì ‚Äì\\nGPT-2 XL‚Äì ‚Äì 20.0 12.4 7.8 8.0 8.0\\nBM25 ¬ß5 ‚Äì 16.6 10.1 7.2 7.4 6.4\\nBM25 Zero-shot ¬ß6.1 16.1 9.8 6.8 7.1 6.0\\nBM25 Predictive ¬ß6.2 15.4 ‚Äì ‚Äì ‚Äì ‚Äì\\nTable 1: Perplexity on the test set of WikiText-103, RealNews and three datasets from the Pile. For\\neach LM, we report: (a) its performance without retrieval, (b) its performance when fed the top-scored\\npassage by BM25 (¬ß5), and (c) its performance when applied on the top-scored passage of each of our two\\nsuggested rerankers (¬ß6). All models share the same vocabulary, thus token-level perplexity ( token ppl )\\nnumbers are comparable. For WikiText we follow prior work and report word-level perplexity ( word ppl ).\\nModel RetrievalWikiText-103\\nword ppl\\nLLaMA-7B- 9.9\\nBM25, ¬ß5 8.8\\nLLaMA-13B- 8.5\\nBM25, ¬ß5 7.6\\nLLaMA-33B- 6.3\\nBM25, ¬ß5 6.1\\nTable 2: The performance of models from the\\nLLaMA family, measured by word-level perplexity\\non the test set of WikiText-103.\\nContext RALM: applying a sparse BM25 retriever\\nthat receives ‚Ñì= 32 query tokens and is applied\\nas frequently as possible. Practically, we retrieve\\nevery s= 4 tokens ( ‚Ñìandsare defined in ¬ß3).\\nTable 1 shows for the GPT-2 models that across\\nall the examined corpora, employing In-Context\\nRALM with an off-the-shelf retriever improved\\nLM perplexity to a sufficient extent that it matched\\nthat of a 2‚Äì3√ólarger model. Figure 4 and Tables 2\\nand 5 show that this trend holds across model sizes\\nup to 66B parameters, for both WikiText-103 andRealNews.\\n5.1 BM25 Outperforms Off-the-Shelf Neural\\nRetrievers in Language Modeling\\nWe experimented with different off-the-shelf gen-\\neral purpose retrievers, and found that the sparse\\n(lexical) BM25 retriever (Robertson and Zaragoza,\\n2009) outperformed three popular dense (neu-\\nral) retrievers: the self-supervised retrievers Con-\\ntriever (Izacard et al., 2022a) and Spider (Ram et al.,\\n2022), as well as a retriever based on the average\\npooling of BERT embeddings that was used in\\nthe RETRO system (Borgeaud et al., 2022). We\\nconducted a minimal hyper-parameter search on\\nthe query length ‚Ñìfor each of the retrievers, and\\nfound that ‚Ñì= 32 was optimal for BM25 (Fig-\\nure 6), and ‚Ñì= 64 worked best for dense retrievers\\n(Figures 9, 10).\\nFigure 3 compares the performance gains of In-\\nContext RALM with these four general-purpose re-\\ntrievers. The BM25 retriever clearly outperformed\\nall dense retrievers. This outcome is consistent\\nwith prior work showing that BM25 outperforms\\nneural retrievers across a wide array of tasks, when\\napplied in zero-shot settings (Thakur et al., 2021).\\nThis result renders In-Context RALM even more', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 5}),\n",
       " Document(page_content='WikiText-103Perplexity\\n10.020.030.040.0\\nOPT-125M OPT-350M OPT-1.3B OPT-2.7B OPT-6.7B OPT-13B OPT-30B OPT-66BNo Retrieval In-Context RALM (BM25)\\nRealNewsPerplexity\\n3.08.013.018.0\\nOPT-125M OPT-350M OPT-1.3B OPT-2.7B OPT-6.7B OPT-13B OPT-30B OPT-66BNo Retrieval In-Context RALM (BM25)Figure 4: Results of OPT models (Zhang et al., 2022) on the test set of WikiText-103 (word-level\\nperplexity) and the development set of RealNews (token-level perplexity). In-Context RALM models use\\na BM25 retriever with s= 4(i.e., the retriever is called every four tokens) and ‚Ñì= 32 (i.e., the retriever\\nquery is comprised of the last 32 tokens of the prefix). In-Context RALM with an off-the-shelf retriever\\nimproved the performance of a 6.7B parameter OPT model to match that of a 66B parameter OPT model.\\nappealing since applying a BM25 retriever is sig-\\nnificantly cheaper than the neural alternatives.\\n5.2 Frequent Retrieval Improves Language\\nModeling\\nWe investigated the effect of varying the retrieval\\nstride s(i.e., the number of tokens between consec-\\nutive retrieval operations). Figure 5 shows that LM\\nperformance improved as the retrieval operation\\nbecame more frequent. This supports the intuition\\nthat retrieved documents become more relevant the\\ncloser the retrieval query becomes to the gener-\\nated tokens. Of course, each retrieval operation\\nimposes a runtime cost. To balance performance\\nand runtime, we used s= 4 in our experiments.\\nFor comparison, RETRO employed a retrieval fre-\\nquency of s= 64 (Borgeaud et al., 2022), which\\nleads to large degradation in perplexity. Intuitively,\\nretrieving with high frequency (low retrieval stride)\\nallows to ground the LM in higher resolution.5.3 A Contextualization vs. Recency Tradeoff\\nin Query Length\\nWe also investigated the effect of varying ‚Ñì, the\\nlength of the retrieval query for BM25. Figure 6\\nreveals an interesting tradeoff and a sweet spot\\naround a query length of 32tokens. Similar ex-\\nperiments for dense retrievers are given in App. A.\\nWe conjecture that when the retriever query is too\\nshort, it does not include enough of the input con-\\ntext, decreasing the retrieved document‚Äôs relevance.\\nConversely, excessively growing the retriever query\\ndeemphasizes the tokens at the very end of the pre-\\nfix, diluting the query‚Äôs relevance to the LM task.\\n6 Improving In-Context RALM with\\nLM-Oriented Reranking\\nSince In-Context RALM uses a fixed document\\nreading component by definition, it is natural to\\nask whether performance can be improved by spe-\\ncializing its document retrieval mechanism to the\\nLM task. Indeed, there is considerable scope for\\nimprovement: the previous section considered con-\\nditioning the model only on the first document re-', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 6}),\n",
       " Document(page_content='Retrieval Stride ( ùë†)Perplexity\\n10.020.030.040.0\\n1 2 4 8 16 32 64GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)Figure 5: An analysis of perplexity as a function\\nofs, the retrieval stride ,i.e., the number of tokens\\nbetween consecutive retrieval operations, on the\\ndevelopment set of WikiText-103. Throughout the\\npaper, we use s= 4 to balance perplexity and\\nruntime.\\nRetrieval Query Length (‚Ñì)Perplexity\\n10.015.020.025.030.035.0\\n16 32 64GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)\\nFigure 6: An analysis of perplexity as a function\\nofthe number of tokens in the query ‚Ñìfor BM25\\non the development set of WikiText-103. In the\\nappendix, we show similar trade-offs for dense\\nretrievers within WikiText-103. Throughout the\\npaper, we use a query length of ‚Ñì= 32 tokens.\\ntrieved by the BM25 retriever. This permits very\\nlimited semantic understanding of the query, since\\nBM25 is based only on the bag of words signal.\\nMoreover, it offers no way to accord different de-\\ngrees of importance to different retrieval query to-\\nkens, such as recognizing that later query tokens\\nare more relevant to the generated text.\\nIn this section, we focus on choosing which doc-\\nument to present to the model, by reranking the\\ntop-kdocuments returned by the BM25 retriever.5\\nWe use Figure 7 as motivation: it shows the large\\npotential for improvement among the top- 16docu-\\nments returned by the BM25 retriever. We act upon\\n5In both ¬ß6.1 and ¬ß6.2 we use k= 16 .\\nPerplexity\\n10.020.030.040.0\\nNo Retrieval BM25 (Top-1) Oracle: BM25 (Top-16)GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)Figure 7: Potential for gains from reranking: per-\\nplexity improvement (on the development set of\\nWikiText-103) from an oracle that takes the best\\nof the top-16 documents retrieved by BM25 rather\\nthan the first.\\nthis motivation by using two rerankers. Specifi-\\ncally, in ¬ß6.1 we show performance gains across\\nour evaluation suite obtained by using an LM to\\nperform zero-shot reranking of the top- kBM25\\nretrieved documents (results in third row for each\\nof the models in Table 1). Then, in ¬ß6.2 we show\\nthat training a specialized bidirectional reranker\\nof the top- kBM25 retrieved documents in a self-\\nsupervised manner via the LM signal can provide\\nfurther LM gains (results in forth row for each of\\nthe models in Table 1).\\n6.1 LMs as Zero-Shot Rerankers\\nFirst, we used off-the-shelf language models as\\ndocument rerankers for the In-Context RALM set-\\nting. Formally, for a query qconsisting of the\\nlast‚Ñìtokens in the prefix of the LM input x, let\\n{d1, ..., d k}be the top- kdocuments returned by\\nBM25. For retrieval iteration j, let the text for\\ngeneration be y:=xs¬∑j+1, ..., x s¬∑j+s. Ideally, we\\nwould like to find the document di‚àóthat maximizes\\nthe probability of the text for generation, i.e.,\\ni‚àó= arg max\\ni‚àà[k]pŒ∏(y|[di;x‚â§s¬∑j]). (5)\\nHowever, at test time we do not have access to\\nthe tokens of y. Instead, we used the last pre-\\nfixtokens (which areavailable at test time), de-\\nnoted by y‚Ä≤, for reranking. Formally, let s‚Ä≤be\\na hyper-parameter that determines the number of\\nthe prefix tokens by which to rerank. We define\\ny‚Ä≤:=xs¬∑j‚àís‚Ä≤+1, ..., x s¬∑j(i.e., the stride of length s‚Ä≤\\nthat precedes y) and choose the document dÀÜisuch', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 7}),\n",
       " Document(page_content='ModelReranking\\nModelWikiText-103 RealNews\\nword ppl token ppl\\nGPT-2 345M (M)GPT-2 110M (S) 20.8 12.1\\nGPT-2 345M (M) 20.8 12.0\\nGPT-2 762M (L)GPT-2 110M (S) 17.7 10.7\\nGPT-2 762M (L) 17.6 10.6\\nGPT-2 1.5B (XL)GPT-2 110M (S) 16.2 9.9\\nGPT-2 1.5B (XL) 16.1 9.8\\nTable 3: Perplexity for zero-shot reranking (¬ß6.1) where the reranking models is smaller than the LM, or\\nthe LM itself. Reranking is performed on the top 16 documents retrieved by BM25. Using a GPT-2 110M\\n(S) instead of a larger language model as a reranker leads to only a minor degradation.\\nthat\\nÀÜi= arg max\\ni‚àà[k]pœï(y‚Ä≤|\\x02\\ndi;x‚â§(s¬∑j‚àís‚Ä≤)\\x03\\n).(6)\\nThe main motivation is that since BM25 is a lexical\\nretriever, we want to incorporate a semantic signal\\ninduced by the LM. Also, this reranking shares con-\\nceptual similarities with the reranking framework\\nof Sachan et al. (2022) for open-domain question\\nanswering, where y‚Ä≤(i.e., the last prefix tokens) can\\nbe thought of as their ‚Äúquestion‚Äù.\\nNote that our zero-shot reranking does not re-\\nquire that the LM used for reranking is the same\\nmodel as the LM used for generation ( i.e., the LM\\nin Eq. (6), parameterized by œï, does not need to be\\nthe LM in Eq. (2), parameterized by Œ∏). This ob-\\nservation unlocks the possibility of reranking with\\nsmaller (and thus faster) models, which is impor-\\ntant for two main reasons: (i) Reranking kdocu-\\nments requires kforward passes; and (ii) it allows\\nour methods to be used in cases where the actual\\nLM‚Äôs log probabilities are not available (for exam-\\nple, when the LM is accessed through an API).6\\nResults A minimal hyper-parameter search on\\nthe development set of WikiText-103 revealed that\\nthe optimal query length is s‚Ä≤= 16 ,7so we proceed\\nwith this value going forward. Table 1 shows the\\nresults of letting the LM perform zero-shot rerank-\\ning on the top-16 documents retrieved by BM25\\n(third row for each of the models). It is evident\\nthat reranking yielded consistently better results\\nthan simply taking the first result returned by the\\nretriever.\\n6Note we do not require that the two models share the\\nsame vocabulary.\\n7We experimented with s‚Ä≤‚àà {4,8,16,32}.Table 3 shows that a small LM (GPT-2 117M)\\ncan be used to rerank the documents for all larger\\nGPT-2 models, with roughly the same performance\\nas having each LM perform reranking for itself,\\nsupporting the applicability of this method for LMs\\nthat are only accessible via an API.\\n6.2 Training LM-dedicated Rerankers\\nNext, we trained a reranker to choose one of the\\ntop-kdocuments retrieved by BM25. We refer to\\nthis approach as Predictive Reranking , since the\\nreranker learns to choose which document will help\\nin ‚Äúpredicting‚Äù the upcoming text. For this process,\\nwe assume availability of training data from the\\ntarget corpus. Our reranker is a classifier that gets\\na prefix x‚â§s¬∑jand a document di(fori‚àà[k]), and\\nproduces a scalar f(x‚â§s¬∑j, di)that should resemble\\nthe relevance of diforthe continuation ofx‚â§s¬∑j.\\nWe then normalize these relevance scores:\\nprank(di|x‚â§s¬∑j) =exp(f(x‚â§s¬∑j, di))Pk\\ni‚Ä≤=1exp(f(x‚â§s¬∑j, di‚Ä≤)),(7)\\nand choose the document dÀÜisuch that\\nÀÜi= arg max\\ni‚àà[k]prank(di|x‚â§s¬∑j). (8)\\nCollecting Training Examples To train our pre-\\ndictive reranker, we collected training examples\\nas follows. Let x‚â§s¬∑jbe a prefix we sample from\\nthe training data, and y:=xs¬∑j+1, ..., x s¬∑j+sbe the\\ntext for generation upcoming in its next stride. We\\nrun BM25 on the query qs,‚Ñì\\njderived from x‚â§s¬∑j\\n(see ¬ß3.2) and get kdocuments {d1, ..., d k}. For\\neach document di, we then run the LM to compute\\npŒ∏(y|[di;x‚â§s¬∑j])similar to Eq. (4).', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 8}),\n",
       " Document(page_content='Number of documentsEM\\n0.020.040.060.080.0\\n0 1 2 3 4TriviaQA: LLaMa-7B TriviaQA: LLaMa-13B\\nNQ: LLaMa-7B NQ: LLaMa-13BFigure 8: Zero-shot performance of In-Context\\nRALM on the development set of Natural Ques-\\ntions and TriviaQA, when varying the number of\\ndocuments (retrieved by DPR) shown in-context.\\nTraining Our reranker was a fine-tuned\\nRoBERTa-base (Liu et al., 2019) that trained for\\n10,000 steps with a peak learning rate of 10‚àí5and\\na batch size of 32. Overall, we created 300,000\\nexamples from the training set of WikiText-103 as\\nexplained above. The loss function we use to train\\nthe reranker follows previous work (Guu et al.,\\n2020; Lewis et al., 2020):\\n‚àílogkX\\ni=1prank(di|x‚â§s¬∑j)¬∑pŒ∏(y|[di;x‚â§s¬∑j]).(9)\\nNote that unlike those works, we train only the\\nreranker ( prank), keeping the LM weights Œ∏frozen.\\nResults Table 1 shows the result of our predictive\\nreranker, trained on WikiText-103. Specifically, we\\ntrained it with data produced by GPT-2 110M (S),\\nand tested its effectiveness for all GPT-2 models.\\nWe observed significant gains obtained from Predic-\\ntive Reranking. For example, the perplexity of GPT-\\n2 110M (S) improved from 29.6 to 26.8, and that of\\nGPT-2 1.5B (XL) improved from 16.6 to 15.4. This\\ntrend held for the other two models as well. Overall,\\nthese results demonstrate that training a reranker\\nwith domain-specific data was more effective than\\nzero-shot reranking (Section 6.1). Note that these\\nresults‚Äîwhile impressive‚Äîstill leave room for fur-\\nther improvements, compared to the top-16 BM25\\noracle results (see Figure 7). Moreover, the oracle\\nresults themselves can be improved by retrieving\\nk > 16documents via a BM25 retriever, or by\\ntraining stronger retrievers dedicated to the RALM\\ntask. We leave this direction for future work.Model Retrieval NQ TriviaQA\\nLLaMA-7B- 10.3 47.5\\nDPR 28.0 56.0\\nLLaMA-13B- 12.0 54.8\\nDPR 31.0 60.1\\nLLaMA-33B- 13.7 58.3\\nDPR 32.3 62.7\\nTable 4: Zero-shot results of In-Context RALM on\\nthe test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our ‚Äúreader‚Äù ( i.e., the model that gets the\\nquestion along with its corresponding retrieved doc-\\numents, and returns the answer) is simply a frozen\\nlarge LM: notpretrained, fine-tuned or prompted\\nto be retrieval-augmented. For the closed-book set-\\nting, we utilize the prompt of Touvron et al. (2023).\\nFor the open-book setting, we extend this prompt\\nto include retrieved documents (see App. C). We\\nuse DPR (Karpukhin et al., 2020) as our retriever.\\nVarying the Number of Documents To inves-\\ntigate the the effect of the number of documents\\nshown to the model, we performed a minimal anal-\\nysis on the development set of NQ and TriviaQA.\\nFigure 8 demonstrates that showing documents in-\\ncontext significantly improves the model‚Äôs perfor-\\nmance. In addition, most of the gain can be ob-\\ntained by using only two documents (or even a\\nsingle one in some cases).', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 9}),\n",
       " Document(page_content='Results Table 4 gives the results of In-Context\\nRALM on the test set of Natural Questions and\\nTriviaQA. Motivated by our previous findings,\\nwe used two retrieved documents. It is evident\\nthat showing the model relevant documents sig-\\nnificantly boosted its performance. For example,\\nadding retrieved documents improved LLaMA-\\n13B in the zero-shot setting by more than 18 points\\non NQ (from 12.0% to 31.0%) and more than 5\\npoints on TriviaQA (from 54.8% to 60.1%).\\n8 Discussion\\nRetrieval from external sources has become a com-\\nmon practice in knowledge-intensive tasks (such\\nas factual question answering, fact checking, and\\nmore; Petroni et al. 2021). In parallel, recent break-\\nthroughs in LM generation capabilities has led to\\nLMs that can generate useful long texts. How-\\never, factual inaccuracies remain a common way in\\nwhich machine-generated text can fall short, and\\nlack of direct provenance makes it hard to trust\\nmachine generated text. This makes language mod-\\neling both a promising and an urgent new applica-\\ntion area for knowledge grounding, and motivates\\npromoting RALM approaches. Prior research has\\nalready investigated RALM, of course, but it is\\nnot yet widely deployed. One likely reason is that\\nexisting approaches rely upon fine-tuning the LM,\\nwhich is typically difficult and costly, and is even\\nimpossible for LMs accessible only via an API.\\nThis paper presented the framework of In-\\nContext RALM , enabling frozen, off-the-shelf LMs\\nto benefit from retrieval. We demonstrated that\\nsubstantial performance gains can be achieved by\\nusing general purpose retrievers, and showed that\\nadditional gains can be achieved by tailoring the\\ndocument selection to the LM setting. A recent\\nwork by Muhlgay et al. (2023) demonstrates that\\nIn-Context RALM is indeed able to improve the\\nfactuality of large LMs.\\nSeveral directions for further improvement re-\\nmain for future work. First, this paper considers\\nonly the case of prepending a single external docu-\\nment to the context; adding more documents could\\ndrive further gains (for example, using the frame-\\nwork of Ratner et al. 2022). Second, we retrieved\\ndocuments every fixed interval of stokens, but see\\npotential for large latency and cost gains by retriev-\\ning more sparsely, such as only when a specialized\\nmodel predicts that retrieval is needed.\\nWe release the code used in this work, for thecommunity to use and improve over. We hope it\\nwill drive further research of RALM, which will\\nenable its wider adoption.\\nAcknowledgements\\nWe would like to thank the reviewers and the Ac-\\ntion Editor for their valuable feedback.\\nReferences\\nUri Alon, Frank Xu, Junxian He, Sudipta Sengupta,\\nDan Roth, and Graham Neubig. 2022. Neuro-\\nsymbolic language modeling with automaton-\\naugmented retrieval. In ICML .\\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\\nand Stella Biderman. 2021. GPT-Neo: Large\\nScale Autoregressive Language Modeling with\\nMesh-Tensorflow.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Mil-\\nlican, George Bm Van Den Driessche, Jean-\\nBaptiste Lespiau, Bogdan Damoc, Aidan Clark,\\nDiego De Las Casas, Aurelia Guy, Jacob Menick,\\nRoman Ring, Tom Hennigan, Saffron Huang,\\nLoren Maggiore, Chris Jones, Albin Cassirer,\\nAndy Brock, Michela Paganini, Geoffrey Irv-\\ning, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack Rae, Erich Elsen, and Laurent\\nSifre. 2022. Improving language models by re-\\ntrieving from trillions of tokens. In ICML .\\nTom B. Brown, Benjamin Mann, Nick Ryder,\\nMelanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish\\nSastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-V oss, Gretchen Krueger, Tom Henighan,\\nRewon Child, Aditya Ramesh, Daniel Ziegler,\\nJeffrey Wu, Clemens Winter, Christopher Hesse,\\nMark Chen, Eric Sigler, Mateusz Litwin, Scott\\nGray, Benjamin Chess, Jack Clark, Christopher\\nBerner, Sam McCandlish, Alec Radford, Ilya\\nSutskever, and Dario Amodei. 2020. Language\\nmodels are few-shot learners. In Advances in\\nNeural Information Processing Systems .\\nDanqi Chen, Adam Fisch, Jason Weston, and An-\\ntoine Bordes. 2017. Reading Wikipedia to an-\\nswer open-domain questions. In Proceedings\\nof the 55th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 10}),\n",
       " Document(page_content='Papers) , pages 1870‚Äì1879, Vancouver, Canada.\\nAssociation for Computational Linguistics.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training\\nof deep bidirectional transformers for language\\nunderstanding. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the\\nAssociation for Computational Linguistics: Hu-\\nman Language Technologies, Volume 1 (Long\\nand Short Papers) , pages 4171‚Äì4186, Minneapo-\\nlis, Minnesota. Association for Computational\\nLinguistics.\\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu,\\nLei Li, and Zhifang Sui. 2023. A survey on\\nin-context learning.\\nMartin Fajcik, Martin Docekal, Karel Ondrej, and\\nPavel Smrz. 2021. R2-D2: A modular baseline\\nfor open-domain question answering. In Find-\\nings of the Association for Computational Lin-\\nguistics: EMNLP 2021 , pages 854‚Äì870, Punta\\nCana, Dominican Republic. Association for\\nComputational Linguistics.\\nLeo Gao, Stella Biderman, Sid Black, Laurence\\nGolding, Travis Hoppe, Charles Foster, Jason\\nPhang, Horace He, Anish Thite, Noa Nabeshima,\\nShawn Presser, and Connor Leahy. 2021. The\\npile: An 800gb dataset of diverse text for lan-\\nguage modeling.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong\\nPasupat, and Ming-Wei Chang. 2020. REALM:\\nRetrieval-augmented language model pre-\\ntraining. In ICML .\\nJunxian He, Graham Neubig, and Taylor Berg-\\nKirkpatrick. 2021. Efficient nearest neighbor\\nlanguage models. In Proceedings of the 2021\\nConference on Empirical Methods in Natural\\nLanguage Processing , pages 5703‚Äì5714, Online\\nand Punta Cana, Dominican Republic. Associa-\\ntion for Computational Linguistics.\\nMinlie Huang, Xiaoyan Zhu, and Jianfeng Gao.\\n2020. Challenges in building intelligent open-\\ndomain dialog systems. ACM Trans. Inf. Syst. ,\\n38(3).\\nGautier Izacard, Mathilde Caron, Lucas Hosseini,\\nSebastian Riedel, Piotr Bojanowski, ArmandJoulin, and Edouard Grave. 2022a. Unsu-\\npervised dense information retrieval with con-\\ntrastive learning. Transactions on Machine\\nLearning Research .\\nGautier Izacard and Edouard Grave. 2021. Lever-\\naging passage retrieval with generative models\\nfor open domain question answering. In Pro-\\nceedings of the 16th Conference of the European\\nChapter of the Association for Computational\\nLinguistics: Main Volume , pages 874‚Äì880, On-\\nline. Association for Computational Linguistics.\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel,\\nand Edouard Grave. 2022b. Atlas: Few-shot\\nlearning with retrieval augmented language mod-\\nels.\\nJeff Johnson, Matthijs Douze, and Herv√© J√©gou.\\n2021. Billion-scale similarity search with GPUs.\\nIEEE Transactions on Big Data , 7(3):535‚Äì547.\\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale\\ndistantly supervised challenge dataset for read-\\ning comprehension. In Proceedings of the 55th\\nAnnual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers) ,\\npages 1601‚Äì1611, Vancouver, Canada. Associa-\\ntion for Computational Linguistics.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min,\\nPatrick Lewis, Ledell Wu, Sergey Edunov, Danqi\\nChen, and Wen-tau Yih. 2020. Dense passage re-\\ntrieval for open-domain question answering. In\\nProceedings of the 2020 Conference on Empir-\\nical Methods in Natural Language Processing\\n(EMNLP) , pages 6769‚Äì6781, Online. Associa-\\ntion for Computational Linguistics.\\nUrvashi Khandelwal, Omer Levy, Dan Juraf-\\nsky, Luke Zettlemoyer, and Mike Lewis. 2020.\\nGeneralization through memorization: Nearest\\nneighbor language models. In International\\nConference on Learning Representations .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia\\nRedfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob\\nDevlin, Kenton Lee, Kristina Toutanova, Llion\\nJones, Matthew Kelcey, Ming-Wei Chang, An-\\ndrew M. Dai, Jakob Uszkoreit, Quoc Le, and', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 11}),\n",
       " Document(page_content='Slav Petrov. 2019. Natural questions: A bench-\\nmark for question answering research. Trans-\\nactions of the Association for Computational\\nLinguistics , 7:452‚Äì466.\\nYoav Levine, Itay Dalmedigos, Ori Ram, Yoel\\nZeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin,\\nOpher Lieber, Barak Lenz, Shai Shalev-Shwartz,\\nAmnon Shashua, Kevin Leyton-Brown, and\\nYoav Shoham. 2022a. Standing on the shoul-\\nders of giant frozen language models.\\nYoav Levine, Ori Ram, Daniel Jannai, Barak Lenz,\\nShai Shalev-Shwartz, Amnon Shashua, Kevin\\nLeyton-Brown, and Yoav Shoham. 2022b. Huge\\nfrozen language models as readers for open-\\ndomain question answering. In ICML 2022\\nWorkshop on Knowledge Retrieval and Lan-\\nguage Models .\\nYoav Levine, Noam Wies, Daniel Jannai, Dan\\nNavon, Yedid Hoshen, and Amnon Shashua.\\n2022c. The inductive bias of in-context learn-\\ning: Rethinking pretraining example design. In\\nInternational Conference on Learning Represen-\\ntations .\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus,\\nFabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich K√ºttler, Mike Lewis, Wen-tau\\nYih, Tim Rockt√§schel, Sebastian Riedel, and\\nDouwe Kiela. 2020. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. In\\nAdvances in Neural Information Processing Sys-\\ntems, pages 9459‚Äì9474.\\nZonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022.\\nDecoupled context processing for context aug-\\nmented language modeling. In Advances in Neu-\\nral Information Processing Systems .\\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\\nShoham. 2021. Jurassic-1: Technical details and\\nevaluation.\\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin,\\nJheng-Hong Yang, Ronak Pradeep, and Rodrigo\\nNogueira. 2021. Pyserini: A python toolkit for\\nreproducible information retrieval research with\\nsparse and dense representations. In Proceed-\\nings of the 44th International ACM SIGIR Con-\\nference on Research and Development in Infor-\\nmation Retrieval , SIGIR ‚Äô21, page 2356‚Äì2362,\\nNew York, NY , USA. Association for Comput-\\ning Machinery.Stephanie Lin, Jacob Hilton, and Owain Evans.\\n2022. TruthfulQA: Measuring how models\\nmimic human falsehoods. In Proceedings of the\\n60th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers) ,\\npages 3214‚Äì3252, Dublin, Ireland. Association\\nfor Computational Linguistics.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\\nMandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019. RoBERTa: A robustly optimized bert\\npretraining approach.\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet,\\nand Ryan McDonald. 2020. On faithfulness and\\nfactuality in abstractive summarization. In Pro-\\nceedings of the 58th Annual Meeting of the As-\\nsociation for Computational Linguistics , pages\\n1906‚Äì1919, Online. Association for Computa-\\ntional Linguistics.\\nStephen Merity, Caiming Xiong, James Bradbury,\\nand Richard Socher. 2016. Pointer sentinel mix-\\nture models.\\nDor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\\nNir Ratner, Yonatan Belinkov, Omri Abend,\\nKevin Leyton-Brown, Amnon Shashua, and\\nYoav Shoham. 2023. Generating benchmarks\\nfor factuality evaluation of language models.\\nFabio Petroni, Aleksandra Piktus, Angela Fan,\\nPatrick Lewis, Majid Yazdani, Nicola De Cao,\\nJames Thorne, Yacine Jernite, Vladimir\\nKarpukhin, Jean Maillard, Vassilis Plachouras,\\nTim Rockt√§schel, and Sebastian Riedel. 2021.\\nKILT: a benchmark for knowledge intensive lan-\\nguage tasks. In Proceedings of the 2021 Con-\\nference of the North American Chapter of the\\nAssociation for Computational Linguistics: Hu-\\nman Language Technologies , pages 2523‚Äì2544,\\nOnline. Association for Computational Linguis-\\ntics.\\nAlec Radford, Karthik Narasimhan, Tim Salimans,\\nand Ilya Sutskever. 2018. Improving language\\nunderstanding by generative pre-training.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Lan-\\nguage models are unsupervised multitask learn-\\ners.', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 12}),\n",
       " Document(page_content='Ori Ram, Gal Shachaf, Omer Levy, Jonathan Be-\\nrant, and Amir Globerson. 2022. Learning to re-\\ntrieve passages without supervision. In Proceed-\\nings of the 2022 Conference of the North Amer-\\nican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technolo-\\ngies, pages 2687‚Äì2700, Seattle, United States.\\nAssociation for Computational Linguistics.\\nNir Ratner, Yoav Levine, Yonatan Belinkov,\\nOri Ram, Omri Abend, Ehud Karpas, Am-\\nnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2022. Parallel context windows im-\\nprove in-context learning of large language mod-\\nels.\\nStephen Robertson and Hugo Zaragoza. 2009. The\\nprobabilistic relevance framework: BM25 and\\nbeyond. Found. Trends Inf. Retr. , 3(4):333‚Äì389.\\nDevendra Sachan, Mike Lewis, Mandar Joshi, Ar-\\nmen Aghajanyan, Wen-tau Yih, Joelle Pineau,\\nand Luke Zettlemoyer. 2022. Improving passage\\nretrieval with zero-shot question generation. In\\nProceedings of the 2022 Conference on Empir-\\nical Methods in Natural Language Processing ,\\npages 3781‚Äì3797, Abu Dhabi, United Arab Emi-\\nrates. Association for Computational Linguis-\\ntics.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\\nmoyer, and Wen tau Yih. 2023. REPLUG:\\nRetrieval-augmented black-box language mod-\\nels.\\nNandan Thakur, Nils Reimers, Andreas R√ºckl√©,\\nAbhishek Srivastava, and Iryna Gurevych. 2021.\\nBEIR: A heterogeneous benchmark for zero-shot\\nevaluation of information retrieval models. In\\nProceedings of the Neural Information Process-\\ning Systems Track on Datasets and Benchmarks ,\\nvolume 1.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard,\\nXavier Martinet, Marie-Anne Lachaux, Timo-\\nth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal,\\nEric Hambro, Faisal Azhar, Aurelien Rodriguez,\\nArmand Joulin, Edouard Grave, and Guillaume\\nLample. 2023. LLaMA: Open and efficient foun-\\ndation language models.\\nAshish Vaswani, Noam Shazeer, Niki Parmar,\\nJakob Uszkoreit, Llion Jones, Aidan Gomez,≈Å ukasz Kaiser, and Illia Polosukhin. 2017. At-\\ntention is all you need. In Advances in Neural\\nInformation Processing Systems 30 , pages 5998‚Äì\\n6008.\\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\\n6B: A 6 Billion Parameter Autoregressive Lan-\\nguage Model.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi,\\nPierric Cistac, Tim Rault, Remi Louf, Morgan\\nFuntowicz, Joe Davison, Sam Shleifer, Patrick\\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu,\\nCanwen Xu, Teven Le Scao, Sylvain Gugger,\\nMariama Drame, Quentin Lhoest, and Alexan-\\nder Rush. 2020. Transformers: State-of-the-art\\nnatural language processing. In Proceedings of\\nthe 2020 Conference on Empirical Methods in\\nNatural Language Processing: System Demon-\\nstrations , pages 38‚Äì45, Online. Association for\\nComputational Linguistics.\\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\\nYonatan Bisk, Ali Farhadi, Franziska Roesner,\\nand Yejin Choi. 2019. Defending against neural\\nfake news. In Advances in Neural Information\\nProcessing Systems , volume 32. Curran Asso-\\nciates, Inc.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\\nArtetxe, Moya Chen, Shuohui Chen, Christopher\\nDewan, Mona Diab, Xian Li, Xi Victoria Lin,\\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt\\nShuster, Daniel Simig, Punit Singh Koura, An-\\njali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\\n2022. OPT: Open pre-trained transformer lan-\\nguage models.\\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022.\\nTraining language models with memory augmen-\\ntation. In Proceedings of the 2022 Conference\\non Empirical Methods in Natural Language Pro-\\ncessing , pages 5657‚Äì5673, Abu Dhabi, United\\nArab Emirates. Association for Computational\\nLinguistics.\\nA Query Length Ablations\\nFigure 9 and Figure 10 show ablations on the opti-\\nmal query length ‚Ñìfor off-the-shelf dense retrievers\\n(BERT and Contriever respectively). We omit the\\nresults of Spider as they are almost identical to\\nthose of Contriever. Consistently, using ‚Ñì= 64', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 13}),\n",
       " Document(page_content='(tokens) is optimal. This is in contrast to similar\\nexperiments we conducted for BM25 ( cf. Figure 6),\\nwhere ‚Ñì= 32 is optimal.\\nB GPT-Neo Results\\nTable 5 gives the results of applying In-Context\\nRALM to the models from the GPT-Neo model\\nfamily on WikiText-103 and RealNews.\\nC Open-Domain Question Answering\\nExperiments: Further Details\\nClosed-Book Setting For the closed-book set-\\nting, we adopt the prompt of Touvron et al. (2023):\\nAnswer these questions:\\nQ: Who got the first nobel\\nprize in physics?\\nA:\\nOpen-Book Setting For the open-book setting,\\nwe extend the above prompt as follows:\\nNobel Prize\\nA group including 42\\nSwedish writers, artists,\\nand literary critics\\nprotested against this\\ndecision, having expected\\nLeo Tolstoy to be awarded.\\nSome, including Burton\\nFeldman, have criticised\\nthis prize because they...\\nNobel Prize in Physiology\\nor Medicine\\nIn the last half century\\nthere has been an\\nincreasing tendency\\nfor scientists to work\\nas teams, resulting in\\ncontroversial exclusions.\\nAlfred Nobel was born\\non 21 October 1833 in\\nStockholm, Sweden, into\\na family of engineers...\\nBased on these texts,\\nanswer these questions:\\nQ: Who got the first nobel\\nprize in physics?\\nA:Model RetrievalWiki-103 RealNews\\nword ppl token ppl\\nGPT-Neo 1.3B- 17.5 12.3\\nBM25, ¬ß5 14.6 9.9\\nGPT-Neo 2.7B- 15.1 11.0\\nBM25, ¬ß5 12.8 9.0\\nGPT-J 6B- 11.6 9.2\\nBM25, ¬ß5 10.0 7.7\\nTable 5: The performance of models from the GPT-\\nNeo family, measured by word-level perplexity on\\nthe test set of WikiText-103 and token-level per-\\nplexity on the development set of RealNews.\\nRetrieval Query Length (‚Ñì)Perplexity\\n15.025.035.045.0\\n16 32 64 128 256GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)\\nFigure 9: An analysis of perplexity as a function\\nofthe number of tokens in the query for an off-\\nthe-shelf BERT retriever on the development set of\\nWikiText-103.\\nRetrieval Query Length (‚Ñì)Perplexity\\n10.020.030.040.0\\n16 32 64 128 256GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)\\nFigure 10: An analysis of perplexity as a function\\nofthe number of tokens in the query for Contriever\\non the development set of WikiText-103.', metadata={'source': './data\\\\In context retrieval.pdf', 'page': 14}),\n",
       " Document(page_content='MuRAG: Multimodal Retrieval-Augmented Generator\\nfor Open Question Answering over Images and Text\\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen\\nGoogle Research\\n{wenhuchen,hexiang,patverga,wcohen}@google.com\\nAbstract\\nWhile language Models store a massive\\namount of world knowledge implicitly in their\\nparameters, even very large models often fail\\nto encode information about rare entities and\\nevents, while incurring huge computational\\ncosts. Recently, retrieval-augmented models,\\nsuch as REALM, RAG, and RETRO, have\\nincorporated world knowledge into language\\ngeneration by leveraging an external non-\\nparametric index and have demonstrated im-\\npressive performance with constrained model\\nsizes. However, these methods are restricted\\nto retrieving only textual knowledge, neglect-\\ning the ubiquitous amount of knowledge in\\nother modalities like images ‚Äì much of which\\ncontains information not covered by any text.\\nTo address this limitation, we propose the\\nÔ¨Årst Multimodal Retrieval-Augmented Trans-\\nformer (MuRAG), which accesses an external\\nnon-parametric multimodal memory to aug-\\nment language generation. MuRAG is pre-\\ntrained with a mixture of large-scale image-\\ntext and text-only corpora using a joint con-\\ntrastive and generative loss. We perform ex-\\nperiments on two different datasets that re-\\nquire retrieving and reasoning over both im-\\nages and text to answer a given query: We-\\nbQA, and MultimodalQA. Our results show\\nthat MuRAG achieves state-of-the-art accu-\\nracy, outperforming existing models by 10-\\n20% absolute on both datasets and under both\\ndistractor and full-wiki settings.\\n1 Introduction\\nPre-trained language models like GPT-3 (Brown\\net al., 2020), PaLM (Chowdhery et al., 2022), etc\\nhave been shown to capture a massive amount\\nof world knowledge implicitly in their parame-\\nters. However, using such large models incurs an\\nextremely high computation cost. As an alterna-\\ntive to a singular monolithic transformer, retrieval-\\naugmented architectures like KNN-LM (Khandel-\\nwal et al., 2019), REALM (Guu et al., 2020),\\nFigure 1: Visual information-seeking queries : These\\nqueries are unanswerable with text-only retrieval and\\nrequire retrieving and reasoning over images.\\nRAG (Lewis et al., 2020), FiD (Izacard and Grave,\\n2021), and RETRO (Borgeaud et al., 2021) have\\nbeen proposed to decouple world knowledge from\\nthe model‚Äôs parameters. More speciÔ¨Åcally, these\\nmodels are trained to access an external mem-\\nory to enhance the model‚Äôs predictions. Such\\nretrieval-augmented architectures have multiple\\nbeneÔ¨Åcial properties including: decreased model\\nsize (Borgeaud et al., 2021), better attribution/-\\nexplanation for model predictions (Lewis et al.,\\n2020), and adaptability to new information with-\\nout retraining (Verga et al., 2021). However, pre-\\nvious retrieval-augmented models are limited to\\nmemories that contain only text or structured data\\nand hence cannot make use of the massive amount\\nof multimodal knowledge available on the web‚Äî\\nmuch of which contains information only available\\nin non-text modalities.\\nFigure 1, shows several information-seeking\\nqueries that require retrieving and reasoning over\\nvisual knowledge. Here, a user Ô¨Årst poses a ques-\\ntion such as ‚ÄúWhat can be found on the White\\nHouse balconies at Christmas‚Äù . The system then\\nretrieves relevant items from its memory, for exam-arXiv:2210.02928v2  [cs.CL]  20 Oct 2022', metadata={'source': './data\\\\MuRAG-Multimodal Retrieval-Augmented Generator.pdf', 'page': 0}),\n",
       " Document(page_content='ple, the Ô¨Årst image of Figure 1 with the caption\\n‚ÄúWhite House during Christmas‚Äù , which it uses to\\nproduce the answer ‚Äúwreaths and garlands‚Äù . Ex-\\nisting text retrieval-augmented models would strug-\\ngle with such queries because, in many cases, they\\nwould simply not have access to the answer as some\\nknowledge does not exist in text form. That, cou-\\npled with the abundance of multimodal knowledge\\nthat exists, leads to the conclusion that retrieval-\\naugmented models should ultimately be developed\\nto retrieve and reason over multiple modalities.\\nFigure 2: Model Overview : retrieval-and-predict pro-\\ncess of MuRAG on downstream datasets.\\nIn this paper, we are speciÔ¨Åcally interested in\\nendowing pre-trained language models with a non-\\nparametric multimodal memory containing images,\\ntext, or image-text pairs. To accomplish this, we\\nÔ¨Årst combine pre-trained T5 (Raffel et al., 2020)\\nand ViT (Dosovitskiy et al., 2020) models to build\\na backbone encoder (Figure 3), which encodes\\nimage-text pairs, image-only, and text-only inputs\\ninto a multimodal representation. MuRAG uses the\\nbackbone encoder to embed items into an external\\nmemory as well as queries to retrieve multimodal\\nknowledge from that memory. These retrievals\\nthen augment a language model to generate more\\nvisually-grounded outputs.\\nWe pre-train MuRAG with a mixture of\\nimage-text and text-only datasets including\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption (Sharma et al., 2018), VQA (An-\\ntol et al., 2015) and Probably-Asked-Questions\\n(PAQ) (Lewis et al., 2021). More speciÔ¨Åcally, we\\nreformulate these datasets in a retrieve-and-predict\\nformat. Here, the model‚Äôs input is an image along\\nwith a text prompt. The model then retrieves from\\na memory containing captions and passages, which\\nit uses to generate a target token sequence. The\\nmodel is trained with both a contrastive and a gen-erative loss; this teaches the model to discriminate\\nrelevant from irrelevant memory entries, and guides\\nthe model to leverage the multimodal knowledge\\ninto generation.\\nUnlike the pre-training stage, during Ô¨Åne-\\ntuning Figure 2 the model‚Äôs input is a question,\\nand the memory contains a collection of captioned\\nimages and text snippets. We Ô¨Åne-tune MuRAG\\non the downstream datasets with a contrastive and\\ngenerative loss similar to pre-training. To avoid ex-\\ncessive computation cost, we develop a two-stage\\ntraining pipeline to Ô¨Årst train with small in-batch\\nmemory, and then with a statically encoded and\\nindexed large global memory.\\nOur experiments show that MuRAG achieves\\nstate-of-the-art performance on two different open-\\nmultimodal-QA datasets, both of which require\\nretrieving images and text from a large corpus to\\nanswer factoid questions: WebQA (Chang et al.,\\n2022) and MultimodalQA (Talmor et al., 2021). On\\nboth datasets, we outperform sophisticated base-\\nlines (Li et al., 2020; Radford et al., 2021; Zhang\\net al., 2021) by 10-20% accuracy under both dis-\\ntractor (from 40+ candidates) and full-wiki settings\\n(from 1M candidates). We also perform a compre-\\nhensive study to ablate different components of the\\npre-training to see their contributions. These em-\\npirical results demonstrate the effectiveness of our\\nproposed models to integrate multimodal knowl-\\nedge into pre-trained generation models and pave\\nthe way to uniÔ¨Åed retrieval-augmented frameworks.\\n2 Related Work\\nRetrieval Augmented Models Retrieval aug-\\nmented models are hybrid models containing\\nboth parameterized sequence models and a non-\\nparametric memory, infusing world knowledge into\\nexisting language models. Among them, KNN-\\nLM (Khandelwal et al., 2019) was Ô¨Årst proposed\\nto retrieve instances from a text training corpus to\\nhelp language modeling. Later, RETRO (Borgeaud\\net al., 2021) was proposed to scale up the text cor-\\npus to trillions of tokens, enabling the model to\\nachieve similar perplexity to GPT-3 (Brown et al.,\\n2020) with 25x fewer model parameters. Another\\nfamily of models, such as REALM (Guu et al.,\\n2020), RAG (Lewis et al., 2020), and FiD (Izacard\\nand Grave, 2021), integrate Wikipedia passages as\\na datastore to beneÔ¨Åt downstream knowledge in-\\ntensive tasks ( e.g.Question Answering). REALM\\nis an encoder-only model trained with masked lan-', metadata={'source': './data\\\\MuRAG-Multimodal Retrieval-Augmented Generator.pdf', 'page': 1}),\n",
       " Document(page_content='guage modeling, while RAG and FiD adopt an\\nencoder-decoder model with a generative language\\nmodeling objective. Compared to them, MuRAG\\nis the Ô¨Årst retrieval-augmented model that is ca-\\npable of using knowledge presented in multiple\\nmodalities ( i.e.visual and textual knowledge data),\\nwhereas all prior methods are restricted to using\\ntext-only knowledge.\\nMultimodal Transformers Multimodal trans-\\nformers have demonstrated strong performances\\nin learning cross-modal representation that are gen-\\nerally beneÔ¨Åcial on downstream vision and lan-\\nguage tasks, such as image-text retrieval (Karpa-\\nthy and Fei-Fei, 2015), image captioning (Chen\\net al., 2015), and VQA (Antol et al., 2015). These\\nmethods typically learn a joint transformer model\\non top of unimodal visual and textual backbones,\\nvia fusing deep features from each modality. The\\nearly version of multimodal transformers (Lu et al.,\\n2019; Chen et al., 2020; Li et al., 2020) usually\\nlearns a Transformer on pre-extracted unimodal\\nfeatures for contextualization, which makes it im-\\npossible to adjust those unimodal features to the\\ntarget tasks. Recently, SimVLM (Wang et al., 2022)\\nand COCA (Yu et al., 2022) proposed end-to-end\\ntraining for both deep multimodal transformers and\\nunimodal featurization networks and demonstrated\\nstrong performance in both multimodal and uni-\\nmodal downstream tasks. The multimodal memory\\nencoder of MuRAG is broadly similar to SimVLM\\nand CoCa, but has a different focus to encode and\\nretrieve multimodal knowledge ( i.e.images and\\ntexts) to augment language generation models.\\nMultimodal Question Answering The problem\\nof multimodal question answering has been ex-\\ntensively studied. VQA was the Ô¨Årst proposed to\\nanswer questions from visual-only inputs. Later,\\nOK-VQA (Marino et al., 2019) enlarged VQA‚Äôs\\nscope to annotate questions requiring both image\\nand implicit textual/common-sense knowledge to\\nanswer. More recently, MuMuQA (Reddy et al.,\\n2021), ManyModelQA (Hannan et al., 2020) and\\nMIMOQA (Singh et al., 2021) provide questions\\nwhich require reasoning over images and explicitly\\nprovided text snippets. However, these datasets\\nare restricted to dealing with given text and images\\nwithout requiring any retrieval from the web: they\\nare analogous to machine-reading approaches to\\nQA from text like SQuAD, rather than open-book\\nQA. To study the more realistic open multimodal\\nQA task, WebQA (Chang et al., 2022) and Multi-modalQA (Talmor et al., 2021) have been proposed\\nto evaluate answers to open queries which require\\nretrieving and reasoning over a large-scale web\\nmultimodal corpus. Our model uses these datasets\\nto study open-world multimodal question answer-\\ning, obtaining state-of-the-art results.\\n3 Model\\n3.1 Backbone Encoder\\nFigure 3: Backbone encoder: ViT encodes image\\npatches into a sequence of vectors eI, while word em-\\nbedding converts text tokens into another sequence of\\nvectors eT. These vectors are concatenated to form\\nfŒ∏(e)and fed to a decoder for text generation.\\nMuRAG is built on top of a simpler model we\\ncall a ‚Äúbackbone‚Äù model, which is pre-trained to\\nencode image-text pairs such that they are suitable\\nfor both answer generation and retrieval. The back-\\nbone model‚Äôs encoder is used as a component of\\nthe MuRAG model. The backbone model is built\\nwith a pre-trained visual Transformer (Dosovitskiy\\net al., 2020) and a T5 text Transformer (Raffel et al.,\\n2020), and consists of a multimodal encoder fŒ∏and\\ndecoder gŒ∏. The encoder takes as input a sequence\\nof image-text pairs, where either the image or the\\ntext component can be empty to accommodate text-\\nonly and image-only cases.\\nAs depicted in Figure 3, the encoder can take a\\nsequence of images and text. For image input, we\\nÔ¨Årst split each into 16x16 patches and feed them\\nto a ViT (Dosovitskiy et al., 2020) transformer to\\ngenerate a sequence of visual embedding denoted\\naseI‚ààRLi√óD, where Liis the length of the im-\\nage tokens. For text input, we use word embedding\\nto produce another sequence of textual embedding\\neT‚ààRLt√óD. Forkimages and ntext inputs, we\\nconcatenate all their embeddings in the input or-\\nder as e= [e1\\nI;e1\\nT;¬∑¬∑¬∑;ek\\nI;en\\nT]‚ààR(kLt+nLi)√óD,\\nwhich is fed to another bi-directional transformer\\nfŒ∏initialized from T5. We enable cross-attention', metadata={'source': './data\\\\MuRAG-Multimodal Retrieval-Augmented Generator.pdf', 'page': 2}),\n",
       " Document(page_content='between the two modalities to produce a fused rep-\\nresentation, denoted as fŒ∏(e)‚ààR(kLt+nLi)√óD.\\nWe add a [CLS] token to obtain a pooled repre-\\nsentation fŒ∏(e)[CLS]‚ààRDfor dense retrieval.\\n3.2 MuRAG\\nWe build MuRAG (shown in Figure 4) on top of\\nthe backbone model. During the retriever stage,\\nMuRAG takes a query qof any modality as in-\\nput and retrieves from a memory Mof image-text\\npairs. SpeciÔ¨Åcally, we apply the backbone encoder\\nfŒ∏to encode a query q, and use maximum inner\\nproduct search (MIPS (Guo et al., 2020)) over all of\\nthe memory candidates m‚ààM to Ô¨Ånd the Top-K\\nnearest neighbors TopK(M|q) = [m1,¬∑¬∑¬∑, mk].\\nFormally, we deÔ¨Åne TopK(M|q)as follows:\\nTopK(M|q) =TopK\\nm‚ààMfŒ∏(q)[CLS]¬∑fŒ∏(m)[CLS]\\nDuring the reader stage, the retrievals (the raw im-\\nage patches) are combined with the query qas\\nan augmented input [m1,¬∑¬∑¬∑, mk, q], which is fed\\nto the backbone encoder fŒ∏to produce retrieval-\\naugmented encoding. The decoder model gŒ∏uses\\nattention over this representation to generate tex-\\ntual outputs y=y1,¬∑¬∑¬∑, yntoken by token.\\np(yi|yi‚àí1) =gŒ∏(yi|fŒ∏(TopK(M|q);q);y1:i‚àí1)\\nwhere yis decoded from a given vocabulary V.\\n3.3 Pre-training\\nThe pre-training implementation is depicted in the\\nupper portion of Figure 4, where the input query\\nis an image xIplus a text prompt xp. The exter-\\nnal memoryMcontains textual-only entries mT.\\nThe Top-K retrievals mT\\n1,¬∑¬∑¬∑, mT\\nkare leveraged to\\ngenerate the textual output. To avoid the excessive\\ncomputation cost of backpropagation over the mas-\\nsive external memory, we adopt an in-batch mem-\\noryMB, dynamically constructed from the input\\nexamples in a batch. The small in-batch memory\\nenables MuRAG to continuously update the mem-\\nory encoder efÔ¨Åciently similar to TOME (de Jong\\net al., 2022) and QAMAT (Chen et al., 2022).\\nDataset The pre-training corpus consists of\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption-12M+3M (CC) (Sharma et al., 2018;\\nChangpinyo et al., 2021), VQA (Antol et al., 2015)\\nand PAQ (Lewis et al., 2021) Table 1. LAION is\\na publicly-released image-text dataset containingcrawled image-text pairs Ô¨Åltered by CLIP (Rad-\\nford et al., 2021). We apply rules to Ô¨Ålter LAION\\nfrom 400M to 200M by removing text with HTTP-\\nURLs or image width/height beyond 1000 pixels.\\nCC contains 15M (image, anonymized alt-text)\\npairs crawled from the web but Ô¨Åltered more ex-\\ntensively to maintain high alignment quality. VQA\\ncontains annotated QA pairs aligned to MSCOCO\\nimages. We further add captions to each image\\nfrom MSCOCO-Captioning (Lin et al., 2014) to\\ncreate (Image, Caption, QA) triples. PAQ is a text-\\nonly dataset containing 65M machine-generated\\nQA pairs along with their source Wikipedia pas-\\nsage.\\nDataset #Size Format Source\\nCC 15M (Image, Caption) Crawled\\nLAION 200M (Image, Alt-Text) Crawled\\nPAQ 65M (Passage, QA) Generated\\nVQA 400K (Image, Caption, QA) Annotated\\nTable 1: Pre-training Dataset Statistics\\nFor LAION and CC, we use the input image as\\nxI, and ‚Äògenerate caption:‚Äô as the text prompt xp.\\nFor VQA, we use the input image as xIand the\\nquestion as the prompt xp. For PAQ, we use an\\nempty array as the input image and the question\\nas the prompt. The in-batch memory MBis con-\\nstructed by stacking the captions associated with\\nthe input images in LAION/CC/VQA and the pas-\\nsages associated with the questions in PAQ. Each\\ntextual memory entry is denoted as mT. The de-\\ncoder is optimized to generate either a caption or\\nan answer, depending on the source dataset. Since\\nthe four dataset sizes are highly unbalanced, we\\nuse Ô¨Åxed mixture sampling ratios to balance their\\npresence during pre-training.\\nWe train the model with a joint loss L=Lgen+\\nLconas follows:\\nLcon=‚àílogexp(fŒ∏(xI, xp)¬∑fŒ∏(mT))‚àë\\nm‚ààMBexp(fŒ∏(xI, xp)¬∑fŒ∏(mT))\\nLgen=‚àíloggŒ∏(y|fŒ∏(Mp;xI;xp))\\nMp={\\nTopK(MB|xI, xp)If(xI, xp)‚ààPAQ/VQA\\n√ò If (xI, xp)‚ààLAION/CC\\nwhere Mpis the retrieved augmentation: if the\\ninput query is from PAQ/VQA, we use the retrieved\\nmemory entries, otherwise, we use null. The reason\\nfor setting it to null for LAION/CC is to avoid a\\ntrivial solution when the generation target (caption)\\nalso exactly appears in the memory.\\nThe contrastive loss Lconis minimized to dis-\\ncriminate between the positive query-memory pairs', metadata={'source': './data\\\\MuRAG-Multimodal Retrieval-Augmented Generator.pdf', 'page': 3}),\n",
       " Document(page_content='Figure 4: Model Architecture: the model accesses an external memory to obtain multimodal knowledge contained\\nin images or text snippets, which is used to augment the generation. The upper part deÔ¨Ånes the pre-training\\nimplementation, while the lower part deÔ¨Ånes Ô¨Åne-tuning implementation.\\nand all other query-memory pairs from the mem-\\nory. The pairwise matching score is computed as\\nthe dot product between query fŒ∏(xI;xp)[CLS] and\\ncandidates fŒ∏(mT)[CLS]. This objective enables\\nthe model to retrieve the most relevant knowledge\\nfrom the memory. The generative loss Lgenis min-\\nimized to generate target tokens yconditioned on\\nthe retrieval-augmented representation. This ob-\\njective enables the model to combine information\\nacross different modalities for text generation.\\n3.4 Fine-tuning\\nWe Ô¨Ånetune MuRAG to align with the expected\\ninputs of the downstream datasets which require an-\\nswering text questions by retrieving image-caption\\npairs or text snippets from the external knowledge\\ndatastore. As depicted in the lower part of Figure 4,\\nthe input query for the downstream task is a text\\nquestion xq, and the memory Mcontaining (im-\\nage, text) pairs (mI, mT).1The Top-K retrievals\\n{(mI\\n1, mT\\n1),¬∑¬∑¬∑,(mI\\nk, mT\\nk)}are leveraged to gen-\\nerate the answer a. To minimize the computation\\ncost, we develop a two-stage pipeline to optimize\\nwith an in-batch memory and then resume with\\nÔ¨Åxed retrieval from global memory.\\nIn-Batch Training In this stage, we aim to mini-\\nmize the joint loss function L=Lcon+Lgenbased\\n1We set the image to a zero array if the memory entry is a\\ntext snippet.on the in-batch memory MBas follows:\\nLcon=‚àílogexp(fŒ∏(xq)¬∑fŒ∏(mI;mT))‚àë\\nm‚ààMBexp(fŒ∏(xq)¬∑fŒ∏(mI;mT))\\nLgen=‚àíloggŒ∏(y|fŒ∏(TopK(MB|xq);xq))\\nThe in-batch memory MBis constructed in the\\nfollowing way: the k-th example in the dataset is\\nrepresented as (xq,k, yk,{mI\\ni, mI\\ni}k,{¬ØmI\\nj,¬ØmT\\nj}k),\\nwhere mrepresents the positive (image, text)\\nsource, and ¬Ømrepresents the hard negative\\n(image, text) source provided by the dataset2.\\nFor a batch with Bexamples, we assemble\\nall the associated positive and negative knowl-\\nedge source as our in-batch memory MB=\\n{{mI\\ni, mI\\ni}1,{¬ØmI\\nj,¬ØmT\\nj}1,¬∑¬∑¬∑,{¬ØmI\\nj,¬ØmT\\nj}B}.\\nFixed-Retrieval Training After in-batch train-\\ning, we encode all available cross-modal pairs, and\\nindex these encodings for fast MIPS retrieval. We\\nthen apply the trained retriever to search over the\\nfull multimodal corpus Mto obtain the global top-\\nK retrievals TopK(M|xq)and continue to opti-\\nmizeLgen. During this training phase, the stored\\nencodings are not updated. During inference time,\\nwe use Ô¨Åxed encodings to generate the answers.\\n2These hard negatives are mined through Bing Search API\\nand Wikipedia page, refer to (Chang et al., 2022) for details.', metadata={'source': './data\\\\MuRAG-Multimodal Retrieval-Augmented Generator.pdf', 'page': 4}),\n",
       " Document(page_content='4 Experiments\\n4.1 Implementation Details\\nThe backbone model uses T5-base (Raffel et al.,\\n2020) and a ViT-large model (Dosovitskiy et al.,\\n2020) as described in Table 2. We adopt the\\nsentence-piece model from T5 with a vocabulary\\nsize of 32128. The ViT model was pre-trained\\non the JFT dataset. We resize every image into\\n224x224 pixels and split them into a sequence of\\n16x16 patches. The output of ViT is a sequence\\nof 1024-dimension vectors, which are projected\\nto 768-dimension for consistency with T5 model.\\nMuRAG reuses the model as retriever and reader,\\nthus the full model size is 527M parameters.\\nModel #Enc #Dec Hidden Heads Params\\nViT-large 24 0 1024 16 307M\\nT5-base 12 12 768 12 220M\\nTable 2: The model size and conÔ¨Ågurations, with\\n#Enc/#Dec denoting encoder/decoder layers.\\nOur model is implemented in JAX (Bradbury\\net al., 2018), based on the T5X codebase (Roberts\\net al., 2022). During pre-training, we Ô¨Årst train the\\nmodel on LAION for 1M steps, and then continue\\ntraining on CC/PAQ/VQA with 1:1:1 sample ratio\\nfor another 200K steps. We optimize the model\\nwith Adafactor (Shazeer and Stern, 2018). For both\\nstages, we adopt a constant learning rate of 5e-4\\nand a batch size of 4096. The models are trained\\non 64 Cloud v4 TPUs (Jouppi et al., 2020).\\nWe then Ô¨Åne-tune MuRAG on WebQA and Mul-\\ntimodalQA with a constant learning rate of 3e-4\\nfor 20K steps. The checkpoint with the highest\\nvalidation score is run on the test set. We use a\\nbatch size of 64 and set TopK=4 for both in-batch\\ntraining and Ô¨Åxed-retrieval training. We noticed\\nthat increasing Top-K further does not yield further\\nimprovement. We use a beam size of 2 to search\\nfor the best hypothesis for both datasets (increasing\\nit further doesn‚Äôt yield better performance).\\n4.2 Datasets\\nFor evaluation, we choose two multimodal QA\\ndatasets: WebQA (Chang et al., 2022) and Mul-\\ntimodalQA (Talmor et al., 2021) and demonstrate\\ntheir statistics in Table 3.\\nWebQA This dataset contains multi-hop, multi-\\nmodal question-answer pairs where all questions\\nare knowledge-seeking queries. The queries re-\\nquire 1-2 images or 1-2 text snippets to answer.Dataset Train Dev Test\\nImage/Text Image/Text Image/Text\\nWebQA 18K/17K 2.5K/2.4K 3.4K/4K\\nMultimodalQA 2.1K/7.4K 230/721 -\\nTable 3: Overall Statistics of downstream dataset.\\nEach query in WebQA is associated with a set of\\nvisual/text distractors (hard negatives). The an-\\nswers in WebQA are normally complete sentences\\nto better assess the model‚Äôs generation capabil-\\nity. Two evaluation setups are used, namely dis-\\ntractor and full-wiki. Under the distractor setup,\\nthe model needs to retrieve from these hard neg-\\natives + positives to answer the question. Under\\nthe full-wiki setup, the model needs to search over\\n1.1M text and visual sources from Wikipedia to an-\\nswer the question. For evaluation, WebQA uses\\nBARTScore (Yuan et al., 2021) to measure the\\nÔ¨Çuency between the generation and the reference,\\nand keyword accuracy score to measure the cor-\\nrectness/truthfulness of the generation. These two\\nscores are multiplied to calculate the overall score.\\nMultimodalQA-Subset This dataset contains\\nhuman-annotated multimodal questions over differ-\\nent modalities including tables, text, and images.\\nWikipedia tables are used as anchors to connect dif-\\nferent modalities. The authors Ô¨Årst use the template\\nto generate questions and then ask crowd-workers\\nto Ô¨Ålter and paraphrase the generated questions.\\nSince tables are outside the scope of our paper, we\\nfocus on the subset of queries requiring only text\\nand image information. SpeciÔ¨Åcally, we choose the\\nquestions with types of ‚ÄòTextQ‚Äô and ‚ÄòImageQ‚Äô to\\nconstruct the subset. The query requires 1 image\\nor 1 text snippet to answer. Each query in Multi-\\nmodalQA is also associated with visual and text dis-\\ntractors (hard negatives). Similarly, two evaluation\\nsetups are used as before. Under a full-wiki setup,\\nMultimodalQA uses a database containing 500K\\ntext and visual sources. The evaluation scores are\\nbased on Exact Match and F1.\\n4.3 Baselines\\nFor WebQA and MultimodalQA, we mainly\\ncompare different variants of pre-trained vision-\\nlanguage models.\\nVLP In WebQA, VLP-like models (Zhou et al.,\\n2020) like Oscar (Li et al., 2020) and VinvL (Zhang\\net al., 2021) are used as the standard baselines.\\nThese models were pre-trained on Conceptual', metadata={'source': './data\\\\MuRAG-Multimodal Retrieval-Augmented Generator.pdf', 'page': 5}),\n",
       " Document(page_content='3M (Sharma et al., 2018) with a masked language\\nobjective. During Ô¨Åne-tuning, the VLP model takes\\na set of token inputs <[CLS], si, [SEP], Q, [SEP]>\\nÔ¨Årst to select the most plausible source si, and then\\nfeedsiin the form of <[CLS], S,Q,A, [SEP]>\\nto autoregressively decode answer Awith masked\\nlanguage model prediction.\\nAutoRouting In MultimodalQA, this method\\nÔ¨Årst applies a question type classiÔ¨Åer to detect the\\nmodality of the question (either a passage or an\\nimage), and then routes the question to its sub-\\nmodel. The method uses RoBERTa-large (Roberts\\net al., 2022) for text-questions and VilBERT (Lu\\net al., 2019) with features extracted from Faster-\\nRCNN (Ren et al., 2015) for image questions.\\nCLIP (K) CLIP (Radford et al., 2021) is used for\\nfull-wiki retrieval. SpeciÔ¨Åcally, the baselines sys-\\ntems adopt CLIP to encode queries and all the im-\\nage/text candidates separately into vectors and then\\nrun approximated nearest neighbor searches to Ô¨Ånd\\na set of K potential candidates. After the coarse-\\nlevel retrieval without cross-attention, it adopts a\\nreranker to further narrow down to the 1-2 candi-\\ndates to feed as input Sto the QA model.\\n4.4 Experimental Results\\nWe demonstrate WebQA‚Äôs results in Table 4. All\\nresults reported are the medium score from three\\nruns with different random seeds, and the variance\\nof the Overall score is within 0.2%. We can observe\\nthat MuRAG can signiÔ¨Åcantly outperform VLP\\nwith different backends including Oscar, ResNet,\\nand VinVL. In retrieval performance, our model\\noutperforms VLP by 15% in the full-wiki setting.\\nFor Fluency, our model outperforms VLP by 12%\\nunder the distractor setting and 14% under the full-\\nwiki setting. For Accuracy, our model manages\\nto achieve 16% under the distractor setting and\\neven 20% the under the full-wiki setting. These\\nimprovements reÔ¨Çect the high Ô¨Çuency and accuracy\\nof MuRAG‚Äôs generation, and the improvement is\\nmore pronounced for full wiki.\\nWe show the MultimodalQA results in Table 5.\\nWe can see that MuRAG is also able to vastly\\noutperform the routing-based multimodality QA\\nmodel. For text questions, our model improves\\nover AutoRouting by 10+% EM under both set-\\ntings. For image questions, the gap becomes more\\nsigniÔ¨Åcant, with 20+% improvement under both\\nsettings. Similarly, we Ô¨Ånd that our model is more\\ncapable of handling full-wiki corpus.Evaluation Distractor\\nMetrics Retr FL Accuracy Overall\\nQuestion-Only - 34.9 22.2 13.4\\nVLP (Oscar) 68.9 42.6 36.7 22.6\\nVLP + ResNeXt 69.0 43.0 37.0 23.0\\nVLP + VinVL 70.9 44.2 38.9 24.1\\nMuRAG 74.6 55.7 54.6 36.1\\nEvaluation Full-Wiki\\nCLIP (2) + VLP 11.9 34.2 24.1 14.6\\nCLIP (20) + VLP 24.0 36.1 27.2 16.1\\nMuRAG 39.7 50.7 47.8 31.5\\nTable 4: WebQA ofÔ¨Åcial test-set results indicated\\non leaderboard3as of May 2022. Retr denotes\\nthe retrieval-F1 score. FL refers to Ô¨Çuency metric\\nBARTSCcore, and Accuracy refers to keyword match-\\ning F1 score, they are combined as Overall.\\nEvaluation Distractor\\nMetricsText Image All\\nEM F1 EM F1 EM\\nQuestion-Only 15.4 18.4 11.0 15.6 13.8\\nAutoRouting 49.5 56.9 37.8 37.8 46.6\\nMuRAG 60.8 67.5 58.2 58.2 60.2\\nEvaluation Full-Wiki\\nMetricsText Image All\\nEM F1 EM F1 EM\\nCLIP (10) +\\nAutoRouting35.6 40.2 32.5 32.5 34.7\\nMuRAG 49.7 56.1 56.5 56.5 51.4\\nTable 5: Multimodal dev-set results on the subset.\\n4.5 Ablation Study\\nHere we ablate the properties of MuRAG to better\\nunderstand our experimental results.\\nPre-training Corpus In order to study the contri-\\nbutions of different pre-training corpora, we investi-\\ngated several pre-training corpus combinations. We\\nreport their Ô¨Åne-tuned results on WebQA test set\\nin Table 6. As can be seen, without any pre-training,\\nour model only achieves an overall score of 23.5,\\nwhich lags behind the baseline models. After pre-\\ntraining on different singular datasets, MuRAG is\\nable to achieve better performance than the base-\\nlines. Among the individual datasets, LAION is\\nshown to yield the highest score, and adding CC,\\nPAQ, and VQA to the pre-training corpus set one\\nby one produces steady improvements.\\nTwo-Stage Fine-tuning In order to study the ne-\\ncessity of the two-stage Ô¨Åne-tuning, we perform an\\nablation study to see the impact of the two stages.\\nWe display our results in Table 7. (Only In-Batch)', metadata={'source': './data\\\\MuRAG-Multimodal Retrieval-Augmented Generator.pdf', 'page': 6}),\n",
       " Document(page_content='Pre-train Dataset FL Accuracy Overall\\nNone 42.5 36.1 23.5\\nCC 46.4 41.3 25.6\\nLAION 47.8 44.8 28.3\\nVQA 47.0 44.4 27.4\\nPAQ 46.8 42.8 27.0\\nLAION+CC 49.5 47.4 30.7\\nLAION+CC+PAQ 53.7 51.8 34.4\\nLAION+CC+PAQ+VQA 55.7 54.6 36.1\\nTable 6: Ablation Study for different pre-training cor-\\npus, score under distractor setting.\\nModel WebQA Multimodal\\nMuRAG (Only In-Batch) 29.4 49.6\\nMuRAG (Only Fixed-Retrieval) 25.8 40.7\\nMuRAG (Two Stage) 31.5 51.4\\nTable 7: Ablation Study for different Ô¨Åne-tuning stages\\nto see their contributions. WebQA uses the overall\\nscore, and MultimodalQA refers to EM-all score.\\nEvaluation Model Correct Wrong\\nDistractorMuRAG (Text) 80% 20%\\nMuRAG (Image) 64% 36%\\nFull-WikiMuRAG (Text) 72% 28%\\nMuRAG (Image) 54% 46%\\nTable 8: The human evaluation results on WebQA\\ndataset separately for image/text queries.\\nrefers to the model trained only with in-batch mem-\\nory are directly used to generate outputs by access-\\ning the global memory. Without further tuning,\\nthe performance will drop by roughly 2% on both\\ndatasets. (Only Fixed-Retrieval) refers to using the\\npre-trained retriever directly to obtain Top-K and\\nthen optimize the generative loss. As can be seen,\\nthe performance drop is more severe in this case\\nfor both datasets. This is understandable due the\\nmisalignment between pre-training retrieval is (im-\\nage + text->text) while the Ô¨Åne-tuning retrieval is\\n(text -> image+text). Thus, it is necessary to adapt\\nthe MuRAG‚Äôs pre-trained retriever to different use\\ncases depending on the downstream datasets.\\n4.6 Human Analysis\\nIn order to better understand the model‚Äôs perfor-\\nmance, we manually study 200 model outputs and\\nclassify them into three categories and show our\\nmanual analysis results in Table 8. As can be seen,\\nimage queries are much harder than text queries.\\nMuRAG only achieves 64% accuracy for the dis-\\ntractor setting and 54% accuracy for the full-wiki\\nsetting, falling signiÔ¨Åcantly behind text accuracy.\\nWe further categorize the image-query errors\\nFigure 5: Upper left: correct prediction, Upper Right:\\nerror due to miscounting, Lower: error due to misrecog-\\nnition (multiple image reasoning). Q refers to the ques-\\ntion, P refers to prediction and R refers to the reference.\\nmanually into the categories of Table 9. Counting\\nis the most difÔ¨Åcult question type, and constitutes\\n52% of the total errors, while object recognition\\nerrors rank second, constituting 29% of errors. In\\ncontrast, identifying color, shape, and gender is\\ncomparatively easier, with fairly low error rates.\\nWe demonstrate some correct and typical error\\ncases in Figure 5 including miscounting and mis-\\nrecognizing objects. We observe that these errors\\nare mostly due to several reasons: 1) the question\\nis related to infrequent objects, thus making recog-\\nnition errors, 2) the image scene is highly complex\\nwith a large number of objects, thus grounding to a\\nspeciÔ¨Åc region is difÔ¨Åcult, 3) the questions require\\noptical character recognition ability from images.\\nHence, the bottleneck of MuRAG is still in the\\nvisual understanding module.\\nCategory Count Object Color Shape Gender\\nRatio 52% 29.4% 5.8% 5.8% 5.8%\\nTable 9: Error categorization and their ratios on sam-\\npled WebQA-dev image queries.\\n5 Examples\\nWe list more examples in Figure 6 and Figure 7.\\nAs can be seen, in the Ô¨Årst example, the model is', metadata={'source': './data\\\\MuRAG-Multimodal Retrieval-Augmented Generator.pdf', 'page': 7}),\n",
       " Document(page_content='grounded on the oracle image-text pair to make the\\ncorrect prediction. However, in the second exam-\\nple, though the model retrieves the wrong image-\\ntext pair, it is able to make the correct prediction of\\n‚Äòthe angel is holding a dead body‚Äô. We conjecture\\nthat the model utilizes textual clues to make the pre-\\ndiction rather than grounding on the image itself.\\nSuch shortcut learning is concerning and needs to\\nbe addressed through better learning algorithms.\\nFigure 6: Examples: we demonstrate model retrieval\\nvs. groundtruth and model answer vs. reference.\\n6 Conclusion\\nIn this paper, we build the Ô¨Årst visually-grounded\\nlanguage generator capable of retrieving multi-\\nmodal knowledge from a large-scale corpus. Our\\nexperiments show the promise of this approach, as\\nit outperforms existing baselines by a large margin.\\nAt the same time, the performance on knowledge-\\nseeking queries that require reasoning over images\\nis still signiÔ¨Åcantly lower than the performance on\\nqueries requiring only text. This indicates that there\\nis still ample room for further improvements and\\nwe hope our study can motivate more research on\\nbetter multimodal retrieval-augmented models.\\nLimitations\\nThe current approach has several limitations: 1)\\nsince we do not mine hard negatives during pre-\\ntraining, negatives come from other examples\\nwithin the same batch. This requires that we set the\\nbatch size sufÔ¨Åciently large enough to collect hard-\\nenough negatives. This results in the pre-training\\nFigure 7: Examples: we demonstrate model retrieval\\nvs. groundtruth, and model answer vs. reference.\\nrequiring a large number of computation resources\\nto reach competitive retrieval abilities. 2) our pre-\\ntraining corpus‚Äôs format (image -> text) is differ-\\nent from Ô¨Åne-tuning (text -> image+text). This\\nmisalignment limits the model‚Äôs performance. Fu-\\nture work should consider how to design a better-\\naligned pre-training objective to achieve better per-\\nformance. 3) Current visual representation in the\\nreader stage is relatively expensive, i.e. 16x16=196\\ntokens per image, which poses great challenges for\\nthe transformer encoder to scale up to large Top-K\\nvalues due to the quadratic attention complexity.\\nEthical Statement\\nOur work uses the LAION dataset, a widely-used\\nand publicly available large-scale visual-language\\ncorpus crawled from the web. The authors have\\nconducted automatic Ô¨Åltering to greatly reduce\\nharmful content. However, it is not possible to\\nfully remove all of the potential risks from the data\\ngiven its tremendous size. Being trained on this\\ndataset, we anticipate our model to contain some\\nbiases (racial, gender, etc.). During our manual\\ninspection, we saw some such biases, for example,\\n5% of errors are caused by misrecognition of gen-\\nder. However, there are other many other forms of\\nbiases that we cannot fully enumerate or observe', metadata={'source': './data\\\\MuRAG-Multimodal Retrieval-Augmented Generator.pdf', 'page': 8}),\n",
       " Document(page_content='explicitly.\\nReferences\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\\nand Devi Parikh. 2015. Vqa: Visual question an-\\nswering. In Proceedings of the IEEE international\\nconference on computer vision , pages 2425‚Äì2433.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\nGeorge van den Driessche, Jean-Baptiste Lespiau,\\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\\ning language models by retrieving from trillions of\\ntokens. arXiv preprint arXiv:2112.04426 .\\nJames Bradbury, Roy Frostig, Peter Hawkins,\\nMatthew James Johnson, Chris Leary, Dougal\\nMaclaurin, George Necula, Adam Paszke, Jake\\nVanderPlas, Skye Wanderman-Milne, and Qiao\\nZhang. 2018. JAX: composable transformations of\\nPython+NumPy programs.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems , 33:1877‚Äì1901.\\nYingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\\nhong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.\\nWebqa: Multihop and multimodal qa. The Confer-\\nence on Computer Vision and Pattern Recognition .\\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\\nscale image-text pre-training to recognize long-tail\\nvisual concepts. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recog-\\nnition , pages 3558‚Äì3568.\\nWenhu Chen, Pat Verga, Michiel de Jong, John Wi-\\neting, and William Cohen. 2022. Augmenting\\npre-trained language models with qa-memory for\\nopen-domain question answering. arXiv preprint\\narXiv:2204.04581 .\\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\\nishna Vedantam, Saurabh Gupta, Piotr Doll√°r, and\\nC Lawrence Zitnick. 2015. Microsoft coco captions:\\nData collection and evaluation server. arXiv preprint\\narXiv:1504.00325 .\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\\nJingjing Liu. 2020. Uniter: Universal image-text\\nrepresentation learning. In European conference on\\ncomputer vision , pages 104‚Äì120. Springer.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, et al. 2022. Palm: Scaling\\nlanguage modeling with pathways. arXiv preprint\\narXiv:2204.02311 .\\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\\nald, Fei Sha, and William Cohen. 2022. Mention\\nmemory: incorporating textual knowledge into trans-\\nformers through entity mention attention. ICLR .\\nAlexey Dosovitskiy, Lucas Beyer, Alexander\\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\\nAn image is worth 16x16 words: Transformers\\nfor image recognition at scale. In International\\nConference on Learning Representations .\\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\\nDavid Simcha, Felix Chern, and Sanjiv Kumar. 2020.\\nAccelerating large-scale inference with anisotropic\\nvector quantization. In International Conference on\\nMachine Learning , pages 3887‚Äì3896. PMLR.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\\nsupat, and Mingwei Chang. 2020. Retrieval aug-\\nmented language model pre-training. In Proceed-\\nings of the 37th International Conference on Ma-\\nchine Learning , volume 119 of Proceedings of Ma-\\nchine Learning Research , pages 3929‚Äì3938. PMLR.\\nDarryl Hannan, Akshay Jain, and Mohit Bansal. 2020.\\nManymodalqa: Modality disambiguation and qa\\nover diverse inputs. In Proceedings of the AAAI Con-\\nference on ArtiÔ¨Åcial Intelligence , volume 34, pages\\n7879‚Äì7886.\\nGautier Izacard and √âdouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open\\ndomain question answering. In Proceedings of the\\n16th Conference of the European Chapter of the As-\\nsociation for Computational Linguistics: Main Vol-\\nume, pages 874‚Äì880.\\nNorman P Jouppi, Doe Hyun Yoon, George Kurian,\\nSheng Li, Nishant Patil, James Laudon, Cliff Young,\\nand David Patterson. 2020. A domain-speciÔ¨Åc\\nsupercomputer for training deep neural networks.\\nCommunications of the ACM , 63(7):67‚Äì78.\\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\\nsemantic alignments for generating image descrip-\\ntions. In Proceedings of the IEEE conference\\non computer vision and pattern recognition , pages\\n3128‚Äì3137.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2019. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. In International Conference on Learning\\nRepresentations .\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\nt√§schel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. Advances in Neu-\\nral Information Processing Systems , 33:9459‚Äì9474.', metadata={'source': './data\\\\MuRAG-Multimodal Retrieval-Augmented Generator.pdf', 'page': 9}),\n",
       " Document(page_content='Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale\\nMinervini, Heinrich K√ºttler, Aleksandra Piktus, Pon-\\ntus Stenetorp, and Sebastian Riedel. 2021. Paq: 65\\nmillion probably-asked questions and what you can\\ndo with them. Transactions of the Association for\\nComputational Linguistics , 9:1098‚Äì1115.\\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\\nLi Dong, Furu Wei, et al. 2020. Oscar: Object-\\nsemantics aligned pre-training for vision-language\\ntasks. In European Conference on Computer Vision ,\\npages 121‚Äì137. Springer.\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,\\nand C Lawrence Zitnick. 2014. Microsoft coco:\\nCommon objects in context. In European confer-\\nence on computer vision , pages 740‚Äì755. Springer.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\\nolinguistic representations for vision-and-language\\ntasks. Advances in neural information processing\\nsystems , 32.\\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual\\nquestion answering benchmark requiring external\\nknowledge. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recogni-\\ntion, pages 3195‚Äì3204.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\\net al. 2021. Learning transferable visual models\\nfrom natural language supervision. In International\\nConference on Machine Learning , pages 8748‚Äì8763.\\nPMLR.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\\nthe limits of transfer learning with a uniÔ¨Åed text-to-\\ntext transformer. Journal of Machine Learning Re-\\nsearch , 21(140):1‚Äì67.\\nRevanth Gangi Reddy, Xilin Rui, Manling Li, Xudong\\nLin, Haoyang Wen, Jaemin Cho, Lifu Huang, Mo-\\nhit Bansal, Avirup Sil, Shih-Fu Chang, et al. 2021.\\nMumuqa: Multimedia multi-hop news question an-\\nswering via cross-media knowledge extraction and\\ngrounding. arXiv preprint arXiv:2112.10728 .\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\\nSun. 2015. Faster r-cnn: Towards real-time object\\ndetection with region proposal networks. Advances\\nin neural information processing systems , 28.\\nAdam Roberts, Hyung Won Chung, Anselm Lev-\\nskaya, Gaurav Mishra, James Bradbury, Daniel An-\\ndor, Sharan Narang, Brian Lester, Colin Gaffney,\\nAfroz Mohiuddin, et al. 2022. Scaling up mod-\\nels and data with t5x and seqio. arXiv preprint\\narXiv:2203.17189 .Christoph Schuhmann, Richard Vencu, Romain Beau-\\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\\nsuzaki. 2021. Laion-400m: Open dataset of clip-\\nÔ¨Åltered 400 million image-text pairs. arXiv preprint\\narXiv:2111.02114 .\\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\\nRadu Soricut. 2018. Conceptual captions: A\\ncleaned, hypernymed, image alt-text dataset for au-\\ntomatic image captioning. In Proceedings of the\\n56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers) , pages\\n2556‚Äì2565.\\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\\nAdaptive learning rates with sublinear memory cost.\\nInInternational Conference on Machine Learning ,\\npages 4596‚Äì4604. PMLR.\\nHrituraj Singh, Anshul Nasery, Denil Mehta, Aish-\\nwarya Agarwal, Jatin Lamba, and Balaji Vasan Srini-\\nvasan. 2021. Mimoqa: Multimodal input multi-\\nmodal output question answering. In Proceedings\\nof the 2021 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies , pages 5317‚Äì5332.\\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\\nnaneh Hajishirzi, and Jonathan Berant. 2021. Multi-\\nmodalqa: complex question answering over text, ta-\\nbles and images. In ICLR .\\nPat Verga, Haitian Sun, Livio Baldini Soares, and\\nWilliam Weston Cohen. 2021. Adaptable and inter-\\npretable neural memory over symbolic knowledge.\\nInProceedings of NAACL-HLT , pages 3678‚Äì3691.\\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\\nlia Tsvetkov, and Yuan Cao. 2022. Simvlm: Simple\\nvisual language model pretraining with weak super-\\nvision. ICLR .\\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\\nCoca: Contrastive captioners are image-text founda-\\ntion models. arXiv preprint arXiv:2205.01917 .\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text gener-\\nation. Advances in Neural Information Processing\\nSystems , 34.\\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\\nfeng Gao. 2021. Vinvl: Revisiting visual representa-\\ntions in vision-language models. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and\\nPattern Recognition , pages 5579‚Äì5588.\\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\\nHu, Jason Corso, and Jianfeng Gao. 2020. Uni-\\nÔ¨Åed vision-language pre-training for image caption-\\ning and vqa. In Proceedings of the AAAI Conference\\non ArtiÔ¨Åcial Intelligence , volume 34, pages 13041‚Äì\\n13049.', metadata={'source': './data\\\\MuRAG-Multimodal Retrieval-Augmented Generator.pdf', 'page': 10}),\n",
       " Document(page_content='A Pre-training\\nDuring Pre-trainnig, we found that directly train-\\ning with a mixture of all four datasets will lead to\\ninstability. We experimented with different vari-\\nants and found that a scheduled pre-training can\\nlead to a stable solution. We propose to Ô¨Årst pre-\\ntrain the model on the largest LAION dataset for\\n1M steps, and then continue training on the other\\nthree datasets with a Ô¨Åxed sample ratio. We plot\\nthe Ô¨Årst stage of LAION training in Figure 8. We\\nmonitor the generation quality (LAION image ->\\ntext captioning), and the retrieval quality (image ->\\n4096 in-batch caption retrieval). As can be seen,\\nthe LAION pre-training converges after 1M steps,\\nwhere we Ô¨Årst warm up and then decrease the learn-\\ning rate using a scheduler.\\nFigure 8: LAION Pre-training, validation accuracy,\\ngeneration Cider score and retrieval recall score from\\nthe in-batch memory.\\nWe further the pre-training on a mixture of the\\nother three datasets. We plot their inference eval-\\nuation scores in Figure 9. We can see that the\\nmodel is able to achieve very strong performance\\non these datasets, i.e. higher than 1.2 CiDEron CC12M+3M validation set. The model also\\nachieves strong performance on text-only reading\\ncomprehension on PAQ (similar to NQ), i.e. higher\\nthan 55% EM score. On the VQA dataset, the\\nmodel is able to achieve higher than 72% VQA ac-\\ncuracy on the validation set. These results demon-\\nstrate the efÔ¨Åciency and multi-tasking capabilities\\nof the pre-trained model. The overall retrieval\\naccuracy from the multimodal memory consist-\\ning of captions, and passages are plotted in Fig-\\nure 10, where the model is able to achieve 85%\\nRECALL@1 from a 4K memory.\\nB Model ConÔ¨Åguration\\nWe demonstrate the ViT conÔ¨Åguration as follows:\\n\" v i t _ c o n f i g \" : {\\n\" model \" : \" ViT \" ,\\n\" p a t c h e s \" : {\\n\" s i z e \" : [ 1 6 , 16]\\n} ,\\n\" h i d d e n _ s i z e \" : 1024 ,\\n\" i m a g e _ s i z e \" : [ 2 2 4 , 2 2 4 ] ,\\n\" num_heads \" : 16 ,\\n\" num_layers \" : 24 ,\\n\" mlp_dim \" : 4096 ,\\n\" r e t u r n _ p o o l e d _ o u t p u t \" : f a l s e ,\\n\" d r o p o u t _ r a t e \" : 0 . 1\\n} ,\\nWe demonstrate the T5-EncDec conÔ¨Åguration as\\nfollows:\\n\" m o d e l _ c o n f i g \" : {\\n\" v o c a b _ s i z e \" : 32128 ,\\n\" h i d d e n _ s i z e \" : 768 ,\\n\" i n t e r m e d i a t e _ d i m \" : 2048 ,\\n\" n u m _ a t t e n t i o n _ h e a d s \" : 12 ,\\n\" memory_key_dim \" : 768 ,\\n\" e n c o d e r _ l a y e r s \" : 12 ,\\n\" d e c o d e r _ l a y e r s \" : 12 ,\\n\" d r o p o u t _ r a t e \" : 0 . 1 ,\\n\" m a x _ d i s t a n c e \" : 128 ,\\n\" num_buckets \" : 32 ,\\n\" s c a l e \" : 1 . 0 ,\\n\" r e t r i e v a l _ w e i g h t \" : 0 . 5 ,\\n}', metadata={'source': './data\\\\MuRAG-Multimodal Retrieval-Augmented Generator.pdf', 'page': 11}),\n",
       " Document(page_content='Figure 9: Mixture Pre-training, CiDEr, EM, and VQA\\naccuracy for CC, PAQ, and VQA datasets.\\nFigure 10: Mixture Pre-training retrieval accuracy over\\nCC, PAQ, and VQA datasets.', metadata={'source': './data\\\\MuRAG-Multimodal Retrieval-Augmented Generator.pdf', 'page': 12}),\n",
       " Document(page_content='RETRIEVAL-GENERATION SYNERGY AUGMENTED LARGE LANGUAGE MODELS\\nZhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, Bing Qin\\nHarbin Institute of Technology, China\\nABSTRACT\\nLarge language models augmented with task-relevant docu-\\nments have demonstrated impressive performance on knowl-\\nedgeintensive tasks. However, regarding how to obtain effec-\\ntive documents, the existing methods are mainly divided into\\ntwo categories. One is to retrieve from an external knowledge\\nbase, and the other is to utilize large language models to gen-\\nerate documents. We propose an iterative retrieval-generation\\ncollaborative framework. It is not only able to leverage both\\nparametric and non-parametric knowledge, but also helps to\\nfind the correct reasoning path through retrieval-generation\\ninteractions, which is very important for tasks that require\\nmulti-step reasoning. We conduct experiments on four ques-\\ntion answering datasets, including single-hop QA and multi-\\nhop QA tasks. Empirical results show that our method signif-\\nicantly improves the reasoning ability of large language mod-\\nels and outperforms previous baselines.\\nIndex Terms ‚Äîlarge language models, retrieval aug-\\nmented, question answering\\n1. INTRODUCTION\\nLarge Language models (LLMs) have demonstrated im-\\npressive performance on diverse language tasks through in-\\ncontext learning [1, 2, 3, 4, 5, 6]. However, they still struggle\\nwith knowledge-intensive tasks that require access to a large\\namount of knowledge, such as open-domain question answer-\\ning [7] and commonsense reasoning [8], since the implicit\\nknowledge preserved in the parameters may be partial and\\ninsufficient. As shown in the top of Figure 1, one promising\\ndirection is to incorporate non-parametric knowledge to help\\nalleviate this problem with large language models.\\nRecent research shows that retrieving relevant documents\\nfrom an external datastore [9, 10, 11] or directly generating\\ncontextual documents from LLMs [12, 13] both can improve\\nLLMs‚Äô performance on knowledge-intensive tasks. The for-\\nmer, called retrieve-then-read, requires a retriever to retrieve\\nrelevant documents. The latter, known as generate-then-read,\\nleverages large language models to generate relevant docu-\\nments before answering questions. However, as shown in\\nFigure 1, the above two methods are isolated and lack co-\\nordination with each other. To fill this gap, in this paper, we\\nexplore an effective retrieval-generation collaboration frame-\\nWho is the lead singer of Depeche Mode?Depeche Mode currently consists of Dave Gahan (lead vocals, co-songwriting) and Martin Gore (keyboards, guitar, co-lead vocals, primary songwriting)‚Ä¶LLMsQuestionDocumentDave GahanAnswer\\nQuestionLLMsRetrieverLLMsRetrieverDocumentQuestion\\nDocumentQuestion\\nDocument(1) Retrieval(2) Generation(3) Retrieval-Generation SynergyFig. 1 : The top is the standard method utilizing LLMs for\\nquestion answering with relevant documents. The bottom\\nshows three methods to generate relevant documents.\\nwork to further improve the ability of large language models\\nto solve knowledge-intensive tasks.\\nIn this work, we present ITRG, an ITerative Retrieval-\\nGeneration synergy framework to generate relevant doc-\\numents that simultaneously exploits parametric and non-\\nparametric knowledge. In each iteration, ITRG consists of\\ntwo important steps: generation augmented retrieval (GAR)\\nand retrieval augmented generation (RAG). In the GAR step,\\nwe propose a simple and effective method to expand queries\\nby concatenating pseudo-documents generated from large\\nlanguage models and original questions. And expanded\\nqueries improve the accuracy of retrieving relevant docu-\\nments. In the RAG step, we use large language models to\\ncomprehensively understand retrieved documents to generate\\nnew documents for answering questions. We repeat these\\nsteps until we reach the maximum allowed number of itera-\\ntions. Through multiple retrieval generation collaborations,\\nour method aids in discovering the appropriate reasoning path\\nand providing correct answers to questions.\\nWe evaluate the efficacy of our method on 4 question\\nanswering datasets, including Natural Questions, TriviaQA,\\n2WikiMultiHopQA, and HotpotQA. Experimental results\\nshow that our method performs better than previous baselines\\non all datasets. In summary, our main contributions can be\\nsummarized as follows: (1) We propose ITRG, an iterative\\nretrieval-generation synergy framework using both paramet-\\nric and non-parametric knowledge. (2) We propose a simple\\nand effective generation-augmented retrieval strategy and\\ntwo retrieval-augmented generation strategies. (3) Empiri-\\ncal results show that ITRG outperforms previous retrieval-\\naugmented methods.arXiv:2310.05149v1  [cs.CL]  8 Oct 2023', metadata={'source': './data\\\\RAG on large language models.pdf', 'page': 0}),\n",
       " Document(page_content=\"2. ITERATIVE RETRIEVAL-GENERATION\\nSYNERGY\\nIn this section, we first introduce the overall framework, and\\nthen introduce the retrieval-generation collaboration frame-\\nwork in detail, including generation augmented retrieval and\\nretrieval augmented generation.\\n2.1. Overview\\nWe show the framework of ITRG in Figure 2. Given a user\\nquestion qand a document corpus D={di}|D|\\ni=1(i.e,diis a\\nWikipedia paragraph.), ITRG repeats generation augmented\\nretrieval (GAR) and retrieval augmented generation (RAG)\\nforTiterations. In the GAR process of iteration t, we con-\\ncatenate the output yt‚àí1of the last iteration and question qto\\nform a new query, and then use a dense retriever to retrieve\\ntop-kparagraphs. In the first iteration, we only use the ques-\\ntion as the query. In the RAG process of iteration t, based on\\nthe question qand the retrieved top- kparagraphs, we exploit\\nlarge language models to generate new paragraphs to answer\\nquestions. Specifically, we propose two methods to generate\\nnew paragraphs, which will be introduced in detail in ¬ß2.3.\\n2.2. Generation Augmented Retrieval\\nKnowledge-intensive tasks (e.g., open-domain question an-\\nswering) often require access to additional documents. A\\ncommon approach is to directly employ the question as the\\nquery, and then equip a sparse or dense retriever to retrieve\\nrelevant documents. In practice, we find that in some cases\\nusing the question directly as the query fails to retrieve rel-\\nevant documents because there may exist semantic gaps be-\\ntween them. To alleviate this problem, we propose a simple\\nquery expansion method. At the first iteration ( t= 1), we use\\nthe original question qas the query. At iteration t(t >1), we\\nconcatenate the original question qand the document gener-\\natedyt‚àí1in the last iteration as the new query qt= [q;yt‚àí1].\\nThen, we utilize a pre-trained dense retriever to retrieve top- k\\ndocuments, which are denoted as Rt={d}.\\nGiven an input question q, the retriever aims to retrieve\\na small set of documents from a corpus D={di}|D|\\ni=1that\\nare relevant to q. Following prior work [14], we use a dense\\nretriever based on the dual encoder architecture, where an en-\\ncoder is used to encode both the input context qand the docu-\\nmentd. Specifically, the encoder maps each document d‚àà D\\nto an embedding E(d)by taking the mean pooling of the last\\nhidden representation over the tokens in d. At query time,\\nthe same encoder is applied to the input context qto obtain a\\nquery embedding E(q). The similarity between the query em-\\nbedding and the document embedding is computed by their\\ncosine similarity: s(d, q) = cos( E(d),E(q)). The top- kdoc-\\numents that have the highest similarity scores are retrieved.\\nQuestion: What is the date of birth of Emilie HeghArntzen'smother?Retrieval:Generation:Retrieval:Generation:Retrieval:Generation:Iteration 1Iteration 2\\nIteration 3infoboxname: Emilie HeghArntzen; caption: HeghArntzenin 2018 ; birth_date: January 1, 1994 ; birth_place: Skien, Norway ; nationality: Norwegian ; Emilie HeghArntzenwas born on January 1, 1994 in Skien, Norway. Her mother is unknown.Camilla Marie Gjersemwas born together with a twin sister, Anne Line, on 6 January 1994 in H√∏nefoss, Norway. Their mother, PerlinaBangug, is a Filipina from Ilagan, Isabela, and their father, PetterGjersem, a Norwegian from Raufoss. Camilla Gjersemis a law student at the University of Oslo.Hanne Hegh(born 19 January 1960) is a Norwegian handball player. She played 220 matches for the Norwegian national handball team between 1978 and 1992. She is the mother of Emilie HeghArntzen.infoboxname: Hanne Hegh; caption: Hanne Hegh2008 ; nationality: Norwegian ; birth_date: April 27, 1960; birth_place: Oslo, Norway ; Hanne Heghwas born on April 27, 1960 in Oslo, Norway. She is the mother of Emilie HeghArntzen, who was born on January 1, 1994 in Skien, Norway.Fig. 2 : Iterative retrieval-generation synergy framework con-\\ntains two steps in each iteration: (1) generation augmented\\nretrieval (GAR): utilize the output of the previous iteration to\\nexpand the query to help retrieve more relevant documents;\\n(2) retrieval augmented generation (RAG): utilize retrieved\\ndocuments to generate new documents to answer questions.\\nWe only show three iterations in this figure for brevity. Solid\\narrows indicate RAG within an iteration, and dashed arrows\\nindicate GAR between iterations. Purple represents correct\\nand useful information, and red represents wrong or invalid\\ninformation.\\n2.3. Retrieval Augmented Generation\\nFollowing previous work [13], for a given question q, we\\ncould directly prompt large language models to generate re-\\nlated documents without retrieving them from an external cor-\\npus. However, we find that if only the parametric knowledge\\nlearned by the large model in the pre-training stage is used,\\nthe generated documents may be incomplete. Retrieval aug-\\nmented generation (RAG) aims to comprehensively under-\\nstand the retrieved non-parametric knowledge and the para-\\nmetric knowledge inside large language models to generate\\nmore accurate factual knowledge. Specifically, we propose\\ntwo strategies, which will be described in detail below.\\n2.3.1. Refine\\nAn intuitive idea is to refine the previously generated docu-\\nment yt‚àí1based on the original question qand the retrieved\\ntop-kdocuments at the current iteration step Rtto obtain a\\nnew document yt. We call this method refine. Considering\\nthat the document retrieved in the last iteration Rt‚àí1has been\\nused to generate the last document yt‚àí1, we refine the previ-\\nous output yt‚àí1with updated documents Rupdate .\\nRupdate =Rt‚àíRt‚àí1, (1)\\nyt=M(prompt ( yt‚àí1, q, R update )), (2)\", metadata={'source': './data\\\\RAG on large language models.pdf', 'page': 1}),\n",
       " Document(page_content='where Rupdate means that these documents are only retrieved\\nin the current iteration, not in the last iteration, Mdenotes a\\nwell pre-trained large language model. If Rupdate is an empty\\nset, we do not regenerate a new document and set yt=yt‚àí1.\\n2.3.2. Refresh\\nIn order to avoid the negative effect of errors or hallucinations\\nin the previously generated document yt‚àí1, we do not use\\nyt‚àí1, which is used in refine. We refresh the memory and let\\nthe large language models directly generate the document yt\\nbased on the retrieved document Rtand the original question\\nq. This method is named refresh.\\nyt=M(prompt ( q, R t)) (3)\\nBoth refine and refresh are implemented through prompts.\\nWe give the prompt corresponding to refresh.\\nPrompt for refresh with all documents\\nIn the following task, you should write a document\\nthat contains the answer to the question.\\nPassage: {Rt}\\nQuestion: {q}\\nDocument: {yt}\\n3. EXPERIMENTAL SETUP\\n3.1. Datasets\\nWe evaluate the effectiveness of ITRG on four open domain\\nquestion answering datasets, including Natural Questions\\n(NQ) [15], TriviaQA [16], 2WikiMultiHopQA [17] and Hot-\\npotQA [18]. Following previous works [19, 20], we randomly\\nsub-sample 500 examples from each dataset due to the cost\\nof running experiments. We evaluate our method in 0-shot,\\n1-shot and 5-shot settings. The few-shot demonstrations are\\nrandomly sampled from the data that is not involved in the\\nevaluation process.\\n3.2. Baselines\\nGPT-3.5 [21] We use text-davinci-002 and text-davinci-003\\nas our baselines. Text-davinci-002 is an InstructGPT model\\nwhile Text-davinci-003 is trained with reinforcement learn-\\ning with reward models trained from comparisons by humans.\\nVanilla LM The vanilla LM baselines prompt an LLM to di-\\nrectly generate an answer following the few-shot in-context\\nlearning paradigm [1]. CoT We follow [22] to generate both\\nthe chain-of-thought (CoT) reasoning process and the final\\nanswer. We only evaluate this method on multi-hop reasoning\\ndatasets in 5-shot setting1.Retrieve-then-Read The retrieve-\\n1We also conduct evaluation in 1-shot setting, but the final answer could\\nnot be generated according to the corresponding instructionsthen-read baseline consists of a well-pre-trained dense re-\\ntriever and a large language model. The retriever retrieves\\nrelevant documents for the question, and then the LLM con-\\nditions on both the question and retrieved documents to gen-\\nerate the answer. Generate-then-Read Generate-then-read\\nbaseline first uses few-shot prompts to generate a question-\\nrelated document, and then concatenates it with the question\\nto regenerate the answer.\\n3.3. Details\\nLLaMA [6] is an open source well trained large language\\nmodel. Considering the performance and computational cost\\nof the model, we use LLaMA 33B as the backend LLM. We\\nuse greedy decoding for both document generation and an-\\nswer generation, and set up to generate 200 tokens and 15\\ntokens respectively. We retrieve the top-5 paragraphs for each\\nquery and set the maximum number of iterations Tto 5. We\\ndirectly use the pre-trained dense retriever [23] and used the\\nDecember 2018 Wikipedia dump as the retrieval corpus for all\\ndatasets. Generated answers are evaluated with the standard\\nexact match metric (EM score): a generated answer is con-\\nsidered correct if it matches any answer of the list of answers\\nafter normalization. For this normalization step, we lower-\\ncase generated answers and remove articles, punctuation and\\nduplicate whitespaces.\\n4. RESULTS\\n4.1. Main Results\\nTable 1 reports the results on the single-hop question answer-\\ning datasets. In the 1-shot and 5-shot settings, the perfor-\\nmance of LLaMA-33B based Vanilla LM is very close to\\nthat of text-davinci-003. This shows LLaMA-33B is a strong\\nlanguage model, and it is reasonable to choose LLaMA-33B\\nas our backend LLM. Retrieve-then-read and generate-then-\\nread all exceed vanilla LM, verifying that adding relevant\\nexternal knowledge can improve the reasoning ability of\\nlarge language models. In addition, we observe that our itera-\\ntive retrieval-generation collaborative method ITRG achieves\\nstate-of-the-art performance on both datasets. Specifically,\\nITRG (refresh) performs better on the NQ dataset, and ITRG\\n(refine) performs better on the TriviaQA dataset.\\nTable 2 presents the results on the multi-hop question an-\\nswering datasets. We observe that LLaMA-33B is still com-\\nparable to text-davinci-003 on the multi-hop question answer-\\ning datasets. In addition, CoT can answer questions more\\naccurately than vanilla LM by generating reasoning process.\\nCompared with different baseline models, ITRG significantly\\nimproves the exact match scores. Specifically, on the 2Wiki-\\nMultiHopQA dataset, the exact match score of ITRG (refresh)\\nin the zero-shot setting is 32.2, which exceeds the perfor-\\nmance of vanilla LM in the 5-shot setting with a score of 31.8.\\nIn the 5-shot setting, ITRG (refresh) achieves 38.6 EM score', metadata={'source': './data\\\\RAG on large language models.pdf', 'page': 2}),\n",
       " Document(page_content='Table 1 : Exact match performance on single-hop question answering. All ITRG results are from the last iteration ( T= 5).\\nMethodNatural Questions TriviaQA\\n0-shot 1-shot 5-shot 0-shot 1-shot 5-shot\\nGPT 3.5Text-davinci-002 12.0 24.6 33.0 46.0 74.2 76.0\\nText-davinci-003 29.4 33.0 33.8 75.8 78.6 77.8\\nLLaMA 33BVanilla LM 27.0 29.4 32.4 74.8 70.8 75.8\\nRetrieve-then-Read 27.8 30.6 29.8 74.6 76.0 76.0\\nGenerate-then-Read 28.0 31.4 31.0 73.6 77.2 77.6\\nITRG (refine) 34.4 34.6 34.8 79.0 79.4 80.6\\nITRG (refresh) 37.6 38.4 38.0 77.0 78.6 79.4\\nTable 2 : Exact match performance on multi-hop question answering. All ITRG results are from the last iteration ( T= 5).\\nMethod2WikiMultiHopQA HotpotQA\\n0-shot 1-shot 5-shot 0-shot 1-shot 5-shot\\nGPT 3.5Text-davinci-002 16.4 27.6 30.8 12.2 20.2 22.2\\nText-davinci-003 27.2 27.0 29.8 25.0 25.8 26.6\\nLLaMA 33BVanilla LM 24.4 27.6 31.8 22.6 25.0 27.0\\nCOT - - 32.2 - - 28.6\\nRetrieve-then-Read 27.4 29.2 32.0 28.4 29.8 30.4\\nGenerate-then-Read 30.0 30.4 31.6 25.0 27.0 27.0\\nITRG (refine) 33.0 33.6 37.0 28.8 29.6 30.6\\nITRG (refresh) 32.2 36.2 38.6 31.0 32.6 33.4\\nTable 3 : Exact match performance of ITRG (refresh) at dif-\\nferent iterations in 5-shot setting.\\nIteration 1 2 3 4 5\\nNatural Questions 34.0 35.2 37.0 37.2 38.0\\nTriviaQA 79.8 79.2 79.8 79.8 79.4\\n2WikiMultiHopQA 34.8 37.4 37.2 38.6 38.6\\nHotpotQA 32.6 32.8 34.0 33.4 33.4\\nand improves by 6.8 points in absolute gains. Compared to\\nvanilla LM, ITRG (refresh) can improve the EM score by 9.4,\\n7.6, and 6.4 points respectively in 0-shot, 1-shot, and 5-shot\\nsettings on the Hotpotqa dataset.\\n4.2. Performance at Different Iterations\\nIn this section, we analyze the performance of our model and\\nthe quality of the generated documents during the iteration\\nprocess. Specifically, we present the results of ITRG (refresh)\\nat different iterations in 5-shot setting in Table 3. We measure\\nthe answer recall of generated documents at different itera-\\ntion steps and present results in Table 4. Table 3 shows that\\nthe performance of the model gradually improves with iter-\\nation. And Table 4 shows that the quality of the generated\\ndocuments also gradually improves with iteration. These re-\\nsults verify that our iterative retrieval-generation collaborativeTable 4 : Answer recall of generated documents at different\\niterations with ITRG (refresh).\\nIteration 1 2 3 4 5\\nNatural Questions 44.0 46.4 48.4 48.8 48.0\\nTriviaQA 18.8 19.0 20.2 19.2 19.2\\n2WikiMultiHopQA 34.2 36.6 35.0 40.0 37.0\\nHotpotQA 34.2 34.8 35.6 33.8 33.6\\nframework is effective and can further enhance the reasoning\\ncapabilities of large language models.\\n5. CONCLUSION\\nIn this paper, we present ITRG, which is an iterative retrieval-\\ngeneration synergy framework, containing two important\\nsteps: generation-augmented retrieval and retrieval-augmented\\ngeneration. They form a closed loop, and can improve\\neach other via multiple iterations. We propose a simple\\nand effective generation-augmented retrieval strategy and\\ntwo retrieval-augmented generation strategies. Empirical re-\\nsults show our approach significantly exceeds several strong\\nbaselines, including GPT 3.5, on four open domain ques-\\ntion answering datasets, which indicates that our method can\\nsignificantly improve the reasoning ability of large language\\nmodels.', metadata={'source': './data\\\\RAG on large language models.pdf', 'page': 3}),\n",
       " Document(page_content='6. REFERENCES\\n[1] T. Brown et al. , ‚ÄúLanguage models are few-shot learners,‚Äù Ad-\\nvances in neural information processing systems , vol. 33, pp.\\n1877‚Äì1901, 2020.\\n[2] J. Hoffmann et al. , ‚ÄúTraining compute-optimal large language\\nmodels,‚Äù 2022.\\n[3] A. Zeng et al. , ‚ÄúGlm-130b: An open bilingual pre-trained\\nmodel,‚Äù arXiv preprint arXiv:2210.02414 , 2022.\\n[4] A. Chowdhery et al. , ‚ÄúPalm: Scaling language modeling with\\npathways,‚Äù arXiv preprint arXiv:2204.02311 , 2022.\\n[5] OpenAI, ‚ÄúGpt-4 technical report,‚Äù 2023.\\n[6] H. Touvron et al. , ‚ÄúLlama: Open and efficient foundation lan-\\nguage models,‚Äù 2023.\\n[7] K. Lee, M.-W. Chang, and K. Toutanova, ‚ÄúLatent retrieval\\nfor weakly supervised open domain question answering,‚Äù in\\nProceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics . Florence, Italy: Association\\nfor Computational Linguistics, Jul. 2019, pp. 6086‚Äì6096.\\n[Online]. Available: https://aclanthology.org/P19-1612\\n[8] R. Zellers, Y . Bisk, R. Schwartz, and Y . Choi, ‚ÄúSWAG:\\nA large-scale adversarial dataset for grounded common-\\nsense inference,‚Äù in Proceedings of the 2018 Conference\\non Empirical Methods in Natural Language Processing .\\nBrussels, Belgium: Association for Computational Lin-\\nguistics, Oct.-Nov. 2018, pp. 93‚Äì104. [Online]. Available:\\nhttps://www.aclweb.org/anthology/D18-1009\\n[9] O. Ram et al. , ‚ÄúIn-context retrieval-augmented language mod-\\nels,‚Äù arXiv preprint arXiv:2302.00083 , 2023.\\n[10] O. Khattab et al. , ‚ÄúDemonstrate-search-predict: Composing\\nretrieval and language models for knowledge-intensive nlp,‚Äù\\n2023.\\n[11] W. Shi et al. , ‚ÄúReplug: Retrieval-augmented black-box lan-\\nguage models,‚Äù arXiv preprint arXiv:2301.12652 , 2023.\\n[12] W. Yu et al. , ‚ÄúGenerate rather than retrieve: Large language\\nmodels are strong context generators,‚Äù 2023.\\n[13] Z. Sun, X. Wang, Y . Tay, Y . Yang, and D. Zhou, ‚ÄúRecitation-\\naugmented language models,‚Äù 2023.\\n[14] G. Izacard and E. Grave, ‚ÄúLeveraging passage retrieval with\\ngenerative models for open domain question answering,‚Äù arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , ‚ÄúNatural questions: A benchmark for\\nquestion answering research,‚Äù Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452‚Äì466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, ‚ÄúTriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,‚Äù in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601‚Äì1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n‚ÄúConstructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,‚Äù in Proceedings of the\\n28th International Conference on Computational Linguistics .Barcelona, Spain (Online): International Committee on Com-\\nputational Linguistics, Dec. 2020, pp. 6609‚Äì6625. [Online].\\nAvailable: https://aclanthology.org/2020.coling-main.580\\n[18] Z. Yang et al. , ‚ÄúHotpotQA: A dataset for diverse, explainable\\nmulti-hop question answering,‚Äù in Proceedings of the 2018\\nConference on Empirical Methods in Natural Language\\nProcessing . Brussels, Belgium: Association for Computa-\\ntional Linguistics, Oct.-Nov. 2018, pp. 2369‚Äì2380. [Online].\\nAvailable: https://aclanthology.org/D18-1259\\n[19] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabhar-\\nwal, ‚ÄúInterleaving retrieval with chain-of-thought reasoning\\nfor knowledge-intensive multi-step questions,‚Äù arXiv preprint\\narXiv:2212.10509 , 2022.\\n[20] Z. Jiang et al. , ‚ÄúActive retrieval augmented generation,‚Äù arXiv\\npreprint arXiv:2305.06983 , 2023.\\n[21] L. Ouyang et al. , ‚ÄúTraining language models to follow instruc-\\ntions with human feedback,‚Äù Advances in Neural Information\\nProcessing Systems , vol. 35, pp. 27 730‚Äì27 744, 2022.\\n[22] J. Wei et al. , ‚ÄúChain of thought prompting elicits reasoning\\nin large language models,‚Äù arXiv preprint arXiv:2201.11903 ,\\n2022.\\n[23] G. Izacard et al. , ‚ÄúFew-shot learning with retrieval augmented\\nlanguage models,‚Äù arXiv preprint arXiv:2208.03299 , 2022.', metadata={'source': './data\\\\RAG on large language models.pdf', 'page': 4}),\n",
       " Document(page_content='Lift Yourself Up: Retrieval-augmented Text\\nGeneration with Self-Memory\\nXin Cheng1Di Luo2Xiuying Chen3Lemao Liu4Dongyan Zhao1Rui Yan2\\n1Peking University2Remin University of China\\n3KAUST4Tencent AI Lab\\nchengxin1998@stu.pku.edu.cn\\nAbstract\\nWith direct access to human-written reference as memory, retrieval-augmented\\ngeneration has achieved much progress in a wide range of text generation tasks.\\nSince better memory would typically prompt better generation (we deÔ¨Åne this as\\nprimal problem ). The traditional approach for memory retrieval involves selecting\\nmemory that exhibits the highest similarity to the input. However, this method\\nis constrained by the quality of the Ô¨Åxed corpus from which memory is retrieved.\\nIn this paper, by exploring the duality of the primal problem: better generation\\nalso prompts better memory, we propose a novel framework, Selfmem , which\\naddresses this limitation by iteratively employing a retrieval-augmented generator\\nto create an unbounded memory pool and using a memory selector to choose one\\noutput as memory for the subsequent generation round. This enables the model\\nto leverage its own output, referred to as self-memory, for improved generation.\\nWe evaluate the effectiveness of Selfmem on three distinct text generation tasks:\\nneural machine translation, abstractive text summarization, and dialogue generation,\\nunder two generation paradigms: Ô¨Åne-tuned small model and few-shot LLM. Our\\napproach achieves state-of-the-art results in four directions in JRC-Acquis ,XSum\\n(50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1), demonstrating the potential\\nof self-memory in enhancing retrieval-augmented generation models. Furthermore,\\nwe conduct thorough analyses of each component in the Selfmem framework to\\nidentify bottlenecks and provide insights for future research.\\n1 Introduction\\nIn recent years, retrieval-augmented text generation has garnered increasing interest across vari-\\nous Ô¨Åelds such as neural machine translation [ 1,22,14], dialogue response generation [ 71,6,42],\\nlanguage modeling [ 21,31,90], and others. This novel generation paradigm initially equips the gener-\\nation model, either a Ô¨Åne-tuned small model or a large language model (LLM) with in-context learning\\ncapability [ 5], with access to an external database (typically the training corpus) via information\\nretrieval techniques [63, 33] and then generates text based on the retrieved memory.\\nIn this paradigm, the guiding principle for memory retrieval is to Ô¨Ånd the memory that exhibits the\\nhighest similarity to the current input [ 31,84,45]. This aligns with the human intuition that a more\\nsimilar demonstration sample typically offers more hints. As demonstrated in Figure 1, for a tunable\\nretrieval-augmented translation model, the memory similarity alone exhibits a strong correlation with\\nthe Ô¨Ånal translation quality, regardless of other factors that may inÔ¨Çuence translation quality (e.g.,\\npolysemy, morphology, and coreference). We deÔ¨Åne this as the primal problem :better memory\\nprompts better generation . Consequently, numerous studies have focused on how to retrieve better\\nPreprint. Under review.arXiv:2305.02437v2  [cs.CL]  17 May 2023', metadata={'source': './data\\\\RAG.pdf', 'page': 0}),\n",
       " Document(page_content='memory, ranging from sparse retrieval to dense retrieval [ 10,58], from a Ô¨Åxed retriever to a learnable\\nretriever [ 38,8], and from sentence-level memory to more Ô¨Åne-grained token-level memory [ 31,30].\\n0.0 0.2 0.4 0.6 0.8 1.0\\nMemory Similarity0.20.30.40.50.60.70.80.9Hypothesis BLEU\\nFigure 1: Relation between memory and hy-\\npothesis on JRC-Acquis En‚ÜíDe dataset.\\nThe hypothesis is generated by a retrieval-\\naugmented translator whose memory is re-\\ntrieved from the training set. The X-axis\\nrepresents the similarity between memory\\nand the reference.However, a fundamental limitation exists in all previous\\nworks: the memory is retrieved from a Ô¨Åxed corpus and\\nis constrained by the corpus‚Äôs quality. Due to the Ô¨Ånite\\nretrieval space, bounded memory signiÔ¨Åcantly limits\\nthe potential of memory-augmented generation models.\\nIn this paper, by exploring the primal problem ‚Äôs dual-\\nity,better generation also prompts better memory ,\\nwe propose a novel framework called Selfmem . This\\nframework iteratively employs a retrieval-augmented\\ngenerator to create an unbounded memory pool and\\nuses a memory selector to choose one output as mem-\\nory for the subsequent generation round. By combining\\ntheprimal anddual problem , a retrieval-augmented\\ngeneration model can elevate itself using its own out-\\nput, referred to as self-memory. The key insight behind\\nSelfmem is that the text more closely resembling the\\ndata distribution during inference is not the training\\ndata [76], but the model‚Äôs own output.\\nSelfmem consists of two complementary components:\\na retrieval-augmented generator and a memory selector. The generator operates under two distinct\\nparadigms: Ô¨Åne-tuning a small model or few-shot prompting a LLM. For the former, we supervise the\\ngenerator with labeled data and retrieved memory, while for the latter, we employ a Ô¨Åxed black-box\\nLLM exclusively for inference alongside retrieved in-context learning samples. We then use the\\ngenerator‚Äôs output to train a memory selector based on a speciÔ¨Åc performance metric. By simply\\nreplacing the retrieved memory with unbounded generated memory, we achieve higher-quality\\ngeneration output ( primal problem ), which subsequently serves as memory for the next round after\\nbeing reÔ¨Åned by the memory selector ( dual problem ).\\nTo evaluate the efÔ¨Åcacy of the Selfmem , we carry out comprehensive experiments in three distinct text\\ngeneration tasks: neural machine translation, abstractive text summarization, and dialogue generation.\\nWe witness substantial enhancements over robust baselines, attaining state-of-the-art outcomes in\\nJRC-Acquis (four directions), XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1). To gain\\ndeeper insights into the Selfmem , we meticulously investigate each crucial component and pinpoint\\nthe existing system bottleneck to guide future research endeavors.\\n2 Related Work\\n2.1 Retrieval-augmented Text Generation\\nSince the world is not a snapshot once the training corpus is collected, we can never expect an\\never-large model to capture everything in its parameters, even for LLMs like GPT-4 [ 57]. Therefore,\\nit is crucial to equip these models with an external memory bank to store additional knowledge or\\nuseful demonstration examples for solving various NLP tasks[38, 68, 83].\\nIn the translation domain, retrieval techniques have long been employed by the localization industry\\nto enhance human translators‚Äô productivity and consistency even before the advent of machine\\ntranslation [ 82]. Early works on machine translation primarily focused on utilizing memory for\\nstatistical machine translation (SMT) systems [ 70,46]. For neural machine translation (NMT), [ 22]\\nwere the Ô¨Årst to use search engines to retrieve memory from the training set and incorporate it with\\nan external memory network. Subsequent research explored various aspects of retrieval-augmented\\nNMT, such as memory encoding methods [ 80,81,25], joint training of retrievers and generators\\nwith monolingual data [ 8], memory granularity [ 30], and memory diversity [ 14]. For few-shot LLM\\ngeneration, strategies for in-context example selection have been proposed to improve translation\\nquality [ 1]. Furthermore, in-context machine translation has been shown to be effective for on-the-Ô¨Çy\\nadaptation [ 69]. For dialogue response generation tasks, employing exemplar/template retrieval as\\nan intermediate step has proven advantageous for generating informative responses [ 78,79,6,7].\\nIn-context learning example retrieval also aids in controllable dialogue [ 42]. Other applications\\n2', metadata={'source': './data\\\\RAG.pdf', 'page': 1}),\n",
       " Document(page_content='include abstractive summarization [ 76,11,59], code generation [ 24], paraphrase generation [ 29,73],\\nlanguage modeling [ 31,90], counterfactual data generation [ 18], and question answering [ 32,28,27].\\n2.2 Neural Text Reranking\\nBy alleviating the discrepancy between training and inference (i.e., exposure bias) and directly\\noptimizing desired metrics, two-stage reranking methods have facilitated signiÔ¨Åcant progress in\\nvarious text generation tasks. In machine translation, pioneering works by [ 66] and [ 56] introduced\\nand popularized discriminative reranking for SMT. In the context of NMT, research has focused on\\ntwo primary reranking approaches: generative reranking [ 51,26,77] and discriminative reranking [ 36,\\n64,17]. For syntactic parsing, [ 15] were the Ô¨Årst to employ a two-stage reranking method to select\\noutputs from a base parser, while [ 12] introduced a maximum entropy reranker. In text summarization,\\nRefSum [ 48] proposed a second-stage summarization framework to address train-test distribution\\nmismatches. SimCLS [ 49] used pairwise Learning To Rank (LTR) to select candidates with the\\nhighest matching scores. SummaReranker [ 62] adopted a multi-task mixture-of-experts framework\\nto leverage different metrics capturing various aspects of generated candidates. BRIO [ 50] reused\\nthe base model for a second round of Ô¨Åne-tuning with both cross-entropy loss and a candidate-level\\nranking loss. JGR [67] employed an alternate training paradigm to train the generator and reranker.\\nA key limitation of these reranking methods is that they represent a one-way process, wherein the\\nselected candidates become the system‚Äôs Ô¨Ånal output. In contrast, our framework innovatively utilizes\\nthe chosen candidates as memory for the subsequent generation round of a retrieval-augmented\\ngenerator, which can produce better candidates with enhanced memory.\\n3 Methods\\nIn this section, we begin with a motivating experiment on generation as memory (¬ß 3.1). Then, we\\nintroduce Selfmem , a framework comprising a retrieval-augmented generator (¬ß 3.2) and a memory\\nselector (¬ß 3.3). The complete framework and algorithm are illustrated in Figure 2 and Algorithm 1.\\n3.1 Generation as Memory\\nThe primary motivation behind our framework stems from the observation that the memory, which\\nis more similar in distribution to the data during inference, is not the training data (38.89 BLEU,\\nas shown in Table 1). Instead, it is the model‚Äôs own output (58.58 BLEU) within the unbounded\\ngeneration space. One interesting exploration involves directly utilizing the generated output as\\nmemory in relation to the primal problem : better memory prompts better generation.\\nTable 1: Experiments on a\\nÔ¨Åxed retrieval-augmented trans-\\nlator with different memory.\\nMemory Hypothesis\\nRetrieval 38.89 58.58\\nBeam 58.58 58.43\\nReference 100 90.43\\nRandom 1.14 49.08We conduct experiments on the JRC-Acquis En‚ÜíDe dataset. The\\nÔ¨Årst row in Table 1 represents conventional retrieval-augmented\\ntraining with retrieved memory and achieves a 58.58 BLEU score.\\nHowever, directly incorporating beam output of this trained model\\nas memory (Beam) back into the generation model does not yield\\nany improvements (row 2), despite its higher similarity to the\\nreference compared to the retrieved ones. We hypothesize two\\npotential reasons for this: (1) the retrieval-augmented generator\\nmay not generalize effectively in this context due to the memory\\ndistribution shift (from 38.89 to 58.58), and (2) the beam memory\\ndoes not offer any information gain in comparison to the retrieved\\nmemory, even when it exhibits more overlap with the references.\\nTo eliminate the Ô¨Årst hypothesis, we investigate the best and worst scenarios by using the reference\\nas memory (Reference) and randomly sampled sentences as memory (Random). Table 1 illustrates\\nthat a retrieval-augmented generator trained with retrieved memory has already learned to effectively\\nutilize memory information in both oracle and random scenarios, with Ô¨Åxed parameters.\\nTo evaluate the second conjecture, we Ô¨Årst deÔ¨Åne the token sets of the reference, retrieved memory,\\nand beam memory as R,M, andB, respectively. The overlap token set, denoted by O, is deÔ¨Åned\\nas the tokens that overlap with the references in the beam memory but not in the retrieved memory,\\nwhich is represented as R‚à©B‚àíR‚à©M .Ois considered as the additional information provided\\n3', metadata={'source': './data\\\\RAG.pdf', 'page': 2}),\n",
       " Document(page_content='Target Distribution\\xa0Frozen LLM / T rainable LM\\nNLL LossKL Loss\\nYN\\nY1\\n...\\nY\\n X\\n Y\\nX\\ncandidates\\nsource\\ntarget training\\nmemory\\n... ...\\n... ...\\n(a) Retrieval-augmented Generator (b) Memory SelectorRetrievalPredicted Distribution\\nM\\nPrimalDualFigure 2: Overall framework. There are two components in Selfmem , a retrieval-augmented genera-\\ntor (a) and a memory selector (b). For the primal problem, (a) takes source and memory as input to\\ngenerate candidates for (b). For the dual problem, (b) takes as input source and generated candidates\\nto select memory for (a).\\nby the beam memory. Inspired by the conÔ¨Ådence analysis of NMT model [ 53], we compute the set\\nconÔ¨Ådence score, œà(¬∑), as follows:\\nœà(¬∑) =1\\n|¬∑|‚àë\\nyi‚àà¬∑p(yi|x,y<i) (1)\\nwherep(yi|x,y<i)is deÔ¨Åned by the generation model. œà(¬∑)measures the conÔ¨Ådence with which the\\ngeneration model generates the tokens in the set. The value of œà(R)is 0.58, while that of Ois 0.76,\\nindicating that the generator is relatively conÔ¨Ådent in generating tokens in O, and therefore does not\\nneed to resort to external memory [ 35]. Beam search ranks generated candidates based on p(y|x),\\nwhere the selected memory falls within the conÔ¨Ådence region of the generator and consequently\\nprovides no information gain. This observation motivates us to select memory according to metrics\\nother thanp(y|x)in the memory selector (¬ß3.3).\\n3.2 Retrieval-augmented Generator\\nGiven a text pair (x,y), wherex={x1,...,x|x|}is the source, y={y1,...,y|y|}is the target. They\\ncould be (document, summary) in summarization, (context, response) in dialogue generation or\\n(source, target) in machine translation. The retrieval-augmented generation would Ô¨Årst use xto\\nretrieve memory mfrom datastore D. Then the generator GŒæ(x,m), parameterized by Œæ, would take\\nbothxandmas input to generate the target sentence y. In this paper, following standard practice,\\nwe choose the training set as D={(xi,yi)}|D|\\ni=1. For LLM as GŒæ, we use the standard in-context\\nlearning format to give (x,y)as demonstration example. For tunable generator GŒæ, we only keep the\\ntarget side of top-1 retrieval results as memory and we consider two commonly used architectures:\\nJoint-Encoder [23, 76, 38] and Dual-Encoder [80, 8, 14].\\nJoint-Encoder This architecture is the standard encoder-decoder-based model [ 2,74]. The input is\\nthe concatenation of xandm. The encoder would Ô¨Årst map the input into the hidden states H:\\nH=Encoder (x[SEP]m) (2)\\nAnd the decoder would incorporate Hby attention mechanism and generate tokens in an auto-\\nregressive manner:\\nhi=Decoder (CrossAttn (H),y<i)PGŒæ(¬∑|x,y<i) =Softmax (hi) (3)\\nDual-Encoder Instead of treating xandmas a long sequence, this architecture has two encoders,\\none forxand the other for m. Their outputs are sequentially attended by the decoder with dual cross\\n4', metadata={'source': './data\\\\RAG.pdf', 'page': 3}),\n",
       " Document(page_content='attention as in [14]:\\nHx=SourceEncoder (x)Hm=MemoryEncoder (x) (4)\\nhi=Decoder (CrossAttn (Hx,Hm),y<i) (5)\\nWe use Transformer [ 74] as the building block for both architectures and optimize GŒæwith NLL loss:\\nLnll=‚àí|y|‚àë\\nt=1logPGŒæ(yt|x,m,y<t) (6)\\n3.3 Memory Selector\\nThe role of memory selector SŒ∏(x,c), parameterized by Œ∏, is to select one candidate cfrom the\\ncandidate pool Cgenerated by GŒæbased on a speciÔ¨Åc metric ‚àÜ(¬∑,¬∑). The chosen candidate cis\\nthen utilized as memory mfor the subsequent generation round of GŒæ. As discussed in ¬ß3.1, using\\npGŒæ(y|x)as the metric ‚àÜ(¬∑,¬∑)would result in falling into the conÔ¨Ådence region of GŒæ, leading to\\nno information gain. Moreover, a larger value of pGŒæ(y|x)does not necessarily guarantee improved\\ngeneration quality [ 54]. Consequently, we deÔ¨Åne ‚àÜ(¬∑,¬∑)as model-free metrics that are widely\\nemployed for assessing generation quality, such as BLEU for Neural Machine Translation (NMT)\\nand ROUGE for Summarization. Our memory selector takes the concatenation of the source xand\\ncandidatecias input, and produces a multinomial distribution pSŒ∏(¬∑|x)overC.\\nIn this paper, we focus on the role of the memory selector, SŒ∏(x,c), which is parameterized by Œ∏.\\nThe objective of this selector is to choose a single candidate cfrom the candidate pool C, generated\\nbyGŒæ, based on a speciÔ¨Åc metric, ‚àÜ(¬∑,¬∑).\\npSŒ∏(ci|x) =exp(SŒ∏(x[SEP]ci))\\n‚àë|C|\\nj=1exp(SŒ∏(x[SEP]cj))(7)\\nIn accordance with [ 36], the training goal for SŒ∏is to minimize the discrepancy between the SŒ∏‚Äôs\\npredictions and the scores determined by ‚àÜ(¬∑,¬∑). This divergence is quantiÔ¨Åed using the Kullback-\\nLeibler (KL) divergence.\\nLkl=‚àí|C|‚àë\\ni=1pM(ci)logpSŒ∏(ci|x)wherepM(ci) =exp(‚àÜ(ci,y)/œÑ)\\n‚àë|C|\\nj=1exp(‚àÜ(cj,y)/œÑ)(8)\\nœÑis the temperature to control the smoothness of the distribution. At inference, the output of the SŒ∏\\nisarg max\\nci‚ààCpSŒ∏(ci|x).\\n3.4 Combine Generator and Selector\\nWe deÔ¨Åne two generation modes for GŒæ. The Ô¨Årst mode, referred to as the hypothesis mode , generates\\na single output for each input, which is utilized for system evaluation. The second mode, known as\\nthecandidate mode , produces N outputs for a given input, and is employed for training SŒ∏as well as\\nmemory selection. By integrating two modes together, we present the complete framework of our\\nproposed model, Selfmem , as illustrated in Algorithm 1.\\n4 Experimental Setup\\n4.1 Dataset\\nWe assess the performance of Selfmem on three generation tasks, utilizing a total of seven datasets.\\nTranslation. We evaluate our framework on JRC-Acquis datasets [ 72], a collection of parallel\\nlegislative text of European Union Law. It is the benchmark dataset used in translation memory-\\naugmented NMT task [ 22,80,8,14]. We choose 4 translation directions, namely, Spanish ‚ÜîEnglish\\n(Es‚ÜîEn), German‚ÜîEnglish (De‚ÜîEn).Summarization. We evaluate on 2 summarization datasets:\\n5', metadata={'source': './data\\\\RAG.pdf', 'page': 4}),\n",
       " Document(page_content='Algorithm 1 Selfmem Framework\\nRequire: a dataset D, a retriever R, a memory selection metric ‚àÜ(¬∑,¬∑), a retrieval-augmented\\ngeneratorGŒæ, and a memory selector SŒ∏\\n1:retrieve memory MinDwithR\\n2:trainGŒæwithDandM(if not LLM)\\n3:useGŒæto generate candidate pool CwithMin candidate mode\\n4:trainSŒ∏onCwith‚àÜ(¬∑,¬∑)\\n5:while not converged in the validation set do\\n6:SŒ∏selects memory from CasM\\n7:GŒægenerates candidate pool CwithMin candidate mode\\n8:end while\\n9:GŒægenerates the Ô¨Ånal hypothesis with Min hypothesis mode\\n1)XSum [55], extreme summarization, a single-document summarization dataset with highly abstrac-\\ntive articles from British Broadcasting Corporation. 2) BigPatent [65], consisting of 1.3 million\\nrecords of U.S. patent documents along with human-written abstractive summaries. Dialogue. We\\nexperiment on DailyDialog [40], which contains multi-turn dialogs on daily life topics and is used\\nby [13, 3, 88]. The detailed statistics for these datasets can be found in the Appendix.\\n4.2 Implementation Details\\nWe utilize the BM25 algorithm [ 63] for retrieval purposes. For all tasks, the candidate generation\\nmethod consists of beam search with a beam width of 50. The number of iterations is determined\\nby the performance on the validation set. For translation , we follow the approach of [ 81,8,14],\\nemploying a randomly initialized Transformer basearchitecture as GŒæfor trainable small model and\\nXGLM [ 44] for LLM in-context learning. Evaluation metrics include BLEU, TER, and chrF++\\nobtained from SACRE BLEU [60]. The memory selector SŒ∏utilizes an XLM-R base[16] as back-\\nbone, with BLEU serving as ‚àÜ(¬∑,¬∑).For summarization , we initialize GŒæwith BART base[37]\\nforBigPatent and employ BRIO [ 50] for XSum . The evaluation metric comprises ROUGE (R-\\n1/2/L) [ 43].For dialogue generation , BART baseserves as the backbone for GŒæ. Our dialogue system\\nis evaluated using BLEU (B-1/2) and Distinct (D-1/2) scores [ 39]. For both dialogue and summariza-\\ntion tasks, we adhere to the methods of [ 49,20], adopting RoBERTa base[47] as the backbone for SŒ∏.\\nThe linear combination of B-1/2 is chosen as ‚àÜ(¬∑,¬∑)for Dialogue Generation, while R-1/2/L is used\\nfor Summarization, following [ 67]. For further implementation details, please refer to the Appendix.\\nOur code is open sourced1.\\n5 Experimental Results\\n5.1 Machine Translation\\nWe select four translation directions and experiment with two generation paradigms: trainable\\nsmall models and few-shot prompted LLMs. For trainable models, we explore two architectures\\n(joint and dual, as detailed in ¬ß3.2). The baselines comprise two types of translation systems: one\\nbeing the vanilla sequence-to-sequence model [ 2,74] without memory augmentation, and the other\\nconsisting of retrieval-augmented translation models focusing on memory encoding [ 22,80], memory\\nconstruction [ 86], memory retrieval [ 8], and memory diversity [ 14]. Based on the experimental\\nresults2shown in Table 2, Selfmem signiÔ¨Åcantly enhances the performance of GŒæacross four\\ntranslation datasets and two different architectures. This is noteworthy, given that the parameters of\\ntheGŒæremain Ô¨Åxed, with the only variable being the input memory. This Ô¨Ånding is consistent with\\ntheprimal problem which posits that improved memory typically leads to better generation results.\\nThedual problem is revealed in Table 3. Self-memory, which essentially represents the model‚Äôs\\nown output, exhibits greater similarity with the ground truth and serves as a more effective memory\\nfor generating the Ô¨Ånal output. This observation highlights a key distinction between Selfmem and\\n1https://github.com/hannibal046/selfmemory\\n2As higher BLEU scores in this range do not necessarily guarantee a superior translation system [ 9], we also\\nevaluate our system using TER and chrF++. The results can be found in the Appendix.\\n6', metadata={'source': './data\\\\RAG.pdf', 'page': 5}),\n",
       " Document(page_content='Table 2: Results of translation task on JRC-Acquis measured by BLEU. Models denoted by the\\nsame symbol ( ‚ãÜand‚Ä†) have the same parameters and only differ in memory as input. The bolded\\nnumbers show the SOTA performance and the underlined numbers show the second-best result. ‚àó\\ndenotes the system is signiÔ¨Åcantly better than baselines with p-value < 0.05 tested by [34].\\nSystemEs‚ÜíEn En‚ÜíEs De‚ÜíEn En‚ÜíDe\\nDev Test Dev Test Dev Test Dev Test\\nNone Memory\\nRNNsearch [2] 55.02 59.34 50.54 50.48 50.20 49.74 44.94 43.98\\nTransformer [74] 64.08 64.63 62.02 61.80 60.18 60.16 54.65 55.43\\nRetrieval Memory\\nSEG-NMT [22] 60.28 59.34 57.62 57.27 55.63 55.33 49.26 48.80\\nNMT-pieces [86] 63.97 64.30 61.50 61.56 60.10 60.26 55.54 55.14\\nG-TFM [80] 66.37 66.21 62.50 62.76 61.85 61.72 57.43 56.88\\nMonoNMT [8] 67.73 67.42 64.18 63.86 64.48 64.62 58.77 58.42\\nCMM [14] 67.48 67.76 63.84 64.04 64.22 64.33 58.94 58.69\\nTransformer dual‚ãÜ 66.87 67.12 63.14 63.54 64.09 63.36 58.69 58.06\\nTransformer uni‚Ä† 67.74 67.32 63.93 64.12 64.50 64.40 58.16 58.58\\nSelf-Memory\\nTransformer dual‚ãÜ68.63‚àó69.20‚àó64.12‚àó64.67‚àó65.06‚àó64.98‚àó59.26‚àó59.49‚àó\\nTransformer uni‚Ä† 68.26‚àó68.80‚àó66.07‚àó65.94‚àó65.32‚àó65.65‚àó59.88‚àó60.11‚àó\\nTable 3: Comparison between retrieval memory and self-memory. The quality of memory and\\nhypothesis is measured by the n-gram overlap with reference (BLEU). All experiments are conducted\\nwith Transformer jointonJRC-Acquis .\\nRetrieval Self\\nmemory hypothesis memory hypothesis\\nEn-De‚Üí 38.89 58.58 57.92 60.11\\n‚Üê 42.56 64.40 64.32 65.65\\nEn-Es‚Üí 40.67 64.12 63.57 65.94\\n‚Üê 43.05 67.32 67.78 68.80\\nprevious reranking works [ 36,62]. Reranking aims to select candidates of higher quality than the\\nbeam output, whereas in Selfmem , the chosen candidates serve as memory for the retrieval-augmented\\ngenerator and do not necessarily need to surpass the quality of the beam hypotheses.\\nTable 4: Evaluation results of in-context learning with self-memory.\\nXGLM-1.7B XGLM-4.5B XGLM-7.5B\\nRandom kNN Self Random kNN Self Random kNN Self\\nEn-De‚Üí 11.51 37.87 40.94 17.51 37.60 38.25 18.48 47.82 48.32\\n‚Üê 27.42 51.00 51.88 30.62 48.12 48.36 33.03 55.65 55.12\\nEn-Es‚Üí 23.87 46.20 48.56 31.83 48.37 49.17 29.97 53.86 54.32\\n‚Üê 25.29 51.55 53.13 32.16 48.55 49.22 35.22 57.25 57.56\\nIn Table 4, we present the results of LLM with self-memory. We employ XGLM [ 44] as our backbone\\ngenerator, with three different sizes ranging from 1.7B to 7.5B. We utilize the recommended prompt\\nas described in [ 44]. We select three in-context learning examples and report the average scores\\nfrom three separate runs, taking into account the sensitivity of example selection in ICL [ 45]. From\\nthe table, we Ô¨Årst observe a general trend where few-shot translation performance improves as the\\nsize of the model increases. Furthermore, we Ô¨Ånd that more similar translation demonstrations\\nsigniÔ¨Åcantly enhance performance across all model sizes (from random, kNN to Self). This suggests\\nthat demonstration examples in in-context learning not only act as triggers for model ability but also\\n7', metadata={'source': './data\\\\RAG.pdf', 'page': 6}),\n",
       " Document(page_content='adhere to the primal problem , where better demonstration example leads to better generation. Also,\\nby comparing the results in Table 2 and Table 4, we can conclude that the cross-lingual LLM with\\ndesigned examples still falls short of the supervised baselines in this task.\\n5.2 Summarization\\nIn this paper, we compare the performance of our trainable model with those of REINA [ 76],\\nPEGASUS [ 85], and BART [ 37]. The results are presented in Table5. Initially, it can be observed\\nthat memory has varying impacts on different datasets. The enhancement brought by memory in the\\nBigPatent dataset is signiÔ¨Åcantly larger than that in the XSum dataset. This can be attributed to the\\ninherent characteristics of the BigPatent dataset, which consists of ofÔ¨Åcial patent documents that\\nexhibit considerable similarity. Consequently, this greatly improves the summarization quality in\\naccordance with the primal problem . Furthermore, we discovered that self-memory substantially\\nenhances the performance of both BRIO (+1.2 R1) and BART (+18.5 R1), achieving state-of-the-art\\nresults on both datasets. We selected these baselines for a fair comparison, as they share the same\\nbase generator. Due to space constraints, additional comparisons and the conÔ¨Ådence region of the\\nSOTA model can be found in the Appendix.\\nTable 5: Results of summarization task on XSum andBigPatent measured by ROUGE.\\nSystem Memory R-1 R-2 R-L\\nXSum\\nPEGASUS None 47.2 24.6 39.3\\nBRIO None 49.1 25.6 40.4\\nREINA (PG) Retrieval 48.2 26.0 40.2\\nREINA (B) Retrieval 43.2 21.0 35.5\\nREINA (L) Retrieval 46.5 24.1 38.6\\nBRIO dual‚ãÜ Retrieval 48.6 26.1 40.6\\nBRIO joint‚Ä† Retrieval 49.5 26.5 41.2\\nBRIO dual‚ãÜ Self 49.2 26.2 40.8\\nBRIO joint‚Ä† Self 50.3 26.7 41.6System Memory R-1 R-2 R-L\\nBigPatent\\nPEGASUS None 53.6 33.2 43.2\\nBART None 44.4 21.3 31.0\\nREINA (B) Retrieval 59.5 42.6 50.6\\nREINA (L) Retrieval 60.7 43.3 51.3\\nREINA (PG) Retrieval 44.6 21.5 33.3\\nBART dual‚ãÜ Retrieval 57.4 43.3 49.7\\nBART joint‚Ä† Retrieval 59.6 43.4 51.0\\nBART dual‚ãÜ Self 61.2 44.6 52.3\\nBART joint‚Ä† Self 62.9 48.1 59.6\\n5.3 Dialogue Generation\\nAs demonstrated in Table 6, the self-memory signiÔ¨Åcantly enhances the performance of the retrieval-\\naugmented generator for dialogue generation tasks. By optimizing memory using BLEU as ‚àÜ(¬∑,¬∑),\\nthe self-memory improves the B-1,2 score over retrieved memory by 3.08 B-1 and 0.6 B-2 on\\nBART joint. Intriguingly, although Selfmem surpasses the baselines in terms of B-1/2, it falls behind in\\nD-1 and D-2, which can be attributed to the trade-off between BLEU score and Distinct score when\\nevaluating a dialogue system [ 89]. To address this issue, we opt for D-1,2 as ‚àÜ(¬∑,¬∑)when optimizing\\nSŒ∏, denoted as BART joint‚Ä†(D). The results in Table 6 highlight the remarkable Ô¨Çexibility of Selfmem\\nby directly optimizing memory to achieve the desired attributes for diverse and informative dialogue.\\n1 2 3 4\\nIteration60626466Hypothesis BLEU\\nTransformer-joint\\nrank1\\nrank2\\nrank3rank4\\nrank5\\nrank6\\n1 2 3 4\\nIteration60626466\\nTransformer-dual\\nrank1\\nrank2\\nrank3rank4\\nrank5\\nrank6\\n(a) Hypothesis\\n1 2 3 4 5\\nIteration455055606570Candidates BLEU (b) Candidates\\nFigure 3: (a) shows generation quality in the iteration process with different SŒ∏in both trainable\\ngenerator architectures. (b) shows candidates quality in the iteration process with an oracle SŒ∏.\\n8', metadata={'source': './data\\\\RAG.pdf', 'page': 7}),\n",
       " Document(page_content='Table 6: Results of dialogue generation task on DailyDialog measured by B-1/2 and D-1/2.\\nBART joint(D) denotes the metric ‚àÜ(¬∑,¬∑)forSŒ∏is the average of D-1 and D-2.\\nSystem Memory B-1 B-2 D-1 D-2\\nNCM [75] None 33.60 26.80 3.00 12.80\\niV AE [19] None 30.90 24.90 2.90 25.00\\nPLATO-2 [4] None 34.80 25.12 3.54 25.11\\nDialoFlow [41] None 36.17 27.67 4.56 27.12\\nBART None 20.72 11.36 3.92 19.44\\nBART dual‚ãÜ Retrieval 29.50 21.89 4.74 26.01\\nBART joint‚Ä† Retrieval 36.72 31.55 6.13 35.65\\nBART dual‚ãÜ Self 33.43 22.85 4.66 26.16\\nBART joint‚Ä† Self 39.80 32.15 5.84 32.16\\nBART joint‚Ä†(D) Self 36.92 32.09 9.12 37.05\\n6 Further Analysis\\nTo gain a deeper insight into Selfmem , we Ô¨Årst examine the impact of each key component, namely GŒæ\\nandSŒ∏. Subsequently, we perform a detailed token-level analysis of the generated output concerning\\ntheir frequency in the training set. Experiments are conducted on the JRC-Acquis En‚ÜíDe dataset.\\nTuningSŒ∏We explored various SŒ∏by direct selection from the candidate pool based on gold\\nrankings. As shown in Figure 3a, both architectures with enhanced SŒ∏signiÔ¨Åcantly outperform\\nthe current SOTA performance (60.11 BLEU). Moreover, we assessed the candidate pool quality\\nduring this iterative process using an oracle SŒ∏, as displayed in Figure 3b. A clear pattern emerges\\nin this boxplot, revealing improvements in the oracle ,quartile ,average , and minimum scores of\\nthe candidate pool. These two experiments jointly clarify the Selfmem ‚Äôs underlying intuition: a\\nretrieval-augmented generator proÔ¨Åts from superior memory, which can be chosen from its own\\nunbounded output, and subsequently, the generator with improved memory produces a higher-quality\\ncandidate pool for the next selection round. Consequently, the model lift itself up.\\nThe Least 20% 40% 60% 80% The Most\\nFrequency Rank0.650.700.750.800.850.900.951.00F1Self\\nRetrieval\\nNone\\nFigure 4: 1-gram F1 score sorted by\\ntraining corpus frequency.TuningGŒæAs discussed in ¬ß3.1, we demonstrated that a\\ntrained retrieval-augmented generator, with Ô¨Åxed parameters,\\npossesses the ability to distinguish between \"good\" and \"bad\"\\nmemory. This observation not only justiÔ¨Åes our decision to\\nmaintain a Ô¨Åxed generator within our framework but also im-\\nplies that the GŒæis not the current bottleneck of the Selfmem .\\nFrequency Analysis We conduct a comprehensive token-\\nlevel analysis by computing the 1-gram F1 scores for generated\\ntranslations and subsequently categorizing the tokens based\\non their frequency in the training set. The results are depicted\\nin Figure 4. A noticeable pattern emerges, suggesting that the\\nmore frequently a model encounters a token during training,\\nthe higher the accuracy of the generated output [ 87]. Moreover, our Ô¨Åndings indicate that retrieval-\\naugmented models, particularly those incorporating self-memory augmentation, exhibit superior\\nperformance in handling long-tail inputs which are challenges for parametric models [61, 52].\\n7 Conclusion\\nFor the Ô¨Årst time, we investigate the fundamental limitation of bounded memory in the current\\nretrieval-augmented literature. We combine the primal anddual problems together and propose\\nSelfmem , a general framework for retrieval-augmented text generation by uplifting generation model\\nwith its own output. We conduct comprehensive experiments across various text generation tasks\\nand different generation paradigms, including trainable small model and few-shot prompted LLM.\\nWe surpass strong baselines and improve the state-of-the-art performance in serval datasets. We also\\n9', metadata={'source': './data\\\\RAG.pdf', 'page': 8}),\n",
       " Document(page_content='meticulously investigate each crucial component and pinpoint the existing system bottleneck to guide\\nfuture research endeavors.\\nReferences\\n[1]Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad.\\nIn-context examples selection for machine translation. CoRR , abs/2212.02437, 2022.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. In Yoshua Bengio and Yann LeCun, editors, 3rd International\\nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\\nConference Track Proceedings , 2015.\\n[3]Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. PLATO: pre-trained dialogue\\ngeneration model with discrete latent variable. In Dan Jurafsky, Joyce Chai, Natalie Schluter,\\nand Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages 85‚Äì96. Association for\\nComputational Linguistics, 2020.\\n[4]Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin\\nLiu, and Xinchao Xu. PLATO-2: Towards building an open-domain chatbot via curriculum\\nlearning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 ,\\npages 2513‚Äì2525, Online, August 2021. Association for Computational Linguistics.\\n[5]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo\\nLarochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,\\neditors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural\\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\\n[6]Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, and Shuming Shi.\\nSkeleton-to-response: Dialogue generation guided by retrieval memory. In Jill Burstein, Christy\\nDoran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Technologies,\\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) ,\\npages 1219‚Äì1228. Association for Computational Linguistics, 2019.\\n[7]Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, and Shuming Shi. Retrieval-guided\\ndialogue response generation via a matching-to-generation framework. In Kentaro Inui, Jing\\nJiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 , pages\\n1866‚Äì1875. Association for Computational Linguistics, 2019.\\n[8]Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation\\nwith monolingual translation memory. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto\\nNavigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Conference on Natural Language Processing,\\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pages 7307‚Äì\\n7318. Association for Computational Linguistics, 2021.\\n[9]Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in\\nmachine translation research. In 11th Conference of the European Chapter of the Association\\nfor Computational Linguistics , pages 249‚Äì256, Trento, Italy, April 2006. Association for\\nComputational Linguistics.\\n10', metadata={'source': './data\\\\RAG.pdf', 'page': 9}),\n",
       " Document(page_content='[10] Qian Cao and Deyi Xiong. Encoding gated translation memory into neural machine translation.\\nIn Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun‚Äôichi Tsujii, editors, Proceedings of\\nthe 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium,\\nOctober 31 - November 4, 2018 , pages 3042‚Äì3047. Association for Computational Linguistics,\\n2018.\\n[11] Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. Retrieve, rerank and rewrite: Soft template\\nbased neural summarization. In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers) , pages 152‚Äì161, Melbourne, Australia,\\nJuly 2018. Association for Computational Linguistics.\\n[12] Eugene Charniak and Mark Johnson. Coarse-to-Ô¨Åne n-best parsing and maxent discriminative\\nreranking. In Kevin Knight, Hwee Tou Ng, and Kemal OÔ¨Çazer, editors, ACL 2005, 43rd Annual\\nMeeting of the Association for Computational Linguistics, Proceedings of the Conference,\\n25-30 June 2005, University of Michigan, USA , pages 173‚Äì180. The Association for Computer\\nLinguistics, 2005.\\n[13] Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu,\\nBartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. Dialogved: A pre-trained\\nlatent variable encoder-decoder model for dialog response generation. In Smaranda Muresan,\\nPreslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of\\nthe Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,\\nIreland, May 22-27, 2022 , pages 4852‚Äì4864. Association for Computational Linguistics, 2022.\\n[14] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation\\nwith contrastive translation memories. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang,\\neditors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language\\nProcessing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages\\n3591‚Äì3601. Association for Computational Linguistics, 2022.\\n[15] Michael Collins and Terry Koo. Discriminative reranking for natural language parsing. Comput.\\nLinguistics , 31(1):25‚Äì70, 2005.\\n[16] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,\\nFrancisco Guzm√°n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsu-\\npervised cross-lingual representation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie\\nSchluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages 8440‚Äì8451.\\nAssociation for Computational Linguistics, 2020.\\n[17] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc‚ÄôAurelio Ranzato. Resid-\\nual energy-based models for text generation. In 8th International Conference on Learning\\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020.\\n[18] Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer. CORE: A\\nretrieve-then-edit framework for counterfactual data generation. In Yoav Goldberg, Zornitsa\\nKozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics:\\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages 2964‚Äì2984.\\nAssociation for Computational Linguistics, 2022.\\n[19] Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen. Implicit deep latent\\nvariable models for text generation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun\\nWan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\\nguage Processing and the 9th International Joint Conference on Natural Language Processing,\\nEMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 , pages 3944‚Äì3954. Association\\nfor Computational Linguistics, 2019.\\n[20] Jiazhan Feng, Chongyang Tao, Zhen Li, Chang Liu, Tao Shen, and Dongyan Zhao. Reciprocal\\nlearning of knowledge retriever and response ranker for knowledge-grounded conversations. In\\nNicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun\\nChoi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio,\\nNianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus,\\n11', metadata={'source': './data\\\\RAG.pdf', 'page': 10}),\n",
       " Document(page_content='Francis Bond, and Seung-Hoon Na, editors, Proceedings of the 29th International Conference\\non Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17,\\n2022 , pages 389‚Äì399. International Committee on Computational Linguistics, 2022.\\n[21] Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language modeÔ¨Çs with\\na continuous cache. In 5th International Conference on Learning Representations, ICLR 2017,\\nToulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 2017.\\n[22] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O. K. Li. Search engine guided neural\\nmachine translation. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\\nthe Thirty-Second AAAI Conference on ArtiÔ¨Åcial Intelligence, (AAAI-18), the 30th innovative\\nApplications of ArtiÔ¨Åcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\\nAdvances in ArtiÔ¨Åcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\\n2018 , pages 5133‚Äì5140. AAAI Press, 2018.\\n[23] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval\\naugmented language model pre-training. In Proceedings of the 37th International Conference\\non Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings\\nof Machine Learning Research , pages 3929‚Äì3938. PMLR, 2020.\\n[24] Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, and Percy Liang. A retrieve-and-edit\\nframework for predicting structured outputs. In Samy Bengio, Hanna M. Wallach, Hugo\\nLarochelle, Kristen Grauman, Nicol√≤ Cesa-Bianchi, and Roman Garnett, editors, Advances\\nin Neural Information Processing Systems 31: Annual Conference on Neural Information\\nProcessing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr√©al, Canada , pages\\n10073‚Äì10083, 2018.\\n[25] Qiuxiang He, Guoping Huang, Qu Cui, Li Li, and Lemao Liu. Fast and accurate neural machine\\ntranslation with translation memory. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto\\nNavigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Conference on Natural Language Processing,\\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pages 3170‚Äì\\n3180. Association for Computational Linguistics, 2021.\\n[26] Kenji Imamura and Eiichiro Sumita. Ensemble and reranking: Using multiple models in\\nthe NICT-2 neural machine translation system at WAT2017. In Toshiaki Nakazawa and Isao\\nGoto, editors, Proceedings of the 4th Workshop on Asian Translation, WAT@IJCNLP 2017,\\nTaipei, Taiwan, November 27- December 1, 2017 , pages 127‚Äì134. Asian Federation of Natural\\nLanguage Processing, 2017.\\n[27] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models\\nfor open domain question answering. In Paola Merlo, J√∂rg Tiedemann, and Reut Tsarfaty,\\neditors, Proceedings of the 16th Conference of the European Chapter of the Association for\\nComputational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021 , pages\\n874‚Äì880. Association for Computational Linguistics, 2021.\\n[28] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov,\\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.\\nIn Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020\\nConference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,\\nNovember 16-20, 2020 , pages 6769‚Äì6781. Association for Computational Linguistics, 2020.\\n[29] Amirhossein Kazemnejad, Mohammadreza Salehi, and Mahdieh Soleymani Baghshah. Para-\\nphrase generation by learning how to edit from samples. In Dan Jurafsky, Joyce Chai, Natalie\\nSchluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages 6010‚Äì6021.\\nAssociation for Computational Linguistics, 2020.\\n[30] Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Nearest\\nneighbor machine translation. In 9th International Conference on Learning Representations,\\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021.\\n12', metadata={'source': './data\\\\RAG.pdf', 'page': 11}),\n",
       " Document(page_content='[31] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. General-\\nization through memorization: Nearest neighbor language models. In International Conference\\non Learning Representations , 2020.\\n[32] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts,\\nand Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for\\nknowledge-intensive NLP. CoRR , abs/2212.14024, 2022.\\n[33] Omar Khattab and Matei Zaharia. Colbert: EfÔ¨Åcient and effective passage search via contextu-\\nalized late interaction over BERT. In Jimmy X. Huang, Yi Chang, Xueqi Cheng, Jaap Kamps,\\nVanessa Murdock, Ji-Rong Wen, and Yiqun Liu, editors, Proceedings of the 43rd International\\nACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020,\\nVirtual Event, China, July 25-30, 2020 , pages 39‚Äì48. ACM, 2020.\\n[34] Philipp Koehn. Statistical signiÔ¨Åcance tests for machine translation evaluation. In Proceedings\\nof the 2004 Conference on Empirical Methods in Natural Language Processing , EMNLP 2004,\\nA meeting of SIGDAT, a Special Interest Group of the ACL, held in conjunction with ACL 2004,\\n25-26 July 2004, Barcelona, Spain , pages 388‚Äì395. ACL, 2004.\\n[35] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani,\\nVictor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory\\nnetworks for natural language processing. In Maria Florina Balcan and Kilian Q. Weinberger,\\neditors, Proceedings of The 33rd International Conference on Machine Learning , volume 48 of\\nProceedings of Machine Learning Research , pages 1378‚Äì1387, New York, New York, USA,\\n20‚Äì22 Jun 2016. PMLR.\\n[36] Ann Lee, Michael Auli, and Marc‚ÄôAurelio Ranzato. Discriminative reranking for neural\\nmachine translation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,\\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\\nthe 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021,\\n(Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pages 7250‚Äì7264. Association for\\nComputational Linguistics, 2021.\\n[37] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation, and comprehension. In Dan Jurafsky, Joyce\\nChai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting\\nof the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages\\n7871‚Äì7880. Association for Computational Linguistics, 2020.\\n[38] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, and\\nDouwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Hugo\\nLarochelle, Marc‚ÄôAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,\\neditors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural\\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\\n[39] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\\nobjective function for neural conversation models. In Kevin Knight, Ani Nenkova, and Owen\\nRambow, editors, NAACL HLT 2016, The 2016 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human Language Technologies, San Diego Cal-\\nifornia, USA, June 12-17, 2016 , pages 110‚Äì119. The Association for Computational Linguistics,\\n2016.\\n[40] Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. Dailydialog: A\\nmanually labelled multi-turn dialogue dataset. In Greg Kondrak and Taro Watanabe, editors,\\nProceedings of the Eighth International Joint Conference on Natural Language Processing,\\nIJCNLP 2017, Taipei, Taiwan, November 27 - December 1, 2017 - Volume 1: Long Papers ,\\npages 986‚Äì995. Asian Federation of Natural Language Processing, 2017.\\n[41] Zekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng, and Jie Zhou. Conversations are not Ô¨Çat:\\nModeling the dynamic information Ô¨Çow across dialogue utterances. In Proceedings of the 59th\\n13', metadata={'source': './data\\\\RAG.pdf', 'page': 12}),\n",
       " Document(page_content='Annual Meeting of the Association for Computational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 128‚Äì138,\\nOnline, August 2021. Association for Computational Linguistics.\\n[42] Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and Xifeng Yan. Controllable\\ndialogue simulation with in-context learning. In Findings of the Association for Computational\\nLinguistics: EMNLP 2022 , pages 4330‚Äì4347, Abu Dhabi, United Arab Emirates, December\\n2022. Association for Computational Linguistics.\\n[43] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\\ntion Branches Out , pages 74‚Äì81, Barcelona, Spain, July 2004. Association for Computational\\nLinguistics.\\n[44] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig,\\nMyle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer,\\nPunit Singh Koura, Vishrav Chaudhary, Brian O‚ÄôHoro, Jeff Wang, Luke Zettlemoyer, Zornitsa\\nKozareva, Mona T. Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual\\ngenerative language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,\\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,\\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages 9019‚Äì9052.\\nAssociation for Computational Linguistics, 2022.\\n[45] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.\\nWhat makes good in-context examples for gpt-3? In Eneko Agirre, Marianna Apidianaki, and\\nIvan Vulic, editors, Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge\\nExtraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin,\\nIreland and Online, May 27, 2022 , pages 100‚Äì114. Association for Computational Linguistics,\\n2022.\\n[46] Lemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao, Mo Yu, and Conghui Zhu. Locally\\ntraining the log-linear model for SMT. In Jun‚Äôichi Tsujii, James Henderson, and Marius Pasca,\\neditors, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language\\nProcessing and Computational Natural Language Learning, EMNLP-CoNLL 2012, July 12-14,\\n2012, Jeju Island, Korea , pages 402‚Äì411. ACL, 2012.\\n[47] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT\\npretraining approach. CoRR , abs/1907.11692, 2019.\\n[48] Yixin Liu, Zi-Yi Dou, and Pengfei Liu. Refsum: Refactoring neural summarization. In Kristina\\nToutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T√ºr, Iz Beltagy, Steven Bethard,\\nRyan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021\\nConference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 , pages 1437‚Äì1448.\\nAssociation for Computational Linguistics, 2021.\\n[49] Yixin Liu and Pengfei Liu. Simcls: A simple framework for contrastive learning of abstrac-\\ntive summarization. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,\\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\\nthe 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021,\\n(Volume 2: Short Papers), Virtual Event, August 1-6, 2021 , pages 1065‚Äì1072. Association for\\nComputational Linguistics, 2021.\\n[50] Yixin Liu, Pengfei Liu, Dragomir R. Radev, and Graham Neubig. BRIO: bringing order to\\nabstractive summarization. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, edi-\\ntors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , pages 2890‚Äì2903.\\nAssociation for Computational Linguistics, 2022.\\n[51] Yuchen Liu, Long Zhou, Yining Wang, Yang Zhao, Jiajun Zhang, and Chengqing Zong. A\\ncomparable study on model averaging, ensembling and reranking in NMT. In Min Zhang,\\nVincent Ng, Dongyan Zhao, Sujian Li, and Hongying Zan, editors, Natural Language Processing\\n14', metadata={'source': './data\\\\RAG.pdf', 'page': 13}),\n",
       " Document(page_content='and Chinese Computing - 7th CCF International Conference, NLPCC 2018, Hohhot, China,\\nAugust 26-30, 2018, Proceedings, Part II , volume 11109 of Lecture Notes in Computer Science ,\\npages 299‚Äì308. Springer, 2018.\\n[52] Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg,\\nAlan Blair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented classiÔ¨Åcation for\\nlong-tail visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pages 6959‚Äì6969, 2022.\\n[53] Yu Lu, Jiali Zeng, Jiajun Zhang, Shuangzhi Wu, and Mu Li. Learning conÔ¨Ådence for transformer-\\nbased neural machine translation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio,\\neditors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguis-\\ntics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , pages 2353‚Äì2364.\\nAssociation for Computational Linguistics, 2022.\\n[54] Clara Meister, Ryan Cotterell, and Tim Vieira. If beam search is the answer, what was the\\nquestion? In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of\\nthe 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,\\nOnline, November 16-20, 2020 , pages 2173‚Äì2185. Association for Computational Linguistics,\\n2020.\\n[55] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don‚Äôt give me the details, just the sum-\\nmary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of\\nthe 2018 Conference on Empirical Methods in Natural Language Processing , pages 1797‚Äì1807,\\nBrussels, Belgium, October-November 2018. Association for Computational Linguistics.\\n[56] Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alexan-\\nder M. Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin,\\nand Dragomir R. Radev. A smorgasbord of features for statistical machine translation. In Julia\\nHirschberg, Susan T. Dumais, Daniel Marcu, and Salim Roukos, editors, Human Language\\nTechnology Conference of the North American Chapter of the Association for Computational\\nLinguistics, HLT-NAACL 2004, Boston, Massachusetts, USA, May 2-7, 2004 , pages 161‚Äì168.\\nThe Association for Computational Linguistics, 2004.\\n[57] OpenAI. GPT-4 technical report. CoRR , abs/2303.08774, 2023.\\n[58] Md. Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei\\nChang. Retrieval augmented code generation and summarization. In Marie-Francine Moens,\\nXuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for\\nComputational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic,\\n16-20 November, 2021 , pages 2719‚Äì2734. Association for Computational Linguistics, 2021.\\n[59] Hao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan Dhingra, and Dipanjan Das. Text\\ngeneration with exemplar-based adaptive decoding. In Jill Burstein, Christy Doran, and Thamar\\nSolorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , pages 2555‚Äì2565.\\nAssociation for Computational Linguistics, 2019.\\n[60] Matt Post. A call for clarity in reporting BLEU scores. In Ondrej Bojar, Rajen Chatterjee,\\nChristian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio\\nJimeno-Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aur√©lie N√©v√©ol, Mariana L. Neves,\\nMatt Post, Lucia Specia, Marco Turchi, and Karin Verspoor, editors, Proceedings of the Third\\nConference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October\\n31 - November 1, 2018 , pages 186‚Äì191. Association for Computational Linguistics, 2018.\\n[61] Vikas Raunak, Siddharth Dalmia, Vivek Gupta, and Florian Metze. On long-tailed phenomena\\nin neural machine translation. In Findings of the Association for Computational Linguistics:\\nEMNLP 2020 , pages 3088‚Äì3095, Online, November 2020. Association for Computational\\nLinguistics.\\n15', metadata={'source': './data\\\\RAG.pdf', 'page': 14}),\n",
       " Document(page_content='[62] Mathieu Ravaut, ShaÔ¨Åq R. Joty, and Nancy F. Chen. Summareranker: A multi-task mixture-of-\\nexperts re-ranking framework for abstractive summarization. In Smaranda Muresan, Preslav\\nNakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Associ-\\nation for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May\\n22-27, 2022 , pages 4504‚Äì4524. Association for Computational Linguistics, 2022.\\n[63] Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and\\nbeyond. Found. Trends Inf. Retr. , 3(4):333‚Äì389, 2009.\\n[64] Julian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin Kirchhoff. Masked language model\\nscoring. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceed-\\nings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,\\nOnline, July 5-10, 2020 , pages 2699‚Äì2712. Association for Computational Linguistics, 2020.\\n[65] Eva Sharma, Chen Li, and Lu Wang. BIGPATENT: A large-scale dataset for abstractive and\\ncoherent summarization. In Anna Korhonen, David R. Traum, and Llu√≠s M√†rquez, editors,\\nProceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019,\\nFlorence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , pages 2204‚Äì2213. Association\\nfor Computational Linguistics, 2019.\\n[66] Libin Shen, Anoop Sarkar, and Franz Josef Och. Discriminative reranking for machine transla-\\ntion. In Julia Hirschberg, Susan T. Dumais, Daniel Marcu, and Salim Roukos, editors, Human\\nLanguage Technology Conference of the North American Chapter of the Association for Com-\\nputational Linguistics, HLT-NAACL 2004, Boston, Massachusetts, USA, May 2-7, 2004 , pages\\n177‚Äì184. The Association for Computational Linguistics, 2004.\\n[67] Weizhou Shen, Yeyun Gong, Yelong Shen, Song Wang, Xiaojun Quan, Nan Duan, and Weizhu\\nChen. Joint generator-ranker learning for natural language generation. CoRR , abs/2206.13974,\\n2022.\\n[68] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke\\nZettlemoyer, and Wen-tau Yih. REPLUG: retrieval-augmented black-box language models.\\nCoRR , abs/2301.12652, 2023.\\n[69] Suzanna Sia and Kevin Duh. In-context learning as maintaining coherency: A study of on-the-Ô¨Çy\\nmachine translation using large language models. CoRR , abs/2305.03573, 2023.\\n[70] Michel Simard and Pierre Isabelle. Phrase-based machine translation in a computer-assisted\\ntranslation environment. In Proceedings of Machine Translation Summit XII: Papers, MTSummit\\n2009, Ottawa, Canada, August 26-30, 2009 , 2009.\\n[71] Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and Ming Zhang. Two are better than one: An\\nensemble of retrieval- and generation-based dialog systems. CoRR , abs/1610.07149, 2016.\\n[72] Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan TuÔ¨Ås,\\nand D√°niel Varga. The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages.\\nIn Nicoletta Calzolari, Khalid Choukri, Aldo Gangemi, Bente Maegaard, Joseph Mariani,\\nJan Odijk, and Daniel Tapias, editors, Proceedings of the Fifth International Conference on\\nLanguage Resources and Evaluation, LREC 2006, Genoa, Italy, May 22-28, 2006 , pages\\n2142‚Äì2147. European Language Resources Association (ELRA), 2006.\\n[73] Yixuan Su, David Vandyke, Simon Baker, Yan Wang, and Nigel Collier. Keep the primary,\\nrewrite the secondary: A two-stage approach for paraphrase generation. In Findings of the\\nAssociation for Computational Linguistics: ACL-IJCNLP 2021 , pages 560‚Äì569, Online, August\\n2021. Association for Computational Linguistics.\\n[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von\\nLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman\\nGarnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference\\non Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA ,\\npages 5998‚Äì6008, 2017.\\n16', metadata={'source': './data\\\\RAG.pdf', 'page': 15}),\n",
       " Document(page_content='[75] Oriol Vinyals and Quoc V . Le. A neural conversational model. CoRR , abs/1506.05869, 2015.\\n[76] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu,\\nand Michael Zeng. Training data is more valuable than you think: A simple and effective\\nmethod by retrieving from training data. In Smaranda Muresan, Preslav Nakov, and Aline\\nVillavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 ,\\npages 3170‚Äì3179. Association for Computational Linguistics, 2022.\\n[77] Yuguang Wang, Shanbo Cheng, Liyang Jiang, Jiajun Yang, Wei Chen, Muze Li, Lin Shi,\\nYanfeng Wang, and Hongtao Yang. Sogou neural machine translation systems for WMT17. In\\nOndrej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry\\nHaddow, Matthias Huck, Antonio Jimeno-Yepes, Philipp Koehn, and Julia Kreutzer, editors,\\nProceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen,\\nDenmark, September 7-8, 2017 , pages 410‚Äì415. Association for Computational Linguistics,\\n2017.\\n[78] Jason Weston, Emily Dinan, and Alexander H. Miller. Retrieve and reÔ¨Åne: Improved sequence\\ngeneration models for dialogue. In Aleksandr Chuklin, Jeff Dalton, Julia Kiseleva, Alexey\\nBorisov, and Mikhail S. Burtsev, editors, Proceedings of the 2nd International Workshop on\\nSearch-Oriented Conversational AI, SCAI@EMNLP 2018, Brussels, Belgium, October 31, 2018 ,\\npages 87‚Äì92. Association for Computational Linguistics, 2018.\\n[79] Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhoujun Li, and Ming Zhou. Response gener-\\nation by context-aware prototype editing. In The Thirty-Third AAAI Conference on ArtiÔ¨Åcial\\nIntelligence, AAAI 2019, The Thirty-First Innovative Applications of ArtiÔ¨Åcial Intelligence\\nConference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in ArtiÔ¨Åcial Intel-\\nligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pages 7281‚Äì7288.\\nAAAI Press, 2019.\\n[80] Mengzhou Xia, Guoping Huang, Lemao Liu, and Shuming Shi. Graph based translation memory\\nfor neural machine translation. In The Thirty-Third AAAI Conference on ArtiÔ¨Åcial Intelligence,\\nAAAI 2019, The Thirty-First Innovative Applications of ArtiÔ¨Åcial Intelligence Conference, IAAI\\n2019, The Ninth AAAI Symposium on Educational Advances in ArtiÔ¨Åcial Intelligence, EAAI\\n2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pages 7297‚Äì7304. AAAI Press,\\n2019.\\n[81] Jitao Xu, Josep Maria Crego, and Jean Senellart. Boosting neural machine translation with\\nsimilar translations. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors,\\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL\\n2020, Online, July 5-10, 2020 , pages 1580‚Äì1590. Association for Computational Linguistics,\\n2020.\\n[82] Masaru Yamada. The effect of translation memory databases on productivity. Translation\\nresearch projects , 3:63‚Äì73, 2011.\\n[83] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang,\\nMike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language\\nmodeling. CoRR , abs/2211.12561, 2022.\\n[84] Dani Yogatama, Cyprien de Masson d‚ÄôAutume, and Lingpeng Kong. Adaptive semiparametric\\nlanguage models. Trans. Assoc. Comput. Linguistics , 9:362‚Äì373, 2021.\\n[85] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. PEGASUS: pre-training with\\nextracted gap-sentences for abstractive summarization. In Proceedings of the 37th International\\nConference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of\\nProceedings of Machine Learning Research , pages 11328‚Äì11339. PMLR, 2020.\\n[86] Jingyi Zhang, Masao Utiyama, Eiichiro Sumita, Graham Neubig, and Satoshi Nakamura.\\nGuiding neural machine translation with retrieved translation pieces. In Marilyn A. Walker,\\nHeng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Technologies,\\nNAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers) ,\\npages 1325‚Äì1335. Association for Computational Linguistics, 2018.\\n17', metadata={'source': './data\\\\RAG.pdf', 'page': 16}),\n",
       " Document(page_content='[87] Tong Zhang, Wei Ye, Baosong Yang, Long Zhang, Xingzhang Ren, Dayiheng Liu, Jinan\\nSun, Shikun Zhang, Haibo Zhang, and Wen Zhao. Frequency-aware contrastive learning for\\nneural machine translation. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence ,\\nvolume 36, pages 11712‚Äì11720, 2022.\\n[88] Xueliang Zhao, Lemao Liu, Tingchen Fu, Shuming Shi, Dongyan Zhao, and Rui Yan. Towards\\nefÔ¨Åcient dialogue pre-training with transferable and interpretable latent structure. CoRR ,\\nabs/2210.12461, 2022.\\n[89] Yinhe Zheng, Zikai Chen, Rongsheng Zhang, Shilei Huang, Xiaoxi Mao, and Minlie Huang.\\nStylized dialogue response generation using stylized unpaired texts. In Thirty-Fifth AAAI\\nConference on ArtiÔ¨Åcial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Appli-\\ncations of ArtiÔ¨Åcial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances\\nin ArtiÔ¨Åcial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021 , pages 14558‚Äì14567.\\nAAAI Press, 2021.\\n[90] Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation.\\nCoRR , abs/2205.12674, 2022.\\n18', metadata={'source': './data\\\\RAG.pdf', 'page': 17}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nRETRIEVAL -AUGMENTED GENERATION FOR CODE\\nSUMMARIZATION VIA HYBRID GNN\\nShangqing Liu1‚àó, Yu Chen2‚Ä†, Xiaofei Xie1‚Ä†, Jingkai Siow1, Yang Liu1\\n1Nanyang Technology University\\n2Rensselaer Polytechnic Institute\\nABSTRACT\\nSource code summarization aims to generate natural language summaries from\\nstructured code snippets for better understanding code functionalities. However, au-\\ntomatic code summarization is challenging due to the complexity of the source code\\nand the language gap between the source code and natural language summaries.\\nMost previous approaches either rely on retrieval-based (which can take advantage\\nof similar examples seen from the retrieval database, but have low generalization\\nperformance) or generation-based methods (which have better generalization per-\\nformance, but cannot take advantage of similar examples). This paper proposes\\na novel retrieval-augmented mechanism to combine the beneÔ¨Åts of both worlds.\\nFurthermore, to mitigate the limitation of Graph Neural Networks (GNNs) on\\ncapturing global graph structure information of source code, we propose a novel\\nattention-based dynamic graph to complement the static graph representation of the\\nsource code, and design a hybrid message passing GNN for capturing both the local\\nand global structural information. To evaluate the proposed approach, we release\\na new challenging benchmark, crawled from diversiÔ¨Åed large-scale open-source\\nCprojects (total 95k+ unique functions in the dataset). Our method achieves the\\nstate-of-the-art performance, improving existing methods by 1.42,2.44 and1.29\\nin terms of BLEU-4, ROUGE-L and METEOR.\\n1 I NTRODUCTION\\nWith software growing in size and complexity, developers tend to spend nearly 90% (Wan et al.,\\n2018) effort on software maintenance ( e.g., version iteration and bug Ô¨Åx) in the completed life\\ncycle of software development. Source code summary, in the form of natural language, plays a\\ncritical role in the comprehension and maintenance process and greatly reduces the effort of reading\\nand comprehending programs. However, manually writing code summaries is tedious and time-\\nconsuming, and with the acceleration of software iteration, it has become a heavy burden for software\\ndevelopers. Hence, source code summarization which automates concise descriptions of programs is\\nmeaningful.\\nAutomatic source code summarization is a crucial yet far from the settled problem. The key challenges\\ninclude: 1) the source code and the natural language summary are heterogeneous, which means\\nthey may not share common lexical tokens, synonyms, or language structures and 2) the source\\ncode is complex with complicated logic and variable grammatical structure, making it hard to learn\\nthe semantics. Conventionally, information retrieval (IR) techniques have been widely used in\\ncode summarization (Eddy et al., 2013; Haiduc et al., 2010; Wong et al., 2015; 2013). Since code\\nduplication (Kamiya et al., 2002; Li et al., 2006) is common in ‚Äúbig code‚Äù (Allamanis et al., 2018),\\nearly works summarize the new programs by retrieving the similar code snippet in the existing code\\ndatabase and use its summary directly. Essentially, the retrieval-based approaches transform the code\\nsummarization to the code similarity calculation task, which may achieve promising performance on\\nsimilar programs, but are limited in generalization, i.e. they have poorer performance on programs\\nthat are very different from the code database.\\n‚àóContact:shangqin001@e.ntu.edu.sg\\n‚Ä†Corresponding authors\\n1arXiv:2006.05405v5  [cs.LG]  13 May 2021', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 0}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nTo improve the generalization performance, recent works focus on generation-based approaches.\\nSome works explore Seq2Seq architectures (Bahdanau et al., 2014; Luong et al., 2015) to generate\\nsummaries from the given source code. The Seq2Seq-based approaches (Iyer et al., 2016; Hu et al.,\\n2018a; Alon et al., 2018) usually treat the source code or abstract syntax tree parsed from the source\\ncode as a sequence and follow a paradigm of encoder-decoder with the attention mechanism for\\ngenerating a summary. However, these works only rely on sequential models, which are struggling\\nto capture the rich semantics of source code e.g., control dependencies and data dependencies. In\\naddition, generation-based approaches typically cannot take advantage of similar examples from the\\nretrieval database, as retrieval-based approaches do.\\nTo better learn the semantics of the source code, Allamanis et al. (Allamanis et al., 2017) lighted\\nup this Ô¨Åeld by representing programs as graphs. Some follow-up works (Fernandes et al., 2018)\\nattempted to encode more code structures ( e.g., control Ô¨Çow, program dependencies) into code graphs\\nwith graph neural networks (GNNs), and achieved the promising performance than the sequence-\\nbased approaches. Existing works (Allamanis et al., 2017; Fernandes et al., 2018) usually convert\\ncode into graph-structured input during preprocessing, and directly consume it via modern neural\\nnetworks ( e.g., GNNs) for computing node and graph embeddings. However, most GNN-based\\nencoders only allow message passing among nodes within a k-hop neighborhood (where kis usually\\na small number such as 4) to avoid over-smoothing (Zhao & Akoglu, 2019; Chen et al., 2020a), thus\\ncapture only local neighborhood information and ignore global interactions among nodes. Even\\nthere are some works (Li et al., 2019) that try to address this challenging with deep GCNs (i.e., 56\\nlayers) (Kipf & Welling, 2016) by the residual connection (He et al., 2016), however, the computation\\ncost cannot endure in the program especially for a large and complex program.\\nTo address these challenges, we propose a framework for automatic code summarization, namely\\nHybrid GNN (HGNN) . SpeciÔ¨Åcally, from the source code, we Ô¨Årst construct a code property graph\\n(CPG) based on the abstract syntax tree (AST) with different types of edges ( i.e., Flow To, Reach). In\\norder to combine the beneÔ¨Åts of both retrieval-based and generation-based methods, we propose a\\nretrieval-based augmentation mechanism to retrieve the source code that is most similar to the current\\nprogram from the retrieval database (excluding the current program itself), and add the retrieved code\\nas well as the corresponding summary as auxiliary information for training the model. In order to\\ngo beyond local graph neighborhood information, and capture global interactions in the program,\\nwe further propose an attention-based dynamic graph by learning global attention scores ( i.e., edge\\nweights) in the augmented static CPG. Then, a hybrid message passing (HMP) is performed on both\\nstatic and dynamic graphs. We also release a new code summarization benchmark by crawling data\\nfrom popular and diversiÔ¨Åed projects containing 95k+ functions in Cprogramming language and\\nmake it public1. We highlight our main contributions as follows:\\n‚Ä¢We propose a general-purpose framework for automatic code summarization, which combines the\\nbeneÔ¨Åts of both retrieval-based and generation-based methods via a retrieval-based augmentation\\nmechanism.\\n‚Ä¢We innovate a Hybrid GNN by fusing the static graph (based on code property graph) and dynamic\\ngraph (via structure-aware global attention mechanism) to mitigate the limitation of the GNN on\\ncapturing global graph information.\\n‚Ä¢ We release a new challenging Cbenchmark for the task of source code summarization.\\n‚Ä¢We conduct an extensive experiment to evaluate our framework. The proposed approach achieves\\nthe state-of-the-art performance and improves existing approaches by 1.42,2.44 and1.29 in terms\\nof BLEU-4, ROUGE-L and METEOR metrics.\\n2 H YBRID GNN F RAMEWORK\\nIn this section, we introduce the proposed framework Hybrid GNN (HGNN) , as shown in Figure 1,\\nwhich mainly includes four components: 1) Retrieval-augmented Static Graph Construction (c.f.,\\nSection 2.2), which incorporates retrieved code-summary pairs to augment the original code for\\nlearning. 2) Attention-based Dynamic Graph Construction ( c.f.,Section 2.3), which allows message\\npassing among any pair of nodes via a structure-aware global attention mechanism. 3) HGNN , (c.f.,\\n1https://github.com/shangqing-liu/CCSD-benchmark-for-code-summarization\\n2', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 1}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nRetrievedSummarySourceCode\\nRetrievalCPGAug\\nStatic Message Passing\\nAX(     +     ‚Ä¶     +      )       Dynamic Message PassingHybrid\\nDecoder\\nGeneratedSummaryHybrid GNNRetrieval-augmentedGraph\\nSummaryEncodingGraphEmbeddingGRU Updates\\nNode EmbeddingsAttention-basedGraphRetrievalDatabaseSourceCPGAggregation\\nFigure 1: The overall architecture of the proposed HGNN framework.\\nSection 2.4), which incorporates information from both static graphs and dynamic graphs with Hybrid\\nMessage Passing. 4) Decoder ( c.f.,Section 2.5), which utilizes an attention-based LSTM (Hochreiter\\n& Schmidhuber, 1997) model to generate a summary.\\n2.1 P ROBLEM FORMULATION\\nIn this work, we focus on generating natural language summaries for the given functions (Wan et al.,\\n2018; Zhang et al., 2020). A simple example is illustrated in Listing 1, which is crawled from Linux\\nKernel. Our goal is to generate the best summary ‚Äúset the time of day clock‚Äù based on the given source\\ncode. Formally, we deÔ¨Åne a dataset as D={(c,s)|c‚ààC,s‚ààS}, wherecis the source code of a\\nfunction in the function set Candsrepresents its targeted summary in the summary set S. The task of\\ncode summarization is, given a source code c, to generate the best summary consisting of a sequence\\nof tokens ÀÜs= (t1,t2,...,t T)that maximizes the conditional likelihood ÀÜs= argmaxsP(s|c).\\nSource Code:\\nint pdc_tod_set(unsigned long sec, unsigned long usec){\\nint retval; unsigned long flags;\\nspin_lock_irqsave(&pdc_lock, flags);\\nretval = mem_pdc_call(PDC_TOD, PDC_TOD_WRITE, sec, usec);\\nspin_unlock_irqrestore(&pdc_lock, flags);\\nreturn retval;\\n}\\nGround-Truth: set the time of day clock\\nListing 1: An example in our dataset crawled from Linux Kernel.\\n2.2 R ETRIEVAL -AUGMENTED STATIC GRAPH\\n2.2.1 G RAPH INITIALIZATION\\nThe source code of a function can be represented as Code Property Graph (CPG) (Yamaguchi et al.,\\n2014), which is built on the abstract syntax tree (AST) with different type of edges ( i.e., Flow To,\\nControl, DeÔ¨Åne/Use, Reach). Formally, one raw function ccould be represented by a multi-edged\\ngraphg(V,E), whereVis the set of AST nodes, (v,u)‚ààEdenotes the edge between the node vand\\nthe nodeu. A nodevconsists of two parts: the node sequence and the node type . An illustrative\\nexample is shown in Figure 2. For example, in the red node, a%2 == 0 is the node sequence and\\nCondition is the node type. An edge (v,u)has a type, named edge type ,e.g., AST type and Flow\\nTo type. For more details about the CPG, please refer to Appendix A.\\nInitialization Representation. Given a CPG, we utilize a BiLSTM to encode its nodes. We represent\\neach token of the node sequence and each edge type using the learned embedding matrix Eseqtoken\\nandEedgetype, respectively. Then nodes and edges of the CPG can be encoded as:\\nh1, ...,hl= BiLSTM( Eseqtoken\\nv,1 , ...,Eseqtoken\\nv,l )\\nencode _node (v) = [h‚Üí\\nl;h‚Üê\\n1]\\nencode _edge(v, u) =Eedgetype\\nv,u if(v, u)‚àà Eelse 0(1)\\n3', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 2}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nEntryCFGEntryNodeexample()FunctionDefint a = rand( )IdentiÔ¨ÅerDeclStatementa % 2 == 0Conditionint b = a ++IdentiÔ¨ÅerDeclStatementEXITCFGExitNodecall(b)ExpressionStatement ParameterListvoidReturnTypeintIdentiÔ¨ÅerDeclTypeaIdentiÔ¨Åera = rand ( )AssignmentExprrand ( )CallExpressiona % 2 == 0EqualityExpressiona % 2MultiplicativeExpressionaIdentiÔ¨Åer2PrimaryExpression0PrimaryExpressionintIdentiÔ¨ÅerDeclTypebIdentiÔ¨Åerb = a ++AssignmentExpra ++IncDecOp++IncDecaIdentiÔ¨ÅeraSymbolcallCalleebArgumentbIdentiÔ¨ÅerbSymbolbIdentiÔ¨ÅerASTFlow  ToControlDeÔ¨Åne/UseReachvoid example ( ){   int a = rand( );   if ( a % 2 == 0 )   {        int b = a++;        call(b);   }}A simple code exampleparse\\nFigure 2: An example of Code Property Graph (CPG).\\nwherelis the number of tokens in the node sequence of v. For the sake of simplicity, in the following\\nsection, we use hvandev,uto represent the embedding of the node vand the edge (v,u), respectively,\\ni.e.,encode _node (v)andencode _edge(v,u). Given the source code cof a function as well as the\\nCPGg(V,E),Hc‚ààRm√óddenotes the initial node matrix of the CPG, where mis the total number\\nof nodes in the CPG and dis the dimension of the node embedding.\\n2.2.2 R ETRIEVAL -BASED AUGMENTATION\\nWhile retrieval-based methods can perform reasonably well on examples that are similar to those\\nexamples from a retrieval database, they typically have low generalization performance and might\\nperform poorly on dissimilar examples. On the contrary, generation-based methods usually have better\\ngeneralization performance, but cannot take advantage of similar examples from the retrieval database.\\nIn this work, we propose to combine the beneÔ¨Åts of the two worlds, and design a retrieval-augmented\\ngeneration framework for the task of code summarization.\\nIn principle, the goal of code summarization is to learn a mapping from source code cto the natural\\nlanguage summary s=f(c). In other words, for any source code c‚Ä≤, a code summarization system\\ncan produce its summary s‚Ä≤=f(c‚Ä≤). Inspired by this observation, conceptually, we can derive\\nthe following formulation s=f(c)‚àíf(c‚Ä≤) +s‚Ä≤. This tells us that we can actually compute the\\nsemantic difference between candc‚Ä≤, and further obtain the desired summary sforcby considering\\nboth the above semantic difference and s‚Ä≤which is the summary for c‚Ä≤. Mathmatically, our goal\\nbecomes to learn a function which takes as input (c,c‚Ä≤,s‚Ä≤), and outputs the summary sforc, that\\nis,s=g(c,c‚Ä≤,s‚Ä≤). This motivates us to design our Retrieval-based Augmentation mechanism, as\\ndetailed below.\\nStep 1: Retrieving. For each sample (c,s)‚ààD, we retrieve the most similar sample: (c‚Ä≤,s‚Ä≤) =\\nargmax(c‚Ä≤,s‚Ä≤)‚ààD‚Ä≤sim(c,c‚Ä≤), wherecÃ∏=c‚Ä≤,D‚Ä≤is a given retrieval database and sim(c,c‚Ä≤)is the text\\nsimilarity. Following Zhang et al. (2020), we utilize Lucene for retrieval and calculate the similarity\\nscorezbetween the source code cand the retrieved code c‚Ä≤via dynamic programming (Bellman,\\n1966), namely, z= 1‚àídis(c,c‚Ä≤)\\nmax(|c|,|c‚Ä≤|), wheredis(c,c‚Ä≤)is the text edit distance.\\nStep 2: Retrieved Code-based Augmentation. Given the retrieved source code c‚Ä≤for the current\\nsamplec, we adopt a fusion strategy to inject retrieved semantics into the current sample. The fusion\\nstrategy is based on their initial graph representations ( HcandHc‚Ä≤) with an attention mechanism:\\n‚Ä¢To capture the relevance between candc‚Ä≤, we design an attention function, which computes the\\nattention score matrix Aaugbased on the embeddings of each pair of nodes in CPGs of candc‚Ä≤:\\nAaug‚àùexp(ReLU( HcWC)ReLU( Hc‚Ä≤WQ)T) (2)\\nwhere WC,WQ‚ààRd√ódis the weight matrix with d-dim embedding size and ReLU is the\\nrectiÔ¨Åed linear unit.\\n‚Ä¢We then multiply the attention matrix Aaugwith the retrieved representation Hc‚Ä≤to inject the\\nretrieved features into Hc:\\nH‚Ä≤\\nc=zAaugHc‚Ä≤ (3)\\nwherez‚àà[0,1]is the similarity score and computed from Step 1, which is introduced to weaken\\nthe negative impact of c‚Ä≤on the original training data c,i.e., when the similarity of candc‚Ä≤is low.\\n‚Ä¢ Finally, we merge H‚Ä≤\\ncand the original Hcto get the Ô¨Ånal representation of c.\\ncomp =Hc+H‚Ä≤\\nc (4)\\n4', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 3}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nwhere comp is the augmented node representation additionally encoding the retrieved semantics.\\nStep 3: Retrieved Summary-based Augmentation. We further encode the retrieved summary s‚Ä≤\\nwith another BiLSTM model. We represent each token t‚Ä≤\\niofs‚Ä≤using the learned embedding matrix\\nEseqtoken. Thens‚Ä≤can be encoded as:\\nht‚Ä≤\\n1,...,ht‚Ä≤\\nT= BiLSTM( Eseqtoken\\nt‚Ä≤\\n1,...,Eseqtoken\\nt‚Ä≤\\nT) (5)\\nwhere ht‚Ä≤\\niis the hidden state of the BiLSTM model for the token t‚Ä≤\\niins‚Ä≤andTis the length of\\ns‚Ä≤. We multiply [ht‚Ä≤\\n1;...;ht‚Ä≤\\nT]with the similarity score z, computed from Step 1, and concatenate\\nit with the graph encoding results (i.e., the GNN encoder outputs) to obtain the input, namely,\\n[GNN output ;zht‚Ä≤\\n1;...;zht‚Ä≤\\nT], to the decoder.\\n2.3 A TTENTION -BASED DYNAMIC GRAPH\\nDue to that GNN-based encoders usually consider the k-hop neighborhood, the global relation among\\nnodes in the static graph (see Section 2.2.1) may be ignored. In order to better capture the global\\nsemantics of source code, based on the static graph, we propose to dynamically construct a graph via\\nstructure-aware global attention mechanism, which allows message passing among any pair of nodes.\\nThe attention-based dynamic graph can better capture the global dependency among nodes, and thus\\nsupplement the static graph.\\nStructure-aware Global Attention. The construction of the dynamic graph is motivated by the\\nstructure-aware self-attention mechanism proposed in Zhu et al. (2019). Given the static graph, we\\ncompute a corresponding dense adjacency matrix Adynbased on a structure-aware global attention\\nmechanism, and obtain the constructed graph, namely, attention-based dynamic graph .\\nAdyn\\nv,u=ReLU( hT\\nvWQ)(ReLU( hT\\nuWK) + ReLU( eT\\nv,uWR))T\\n‚àö\\nd(6)\\nwherehv,hu‚ààcomp are the augmented node embedding for any node pair (v,u)in the CPG. Note\\nthat the global attention considers each pair of nodes of the CPG, regardless of whether there is an\\nedge between them. ev,u‚ààRdeis the edge embedding and WQ,WK‚ààRd√ód,WR‚ààRde√ódare\\nparameter matrices, deanddare the dimensions of edge embedding and node embedding, respectively.\\nThe adjacency matrix Adynwill be further row normalized to obtain ÀúAdyn, which will be used to\\ncompute dynamic message passing (see Section 2.4).\\nÀúAdyn= softmax( Adyn) (7)\\n2.4 H YBRID GNN\\nTo better incorporate the information of the static graph and the dynamic graph, we propose the\\nHybrid Message Passing (HMP), which are performed on both retrieval-augmented static graph and\\nattention-based dynamic graph.\\nStatic Message Passing. For every node vat each computation hop kin the static graph, we apply\\nan aggregation function to calculate the aggregated vector hk\\nvby considering a set of neighboring\\nnode embeddings computed from the previous hop.\\nhk\\nv= SUM({hk‚àí1\\nu|‚àÄu‚ààN (v)}) (8)\\nwhereN(v)is a set of the neighboring nodes which are directly connected with v. For each node v,\\nh0\\nvis the initial augmented node embedding of v,i.e.,hv‚ààcomp .\\nDynamic Message Passing. The node information and edge information are propagated on the\\nattention-based dynamic graph with the adjacency matrices ÀúAdyn, deÔ¨Åned as\\nh‚Ä≤k\\nv=‚àë\\nuÀúAdyn\\nv,u(WVh‚Ä≤k‚àí1\\nu+WFev,u)(9)\\nwherevanduare any pair of nodes, WV‚ààRd√ód,WF‚ààRd√ódeare learned matrices, and ev,u\\nis the embedding of the edge connecting vandu. Similarly, h‚Ä≤0\\nvis the initial augmented node\\nembedding of vincomp .\\n5', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 4}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nHybrid Message Passing. Given the static/dynamic aggregated vectors hk\\nv/h‚Ä≤k\\nvfor static and\\ndynamic graphs, we fuse both vectors and feed the resulting vector to a Gated Recurrent Unit (GRU)\\nto update node representations.\\nfk\\nv= GRU( fk‚àí1\\nv,Fuse(hk\\nv,h‚Ä≤k\\nv)) (10)\\nwhere f0\\nvis the augmented node initialization in comp . The fusion function Fuse is designed as a\\ngated sum of two inputs.\\nFuse(a,b) =z‚äôa+ (1‚àíz)‚äôb z =œÉ(Wz[a;b;a‚äôb;a‚àíb] +bz) (11)\\nwhere Wzandbzare learnable weight matrix and vector, ‚äôis the component-wise multiplication, œÉ\\nis a sigmoid function and zis a gating vector. After nhops of GNN computation, we obtain the Ô¨Ånal\\nnode representation fn\\nvand then apply max-pooling over all nodes {fn\\nv|‚àÄv‚ààV} to get the graph\\nrepresentation.\\n2.5 D ECODER\\nThe decoder is similar with other state-of-the-art Seq2seq models (Bahdanau et al., 2014; Luong\\net al., 2015) where an attention-based LSTM decoder is used. The decoder takes the input of the\\nconcatenation of the node representation and the representation of the retrieved summary s‚Ä≤, namely,\\n[fn\\nv1;...;fn\\nvm;zht‚Ä≤\\n1;...;zht‚Ä≤\\nT], wheremis the number of nodes in the input CPG graph. The initial\\nhidden state of the decoder is the fusion (Eq. 11) of the graph representation and the weighted (i.e.,\\nmultiply similarity score z) Ô¨Ånal state of the retrieved summary BiLSTM encoder.\\nWe train the model with the cross-entropy loss, deÔ¨Åned as L=‚àë\\nt‚àílogP(s‚àó\\nt|c,s‚àó\\n<t), wheres‚àó\\ntis\\nthe word at the t-th position of the ground-truth output and cis the source code of the function. To\\nalleviate the exposure bias, we utilize schedule teacher forcing (Bengio et al., 2015). During the\\ninference, we use beam search to generate Ô¨Ånal results.\\n3 E XPERIMENTS\\n3.1 S ETUP\\nWe evaluate our proposed framework against a number of state-of-the-art methods. SpeciÔ¨Åcally,\\nwe classify the selected baseline methods into three groups: 1) Retrieval-based approaches: TF-\\nIDF (Haiduc et al., 2010) and NNGen (Liu et al., 2018), 2) Sequence-based approaches: CODE-\\nNN (Iyer et al., 2016; Barone & Sennrich, 2017), Transformer (Ahmad et al., 2020), Hybrid-\\nDRL (Wan et al., 2018), Rencos (Zhang et al., 2020) and Dual model (Wei et al., 2019), 3) Graph-\\nbased approaches: SeqGNN (Fernandes et al., 2018). In addition, we implemented two another\\ngraph-based baselines: GCN2Seq and GAT2Seq, which respectively adopt the Graph Convolution\\n(Kipf & Welling, 2016) and Graph Attention (Velickovic et al., 2018) as the encoder and a LSTM as\\nthe decoder for generating summaries. Note that Rencos (Zhang et al., 2020) combines the retrieval\\ninformation into Seq2Seq model, we classify it into Sequence-based approaches. More detailed\\ndescription about baselines and the conÔ¨Åguration of HGNN can be found in the Appendix B and C.\\nExisting benchmarks (Barone & Sennrich, 2017; Hu et al., 2018b) are all based on high-level\\nprogramming language i.e., Java, Python. Furthermore, they have been conÔ¨Årmed to have extensive\\nduplication, making the model overÔ¨Åt to the training data that overlapped with the testset (Fernandes\\net al., 2018; Allamanis, 2019). We are the Ô¨Årst to explore neural summarization on Cprogramming\\nlanguage, and make our CCode Summarization Dataset (CCSD) public to beneÔ¨Åt academia and\\nindustry. We crawled from popular Crepositories on GitHub and extracted function-summary pairs\\nbased on the documents of functions. After a deduplication process, we kept 95k+ unique function-\\nsummary pairs. To further test the model generalization ability, we construct in-domain functions\\nand out-of-domain functions by dividing the projects into two sets, denoted as aandb. For each\\nproject ina, we randomly select some of the functions in this project as the training data and the\\nunselected functions are the in-domain validation/test data. All functions in projects bare regarded as\\nout-of-domain test data. Finally, we obtain 84,316 training functions, 4,432 in-domain validation\\nfunctions, 4,203 in-domain test functions and 2,330 out-of-domain test functions. For the retrieval\\naugmentation, we use the training set as the retrieval database, i.e.,D‚Ä≤=D(see Step 1 in Section\\n2.2.2). For more details about data processing, please refer to Appendix D.\\n6', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 5}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nTable 1: Automatic evaluation results (in %) on the CCSD test set.\\nIn-domain Out-of-domain OverallMethodsBLEU-4 ROUGE-L METEOR BLEU-4 ROUGE-L METEOR BLEU-4 ROUGE-L METEOR\\nTF-IDF 15.20 27.98 13.74 5.50 15.37 6.84 12.19 23.49 11.43\\nNNGen 15.97 28.14 13.82 5.74 16.33 7.18 12.76 23.93 11.58\\nCODE-NN 10.08 26.17 11.33 3.86 15.25 6.19 8.24 22.28 9.61\\nHybrid-DRL 9.29 30.00 12.47 6.30 24.19 10.30 8.42 28.64 11.73\\nTransformer 12.91 28.04 13.83 5.75 18.62 9.89 10.69 24.65 12.02\\nDual Model 11.49 29.20 13.24 5.25 21.31 9.14 9.61 26.40 11.87\\nRencos 14.80 31.41 14.64 7.54 23.12 10.35 12.59 28.45 13.21\\nGCN2Seq 9.79 26.59 11.65 4.06 18.96 7.76 7.91 23.67 10.23\\nGAT2Seq 10.52 26.17 11.88 3.80 16.94 6.73 8.29 22.63 10.00\\nSeqGNN 10.51 29.84 13.14 4.94 20.80 9.50 8.87 26.34 11.93\\nHGNN w/o augment & static 11.75 29.59 13.86 5.57 22.14 9.41 9.98 26.94 12.05\\nHGNN w/o augment & dynamic 11.85 29.51 13.54 5.45 21.89 9.59 9.93 26.80 12.21\\nHGNN w/o augment 12.33 29.99 13.78 5.45 22.07 9.46 10.26 27.17 12.32\\nHGNN w/o static 15.93 33.67 15.67 7.72 24.69 10.63 13.44 30.47 13.98\\nHGNN w/o dynamic 15.77 33.84 15.67 7.64 24.72 10.73 13.31 30.59 14.01\\nHGNN 16.72 34.29 16.25 7.85 24.74 11.05 14.01 30.89 14.50\\nSimilar to previous works (Zhang et al., 2020; Wan et al., 2018; Fernandes et al., 2018; Iyer et al.,\\n2016), BLEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005) and ROUGE-L (Lin, 2004)\\nare used as our automatic evaluation metrics. These metrics are popular for evaluating machine\\ntranslation and text summarization tasks. Except for these automatic metrics, we also conduct a\\nhuman evaluation study. We invite 5 PhD students and 10 master students as volunteers, who have\\nrich C programming experiences. The volunteers are asked to rank summaries generated from\\nthe anonymized approaches from 1 to 5 ( i.e., 1: Poor, 2: Marginal, 3: Acceptable, 4: Good, 5:\\nExcellent) based on the relevance of the generated summary to the source code and the degree of\\nsimilarity between the generated summary and the actual summary. SpeciÔ¨Åcally, we randomly choose\\n50 functions for each model with the corresponding generated summaries and ground-truths. We\\ncalculate the average score and the higher the score, the better the quality.\\n3.2 C OMPARISON WITH THE BASELINES\\nTable 1 shows the evaluation results including two parts: the comparison with baselines and the\\nablation study. Consider the comparison with state-of-the-art baselines, in general, we Ô¨Ånd that\\nour proposed model outperforms existing methods by a signiÔ¨Åcant margin on both in-domain and\\nout-of-domain datasets, and shows good generalization performance. Compared with others, on\\nin-domain dataset, the retrieval-based approaches could achieve competitive performance on BLEU-4,\\nhowever ROUGE-L and METEOR are fare less than ours. Moreover, they do not perform well on\\nthe out-of-domain dataset. Compared with the graph-based approaches ( i.e., GCN2Seq, GAT2Seq\\nand SeqGNN), even without augmentation ( HGNN w/o augment ), our approach still outperforms\\nthem, which further demonstrates the effectiveness of Hybrid GNN for additionally capturing global\\ngraph information. Compared with Rencos that also considers the retrieved information in the\\nSeq2Seq model, its performance is still lower than HGNN . On the overall dataset including both\\nof in-domain and out-of-domain data, our model achieves 14.01 ,30.89 and14.50 , outperforming\\ncurrent state-of-the-art method Rencos by 1.42,2.44 and1.29 in terms of BLEU-4, ROUGE-L and\\nMETEOR metrics.\\n3.3 A BLATION STUDY\\nWe also conduct an ablation study to evaluate the impact of different components of our framework,\\ne.g., retrieval-based augmentation, static graph and dynamic graph in the last row of Table 1. Overall,\\nwe found that 1) retrieval-augmented mechanism could contribute to the overall model performance\\n(HGNN vs.HGNN w/o augment ). Compared with HGNN , we see that the performance of HGNN w/o\\nstatic andHGNN w/o dynamic decreases, which demonstrates the effectiveness of the Hybrid GNN\\nand 2) the performance without static graph is worse than the performance without dynamic graph in\\nROUGE-L and METEOR, however, BLEU-4 is higher than the performance without dynamic graph.\\nTo further understand the impact of the static graph and dynamic graph, we evaluate the performance\\nwithout augmentation and static graph/dynamic graph (see HGNN w/o augment& static andHGNN\\nw/o augment& dynamic ). Compared with HGNN w/o augment , the results further conÔ¨Årm the\\neffectiveness of the Hybrid GNN ( i.e., static graph and dynamic graph).\\n7', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 6}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nTable 2: Human evaluation results on the CCSD test set.\\nMetrics NNGen Transformer Rencos SeqGNN HGNN\\nRelevance 3.23 3.17 3.48 3.09 3.69\\nSimilarity 3.18 3.02 3.32 3.06 3.51\\nTable 3: Examples of generated summaries on the CCSD test set.\\nExample Example 1 Example 2\\nSource Codestatic void strInit(Str *p){\\np->z = 0;\\np->nAlloc = 0;\\np->nUsed = 0;\\n}void ReleaseCedar(CEDAR *c){\\nif (c == NULL)\\nreturn;\\nif (Release(c->ref) == 0)\\nCleanupCedar(c);\\n}\\nGround-Truth initialize a str object release reference of the cedar\\nNNGen free the string release the virtual host\\nTransformer reset a string release of the cancel object\\nRencos append a raw string to the json string release of the cancel object\\nSeqGNN initialize the string release cedar communication mode\\nHGNN initialize a string object release reference of cedar\\nWe also conduct experiments to investigate the impact of code-based augmentation and summary-\\nbased augmentation. Overall, we found that the summary-based augmentation could contribute more\\nthan the code-based augmentation. For example, after adding the code-based augmentation, the\\nperformance can be 10.22, 27.54 and 12.49 in terms of BLUE-4, ROUGE-L and METEOR on the\\noverall dataset. With the summary-based augmentation, the results can reach to 13.76, 30.59 and\\n14.11. Compared with the results without augmentation ( i.e., 10.26. 27.17, 12.32 with HGNN w/o\\naugment ), we can see that code-based augmentation could have some improvement, but the effect\\nis not signiÔ¨Åcant compared with summary-based augmentation. We conjecture that, due to that the\\ncode and summary are heterogeneous, the summary-based augmentation has a more direct impact on\\nthe code summarization task. When combining both code-based augmentation and summary-based\\naugmentation, we can achieve the best results ( i.e., 14.01, 30.89, 14.50). We plan to explore more\\ncode-based augmentation ( e.g., semantic-equivalent code transformation) in our future work.\\n3.4 H UMAN EVALUATION\\nAs shown in Table 2, we perform a human evaluation on the overall dataset to assess the quality\\nof the generated summaries by our approach, NNGen, Transformer, Rencos and SeqGNN in terms\\nof relevance and similarity. As depicted in Table 1, NNGen, Rencos and SeqGNN are the best\\nretrieval-based, sequence-based, and graph-based approaches, respectively. We also compare with\\nTransformer as it has been widely used in natural language processing. The results show that our\\nmethod can generate better summaries which are more relevant with the source code and more similar\\nwith the ground-truth summaries.\\n3.5 C ASE STUDY\\nTo perform qualitative analysis, we present two examples with generated summaries by different\\nmethods from the overall data set, shown in Table 3. We can see that, in the Ô¨Årst example, our\\napproach can learn more code semantics, i.e.,pis a self-deÔ¨Åned struct variable. Thus, we could\\ngenerate a token object for the variable p. However, other models can only produce string . Example 2\\nis a more difÔ¨Åcult function with the functionality to ‚Äúrelease reference of cedar‚Äù, as compared to other\\nbaselines, our approach effectively captures the functionality and generates a more precise summary.\\n3.6 E XTENSION ON THE PYTHON DATASET\\nWe conducted additional experiments on a public dataset, i.e., the Python Code Summarization\\nDataset (PCSD), which was also used in Rencos (the most competitive baseline in our paper). We\\nfollow the setting of Rencos and split PCSD into the training set, validation set and testing set with\\nfractions of 60%, 20% and 20%. We construct the static graph based on AST. The decoding step\\nis set to 50, followed by Rencos, and the other settings are the same with CCSD. We compare our\\nmethods on PCSD against various competitive baselines, i.e., NNGen, CODE-NN, Rencos and\\n8', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 7}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nTable 4: Automatic evaluation results (in %) on the PCSD test set.\\nMethods BLEU-4 ROUGE-L METEOR\\nNNGen 21.60 31.61 15.96\\nCODE-NN 16.39 28.99 13.68\\nTransformer 17.06 31.16 14.37\\nRencos 24.02 36.21 18.07\\nHGNN w/o static 24.06 38.28 18.66\\nHGNN w/o dynamic 24.13 38.64 18.93\\nHGNN 24.42 39.91 19.48\\nTransformer, which are either retrieval-based, generation-based or hybrid methods. The results are\\nshown in Table 4. The results indicate that, compared with the best results from NNGen, CODE-NN,\\nRencos and Transformer, our method can improve the performance by 0.40, 3.70 and 1.41 in terms of\\nBLEU-4, ROUGE-L and METEOR. We also conduct the ablation study on PCSD to demonstrate\\nthe usefulness of the static graph ( i.e., HGNN w/o dynamic) and dynamic graph ( i.e., HGNN w/o\\nstatic). The results also demonstrate that both static graph and dynamic graph can contribute to our\\nframework. In summary, the results on both our released benchmark (CCSD) and existing benchmark\\n(PCSD) demonstrate the effectiveness and the scalability of our method.\\n4 R ELATED WORK\\nSource Code Summarization Early works (Eddy et al., 2013; Haiduc et al., 2010; Wong et al., 2015;\\n2013) for code summarization focused on using information retrieval to retrieve summaries. Later\\nworks attempted to employ attentional Seq2Seq model on the source code (Iyer et al., 2016; Siow\\net al., 2020) or some variants, i.e., AST (Hu et al., 2018a; Alon et al., 2018; Liu et al., 2020) for\\ngeneration. However, these works are based on sequential models, ignoring rich code semantics.\\nSome latest attempts (LeClair et al., 2020; Fernandes et al., 2018) embedded program semantics into\\nGNNs. but they mainly rely on simple representations, which are limited to learn full semantics.\\nGraph Neural Networks Over the past few years, GNNs (Li et al., 2015; Hamilton et al., 2017;\\nKipf & Welling, 2016; Chen et al., 2020b) have attracted increasing attention with many successful\\napplications in computer vision (Norcliffe-Brown et al., 2018), natural language processing (Xu et al.,\\n2018a; Chen et al., 2020d;c;e). Because by design GNNs can model graph-structured data, recently,\\nsome works have extended the widely used Seq2Seq architectures to Graph2Seq architectures for\\nvarious tasks including machine translation (Beck et al., 2018), and graph (e.g., AMR, SQL)-to-text\\ngeneration (Zhu et al., 2019; Xu et al., 2018b). Some works have also attempted to encode programs\\nwith graphs for diverse tasks e.g., V ARNAMING/V ARMISUSE (Allamanis et al., 2017), Source\\nCode Vulnerability Detection (Zhou et al., 2019). As compared to these works, we innovate a hybrid\\nmessage passing GNN performed on both static graph and dynamic graph for message fusion.\\n5 C ONCLUSION AND FUTURE WORK\\nIn this paper, we proposed a general-purpose framework for automatic code summarization. A novel\\nretrieval-augmented mechanism is proposed for combining the beneÔ¨Åts of both retrieval-based and\\ngeneration-based approaches. Moreover, to capture global semantics among nodes, we develop a\\nhybrid message passing GNN based on both static and dynamic graphs. The evaluation shows that\\nour approach improves state-of-the-art techniques substantially. Our future work includes: 1) we plan\\nto introduce more information such as API knowledge to learn the better semantics of programs, 2)\\nwe explore more code-based augmentation techniques to improve the performance and 3) we plan to\\nadopt the existing techniques such as (Du et al., 2019; Xie et al., 2019a;b; Ma et al., 2018) to evaluate\\nthe robustness of the trained model.\\n6 A CKNOWLEDGEMENT\\nThis research is supported by the National Research Foundation, Singapore under its AI Singapore\\nProgramme (AISG Award No: AISG2-RP-2020-019), the National Research Foundation under its\\nNational Cybersecurity R&D Program (Award No. NRF2018NCR-NCR005-0001), the Singapore\\nNational Research Foundation under NCR Award Number NRF2018NCR-NSOE003-0001 and NRF\\nInvestigatorship NRFI06-2020-0022.\\n9', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 8}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nREFERENCES\\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. A transformer-based\\napproach for source code summarization. arXiv preprint arXiv:2005.00653 , 2020.\\nMiltiadis Allamanis. The adverse effects of code duplication in machine learning models of code. In\\nProceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms,\\nand ReÔ¨Çections on Programming and Software , pp. 143‚Äì153, 2019.\\nMiltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs\\nwith graphs. arXiv preprint arXiv:1711.00740 , 2017.\\nMiltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of machine\\nlearning for big code and naturalness. ACM Computing Surveys (CSUR) , 51(4):1‚Äì37, 2018.\\nUri Alon, Shaked Brody, Omer Levy, and Eran Yahav. code2seq: Generating sequences from\\nstructured representations of code. arXiv preprint arXiv:1808.01400 , 2018.\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.\\nSatanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved\\ncorrelation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic\\nevaluation measures for machine translation and/or summarization , pp. 65‚Äì72, 2005.\\nAntonio Valerio Miceli Barone and Rico Sennrich. A parallel corpus of python functions and\\ndocumentation strings for automated code documentation and code generation. arXiv preprint\\narXiv:1707.02275 , 2017.\\nDaniel Beck, Gholamreza Haffari, and Trevor Cohn. Graph-to-sequence learning using gated graph\\nneural networks. arXiv preprint arXiv:1806.09835 , 2018.\\nRichard Bellman. Dynamic programming. Science , 153(3731):34‚Äì37, 1966.\\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence\\nprediction with recurrent neural networks. In Advances in Neural Information Processing Systems ,\\npp. 1171‚Äì1179, 2015.\\nDeli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-\\nsmoothing problem for graph neural networks from the topological view. In AAAI , pp. 3438‚Äì3445,\\n2020a.\\nYu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural networks:\\nBetter and robust node embeddings. Advances in Neural Information Processing Systems , 33,\\n2020b.\\nYu Chen, Lingfei Wu, and Mohammed J. Zaki. GraphÔ¨Çow: Exploiting conversation Ô¨Çow with\\ngraph neural networks for conversational machine comprehension. In Proceedings of the Twenty-\\nNinth International Joint Conference on ArtiÔ¨Åcial Intelligence , pp. 1230‚Äì1236. International Joint\\nConferences on ArtiÔ¨Åcial Intelligence Organization, 2020c.\\nYu Chen, Lingfei Wu, and Mohammed J. Zaki. Reinforcement learning based graph-to-sequence\\nmodel for natural question generation. In Proceedings of the 8th International Conference on\\nLearning Representations , Apr. 26-30, 2020d.\\nYu Chen, Lingfei Wu, and Mohammed J Zaki. Toward subgraph guided knowledge graph question\\ngeneration with graph neural networks. arXiv preprint arXiv:2004.06015 , 2020e.\\nXiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. Deepstellar: Model-based\\nquantitative analysis of stateful deep learning systems. In Proceedings of the 2019 27th ACM Joint\\nMeeting on European Software Engineering Conference and Symposium on the Foundations of\\nSoftware Engineering , pp. 477‚Äì487, 2019.\\n10', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 9}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nBrian P Eddy, Jeffrey A Robinson, Nicholas A Kraft, and Jeffrey C Carver. Evaluating source code\\nsummarization techniques: Replication and expansion. In 2013 21st International Conference on\\nProgram Comprehension (ICPC) , pp. 13‚Äì22. IEEE, 2013.\\nPatrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. Structured neural summarization.\\narXiv preprint arXiv:1811.01824 , 2018.\\nSonia Haiduc, Jairo Aponte, Laura Moreno, and Andrian Marcus. On the use of automated text\\nsummarization techniques for summarizing source code. In 2010 17th Working Conference on\\nReverse Engineering , pp. 35‚Äì44. IEEE, 2010.\\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In\\nAdvances in neural information processing systems , pp. 1024‚Äì1034, 2017.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\\npp. 770‚Äì778, 2016.\\nSepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation , 9(8):\\n1735‚Äì1780, 1997.\\nXing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. Deep code comment generation. In Proceedings of\\nthe 26th Conference on Program Comprehension , pp. 200‚Äì210, 2018a.\\nXing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. Summarizing source code with transferred\\napi knowledge. 2018b.\\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Summarizing source code\\nusing a neural attention model. In Proceedings of the 54th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers) , pp. 2073‚Äì2083, 2016.\\nSiyuan Jiang, Ameer Armaly, and Collin McMillan. Automatically generating commit messages\\nfrom diffs using neural machine translation. In 2017 32nd IEEE/ACM International Conference on\\nAutomated Software Engineering (ASE) , pp. 135‚Äì146. IEEE, 2017.\\nToshihiro Kamiya, Shinji Kusumoto, and Katsuro Inoue. CcÔ¨Ånder: a multilinguistic token-based code\\nclone detection system for large scale source code. IEEE Transactions on Software Engineering ,\\n28(7):654‚Äì670, 2002.\\nThomas N Kipf and Max Welling. Semi-supervised classiÔ¨Åcation with graph convolutional networks.\\narXiv preprint arXiv:1609.02907 , 2016.\\nAlexander LeClair, Sakib Haque, Linfgei Wu, and Collin McMillan. Improved code summarization\\nvia a graph neural network. arXiv preprint arXiv:2004.02843 , 2020.\\nGuohao Li, Matthias M√ºller, Ali K. Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep\\nas cnns? In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul,\\nKorea (South), October 27 - November 2, 2019 , pp. 9266‚Äì9275. IEEE, 2019. doi: 10.1109/ICCV .\\n2019.00936. URL https://doi.org/10.1109/ICCV.2019.00936 .\\nYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural\\nnetworks. arXiv preprint arXiv:1511.05493 , 2015.\\nZhenmin Li, Shan Lu, Suvda Myagmar, and Yuanyuan Zhou. Cp-miner: Finding copy-paste and\\nrelated bugs in large-scale software code. IEEE Transactions on software Engineering , 32(3):\\n176‚Äì192, 2006.\\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\\nBranches Out , pp. 74‚Äì81. Association for Computational Linguistics, July 2004.\\nShangqing Liu, Cuiyun Gao, Sen Chen, Nie Lun Yiu, and Yang Liu. Atom: Commit message\\ngeneration based on abstract syntax tree and hybrid ranking. IEEE Transactions on Software\\nEngineering , 2020.\\n11', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 10}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nZhongxin Liu, Xin Xia, Ahmed E Hassan, David Lo, Zhenchang Xing, and Xinyu Wang. Neural-\\nmachine-translation-based commit message generation: how far are we? In Proceedings of the\\n33rd ACM/IEEE International Conference on Automated Software Engineering , pp. 373‚Äì384,\\n2018.\\nMinh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based\\nneural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\nLei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su,\\nLi Li, Yang Liu, et al. Deepgauge: Multi-granularity testing criteria for deep learning systems. In\\nProceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering ,\\npp. 120‚Äì131, 2018.\\nWill Norcliffe-Brown, Stathis Vafeias, and Sarah Parisot. Learning conditioned graph structures for\\ninterpretable visual question answering. In Advances in Neural Information Processing Systems ,\\npp. 8344‚Äì8353, 2018.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\\nevaluation of machine translation. In Proceedings of the 40th annual meeting on association for\\ncomputational linguistics , pp. 311‚Äì318. Association for Computational Linguistics, 2002.\\nJing Kai Siow, Cuiyun Gao, Lingling Fan, Sen Chen, and Yang Liu. Core: Automating review\\nrecommendation for code changes. In 2020 IEEE 27th International Conference on Software\\nAnalysis, Evolution and Reengineering (SANER) , pp. 284‚Äì295. IEEE, 2020.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\\nprocessing systems , pp. 5998‚Äì6008, 2017.\\nPetar Velickovic, Guillem Cucurull, A. Casanova, A. Romero, P. Li√≤, and Yoshua Bengio. Graph\\nattention networks. ArXiv , abs/1710.10903, 2018.\\nYao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S Yu. Improving\\nautomatic source code summarization via deep reinforcement learning. In Proceedings of the 33rd\\nACM/IEEE International Conference on Automated Software Engineering , pp. 397‚Äì407, 2018.\\nBolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. Code generation as a dual task of code summariza-\\ntion. In Advances in Neural Information Processing Systems , pp. 6563‚Äì6573, 2019.\\nEdmund Wong, Jinqiu Yang, and Lin Tan. Autocomment: Mining question and answer sites for\\nautomatic comment generation. In 2013 28th IEEE/ACM International Conference on Automated\\nSoftware Engineering (ASE) , pp. 562‚Äì567. IEEE, 2013.\\nEdmund Wong, Taiyue Liu, and Lin Tan. Clocom: Mining existing source code for automatic\\ncomment generation. In 2015 IEEE 22nd International Conference on Software Analysis, Evolution,\\nand Reengineering (SANER) , pp. 380‚Äì389. IEEE, 2015.\\nXiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jianjun Zhao, Bo Li,\\nJianxiong Yin, and Simon See. Deephunter: a coverage-guided fuzz testing framework for deep\\nneural networks. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software\\nTesting and Analysis , pp. 146‚Äì157, 2019a.\\nXiaofei Xie, Lei Ma, Haijun Wang, Yuekang Li, Yang Liu, and Xiaohong Li. Diffchaser: Detecting\\ndisagreements for deep neural networks. In IJCAI , pp. 5772‚Äì5778, 2019b.\\nKun Xu, Lingfei Wu, Zhiguo Wang, and Vadim Sheinin. Graph2seq: Graph to sequence learning\\nwith attention-based neural networks. arXiv preprint arXiv:1804.00823 , 2018a.\\nKun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Liwei Chen, and Vadim Sheinin. Sql-to-text generation\\nwith graph-to-sequence model. arXiv preprint arXiv:1809.05255 , 2018b.\\nFabian Yamaguchi, Nico Golde, Daniel Arp, and Konrad Rieck. Modeling and discovering vulnera-\\nbilities with code property graphs. In 2014 IEEE Symposium on Security and Privacy , pp. 590‚Äì604.\\nIEEE, 2014.\\n12', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 11}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nJian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. Retrieval-based neural\\nsource code summarization. In Proceedings of the 42nd International Conference on Software\\nEngineering. IEEE , 2020.\\nLingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. arXiv preprint\\narXiv:1909.12223 , 2019.\\nYaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. Devign: Effective vulnera-\\nbility identiÔ¨Åcation by learning comprehensive program semantics via graph neural networks. In\\nAdvances in Neural Information Processing Systems , pp. 10197‚Äì10207, 2019.\\nJie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. Modeling graph\\nstructure in transformer for better amr-to-text generation. arXiv preprint arXiv:1909.00136 , 2019.\\n13', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 12}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nAppendices\\nA D ETAILS ON CODE PROPERTY GRAPH\\nCode Property Graph (CPG) (Yamaguchi et al., 2014), which is constructed on abstract syntax tree\\n(AST), combines different edges (i.e., Flow to, Control) to represent the semantics of the program.\\nWe describe each representation combining with Figure 2 as follows:\\n‚Ä¢Abstract Syntax Tree (AST). AST contains syntactic information for a program and omits irrele-\\nvant details that have no effect on the semantics. Figure 2 shows the completed AST nodes on the\\nleft simple program and each node has a code sequence in the Ô¨Årst line and type attribute in the\\nsecond line. The black arrows represent the child-parent relations among ASTs.\\n‚Ä¢Control Flow Graph (CFG). Compared with AST highlighting the syntactic structure, CFG\\ndisplays statement execution order, i.e., the possible order in which statements may be executed\\nand the conditions that must be met for this to happen. Each statement in the program is treated as\\nan independent node as well as a designated entry and exit node. Based on the keywords if,for,\\ngoto,break andcontinue , control Ô¨Çow graphs can be easily built and ‚ÄúFlow to‚Äù with green dashed\\narrows in Figure 2 represents this Ô¨Çow order.\\n‚Ä¢Program Dependency Graph (PDG). PDG includes data dependencies andcontrol dependen-\\ncies: 1) data dependencies are described as the deÔ¨Ånition of a variable in a statement reaches the\\nusage of the same variable at another statement. In Figure 2, the variable ‚Äú b‚Äù is deÔ¨Åned in the\\nstatement ‚Äú int b = a++ ‚Äù and used in ‚Äú call (b) ‚Äù. Hence, there is a ‚ÄúReach‚Äù edge with blue arrows\\npoint from ‚Äú int b = a++ ‚Äù to ‚Äú call (b) ‚Äù. Furthermore, DeÔ¨Åne/Use edge with orange double arrows\\ndenotes the deÔ¨Ånition and usage of the variable. 2) different from CFG displaying the execution\\nprocess of the complete program, control dependencies deÔ¨Åne the execution of a statement may be\\ndependent on the value of a predicate, which more focus on the statement itself. For instance, the\\nstatements ‚Äú int b = a++ ‚Äù and ‚Äú call(b) ‚Äù are only performed ‚Äúif a is even‚Äù. Therefore, a red double\\narrow ‚ÄúControl‚Äù points from ‚Äú if (a % 2) == 0 ‚Äù to ‚Äú int b = a++ ‚Äù and ‚Äú call(b) ‚Äù.\\nB D ETAILS ON BASELINE METHODS\\nWe compare our approach with existing baselines. They can be divided into three groups: Retrieval-\\nbased approaches, Sequence-based approaches and Graph-based approaches. For papers that provide\\nthe source code, we directly reproduce their methods on CCSD dataset. Otherwise, we reimplement\\ntheir approaches with reference to the papers.\\nB.1 R ETRIEVAL -BASED APPROACHES\\nTF-IDF (Haiduc et al., 2010) is the abbreviation of Term Frequency-Inverse Document Frequency,\\nwhich is adopted in the early code summarization (Haiduc et al., 2010). It transforms programs\\ninto weight vectors by calculating term frequency and inverse document frequency. We retrieve the\\nsummary of the most similar programs by calculating the cosine similarity on the weighted vectors.\\nNNGen (Liu et al., 2018) is a retrieved-based approach to produce commit messages for code changes.\\nWe reproduce such an algorithm on code summarization. SpeciÔ¨Åcally, we retrieve the most similar\\ntop-k code snippets on a bag-of-words model and prioritizes the summary in terms of BLEU-4 scores\\nin top-k code snippets.\\nB.2 S EQUENCE -BASED APPROACHES\\nCODE-NN (Iyer et al., 2016; Barone & Sennrich, 2017) adopts an attention-based Seq2Seq model\\nto generate summaries on the source code.\\nTransformer (Ahmad et al., 2020) adopts the transformer architecture (Vaswani et al., 2017) with\\nself-attention to capture long dependency in the code for source code summrization.\\n14', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 13}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nHybrid-DRL (Wan et al., 2018) is a reinforcement learning-based approach, which incorporates AST\\nand sequential code snippets into a deep reinforcement learning framework and employ evaluation\\nmetrics e.g., BLEU as the reward.\\nDual Model (Wei et al., 2019) propose a dual training framework by training code summarization\\nand code generation tasks simultaneously to boost each task performance.\\nRencos (Zhang et al., 2020) is the retrieval-based Seq2Seq model for code summarization. it utilized\\na pretrained Seq2Seq model during the testing phase by computing a joint probability conditioned on\\nboth the original source code and retrieved the most similar source code for the summary generation.\\nCompared with Rencos, we propose a novel retrieval-augmented mechanism for the similar source\\ncode and use it at the model training phase.\\nB.3 G RAPH -BASED APPROACHES\\nWe also compared with some latest GNN-based works, employing graph neural network for source\\ncode summarization.\\nGCN2Seq, GAT2Seq modify Graph Convolution Network (Kipf & Welling, 2016) and Graph\\nAttention Network (Velickovic et al., 2018) to perform convolution operation and attention operation\\non the code property graph for learning and followed by a LSTM to generate summaries. We\\nimplement the related code from scratch.\\nSeqGNN (Fernandes et al., 2018) combines GGNNs and standard sequence encoders for summa-\\nrization. They take the code and relationships between elements of the code as input. Specially, a\\nBiLSTM is employed on the code sequence to learn representations and each source code token is\\nmodelled as a node in the graph, and employed GGNN for graph-level learning. Since our node\\nsequences are sub-sequence of source code rather than individual token, we adjust to slice the output\\nof BiLSTM and sum each token representation in node sequences as node initial representation for\\nsummarization. Furthermore, we implement the related code from scratch.\\nC M ODEL SETTINGS\\nWe embed the most frequent 40,000 words in the training set with 512-dims and set the hidden size\\nof BiLSTM to 256 and the concatenated state size for both directions is 512. The dropout is set to 0.3\\nafter the word embedding layer and BiLSTM. We set GNN hops to 1 for the best performance. The\\noptimizer is selected with Adam with an initial learning rate of 0.001. The batch size is set to 64 and\\nearly stop for 10. The beam search width is set to 5 as usual. All experiments are conducted on the\\nDGX server with four Nvidia Graphics Tesla V100 and each epoch takes 6 minutes averagely. All\\nhyperparameters are tuned with grid search on the validation set.\\nD D ETAILS ON DATA PREPARATION\\nIt is non-trivial to obtain high-quality datasets for code summarization. We noticed that despite some\\nprevious works (Barone & Sennrich, 2017; Hu et al., 2018b) released their datasets, however, they\\nare all based on high-level programming languages i.e. Java, Python. We are the Ô¨Årst to explore\\nsummarization on Cprogramming language. SpeciÔ¨Åcally, we crawled from popular Crepositories\\n(e.g., Linux and QEMU) on GitHub, and then extracted separate function-summary pairs from these\\nprojects. SpeciÔ¨Åcally, we extracted functions and associated comments marked by special characters\\n\"/**\" and \"*/\" over the function declaration. These comments can be considered as explanations of\\nthe functions. We Ô¨Åltered out functions with line exceeding 1000 and any other comments inside\\nthe function, and the Ô¨Årst sentence was selected as the summary. A similar practice can be found\\nin (Jiang et al., 2017). Totally, we collected 500k+ raw function-summary pairs. Furthermore,\\nfunctions with token size greater than 150 were removed for computational efÔ¨Åciency and there\\nwere 130k+ functions left. Since duplication is very common in existing datasets (Fernandes et al.,\\n2018), followed by Allamanis (2019), we performed a de-duplication process and removed functions\\nwith similarity over 80%. SpeciÔ¨Åcally, we calculated the cosine similarity by encoding the raw\\nfunctions into vectors with sklearn. Finally, we kept 95k+ unique functions. We name this dataset C\\nCode Summarization Dataset (CCSD). To testify model generalization ability, we randomly selected\\n15', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 14}),\n",
       " Document(page_content='Published as a conference paper at ICLR 2021\\nTable 5: More Examples of generated summaries on the CCSD test set.\\nExample Example 1 Example 2\\nSource Codestatic void counterMutexFree\\n(sqlite3_mutex *p){\\nassert(g.isInit);\\ng.m.xMutexFree(p->pReal);\\nif( p->eType==SQLITE_MUTEX_FAST\\n|| p->eType==\\nS QLITE_MUTEX_RECURSIVE)\\n{\\nfree(p);\\n}\\n}static void __exit wimax_subsys_exit(void)\\n{\\nwimax_id_table_release();\\ngenl_unregister_family\\n(&wimax_gnl_family);\\n}\\nGround-Truth free a countable mutex shutdown the wimax stack\\nNNGen enter a countable mutex unregisters pmcraid event family return value none\\nTransformer leave a mutex de initialize wimax driver\\nRencos try to enter a mutex unregister the wimax device subsystem\\nSeqGNN free a mutex allocated by sqlite3 mutex this function is called when the driver is not held\\nHGNN release a mutex free the wimax stack\\nRetrieved_codestatic int counterMutexTry\\n(sqlite3_mutex *p){\\nassert( g.isInit );\\nassert( p->eType>=0 );\\nassert( p->eType<MAX_MUTEXES );\\ng.aCounter[p->eType]++;\\nif( g.disableTry )\\nreturn SQLITE_BUSY;\\nreturn g.m.xMutexTry(p->pReal);\\n}static int __init wimax_subsys_init(void){\\nint result; d_fnstart(4, NULL, \"()\\\\n\");\\nd_parse_params(D_LEVEL, D_LEVEL_SIZE,\\nwimax_debug_params, \"wimax.debug\");\\nresult = genl_register_family\\n(&wimax_gnl_family);\\nif (unlikely(result < 0)) {\\npr_err(\"cannot register generic\\nnetlink family: %d\\\\n\", result);\\ngoto error_register_family;}\\nd_fnend(4, NULL, \"() = 0\\\\n\");\\nreturn 0;\\nerror_register_family:\\nd_fnend(4, NULL, \"() = %d\\\\n\", result);\\nreturn result;\\n}\\nRetrieved_summary try to enter a mutex shutdown the wimax stack\\nExample Example 3 Example 4\\nSource Codestatic void udc_dd_free(\\nstruct lpc32xx_udc *udc,\\nstruct lpc32xx_usbd_dd_gad *dd)\\n{\\ndma_pool_free(udc->dd_cache,\\ndd, dd->this_dma);\\n}void ReleaseSockEvent(SOCK_EVENT *event) {\\nif (event == NULL)\\n{\\nreturn;\\n}\\nif (Release(event->ref) == 0)\\n{\\nCleanupSockEvent(event);\\n}\\n}\\nGround-Truth free a dma descriptor release of the socket event\\nNNGen allocate a dma descriptor clean up of the socket event\\nTransformer free the usb device set the event\\nRencos allocate a dma descriptor set of the sock event\\nSeqGNN free dma buffers release of the socket\\nHGNN free a dma descriptor release the sock event\\nRetrieved_codestatic struct lpc32xx_usbd_dd_gad\\n*udc_dd_alloc(struct\\nlpc32xx_udc *udc) {\\ndma_addr_t dma;\\nstruct lpc32xx_usbd_dd_gad *dd;\\ndd = dma_pool_alloc(udc->dd_cache,\\nGFP_ATOMIC | GFP_DMA, &dma);\\nif (dd)\\ndd->this_dma = dma;\\nreturn dd;\\n}void SetL2TPServerSockEvent(\\nL2TP_SERVER *l2tp,SOCK_EVENT *e){\\nif (l2tp == NULL) {\\nreturn;}\\nif (e != NULL){\\nAddRef(e->ref);}\\nif (l2tp->SockEvent != NULL){\\nReleaseSockEvent(l2tp->SockEvent);\\nl2tp->SockEvent = NULL;}\\nl2tp->SockEvent = e;}\\nRetrieved_summary allocate a dma descriptor set a sock event to the l2tp server\\nsome projects as the out-of-domain test set with 2,330 examples and the remaining were randomly\\nsplit into train/validation/test with 84,316/4,432/4,203 examples. The open-source code analysis\\nplatform Joern (Yamaguchi et al., 2014) was applied to construct the code property graph.\\nE M ORE EXAMPLES\\nWe show more examples along with the retrieved code and summary by dynamic programming in\\nTable 5 and we can Ô¨Ånd that HGNN can generate more high-quality summries based on our approach.\\n16', metadata={'source': './data\\\\Retrieval code for GN.pdf', 'page': 15}),\n",
       " Document(page_content='T-RAG: End-to-End Table Question Answering via Retrieval-Augmented\\nGeneration\\nFeifei Pan1, Mustafa Canim2, Michael Glass2, AlÔ¨Åo Gliozzo2, James Hendler1\\npanf2@rpi.edu ,mustafa@us.ibm.com ,\\nmrglass@us.ibm.com ,gliozzo@us.ibm.com\\nhendler@cs.rpi.edu\\n1Rensselaer Polytechnic Institute\\n2IBM TJ Watson Research Center\\nAbstract\\nMost existing end-to-end Table Question An-\\nswering (Table QA) models consist of a two-\\nstage framework with a retriever to select rel-\\nevant table candidates from a corpus and a\\nreader to locate the correct answers from table\\ncandidates. Even though the accuracy of the\\nreader models is signiÔ¨Åcantly improved with\\nthe recent transformer-based approaches, the\\noverall performance of such frameworks still\\nsuffers from the poor accuracy of using tradi-\\ntional information retrieval techniques as re-\\ntrievers. To alleviate this problem, we intro-\\nduce T-RAG, an end-to-end Table QA model,\\nwhere a non-parametric dense vector index is\\nÔ¨Åne-tuned jointly with BART, a parametric\\nsequence-to-sequence model to generate an-\\nswer tokens. Given any natural language ques-\\ntion, T-RAG utilizes a uniÔ¨Åed pipeline to auto-\\nmatically search through a table corpus to di-\\nrectly locate the correct answer from table cell.\\nWe apply T-RAG on recent open-domain Table\\nQA benchmarks and demonstrate that the Ô¨Åne-\\ntuned T-RAG model is able to achieve state-\\nof-the-art performance in both the end-to-end\\nTable QA and the table retrieval tasks.\\n1 Introduction\\nTabular data is commonly seen in open-domain\\ndocuments (Cafarella et al., 2009; Zhang and Ba-\\nlog, 2018a), such as the Web and Wikipedia, as\\nwell as in domain-speciÔ¨Åc papers, journals, manu-\\nals, and reports. Answering questions over these\\ntables requires table retrieval and understanding\\nof the table structure and content. Table QA task\\nis generally more challenging than executing SQL\\nqueries over relational database tables due to the\\nlack of schema information. Most existing studies\\ntackle Table QA as two separate sub-tasks: (1) Ta-\\nble retrieval (Cafarella et al., 2008, 2009; Zhang\\nand Balog, 2018a; Shraga et al., 2020a,b), and (2)\\nQA over tables (Yu et al., 2018; Herzig et al., 2020;\\nYin et al., 2020; Glass et al., 2020). Recently, theDTR (Herzig et al., 2021) and the CLTR (Pan et al.,\\n2021) models have been proposed as end-to-end\\nsolutions for Table QA. Both models consist of a\\ntwo-step pipeline of a retriever to generate a set of\\ncandidate tables and a reader to answer questions\\nover these tables. The two components are trained\\nindividually, causing error propagation from re-\\ntrievers to readers, i.e. with incorrect table can-\\ndidates, it is impossible for the readers to locate\\nthe correct answer despite the design of the mod-\\nels. While dense retrieval and Retrieval Augmented\\nGeneration (RAG) (Karpukhin et al., 2020; Lewis\\net al., 2020b) have achieved great success in open-\\ndomain QA over free text, none of the studies in\\nthe literature leverage a non-parametric memory\\nmodel along with a parametric memory model for\\nthe open-domain Table QA task.\\nIn this paper, we describe a novel end-to-end\\nTable QA model, T-RAG, replacing the two-step\\nframework with a single training process. To\\ntrain T-RAG, we utilize Dense Passage Retrieval\\n(DPR) (Karpukhin et al., 2020) and RAG strate-\\ngies. SpeciÔ¨Åcally, we jointly train a DPR compo-\\nnent (Glass et al., 2021) together with the BART-\\nbased (Lewis et al., 2020a) sequence-to-sequence\\n(Seq2Seq) model. To the best of our knowledge,\\nT-RAG is the Ô¨Årst Table QA model where the\\nquery encoder for a non-parametric dense vector\\nindex is Ô¨Åne-tuned along with a parametric gen-\\neration model. We evaluate the performance of\\nT-RAG on NQ-TABLES (Herzig et al., 2021) and\\nE2E_WTQ (Pan et al., 2021), two recent end-to-\\nend Table QA benchmarks. The experimental re-\\nsults indicate that T-RAG outperforms the state-of-\\nthe-art models on the end-to-end Table QA task.\\nThe major contribution of this work is that, we\\npropose the Ô¨Årst end-to-end Table QA pipeline,\\nleveraging DPR along with the Seq2Seq compo-\\nnent of RAG. T-RAG employs a simple but effec-\\ntive one-step training that reduces error accumu-\\nlations and simpliÔ¨Åes model Ô¨Åne-tuning. In thearXiv:2203.16714v1  [cs.CL]  30 Mar 2022', metadata={'source': './data\\\\T-RAG End-to-End Table Question Answering via Retrieval-Augmented.pdf', 'page': 0}),\n",
       " Document(page_content='experiments, T-RAG achieves state-of-the-art per-\\nformance on two tasks. We Ô¨Ånd T-RAG improves\\nthe results for end-to-end Table QA on two recent\\nbenchmarks. The RAG component of the end-to-\\nend model Ô¨Åne-tuned over Table QA benchmarks\\nalso yields state-of-the-art results on the table re-\\ntrieval task.\\n2 Related Work\\nTable Retrieval Traditional table retrieval mod-\\nels usually concatenate tables into documents\\nwhile disregarding the underlying tabular structure\\n(Pyreddy and Croft, 1997; Wang and Hu, 2002;\\nLiu et al., 2007; Cafarella et al., 2008, 2009). New\\napproaches are proposed to retrieve tables with a\\nset of features of the table, query and table-query\\npair (Zhang and Balog, 2018b; Sun et al., 2019;\\nBhagavatula et al., 2013; Shraga et al., 2020a).\\nZhang and Balog (2018b) uses semantic similari-\\nties to build an ad-hoc table retrieval model with\\nvarious features. A neural ranking model is in-\\ntroduced in Shraga et al. (2020b), where tables are\\ndeÔ¨Åned as multi-modal objects and the Gated Multi-\\nmodal Units are used to learn the representation of\\nquery-table pairs. Pan et al. (2021) later follows\\nthis work and improves the table retrieval with a\\n2-step retriever. Kosti ¬¥c et al. (2021) discusses the\\nuse of dense vector embeddings to enhance the per-\\nformance of bi- and tri-encoder in retrieving both\\ntable and text.\\nTable QA Most early Table QA solutions are\\nfully supervised models, focusing on converting\\nnatural language questions into SQL format and\\nusing the SQL-format questions to query the given\\ntables, as seen in Yu et al. (2018); Lin et al. (2019);\\nXu et al. (2018). Open-domain QA over text (Yu\\net al., 2020) usually utilizes multiple knowledge\\nsources. For instance, Oguz et al. (2021) pro-\\nposes a model can convert structured, unstruc-\\ntured and semi-structured knowledge into text for\\nopen-domain QA. Therefore, more recent efforts\\nhave been put into investigating the use of exter-\\nnal knowledge in enhancing the performance of\\nTable QA. Jim√©nez-Ruiz et al. (2020) Ô¨Årst proposes\\nthe Semantic Web Challenge on Tabular Data to\\nKnowledge Graph Matching (SemTab) to encour-\\nage such solutions for both table understanding\\nand Table QA. Recently, the transformer-based,\\nweakly supervised solutions have been proposed\\nfor Table QA. These solutions fall into two cat-\\negories: (1) Logic form-based solution, such astheTABERT (Yin et al., 2020) model, which is\\ntrained to capture the representation of natural lan-\\nguage sentences as well as tabular data; (2) Answer\\ncell prediction solutions, such as TAPAS(Herzig\\net al., 2020) and the RCI (Glass et al., 2020) model.\\nThe current state-of-the-art RCI model exploits a\\ntransformer-based framework. Instead of retrieving\\nthe table cells directly for any given question-table\\npairs, the RCI model identiÔ¨Åes the most relevant\\ncolumns and rows independently and locates the\\nintersection table cells as the Ô¨Ånal answers.\\nEnd-to-End Table QA Sun et al. (2016) pub-\\nlishes the Ô¨Årst end-to-end table cell search frame-\\nwork. This work leverages the semantic relations\\nbetween cells and maps queries to table cells with\\nrelational chains. The DTR model (Herzig et al.,\\n2021) addresses the end-to-end Table QA problem\\nwith a table retriever and a TAPAS-based reader\\nmodel. Later, the CLTR model (Pan et al., 2021)\\nintroduces a similar two-step solution, using BM25\\nas the retriever. The model re-ranks the BM25\\nresults and locates the table cells using the RCI\\nscores. Recently, Chen et al. (2021) proposes a\\nnew task for QA over both free text and tables\\nand provides a solution including a retriever with\\nearly fusion techniques and a cross-block reader.\\nIn addition, the open-domain OTT-QA benchmark\\nis released to evaluate models for end-to-end QA\\nover text and table.\\n3 The End-to-End Table QA with T-RAG\\nThe overall architecture of T-RAG is illustrated in\\nFigure 1. In this example, we encode the questions\\n‚Äúwho was the editor for Ikar?‚Äù using the query en-\\ncoder and pre-process the tables, e.g., T 1and T 2,\\nfrom the table corpus for encoding. The encoded ta-\\nbles are later indexed into the Approximate Nearest\\nNeighbors (ANN) data structure for querying. The\\nencoded question is appended to each table before\\ninputting it to the BART-based generator for answer\\nprediction. The DPR and the RAG components are\\ntrained jointly without explicitly considering the\\ntable-level ground truth.\\nSetup We deÔ¨Åne the one-step, end-to-end Table\\nQA task as follows. Given a massive corpus C\\nof tables tiand any natural language question qi,\\nwe train a model to directly generate answer to qi\\nfrom the table cell without any intermediate steps.\\nLabeled datasets are available to us with ground\\ntruth samples in the format of {qi, ti, ai}where ai', metadata={'source': './data\\\\T-RAG End-to-End Table Question Answering via Retrieval-Augmented.pdf', 'page': 1}),\n",
       " Document(page_content='ANNIndexNatural LanguageQuestionWhowastheeditorforIkar?SeriesYears‚Ä¶EditorIvan Zourine1979‚Ä¶Magic-StripSteve Severin1981‚Äì2003‚Ä¶Gl√©nat‚Ä¶‚Ä¶‚Ä¶‚Ä¶Ikar1995‚Äì1997‚Ä¶Gl√©nat‚Ä¶‚Ä¶‚Ä¶‚Ä¶QueryEncoder\\nTableEncoderPierre Makyo at a book fair in Paris, France, in March 2008.BornJuly 16, 1952(age69)NationalityFrench‚Ä¶‚Ä¶TableCorpus\\n‚Ä¶Generator!1!2AnswersGl√©natCitro√´nMagic-Strip‚Ä¶DPRRAGSeries[sep]Ivan Zourine[sep]Title[sep]Year‚Ä¶‚Ä¶Figure 1: An overview of T-RAG, a model trained end-to-end to directly locate answers from table corpus.\\nstands for the answers.\\nTable pre-processing is implemented before the\\ntraining. We process the tables tiinto a structure-\\npreserving format, where: (1) column headers are\\nappended before cell values, separated by a special\\nsymbol ‚Äú |‚Äù; and (2) the separator ‚Äú*‚Äù is appended\\nto the end of each row; (3) for the tables with addi-\\ntional information such as titles, we append them\\nin front of the tables. The tables are segmented\\ninto the length of 512 tokens for training. For each\\nquestion, we retrieve hard negatives from the cor-\\npusCand use them as additional negative samples\\nto enhance the T-RAG training.\\nSoft Hard Negatives We implement a BM25-\\nbased hard negative mining for T-RAG. For each\\nquestion, we Ô¨Årst retrieve a pool of the most rel-\\nevant tables from the corpus using BM25. From\\nthe table pool, we discard the ground truth table.\\nThe top-ranked, non-positive tables are used as the\\nhard negative candidates. In the training process,\\ninstead of using the top 1 negative table, we exploit\\na soft hard negative mechanism, where we select\\nthe hard negative at random from the top knegative\\ntables.\\nRAG For the implementation of RAG, we jointly\\ntrain a DPR-based retriever and a BART-based gen-\\nerator. We index the tables in Cusing a keyword-\\nbased search engine, Anserini1, to harvest the hard\\nnegative training samples using BM25. Later, T-\\nRAG exploits BERT BASE to encode questions\\nalong with the ground truth table and the hard neg-\\native tables. To train RAG, T-RAG employs the\\nanswer-level ground truth and use a Seq2Seq gen-\\n1https://github.com/castorini/anserinierator, the BART LARGE model, for answer predic-\\ntions. The previously encoded tables are indexed\\nwith the open-source FAISS (Johnson et al., 2017)\\nlibrary into the ANN data structure for querying.\\nThe encoded questions are concatenated to each\\nof the top retrieved tables and used as a prompt to\\ngenerate the answer. More concretely, the gener-\\nator predicts probability distributions for possible\\nanswer candidates as the next token. The probabil-\\nity distributions are later marginalized to produce\\na single weighted sequence probability for each\\nanswer candidate. Finally, a standard beam search\\ndecoder (Sutskever et al., 2014) is used to identify\\nthe most relevant candidates as the Ô¨Ånal answers to\\nthe questions at test time. Along with the answers,\\nour model can also return the relevant table ticon-\\ntaining the correct answers from Cfor evaluation\\nand annotation purposes.\\n4 Experiments\\nData We validate T-RAG on two open-domain\\nbenchmarks, NQ-TABLES andE2E_WTQ . NQ-\\nTABLES is the table subset of the Natural Ques-\\ntions dataset (Kwiatkowski et al., 2019), with a\\ntable corpus extracted from the English Wikipedia\\narticles and samples in the {q, T, a }format, where\\nq,T, and adenote question, ground truth table,\\nand answer, respectively. E2E_WTQ contains the\\nlook-up subset of WikiTableQuestions (Pasupat\\nand Liang, 2015). While a substantial amount of\\ntables in NQ-TABLES are transposed infobox ta-\\nbles, the E2E_WTQ only contains well-formatted\\nbut more complex tables. The data statistics are\\nshown in Table 1.', metadata={'source': './data\\\\T-RAG End-to-End Table Question Answering via Retrieval-Augmented.pdf', 'page': 2}),\n",
       " Document(page_content='Data Train Dev Test Corpus\\nNQ-TABLES 9,594 1,068 966 169,898\\nE2E_WTQ 851 124 241 2,108\\nTable 1: Data Statistics\\nModel EM F1 Oracle EM Oracle F1\\nDTR+hn 37.69 47.70 48.20 61.50\\nT-RAG 43.06 50.92 50.62 63.18\\n(a) End-to-end Table QA results on the test set of NQ-TABLES.\\nModel MRR Hit@1\\nCLTR 0.5503 0.4675\\nT-RAG 0.5923 0.5065\\n(b) End-to-end Table QA results on the test set of E2E_WTQ.\\nTable 2: Experimental results on End-to-end Table QA.\\nExperimental Settings In the experiments, we\\nÔ¨Årst encode the questions and tables using\\nBERT BASE , and later jointly train the DPR-based\\nretriever and the Seq2Seq generator of RAG. For\\nthe experiments, we set: (1) training batch size =\\n128; (2) number of epochs = 2; (3) learning rate =\\n3e-5; and (4) gradient accumulation steps = 64.\\nEvaluation metrics: Following the evaluation\\nscript in SQUAD (Rajpurkar et al., 2016), we eval-\\nuate end-to-end Table QA using exact match (EM)\\nand token F1 metrics for NQ-TABLES. The ac-\\ncuracy for the top 1 returned answer and mean\\nreciprocal rank (MRR) are used to measure the per-\\nformance on E2E_WTQ. We also evaluate T-RAG\\non the table retrieval task for a fair comparison\\nwith existing work. We utilize the original met-\\nrics in Herzig et al. (2021) and Pan et al. (2021),\\nwith recall (R) for NQ-TABLES, and precision (P),\\nnormalized discounted gain (N), and mean average\\nprecision (MAP) for E2E_WTQ.\\nExperimental Results We compare the end-to-\\nend Table QA performance of T-RAG against the\\nstate-of-the-art DTR and CLTR models in Table 2.\\nWe Ô¨Ånd T-RAG yields better results than the previ-\\nous best models for both datasets with all evalua-\\ntion metrics.\\nTo further validate T-RAG against the existing\\nmodels, we also evaluate the model performance on\\ntable retrieval. The table retrieval results for NQ-\\nTABLES and E2E_WTQ are shown in Table 3a\\nand 3b, respectively. The results indicate that T-\\nRAG outperforms the simple baselines models such\\nas BM25, as well as the strong state-of-the-art mod-\\nels in the experiments.\\nQualitative Analysis We further evaluate the\\ntable retrieval results on NQ-TABLES. We no-Model R@1 R@10 R@50\\nBM25 16.77 40.06 58.39\\nDTR+hn 42.42 81.13 92.56\\nT-RAG 46.07 85.40 95.03\\n(a) Table retrieval results on the test set of NQ-TABLES.\\nModel P@5 P@10 N@5 N@10 MAP\\nBM25 0.5938 0.6587 0.5228 0.5356 0.4704\\nCLTR 0.7437 0.8735 0.6915 0.7119 0.5971\\nT-RAG 0.7806 0.8943 0.7250 0.7467 0.6404\\n(b) Table retrieval results on the test set of E2E_WTQ.\\nTable 3: Experimental results on Table Retrieval.\\ntice that the DPR-based baseline of our approach\\nachieves 43.89 for R@1 and 81.57 for R@10; both\\noutperform the state-of-the-art DTR results. In\\naddition, the retrieval performance is further im-\\nproved with the more effective end-to-end RAG\\ntraining. To validate the effectiveness of our soft\\nhard negative technique, we test the method on the\\nE2E_WTQ dataset. Instead of using the top 1 nega-\\ntive table from the BM25 results, we set k= 3and\\nachieve a 27.17% absolute gain for Hit@1 accuracy\\nin the end-to-end Table QA task.\\nBesides, we perform thorough error analysis on\\non E2E_WTQ and Ô¨Ånd that over 21% of the errors\\ncome from questions that involve numerical values.\\nThe Ô¨Ånding indicates that understanding different\\ntypes of numbers remains a challenge in Table QA.\\n5 Conclusion and Future Work\\nIn this paper, we present a novel Table QA model\\nthat achieves state-of-the-art performance on recent\\nbenchmarks. Instead of training a retriever and\\na reader model independently, T-RAG uniÔ¨Åes the\\nprocedure into a single pipeline of only one training\\nstep, which reduces the error accumulations from\\ntwo separate models. In the experiments, T-RAG\\noutperforms the current best models for end-to-\\nend Table QA. We additionally demonstrate the\\nadvantages of T-RAG with the table retrieval task,\\nand T-RAG beats the existing numbers on both\\nbenchmarks.\\nIn the future, we plan to validate T-RAG on\\ndomain-speciÔ¨Åc datasets, such as AIT-QA and TAT-\\nQA (Katsis et al., 2021; Zhu et al., 2021) and\\nextend the model to solve multi-modal QA prob-\\nlems, with the corpus containing both tables and\\npassages, as presented in the OTT-QA and Hybrid-\\nQA benchmarks (Chen et al., 2020a,b). To further\\nimprove the model performance, we also plan to\\ninvestigate algorithms to better understand numeric\\nvalues.', metadata={'source': './data\\\\T-RAG End-to-End Table Question Answering via Retrieval-Augmented.pdf', 'page': 3}),\n",
       " Document(page_content='References\\nChandra Bhagavatula, Thanapon Noraset, and Doug\\nDowney. 2013. Methods for exploring and min-\\ning tables on wikipedia. Proceedings of the ACM\\nSIGKDD Workshop on Interactive Data Exploration\\nand Analytics .\\nMichael J Cafarella, Alon Halevy, and Nodira Khous-\\nsainova. 2009. Data integration for the rela-\\ntional web. Proceedings of the VLDB Endowment ,\\n2(1):1090‚Äì1101.\\nMichael J Cafarella, Alon Halevy, Daisy Zhe Wang, Eu-\\ngene Wu, and Yang Zhang. 2008. Webtables: ex-\\nploring the power of tables on the web. Proceedings\\nof the VLDB Endowment , 1(1):538‚Äì549.\\nWenhu Chen, Ming-Wei Chang, Eva Schlinger,\\nWilliam Yang Wang, and William W Cohen. 2020a.\\nOpen question answering over tables and text. In\\nInternational Conference on Learning Representa-\\ntions .\\nWenhu Chen, Ming-Wei Chang, Eva Schlinger,\\nWilliam Yang Wang, and William W. Cohen. 2021.\\nOpen question answering over tables and text. In\\nInternational Conference on Learning Representa-\\ntions .\\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan\\nXiong, Hong Wang, and William Yang Wang. 2020b.\\nHybridqa: A dataset of multi-hop question answer-\\ning over tabular and textual data. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing: Findings , pages 1026‚Äì\\n1036.\\nMichael Glass, Mustafa Canim, AlÔ¨Åo Gliozzo, Saneem\\nChemmengath, Rishav Chakravarti, Avi Sil, Feifei\\nPan, Samarth Bharadwaj, and Nicolas Rodolfo\\nFauceglia. 2020. Capturing row and column seman-\\ntics in transformer based question answering over ta-\\nbles. Proceedings of the Annual Conference of the\\nNorth American Chapter of the Association for Com-\\nputational Linguistics (NAACL-HLT2020) .\\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub\\nChowdhury, and AlÔ¨Åo Gliozzo. 2021. Robust re-\\ntrieval augmented generation for zero-shot slot Ô¨Åll-\\ning. In Proceedings of the 2021 Conference on Em-\\npirical Methods in Natural Language Processing ,\\npages 1939‚Äì1949, Online and Punta Cana, Domini-\\ncan Republic. Association for Computational Lin-\\nguistics.\\nJonathan Herzig, Thomas M√ºller, Syrine Krichene, and\\nJulian Martin Eisenschlos. 2021. Open domain ques-\\ntion answering over tables via dense retrieval. arXiv\\npreprint arXiv:2103.12011 .\\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\\nM√ºller, Francesco Piccinno, and Julian Eisensch-\\nlos. 2020. TaPas: Weakly supervised table pars-\\ning via pre-training. In Proceedings of the 58th An-\\nnual Meeting of the Association for ComputationalLinguistics , pages 4320‚Äì4333, Seattle, Washington,\\nUnited States. Association for Computational Lin-\\nguistics.\\nErnesto Jim√©nez-Ruiz, Oktie Hassanzadeh, Vasilis\\nEfthymiou, Jiaoyan Chen, and Kavitha Srinivas.\\n2020. Semtab 2019: Resources to benchmark tab-\\nular data to knowledge graph matching systems. In\\nESWC , pages 514‚Äì530.\\nJeff Johnson, Matthijs Douze, and Herv√© J√©gou. 2017.\\nBillion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 .\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 6769‚Äì\\n6781, Online. Association for Computational Lin-\\nguistics.\\nYannis Katsis, Saneem Chemmengath, Vishwajeet\\nKumar, Samarth Bharadwaj, Mustafa Canim,\\nMichael Glass, AlÔ¨Åo Gliozzo, Feifei Pan, Jay-\\ndeep Sen, Karthik Sankaranarayanan, and Soumen\\nChakrabarti. 2021. Ait-qa: Question answering\\ndataset over complex tables in the airline industry.\\nBogdan Kosti ¬¥c, Julian Risch, and Timo M√∂ller. 2021.\\nMulti-modal retrieval of tables and texts using tri-\\nencoder models.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nÔ¨Åeld, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral questions: a benchmark for question answering\\nresearch. Transactions of the Association of Compu-\\ntational Linguistics .\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\\n2020a. BART: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation,\\nand comprehension. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics , pages 7871‚Äì7880, Online. Association\\nfor Computational Linguistics.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\nt√§schel, Sebastian Riedel, and Douwe Kiela. 2020b.\\nRetrieval-augmented generation for knowledge-\\nintensive nlp tasks. In Advances in Neural Infor-\\nmation Processing Systems , volume 33, pages 9459‚Äì\\n9474. Curran Associates, Inc.\\nKevin Lin, Ben Bogin, Mark Neumann, Jonathan Be-\\nrant, and Matt Gardner. 2019. Grammar-based neu-\\nral text-to-sql generation.', metadata={'source': './data\\\\T-RAG End-to-End Table Question Answering via Retrieval-Augmented.pdf', 'page': 4}),\n",
       " Document(page_content='Ying Liu, Kun Bai, Prasenjit Mitra, and C Lee Giles.\\n2007. Tableseer: automatic table metadata extrac-\\ntion and searching in digital libraries. In Proceed-\\nings of the 7th ACM/IEEE-CS joint conference on\\nDigital libraries , pages 91‚Äì100.\\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin,\\nStan Peshterliev, Dmytro Okhonko, Michael\\nSchlichtkrull, Sonal Gupta, Yashar Mehdad, and\\nScott Yih. 2021. Unik-qa: UniÔ¨Åed representations\\nof structured and unstructured knowledge for\\nopen-domain question answering.\\nFeifei Pan, Mustafa Canim, Michael Glass, AlÔ¨Åo\\nGliozzo, and Peter Fox. 2021. CLTR: An end-to-\\nend, transformer-based system for cell-level table re-\\ntrieval and table question answering. In Proceed-\\nings of the 59th Annual Meeting of the Association\\nfor Computational Linguistics and the 11th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing: System Demonstrations , pages 202‚Äì209,\\nOnline. Association for Computational Linguistics.\\nPanupong Pasupat and Percy Liang. 2015. Composi-\\ntional semantic parsing on semi-structured tables.\\nPallavi Pyreddy and W Bruce Croft. 1997. Tintin: A\\nsystem for retrieval in text tables. In Proceedings of\\nthe second ACM international conference on Digital\\nlibraries , pages 193‚Äì200.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. SQuAD: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Natu-\\nral Language Processing , pages 2383‚Äì2392, Austin,\\nTexas. Association for Computational Linguistics.\\nRoee Shraga, Haggai Roitman, Guy Feigenblat, and\\nMustafa Canim. 2020a. Ad hoc table retrieval using\\nintrinsic and extrinsic similarities. In Proceedings of\\nThe Web Conference 2020 , pages 2479‚Äì2485.\\nRoee Shraga, Haggai Roitman, Guy Feigenblat, and\\nMustafa Cannim. 2020b. Web table retrieval using\\nmultimodal deep learning. In Proceedings of the\\n43rd International ACM SIGIR Conference on Re-\\nsearch and Development in Information Retrieval ,\\nSIGIR ‚Äô20, page 1399‚Äì1408, New York, NY , USA.\\nAssociation for Computing Machinery.\\nHuan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su,\\nand Xifeng Yan. 2016. Table cell search for question\\nanswering. In Proceedings of the 25th International\\nConference on World Wide Web , pages 771‚Äì782.\\nYibo Sun, Zhao Yan, Duyu Tang, Nan Duan, and Bing\\nQin. 2019. Content-based table retrieval for web\\nqueries. Neurocomputing , 349:183‚Äì189.\\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\\nSequence to sequence learning with neural networks.\\nInAdvances in neural information processing sys-\\ntems, pages 3104‚Äì3112.Yalin Wang and Jianying Hu. 2002. A machine learn-\\ning based approach for table detection on the web.\\nInProceedings of the 11th International Conference\\non World Wide Web , WWW ‚Äô02, page 242‚Äì250, New\\nYork, NY , USA. Association for Computing Machin-\\nery.\\nXiaojun Xu, Chang Liu, and Dawn Song. 2018. SQL-\\nNet: Generating structured queries from natural lan-\\nguage without reinforcement learning.\\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\\nbastian Riedel. 2020. TaBERT: Pretraining for joint\\nunderstanding of textual and tabular data. In Pro-\\nceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics , pages 8413‚Äì\\n8426, Online. Association for Computational Lin-\\nguistics.\\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\\nDongxu Wang, Zifan Li, James Ma, Irene Li,\\nQingning Yao, Shanelle Roman, Zilin Zhang,\\nand Dragomir Radev. 2018. Spider: A large-\\nscale human-labeled dataset for complex and cross-\\ndomain semantic parsing and text-to-SQL task.\\nWenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,\\nQingyun Wang, Heng Ji, and Meng Jiang. 2020.\\nA survey of knowledge-enhanced text generation.\\narXiv preprint arXiv:2010.04389 .\\nShuo Zhang and K. Balog. 2018a. Ad hoc table re-\\ntrieval using semantic similarity. Proceedings of the\\n2018 World Wide Web Conference .\\nShuo Zhang and Krisztian Balog. 2018b. Ad hoc ta-\\nble retrieval using semantic similarity. In Proceed-\\nings of the 2018 World Wide Web Conference , pages\\n1553‚Äì1562.\\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao\\nWang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and\\nTat-Seng Chua. 2021. TAT-QA: A question answer-\\ning benchmark on a hybrid of tabular and textual\\ncontent in Ô¨Ånance. In Proceedings of the 59th An-\\nnual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Confer-\\nence on Natural Language Processing (Volume 1:\\nLong Papers) , pages 3277‚Äì3287, Online. Associa-\\ntion for Computational Linguistics.', metadata={'source': './data\\\\T-RAG End-to-End Table Question Answering via Retrieval-Augmented.pdf', 'page': 5})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61327f19-b85e-4997-8edc-80ce6805d393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
