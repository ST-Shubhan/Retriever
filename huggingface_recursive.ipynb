{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ce6608-e613-4273-9622-b7b77c1bb7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import openai\n",
    "from langchain.llms import openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_transformers import (\n",
    "    EmbeddingsRedundantFilter,\n",
    "    EmbeddingsClusteringFilter,\n",
    ")\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader , PdfWriter, PdfMerger\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af218801-4ea1-460b-af7f-fe58f28ddd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model Loaded..........\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "print(\"Embedding Model Loaded..........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b201479f-57e2-4ab3-ba9b-22d97c33cd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection already exists\n",
      "1.2399999832268804e-05\n"
     ]
    }
   ],
   "source": [
    "def load_chunk_persist_pdf() -> Chroma:\n",
    "    start = timeit.timeit()\n",
    "    pdf_folder_path = \"./data\"\n",
    "    documents = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    client = chromadb.Client()\n",
    "    if client.list_collections():\n",
    "        consent_collection = client.create_collection(\"consent_collection\")\n",
    "    else:\n",
    "        print(\"Collection already exists\")\n",
    "    \n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunked_documents,\n",
    "        embedding=hf,\n",
    "        persist_directory=\"store/chroma/hugging_face\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    end = timeit.timeit()\n",
    "    print(start - end)\n",
    "    return vectordb\n",
    "c_docs = load_chunk_persist_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88fedb21-517f-4b3d-92d2-fac786441ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = c_docs.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":10, \"include_metadata\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896d3dd3-c5a2-4914-af14-371e17d7e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Open-domain question answering?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c66d97b-6598-4292-9248-bc189ce41da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='the test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the', metadata={'page': 9, 'source': './data\\\\In context retrieval.pdf'}),\n",
       " Document(page_content='the test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the', metadata={'page': 9, 'source': './data\\\\In context retrieval.pdf'}),\n",
       " Document(page_content='the test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the', metadata={'page': 9, 'source': './data\\\\In context retrieval.pdf'}),\n",
       " Document(page_content='the test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the', metadata={'page': 9, 'source': './data\\\\In context retrieval.pdf'}),\n",
       " Document(page_content='the test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the', metadata={'page': 9, 'source': './data\\\\In context retrieval.pdf'}),\n",
       " Document(page_content='the test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the', metadata={'page': 9, 'source': './data\\\\In context retrieval.pdf'}),\n",
       " Document(page_content='over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', metadata={'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', metadata={'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', metadata={'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', metadata={'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13dba12e-f2a9-44a3-8fcf-3fee47aa0de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbbfaca2-0532-4a47-9807-d8a6ac84f645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 9\n",
      "Source: ./data\\In context retrieval.pdf\n",
      "Content: the test set of Natural Questions and TriviaQA mea-\n",
      "sured by exact match. In the open-book setting, we\n",
      "include the top two documents returned by DPR.\n",
      "7 In-Context RALM for Open-Domain\n",
      "Question Answering\n",
      "So far, we evaluated our framework on language\n",
      "modeling benchmarks. To test its efficacy in addi-\n",
      "tional scenarios, and specifically downstream tasks,\n",
      "we now turn to evaluate In-Context RALM on open-\n",
      "domain question answering (ODQA; Chen et al.\n",
      "2017). This experiment is intended to verify, in\n",
      "a controlled environment, that LMs can leverage\n",
      "retrieved documents without further training and\n",
      "without any training examples . Specifically, we\n",
      "use the LLaMA family (Touvron et al., 2023) with\n",
      "andwithout In-Context RALM (often referred to\n",
      "in ODQA literature as open-book and closed-book\n",
      "settings, respectively). In contrast to most prior\n",
      "work on ODQA ( e.g., Izacard and Grave 2021; Fa-\n",
      "jcik et al. 2021; Izacard et al. 2022b; Levine et al.\n",
      "2022b), our “reader” ( i.e., the model that gets the\n",
      "-----------------------------------------------------------\n",
      "Page: 9\n",
      "Source: ./data\\In context retrieval.pdf\n",
      "Content: the test set of Natural Questions and TriviaQA mea-\n",
      "sured by exact match. In the open-book setting, we\n",
      "include the top two documents returned by DPR.\n",
      "7 In-Context RALM for Open-Domain\n",
      "Question Answering\n",
      "So far, we evaluated our framework on language\n",
      "modeling benchmarks. To test its efficacy in addi-\n",
      "tional scenarios, and specifically downstream tasks,\n",
      "we now turn to evaluate In-Context RALM on open-\n",
      "domain question answering (ODQA; Chen et al.\n",
      "2017). This experiment is intended to verify, in\n",
      "a controlled environment, that LMs can leverage\n",
      "retrieved documents without further training and\n",
      "without any training examples . Specifically, we\n",
      "use the LLaMA family (Touvron et al., 2023) with\n",
      "andwithout In-Context RALM (often referred to\n",
      "in ODQA literature as open-book and closed-book\n",
      "settings, respectively). In contrast to most prior\n",
      "work on ODQA ( e.g., Izacard and Grave 2021; Fa-\n",
      "jcik et al. 2021; Izacard et al. 2022b; Levine et al.\n",
      "2022b), our “reader” ( i.e., the model that gets the\n",
      "-----------------------------------------------------------\n",
      "Page: 9\n",
      "Source: ./data\\In context retrieval.pdf\n",
      "Content: the test set of Natural Questions and TriviaQA mea-\n",
      "sured by exact match. In the open-book setting, we\n",
      "include the top two documents returned by DPR.\n",
      "7 In-Context RALM for Open-Domain\n",
      "Question Answering\n",
      "So far, we evaluated our framework on language\n",
      "modeling benchmarks. To test its efficacy in addi-\n",
      "tional scenarios, and specifically downstream tasks,\n",
      "we now turn to evaluate In-Context RALM on open-\n",
      "domain question answering (ODQA; Chen et al.\n",
      "2017). This experiment is intended to verify, in\n",
      "a controlled environment, that LMs can leverage\n",
      "retrieved documents without further training and\n",
      "without any training examples . Specifically, we\n",
      "use the LLaMA family (Touvron et al., 2023) with\n",
      "andwithout In-Context RALM (often referred to\n",
      "in ODQA literature as open-book and closed-book\n",
      "settings, respectively). In contrast to most prior\n",
      "work on ODQA ( e.g., Izacard and Grave 2021; Fa-\n",
      "jcik et al. 2021; Izacard et al. 2022b; Levine et al.\n",
      "2022b), our “reader” ( i.e., the model that gets the\n",
      "-----------------------------------------------------------\n",
      "Page: 9\n",
      "Source: ./data\\In context retrieval.pdf\n",
      "Content: the test set of Natural Questions and TriviaQA mea-\n",
      "sured by exact match. In the open-book setting, we\n",
      "include the top two documents returned by DPR.\n",
      "7 In-Context RALM for Open-Domain\n",
      "Question Answering\n",
      "So far, we evaluated our framework on language\n",
      "modeling benchmarks. To test its efficacy in addi-\n",
      "tional scenarios, and specifically downstream tasks,\n",
      "we now turn to evaluate In-Context RALM on open-\n",
      "domain question answering (ODQA; Chen et al.\n",
      "2017). This experiment is intended to verify, in\n",
      "a controlled environment, that LMs can leverage\n",
      "retrieved documents without further training and\n",
      "without any training examples . Specifically, we\n",
      "use the LLaMA family (Touvron et al., 2023) with\n",
      "andwithout In-Context RALM (often referred to\n",
      "in ODQA literature as open-book and closed-book\n",
      "settings, respectively). In contrast to most prior\n",
      "work on ODQA ( e.g., Izacard and Grave 2021; Fa-\n",
      "jcik et al. 2021; Izacard et al. 2022b; Levine et al.\n",
      "2022b), our “reader” ( i.e., the model that gets the\n",
      "-----------------------------------------------------------\n",
      "Page: 9\n",
      "Source: ./data\\In context retrieval.pdf\n",
      "Content: the test set of Natural Questions and TriviaQA mea-\n",
      "sured by exact match. In the open-book setting, we\n",
      "include the top two documents returned by DPR.\n",
      "7 In-Context RALM for Open-Domain\n",
      "Question Answering\n",
      "So far, we evaluated our framework on language\n",
      "modeling benchmarks. To test its efficacy in addi-\n",
      "tional scenarios, and specifically downstream tasks,\n",
      "we now turn to evaluate In-Context RALM on open-\n",
      "domain question answering (ODQA; Chen et al.\n",
      "2017). This experiment is intended to verify, in\n",
      "a controlled environment, that LMs can leverage\n",
      "retrieved documents without further training and\n",
      "without any training examples . Specifically, we\n",
      "use the LLaMA family (Touvron et al., 2023) with\n",
      "andwithout In-Context RALM (often referred to\n",
      "in ODQA literature as open-book and closed-book\n",
      "settings, respectively). In contrast to most prior\n",
      "work on ODQA ( e.g., Izacard and Grave 2021; Fa-\n",
      "jcik et al. 2021; Izacard et al. 2022b; Levine et al.\n",
      "2022b), our “reader” ( i.e., the model that gets the\n",
      "-----------------------------------------------------------\n",
      "Page: 9\n",
      "Source: ./data\\In context retrieval.pdf\n",
      "Content: the test set of Natural Questions and TriviaQA mea-\n",
      "sured by exact match. In the open-book setting, we\n",
      "include the top two documents returned by DPR.\n",
      "7 In-Context RALM for Open-Domain\n",
      "Question Answering\n",
      "So far, we evaluated our framework on language\n",
      "modeling benchmarks. To test its efficacy in addi-\n",
      "tional scenarios, and specifically downstream tasks,\n",
      "we now turn to evaluate In-Context RALM on open-\n",
      "domain question answering (ODQA; Chen et al.\n",
      "2017). This experiment is intended to verify, in\n",
      "a controlled environment, that LMs can leverage\n",
      "retrieved documents without further training and\n",
      "without any training examples . Specifically, we\n",
      "use the LLaMA family (Touvron et al., 2023) with\n",
      "andwithout In-Context RALM (often referred to\n",
      "in ODQA literature as open-book and closed-book\n",
      "settings, respectively). In contrast to most prior\n",
      "work on ODQA ( e.g., Izacard and Grave 2021; Fa-\n",
      "jcik et al. 2021; Izacard et al. 2022b; Levine et al.\n",
      "2022b), our “reader” ( i.e., the model that gets the\n",
      "-----------------------------------------------------------\n",
      "Page: 0\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: over, as sparse and dense representations are\n",
      "often complementary, G ARcan be easily com-\n",
      "bined with DPR to achieve even better per-\n",
      "formance. G ARachieves state-of-the-art per-\n",
      "formance on Natural Questions and TriviaQA\n",
      "datasets under the extractive QA setup when\n",
      "equipped with an extractive reader, and con-\n",
      "sistently outperforms other retrieval methods\n",
      "when the same generative reader is used.1\n",
      "1 Introduction\n",
      "Open-domain question answering (OpenQA) aims\n",
      "to answer factoid questions without a pre-speciﬁed\n",
      "domain and has numerous real-world applications.\n",
      "In OpenQA, a large collection of documents ( e.g.,\n",
      "Wikipedia) are often used to seek information per-\n",
      "taining to the questions. One of the most com-\n",
      "mon approaches uses a retriever-reader architecture\n",
      "(Chen et al., 2017), which ﬁrst retrieves a small sub-\n",
      "set of documents using the question as the query\n",
      "and then reads the retrieved documents to extract\n",
      "(or generate) an answer. The retriever is crucial as it\n",
      "-----------------------------------------------------------\n",
      "Page: 0\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: over, as sparse and dense representations are\n",
      "often complementary, G ARcan be easily com-\n",
      "bined with DPR to achieve even better per-\n",
      "formance. G ARachieves state-of-the-art per-\n",
      "formance on Natural Questions and TriviaQA\n",
      "datasets under the extractive QA setup when\n",
      "equipped with an extractive reader, and con-\n",
      "sistently outperforms other retrieval methods\n",
      "when the same generative reader is used.1\n",
      "1 Introduction\n",
      "Open-domain question answering (OpenQA) aims\n",
      "to answer factoid questions without a pre-speciﬁed\n",
      "domain and has numerous real-world applications.\n",
      "In OpenQA, a large collection of documents ( e.g.,\n",
      "Wikipedia) are often used to seek information per-\n",
      "taining to the questions. One of the most com-\n",
      "mon approaches uses a retriever-reader architecture\n",
      "(Chen et al., 2017), which ﬁrst retrieves a small sub-\n",
      "set of documents using the question as the query\n",
      "and then reads the retrieved documents to extract\n",
      "(or generate) an answer. The retriever is crucial as it\n",
      "-----------------------------------------------------------\n",
      "Page: 0\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: over, as sparse and dense representations are\n",
      "often complementary, G ARcan be easily com-\n",
      "bined with DPR to achieve even better per-\n",
      "formance. G ARachieves state-of-the-art per-\n",
      "formance on Natural Questions and TriviaQA\n",
      "datasets under the extractive QA setup when\n",
      "equipped with an extractive reader, and con-\n",
      "sistently outperforms other retrieval methods\n",
      "when the same generative reader is used.1\n",
      "1 Introduction\n",
      "Open-domain question answering (OpenQA) aims\n",
      "to answer factoid questions without a pre-speciﬁed\n",
      "domain and has numerous real-world applications.\n",
      "In OpenQA, a large collection of documents ( e.g.,\n",
      "Wikipedia) are often used to seek information per-\n",
      "taining to the questions. One of the most com-\n",
      "mon approaches uses a retriever-reader architecture\n",
      "(Chen et al., 2017), which ﬁrst retrieves a small sub-\n",
      "set of documents using the question as the query\n",
      "and then reads the retrieved documents to extract\n",
      "(or generate) an answer. The retriever is crucial as it\n",
      "-----------------------------------------------------------\n",
      "Page: 0\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: over, as sparse and dense representations are\n",
      "often complementary, G ARcan be easily com-\n",
      "bined with DPR to achieve even better per-\n",
      "formance. G ARachieves state-of-the-art per-\n",
      "formance on Natural Questions and TriviaQA\n",
      "datasets under the extractive QA setup when\n",
      "equipped with an extractive reader, and con-\n",
      "sistently outperforms other retrieval methods\n",
      "when the same generative reader is used.1\n",
      "1 Introduction\n",
      "Open-domain question answering (OpenQA) aims\n",
      "to answer factoid questions without a pre-speciﬁed\n",
      "domain and has numerous real-world applications.\n",
      "In OpenQA, a large collection of documents ( e.g.,\n",
      "Wikipedia) are often used to seek information per-\n",
      "taining to the questions. One of the most com-\n",
      "mon approaches uses a retriever-reader architecture\n",
      "(Chen et al., 2017), which ﬁrst retrieves a small sub-\n",
      "set of documents using the question as the query\n",
      "and then reads the retrieved documents to extract\n",
      "(or generate) an answer. The retriever is crucial as it\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def convert_docs_to_dict(docs):\n",
    "    \"\"\"\n",
    "    Convert a list of Document objects to a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "    - docs (list): List of Document objects.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of dictionaries containing 'page_content' and 'metadata'.\n",
    "    \"\"\"\n",
    "    doc_dicts = []\n",
    "    for doc in docs:\n",
    "        doc_dict = {\n",
    "            'page_content': str(doc.page_content),\n",
    "            'metadata': doc.metadata\n",
    "        }\n",
    "        doc_dicts.append(doc_dict)\n",
    "    return doc_dicts\n",
    "\n",
    "# Example usage:\n",
    "docs_dict_list = convert_docs_to_dict(docs)\n",
    "for doc_dict in docs_dict_list:\n",
    "    print(\"Page:\", doc_dict['metadata']['page'])\n",
    "    print(\"Source:\", doc_dict['metadata']['source'])\n",
    "    print(\"Content:\", doc_dict['page_content'])\n",
    "    print(\"-----------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71c982aa-ddef-47e3-8671-eea833abdaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.634298\n",
      "4.634298\n",
      "4.634298\n",
      "4.634298\n",
      "4.634298\n",
      "4.634298\n",
      "6.049387\n",
      "6.049387\n",
      "6.049387\n",
      "6.049387\n",
      "[{'page_content': 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'metadata': {'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'metadata': {'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'metadata': {'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'metadata': {'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': 'the test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the', 'metadata': {'page': 9, 'source': './data\\\\In context retrieval.pdf'}}, {'page_content': 'the test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the', 'metadata': {'page': 9, 'source': './data\\\\In context retrieval.pdf'}}, {'page_content': 'the test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the', 'metadata': {'page': 9, 'source': './data\\\\In context retrieval.pdf'}}, {'page_content': 'the test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the', 'metadata': {'page': 9, 'source': './data\\\\In context retrieval.pdf'}}, {'page_content': 'the test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the', 'metadata': {'page': 9, 'source': './data\\\\In context retrieval.pdf'}}, {'page_content': 'the test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the', 'metadata': {'page': 9, 'source': './data\\\\In context retrieval.pdf'}}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming model is your machine learning model\n",
    "# query is the query you want to predict scores for\n",
    "query = \"What is Open-domain question answering?\"\n",
    "\n",
    "# Assuming docs_dict_list is a list of dictionaries with 'page_content' key\n",
    "# containing the content of each document\n",
    "docs_content_list = [doc['page_content'] for doc in docs_dict_list]\n",
    "\n",
    "# Initialize an empty list to store the scores along with document indices\n",
    "scores_list = []\n",
    "\n",
    "# Loop through each document and calculate the score\n",
    "for i, doc_content in enumerate(docs_content_list):\n",
    "    # Assuming model.predict returns a scalar score\n",
    "    score = model.predict([query, doc_content])\n",
    "    print(score)\n",
    "    \n",
    "    # Append the score along with the document index to the scores_list\n",
    "    scores_list.append((i, score))\n",
    "\n",
    "# Sort the scores_list in descending order based on scores\n",
    "scores_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Now, scores_list contains tuples of (document_index, score) sorted in descending order\n",
    "# You can use these indices to access the corresponding documents in docs_dict_list if needed\n",
    "# Example:\n",
    "sorted_documents = [docs_dict_list[i] for i, _ in scores_list]\n",
    "\n",
    "# Print or use the sorted_documents as needed\n",
    "print(sorted_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db0327-f1bd-49fe-8173-84bbe47f2a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
