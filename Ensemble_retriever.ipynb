{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5616410-3af2-45e2-b14d-88c09d294ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import openai\n",
    "from langchain.llms import openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_transformers import (\n",
    "    EmbeddingsRedundantFilter,\n",
    "    EmbeddingsClusteringFilter,\n",
    ")\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader , PdfWriter, PdfMerger\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7613a0-2948-4702-a1d2-b39acce16861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection already exists\n",
      "0.004199800000151299\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = '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'\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "def load_chunk_persist_pdf() -> Chroma:\n",
    "    start = timeit.timeit()\n",
    "    pdf_folder_path = \"./data\"\n",
    "    documents = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    client = chromadb.Client()\n",
    "    if client.list_collections():\n",
    "        consent_collection = client.create_collection(\"consent_collection\")\n",
    "    else:\n",
    "        print(\"Collection already exists\")\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunked_documents,\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "        persist_directory=\"store/chroma/new\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    end = timeit.timeit()\n",
    "    print(start - end)\n",
    "    return vectordb\n",
    "\n",
    "c_docs = load_chunk_persist_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf03ce2-061e-42a9-abd5-ef4d56e68f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = c_docs.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":10, \"include_metadata\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "589b833a-a8ad-41f7-989b-aed3f4ab084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Open-domain question answering?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d4cdcbb-09ca-418d-9012-a3c5d4232840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='guistics.\\nDanqi Chen and Wen-tau Yih. 2020. Open-domain\\nquestion answering. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics: Tutorial Abstracts , pages 34–37, On-\\nline. Association for Computational Linguistics.\\nMingda Chen, Qingming Tang, Sam Wiseman, and\\nKevin Gimpel. 2019. Controllable paraphrase gen-\\neration with a syntactic exemplar. In Proceedings of\\nthe 57th Conference of the Association for Compu-\\ntational Linguistics, ACL 2019, Florence, Italy, July\\n28- August 2, 2019, Volume 1: Long Papers , pages\\n5972–5984. Association for Computational Linguis-\\ntics.\\nDavid Chiang. 2007. Hierarchical phrase-based trans-\\nlation. computational linguistics , 33(2):201–228.\\nSarah Dillon and Janet Fraser. 2006. Translators and\\ntm: An investigation of translators’ perceptions of\\ntranslation memory adoption. Machine Translation ,\\n20(2):67–79.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\\nFan, Michael Auli, and Jason Weston. 2018. Wizard', metadata={'page': 8, 'source': './data\\\\a survey.pdf'}),\n",
       " Document(page_content='the test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the', metadata={'page': 9, 'source': './data\\\\In context retrieval.pdf'}),\n",
       " Document(page_content='[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', metadata={'page': 4, 'source': './data\\\\RAG on large language models.pdf'}),\n",
       " Document(page_content='models are few-shot learners. In Advances in\\nNeural Information Processing Systems .\\nDanqi Chen, Adam Fisch, Jason Weston, and An-\\ntoine Bordes. 2017. Reading Wikipedia to an-\\nswer open-domain questions. In Proceedings\\nof the 55th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long', metadata={'page': 10, 'source': './data\\\\In context retrieval.pdf'}),\n",
       " Document(page_content='Generation-Augmented Retrieval for Open-Domain Question Answering\\nYuning Mao1∗, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\\nJianfeng Gao3, Jiawei Han1, Weizhu Chen2\\n1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\\n1{yuningm2, hanj}@illinois.edu\\n2,3{penhe, xiaodl, yeshe, jfgao,wzchen }@microsoft.com\\nAbstract\\nWe propose Generation-Augmented Retrieval\\n(GAR) for answering open-domain questions,\\nwhich augments a query through text genera-\\ntion of heuristically discovered relevant con-\\ntexts without external resources as supervi-\\nsion. We demonstrate that the generated con-\\ntexts substantially enrich the semantics of the\\nqueries and G ARwith sparse representations\\n(BM25) achieves comparable or better per-\\nformance than state-of-the-art dense retrieval\\nmethods such as DPR (Karpukhin et al., 2020).\\nWe show that generating diverse contexts for a\\nquery is beneﬁcial as fusing their results con-\\nsistently yields better retrieval accuracy. More-', metadata={'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='tition: Systems, analyses and lessons learned. arXiv\\npreprint arXiv:2101.00133 .\\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2019a. A discrete hard EM ap-\\nproach for weakly supervised question answering.\\nInProceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the\\n9th International Joint Conference on Natural Lan-\\nguage Processing (EMNLP-IJCNLP) , pages 2851–\\n2864, Hong Kong, China. Association for Computa-\\ntional Linguistics.\\nSewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\\nnaneh Hajishirzi. 2019b. Knowledge guided text re-\\ntrieval and reading for open domain question answer-\\ning. arXiv preprint arXiv:1911.03868 .\\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2020. Ambigqa: Answering\\nambiguous open-domain questions. arXiv preprint\\narXiv:2004.10645 .\\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\\n2016. Ms marco: A human-generated machine read-', metadata={'page': 10, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 6769–\\n6781, Online. Association for Computational Lin-\\nguistics.\\nYannis Katsis, Saneem Chemmengath, Vishwajeet\\nKumar, Samarth Bharadwaj, Mustafa Canim,\\nMichael Glass, Alﬁo Gliozzo, Feifei Pan, Jay-\\ndeep Sen, Karthik Sankaranarayanan, and Soumen\\nChakrabarti. 2021. Ait-qa: Question answering\\ndataset over complex tables in the airline industry.\\nBogdan Kosti ´c, Julian Risch, and Timo Möller. 2021.\\nMulti-modal retrieval of tables and texts using tri-\\nencoder models.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob', metadata={'page': 4, 'source': './data\\\\T-RAG End-to-End Table Question Answering via Retrieval-Augmented.pdf'}),\n",
       " Document(page_content='Unlike generative QA, the goal of GARis not to\\ngenerate perfect answers to the questions but perti-\\nnent contexts that are helpful for retrieval. Another\\nline in generative QA learns to generate answers\\nwithout relevant passages as the evidence but solely\\nthe question itself using PLMs (Roberts et al., 2020;\\nBrown et al., 2020). GARfurther conﬁrms that one\\ncan extract factual knowledge from PLMs, which\\nis not limited to the answers as in prior studies but\\nalso other relevant contexts.\\n3 Generation-Augmented Retrieval\\n3.1 Task Formulation\\nOpenQA aims to answer factoid questions with-\\nout pre-speciﬁed domains. We assume that a large\\ncollection of documents C(i.e., Wikipedia) are\\ngiven as the resource to answer the questions and\\na retriever-reader architecture is used to tackle the\\ntask, where the retriever retrieves a small subset\\nof the documents D⊂Cand the reader reads the\\ndocuments Dto extract (or generate) an answer.\\nOur goal is to improve the effectiveness and efﬁ-', metadata={'page': 2, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='Recent Query Reformulation . There are recent\\nor concurrent studies (Nogueira and Cho, 2017;\\nZaiem and Sadat, 2019; Yu et al., 2020; Vaku-\\nlenko et al., 2020; Lin et al., 2020) that reformu-\\nlate queries with generation models for other re-\\ntrieval tasks. However, these studies are not eas-\\nily applicable or efﬁcient enough for OpenQA be-\\ncause: (1) They require external resources such as\\nparaphrase data (Zaiem and Sadat, 2019), search\\nsessions (Yu et al., 2020), or conversational con-\\ntexts (Lin et al., 2020; Vakulenko et al., 2020)\\nto form the reformulated queries, which are not\\navailable or showed inferior domain-transfer per-\\nformance in OpenQA (Zaiem and Sadat, 2019);\\n(2) They involve time-consuming training process\\nsuch as RL. For example, Nogueira and Cho (2017)\\nreported a training time of 8 to 10 days as it uses\\nretrieval performance in the reward function and\\nconducts retrieval at each iteration. In contrast,\\nGARuses freely accessible in-domain contexts like', metadata={'page': 1, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\nNatural questions: A benchmark for question an-\\nswering research. Transactions of the Association\\nfor Computational Linguistics , 7:452–466.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 6086–6096, Florence,\\nItaly. Association for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\\nBart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and', metadata={'page': 9, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12868594-77b3-4fec-baff-7a0a6f7c83d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "A Resource -efficient FIR Filter Design Based on an \n",
      "RAG Improve d Algorithm  \n",
      " \n",
      "Mengwei Hu  \n",
      "National Demonstration Center for \n",
      "Experimental Physics and Education  \n",
      "School of Physics and Technology, \n",
      "Wuhan University  \n",
      "Wuhan, China  \n",
      "humengv@whu.edu.cn  Xianyang Jiang*  \n",
      "National Demonstration Center for \n",
      "Experimental Physics and Education  \n",
      "School of Physics and Technology, \n",
      "Wuhan University  \n",
      "Wuhan, China  \n",
      "jiang@whu. edu.cn  Zhengxiong Li  \n",
      "National Demonstration Center for \n",
      "Experimental Physics and Education  \n",
      "Hongyi Honor College of Wuhan \n",
      "University  \n",
      "Wuhan, China  \n",
      "li_zhengxiong@whu.edu.cn  \n",
      " \n",
      "Abstract —In modern digital filter chip design, efficient \n",
      "resource u tilization is a hot topic. Due to linear phase \n",
      "characteristics of FIR filters, a pulsed fully parallel structure \n",
      "can be applied to attack the problem . In order to further reduce \n",
      "hardware resource consumption especially caused by \n",
      "multiplication function, an improved RAG algorithm is \n",
      "proposed. Filters with different orders and for different \n",
      "algorithms are compared, and the experimental results show \n",
      "that the improved RAG algorithm is excellent in terms of logic \n",
      "resource utilization, resource allocation, running  speed, and \n",
      "power consumption under  different application scenarios. The \n",
      "proposed algorithm invokes a better circuit structure for FIR \n",
      "filter , it gives full play to resource allocation strategy and \n",
      "reduces logic resource consumption. The proposed circuit i s \n",
      "faster and more stable, and suitable for a variety of complex \n",
      "application scenarios.  \n",
      "Keywords —FIR filter, pulsed fully parallel structure, \n",
      "improved RAG algorithm, resource allocation strategy.  \n",
      "I. INTRODUCTION  \n",
      "FIR filters have a wide range of applications in  \n",
      "communication, audio processing, image processing, and \n",
      "other fields. With blowout type increase ment  of portable \n",
      "devices and Internet of Things, there is an increasing demand \n",
      "for low power consumption and small size in this field. FIR \n",
      "filter design also ev olves in this tide to meet requirements of \n",
      "embedded systems. Efficient FIR filter design methods have \n",
      "been continuously explored to reduce computational cost and \n",
      "product cost, which may involve new optimization algorithms, \n",
      "approximation techniques, and res ource allocation strategies.  \n",
      "FIR filters have an important linear phase property, which \n",
      "allows us to exploit the symmetry of coefficients to build both \n",
      "serial and parallel structures. Compared to IIR filters, FIR \n",
      "filters have many significant advantages su ch as bounded \n",
      "input and output stability, phase linearity, and low coefficient \n",
      "sensitivity, which makes it more suitable under many \n",
      "applications [1]. However, FIR filters involve a large number \n",
      "of arithmetic operations , which limits their processing speed \n",
      "[2]. In order to overcome this limitation, a fully parallel \n",
      "structure can be exploited which allows a single filtering \n",
      "operation to perform multiple multiplications simultaneously \n",
      "to improve performance. Compared to fully parallel FIR \n",
      "filters, adoption of improved Reduced Adder Graph (RAG) \n",
      "algorithm [3] can significantly reduce hardware consumption \n",
      "by exploiting redundancy between coefficients [4]. Our work focuses on how to efficiently implement FIR filters with fixed \n",
      "coefficients. In the proposed design, fully parallel structure \n",
      "and RAG algorithm characteristics have been utilized to \n",
      "effectively reduce hardware cost. Meanwhile, a better \n",
      "resource allocation strategy is taken to further improve FIR \n",
      "filter implementation.  \n",
      "II. PULSED F ULLY PARALLEL FIR  FILTERS  \n",
      "FIR filters differ from IIR filters in that their impulse \n",
      "response can be expressed in terms of a finite number of \n",
      "sampled values and can be described by a difference equation \n",
      "(1), where N is the number of filters tap coefficients and x(n) \n",
      "is the input time s eries [5].  \n",
      "   𝑦(𝑛)=∑ℎ(𝑘)𝑥(𝑛−𝑘)𝑁−1\n",
      "𝑘=0       () \n",
      "In order to implement an efficient specific circuit for FIR, \n",
      "the \"pulsation\" structure, originally proposed by H.T. Kung, \n",
      "represents a parallel pipelined approach for high -speed signal \n",
      "processing and data processing. This architecture is known for \n",
      "a number  of advantages such as modularity, regularity, local \n",
      "linking, and high degree of pipelining. In pulsation \n",
      "architecture, Process Element (PE) constitutes a \n",
      "multiprocessor system and these PEs work together in a \n",
      "synchronized manner so that this architecture offers \n",
      "significant performance in processing large -scale data. The \n",
      "hardware structure of a pulsation FIR filter is shown in Fig. 1, \n",
      "and N PEs are required to accomplish one such operation.  \n",
      " \n",
      "Fig. 1  Hardware structure of pulsation FIR filter  \n",
      "As for an FIR fil ter with symmetric coefficients, its linear \n",
      "phase property can be further utilized to reduce the number of \n",
      "PEs by pre -addition. For example, a hardware structure of \n",
      "even -symmetric filter is shown in Fig. 2, and it is clear that the \n",
      "number of PEs is reduced  to N/2 for the same N tap \n",
      "coefficients.   \n",
      "Z -1 Z -1 Z -1Z -1\n",
      "out\n",
      "……\n",
      "PE1din\n",
      "PE2 PE3 PEn0h(0) h(1) h(2) h(n-1)Fig. 2  Structure of pulsation FIR filter with even symmetry of coefficients  \n",
      "III. FILTER IMPLEMENTATION OF RAG  IMPROVEMENT \n",
      "ALGORITHM  \n",
      "A. RAG Improved Algorithm  \n",
      "Pulsation filter designs are usually based on PEs that have \n",
      "identical coefficients. Since such coefficients are constant and \n",
      "shift operations can be easily implemented in hardware, \n",
      "multiplication can be replaced using shift, add, and subtract \n",
      "operations to reduc e resource consumption of multiplier. The \n",
      "concept of u sing an addition tree in a multiplier was first \n",
      "introduced by Bull, who advocated the implementation of \n",
      "multiplication by constructing a structural graph consisting of \n",
      "simple addition and shift operations, A.G. Dempster and M.D. \n",
      "Macleod proposed RAG algori thm [ 6], the core idea of this \n",
      "algorithm is to use an equivalent structure to convert all \n",
      "coefficients to bases, and at the same time, introduce \n",
      "subtraction structure to make all intermediate values positive, \n",
      "which greatly simplifies the structure of addit ion tree. The \n",
      "redundancy relationship between coefficients is also utilized \n",
      "to reduce logical depth in order to cut down the total resource \n",
      "consumption [ 7,8]. \n",
      "In addition, the RAG algorithm introduces a concept \n",
      "called adder depth \"cost\". Despite obvious ad vantages of \n",
      "RAG algorithm, its disadvantage lies in the need to calculate \n",
      "cost value for each coefficient. For smaller coefficients, cost \n",
      "value can be obtained directly by looking up the table, but \n",
      "when coefficients are larger, cost value calculation becom es \n",
      "more difficult [1 1]. \n",
      "In order to address shortcomings of the RAG algorithm in \n",
      "circuit design, an improved algorithm is proposed, which \n",
      "combines the advantages of pulsation structure fully parallel \n",
      "FIR filter and the RAG algorithm structure FIR filter.  \n",
      "Let \"coeff\" be all filter coefficients to be realized, \"coeff -\n",
      "r\" is the set of smaller coefficients, \"coeff -s\" is the set of larger \n",
      "coefficients. \"cost -n\" (n=1,2,3,4) is the set of coefficients with \n",
      "different adder depths, \"cost -o\" is the set of coefficient s with \n",
      "other adder depths, and the improved algorithm [ 9-12] are as \n",
      "follows:  \n",
      "⚫ Take the absolute values of all coefficients and store the \n",
      "results in \"coeff\" set;  \n",
      "⚫ Remove duplicate coefficients and coefficients with \n",
      "value 2n, and the number of remaining coefficients is \n",
      "denoted as N;  \n",
      "⚫ The smaller coefficients are deposited into set \"coeff -r\", and the number of coefficients deposited is N/2 or (N -\n",
      "1)/2; \n",
      "⚫ Deposit the remaining larger coefficients into set \n",
      "\"coeff -s\"; \n",
      "⚫ Divide the even numbers in the \" coeff -r\" set by 2n to \n",
      "obtain the base;  \n",
      "⚫ Look up the table to get the depth of adder \n",
      "corresponding to each base number, store these \n",
      "coefficients in \"cost -n\" set, and store the coefficients \n",
      "that cannot be categorized by the table in \"cost -o\" set;  \n",
      "⚫ Realize coef ficients in \"cost -1\" set;  \n",
      "⚫ Check the sum/difference of coefficients in all realized \n",
      "cost sets, realize the coefficients in higher cost sets by \n",
      "the sum/difference of coefficients and the realized \n",
      "coefficients, and finally realize the coefficients in \n",
      "\"cost -o\" set; \n",
      "⚫ Realize the coefficients in \"coeff -s\" set according to \n",
      "the hardware structure of pulsation FIR filter with \n",
      "symmetric coefficients.  \n",
      "B. Implementation Example  \n",
      "Taking a 64th order filter as an example, Fs=250KHz \n",
      "and Fc=20KHz, the filter coefficients after  quantization and \n",
      "rounding [1 3] are shown in Table 1, and due to the symmetry \n",
      "of the coefficients of the FIR filters, only coefficients with \n",
      "0≤n≤31 need to be di scussed here.  \n",
      "TABLE I.  FILTER COEFFICIENTS , H(N) = H(63-N), 32≤N≤63 \n",
      "h(0)=219  h(1)=137  h(2)=162  h(3)=174  \n",
      "h(4)=168  h(5)=137  h(6)=79  h(7)= -9 \n",
      "h(8)= -127 h(9)= -269 h(10)= -428 h(11)= -592 \n",
      "h(12)= -747 h(13)= -875 h(14)= -957 h(15)= -972 \n",
      "h(16)= -903 h(17)= -733 h(18)= -450 h(19)= -49 \n",
      "h(20)=470  h(21)=1100  h(22)=1825  h(23)=2622  \n",
      "h(24)=3462  h(25)=4311  h(26)=5134  h(27)=5891  \n",
      "h(28)=6548  h(29)=7072  h(30)=7437  h(31)=7624  \n",
      "Firstly, all the coefficients in the above table are taken as \n",
      "absolute values, and duplicates  and numbers divisible by 2n \n",
      "are removed, and then the coefficients are divided into the two \n",
      "parts, one part of the coefficients is smaller, which is easy to \n",
      "optimize using the RAG improvement algorithm, and the \n",
      "coefficients are stored into the \"coeff -r\" set = [9, 49, 79, 127, \n",
      "137, 1 62, 168, 174, 219, 269, 428, 450, 470, 592, 733], and \n",
      "one part is larger, and the corresponding coefficients can be \n",
      "stored according to the pulsation structure and the symmetry \n",
      "of the coefficients. 162, 168, 174, 219, 269, 428, 450, 470, \n",
      "592, 733], part s of the coefficients are larger, according to the \n",
      "pulsation structure and the symmetry of the coefficients, the \n",
      "corresponding input signals can be pre -added or subtracted, \n",
      "and then multiplied by the filter coefficients, which are \n",
      "deposited in the \"coeff -s\" collection. The coefficients are \n",
      "stored in the set of \"coeff -s\" = [747, 875, 957, 972, 903, 1100, \n",
      "1825, 2622, 3462, 4311, 5134, 5891, 6548, 7072, 7437, 7624].  \n",
      "Check the adder depth table to categorize the coefficients \n",
      "and store the coefficients in the corre sponding sets:  \n",
      "\"cost -1\" set = [9, 127];   \n",
      "out……\n",
      "PE1din\n",
      "PE2 PE3 PEn0h(0) h(1) h(2) h(n-1)Z -1Z -1\n",
      "Z -1 Z -1Z -1 Z -1Z -1 ……\"cost -2\" set = [49, 79, 137, 162, 168];  \n",
      "\"cost -3\" set = [174, 219];  \n",
      "\"cost -o\" set = [269, 428, 450, 470, 592, 733].  \n",
      "First realize the coefficients of cost -1: \n",
      " {x9 = xin<<3 + xin,   \n",
      "x127 = xin<<7 – xin, \n",
      "Continue to reali ze the cost -2 factor : \n",
      " \n",
      "{    x49 = x9<<4 + x9 + xin<<2,        \n",
      "x79 = x127 – x49 + xin,                           \n",
      "x137 = x127 + x9 + xin,                         \n",
      "x162 = x137 + x9 + xin<<4,             \n",
      "x168 = x162 + xin<<2 + xin<<1, \n",
      "The cost -3 factor is then reali zed: \n",
      " {x174 = x127 + x49 – xin<<1,\n",
      "x219 = x168 + x49 – xin<<1, \n",
      "The flexible use of multiplexing makes it possible to use \n",
      "only two layers of adder depth for the coefficients of cost -3 as \n",
      "well, and then gradually implement the other coefficients at \n",
      "the end. The final result of the RAG improvement algorithm \n",
      "design is as follows:  \n",
      " {x9 = xin<<3 + xin,   \n",
      "x127 = xin<<7 – xin, \n",
      " \n",
      "{    x49 = x9<<4 + x9 + xin<<2,        \n",
      "x79 = x127 – x49 + xin,                           \n",
      "x137 = x127 + x9 + xin,                         \n",
      "x162 = x137 + x9 + xin<<4,             \n",
      "x168 = x162 + xin<<2 + xin<<1, \n",
      " {x174 = x127 + x49 – xin<<1,\n",
      "x219 = x168 + x49 – xin<<1, \n",
      " {x450 = x428 + x9<<1 + xin<<2,\n",
      "x470 = x450 + x9<<1 + xin<<1,\n",
      "x592 = x450 + x269 – x127,                 \n",
      "x733 = x592 + x137 + xin<<2,       \n",
      "Comparing to the pre -optimization design, which uses a \n",
      "total of  28 adders, performs 7 shift operations and keeps the \n",
      "adder depth at 2 and below, saves more than half of the total \n",
      "number of adders and a large number of shift operations, while \n",
      "also reducing the adder depth, compared to the unimproved \n",
      "algorithm.  \n",
      "IV. COMPARI SON OF HARDWARE SYNTHESIS RESULTS  \n",
      "The consumption of FPGA hardware resources can be \n",
      "measured by FPGA LUT resources, FF register resources, and \n",
      "DSP resources, and the hardware performance can be \n",
      "measured by power consumption and device junction \n",
      "temperature [9,14]. In our design , we adopt a  Virtex -7 series \n",
      "xc7vx485tffg1157 -1 FPGA, and implement and compare \n",
      "pulsed fully parallel structure, traditional RAG algorithmic \n",
      "structure, and RAG improved algorithmic structure according \n",
      "to these measuements . The realizat ion results are shown in \n",
      "Table 2 and Figure 3.  \n",
      "TABLE II.  COMPARISON OF 64TH ORDER FILTER HARDWARE  Resource \n",
      "performance \n",
      "indicators  different algorithmic structures  \n",
      "Pulsed Fully \n",
      "Parallel  RAG \n",
      "algorithm  RAG Improved \n",
      "Algorithm  \n",
      "LUT  574 4956 934 \n",
      "FF 1286 528 904 \n",
      "DSP 4 0 2 \n",
      "Power( W) 32.8 234.7 38.6 \n",
      "Tem(℃) 70.8 125.0 79 \n",
      " \n",
      " \n",
      "Fig. 3  Comprehensive comparison of 64th order filter hardware  \n",
      "From the results, we can see that the pulsation structure \n",
      "fully parallel filter uses the most DSP and FF and the least \n",
      "LUT resources, while the conventional RAG algorithmic \n",
      "structure filter does not use DSP and consumes the least FF \n",
      "resources, but it is cl ear that the RAG algorithmic structure \n",
      "uses too much LUT resources compared  to the other two \n",
      "structures to the extent that the power and junction \n",
      "temperatures are too high to be used for practical applications. \n",
      "The RAG improved algorithmic structure filter reduces the \n",
      "DSP usage by half and the FF resources by about 29.7% \n",
      "compared  to the pulsed fully parallel structure, while the LUT \n",
      "resources are reduced by about 81.2% compared  to the RAG \n",
      "algorithmic structure while achieving low power consumption \n",
      "and junction temperature.  \n",
      "Table 3 and 4 show the hardware synthesis comparison of \n",
      "the 8th and 32nd order filters, respectively, to validate the \n",
      "hardware synthesis results for different order filters.  \n",
      "TABLE III.  COMPARISON OF 8TH ORDER FILTER HARDWARE  \n",
      "Resource \n",
      "performance \n",
      "indicators  different algorithmic structures  \n",
      "Pulsed Fully \n",
      "Parallel  RAG \n",
      "algorithm  RAG Improved \n",
      "Algorithm  \n",
      "LUT  141 212 185 \n",
      "FF 203 120 222 \n",
      "DSP 4 0 2 \n",
      "Power( W) 41.762  33.673  36.75  \n",
      "Tem(℃) 83.4 72.1 76.4 \n",
      "It can be seen that the traditional RAG algorithm structure \n",
      "has the best integrated performance when the filter order is 8, \n",
      "the RAG improved algorithm structure has excellent \n",
      "performance, and the puls ed fully parallel structure has the \n",
      "worst integrated performance, and there is little difference in \n",
      "the integrated performance of the three except for the DSP \n",
      "resource consumption.    \n",
      "TABLE IV.  COMPARISON OF 32ND ORDER FILTER HARDWARE  \n",
      "Resource \n",
      "performance \n",
      "indicators  different algorithmic structures  \n",
      "Pulsed Fully \n",
      "Parallel  RAG \n",
      "algorithm  RAG Improved \n",
      "Algorithm  \n",
      "LUT  358 695 555 \n",
      "FF 679 287 538 \n",
      "DSP 4 0 2 \n",
      "Power( W) 21.34  24.52  19.75  \n",
      "Tem(℃) 54.8 59.3 52.6 \n",
      " While the puls ed fully parallel structure consumes the \n",
      "most FF and DSP resources and the RAG algorithm structure \n",
      "consumes the most LUT resources when the filter order is 32, \n",
      "the LUT, FF, and DSP resource consumption of the RAG \n",
      "improved algorithm structure filter are all in the middle \n",
      "between the puls ed fully parallel structure and the traditional \n",
      "RAG algorithm filter, which are more balanced in logic \n",
      "distribution, and the power consumption and junction \n",
      "temperature are the lowest  among the three [1 5]. \n",
      " Comprehensive comparison concluded that the puls ed \n",
      "fully parallel structure is more suitable for higher order filters, \n",
      "the traditional RAG algorithm structure is  more  suitable for \n",
      "low order filters, and regardless of the order  numbe r, the RAG \n",
      "improved algorithm structure filters have excellent \n",
      "performance, effectively take advantage of the resource \n",
      "allocation strategy,  and achieve low power consumption, \n",
      "low junction temperature , and optimal resource consumption \n",
      "for 32-order and 64 -order filters . Specifically, the 64th order \n",
      "filter reduces DSP usage and balances logic resource \n",
      "consumption, and stabilizes power consumption and junction \n",
      "temperature  as well. The proposed design  is very suitable for \n",
      "application s in the case of many coefficients and high \n",
      "complexity.  \n",
      "V. CONCLUSION  \n",
      "FIR filters exists in more and more application scenarios, \n",
      "and practical requirements are becoming more and more \n",
      "individualized, so novel design solutions must b e \n",
      "continuously explored to meet these requirements under \n",
      "various scenarios. The algorithmic structure described \n",
      "provides a new design scheme that is suitable for most \n",
      "application scenarios and is well suited when the number of \n",
      "filter orders is large or the  coefficients are large. FIR filter \n",
      "design for improved RAG algorithm gives full play to \n",
      "resource allocation strategy by using shift and add operations \n",
      "instead of a direct multiplier structure. Combining with \n",
      "pulsation fully parallel structure, both operat ion speed and \n",
      "resource utilization efficiency are enhanced . Comparative \n",
      "simulation experiments demonstrate that the  improved RAG \n",
      "algorithm and filter structure save a large amount of logic \n",
      "resources, meet low -power requirements, and decrease  \n",
      "contradiction between speed and resource -consumption as \n",
      "well.  \n",
      "ACKNOWLEDGMENT  \n",
      "This work was supported in part by National Science \n",
      "Foundation of China under Grant 61072135, 81971702, the \n",
      "Fundamental Research Funds for the Central Universities \n",
      "under Grant 2042017gf0075, 20 42019gf0072, and Natural \n",
      "Science Foundation of Hubei Province under Grant \n",
      "2017CFB721.  REFERENCES  \n",
      "[1] Abhijit Chandra, Sudipta Chattopadhyay,  Design of hardware efficient \n",
      "FIR filter: A review of the state -of-the-art approaches,  Engineering \n",
      "Science and Technolog y, an International Journal,  Volume 19, Issue 1,  \n",
      "2016,  Pages 212 -226, ISSN 2215 -0986.  \n",
      "[2] A. Pathan, A. H. Chandio and R. Aziz, \"An Optimization in \n",
      "Conventional Shift &Add Multiplier for Area -Efficient \n",
      "Implementation on FPGA,\" 2022 International Conference on \n",
      "Emerging Technologies in Electronics, Computing and \n",
      "Communication (ICETECC), Jamshoro, Sindh, Pakistan, 2022, pp. 1 -\n",
      "6, doi: 10.1109/ICETECC56662.2022.10069099.  \n",
      "[3] A. G. Dempster and M. D. Macleod, \"Use of minimum -adder \n",
      "multiplier blocks in FIR digital filters ,\" in IEEE Transactions on \n",
      "Circuits and Systems II: Analog and Digital Signal Processing, vol. 42, \n",
      "no. 9, pp. 569 -577, Sept. 1995, doi: 10.1109/82.466647.  \n",
      "[4] U. Meyer -Baese, J. Chen, C. H. Chang and A. G. Dempster, \"A \n",
      "Comparison of Pipelined RAG -n and DA FPGA -based Multiplierless \n",
      "Filters,\" APCCAS 2006 - 2006 IEEE Asia Pacific Conference on \n",
      "Circuits and Systems, Singapore, 2006, pp. 1555 -1558, doi: \n",
      "10.1109/APCCAS.2006.342540.  \n",
      "[5] U. Meyer -Baese, \"Digital Signal Processing with Field Programmable \n",
      "Gate Arrays,\" T allahassee, USA: Springer Berlin, Heidelberg, Sept. \n",
      "2007, doi: 10.1007/978 -3-540-72613 -5, ISSN 1860 -4862.  \n",
      "[6] O. Gustafsson, \"A Difference Based Adder Graph Heuristic for \n",
      "Multiple Constant Multiplication Problems,\" 2007 IEEE International \n",
      "Symposium on Circuits  and Systems, New Orleans, LA, USA, 2007, \n",
      "pp. 1097 -1100, doi: 10.1109/ISCAS.2007.378201.   \n",
      "[7] M. Potkonjak, M. B. Srivastava and A. P. Chandrakasan, \"Multiple \n",
      "constant multiplications: efficient and versatile framework and \n",
      "algorithms for exploring common subex pression elimination,\" in IEEE \n",
      "Transactions on Computer -Aided Design of Integrated Circuits and \n",
      "Systems, vol. 15, no. 2, pp. 151 -165, Feb. 1996, doi: \n",
      "10.1109/43.486662.  \n",
      "[8] A. G. Dempster and M. D. Macleod, \"Comments on \"Minimum \n",
      "number of adders for implementi ng a multiplier and its application to \n",
      "the design of multiplierless digital filters\",\" in IEEE Transactions on \n",
      "Circuits and Systems II: Analog and Digital Signal Processing, vol. 45, \n",
      "no. 2, pp. 242 -243, Feb. 1998, doi: 10.1109/82.661661.  \n",
      "[9] S. Mirzaei, A. Hos angadi and R. Kastner, \"FPGA Implementation of \n",
      "High Speed FIR Filters Using Add and Shift Method,\" 2006 \n",
      "International Conference on Computer Design, San Jose, CA, USA, \n",
      "2006, pp. 308 -313, doi: 10.1109/ICCD.2006.4380833.  \n",
      "[10] Jeong -Ho Han and I. -C. Park, \"Digita l filter synthesis considering \n",
      "multiple adder graphs for a coefficient,\" 2008 IEEE International \n",
      "Conference on Computer Design, Lake Tahoe, CA, USA, 2008, pp. \n",
      "315-320, doi: 10.1109/ICCD.2008.4751879.  \n",
      "[11] A. Abbaszadeh, A. Azerbaijan and K. D. Sadeghipour, \"A n ew \n",
      "hardware efficient reconfigurable fir filter architecture suitable for \n",
      "FPGA applications,\" 2011 17th International Conference on Digital \n",
      "Signal Processing (DSP), Corfu, Greece, 2011, pp. 1 -4, doi: \n",
      "10.1109/ICDSP.2011.6004958.  \n",
      "[12] H V Kumaraswamy, AmruthKaran th P, Samarth Athreyas and Akash \n",
      "Bharadwaj B R, \"  Comparative Analysis of Different Area-efficient Fir \n",
      "Filter Structures for Symmetric Convolutions ,\" International Journal of \n",
      "Electrical and Electronic Engineering & Telecommunications, Vol. 3, \n",
      "No. 2, pp. 50 -55, April 2014.  \n",
      "[13] Y. J. Yu and Y. C. Lim, \"Design of Linear Phase FIR Filters in \n",
      "Subexpression Space Using Mixed Integer Linear Programming,\" in \n",
      "IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 54, \n",
      "no. 10, pp. 2330 -2338, Oct. 2007, doi: 10. 1109/TCSI.2007.904599.  \n",
      "[14] LI Ying,LU Weijun,YU Dunshan et al. A resource optimization \n",
      "algorithm for implementing FIR digital filters on FPGA[J]. Journal of \n",
      "Peking University (Natural Science Edition),2009,45(02):222 -\n",
      "226.doi:10.13209/j.0479 -8023.2009.034.  \n",
      "[15] Nakka Sivaraju and S Suman, \"  Area Power and Delay Efficiency \n",
      "Evaluation of Truncated and Modified Wallace Fir Filters ,\" \n",
      "International Journal of Electrical and Electronic Engineering & \n",
      "Telecommunications, Vol. 4, No. 4, pp. 43 -52, October 2015.  \n",
      "==================================================\n",
      "\n",
      "Document 2:\n",
      "A Survey on Retrieval-Augmented Text Generation\n",
      "Huayang Li~;\u0003Yixuan Su;\u0003Deng Cai};\u0003Yan Wang|;\u0003Lemao Liu|;\u0003\n",
      "~Nara Institute of Science and TechnologyUniversity of Cambridge\n",
      "}The Chinese University of Hong Kong|Tencent AI Lab\n",
      "li.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\n",
      "thisisjcykcd@gmail.com, brandenwang@tencent.com\n",
      "lemaoliu@gmail.com\n",
      "Abstract\n",
      "Recently, retrieval-augmented text generation\n",
      "attracted increasing attention of the compu-\n",
      "tational linguistics community. Compared\n",
      "with conventional generation models, retrieval-\n",
      "augmented text generation has remarkable ad-\n",
      "vantages and particularly has achieved state-of-\n",
      "the-art performance in many NLP tasks. This\n",
      "paper aims to conduct a survey about retrieval-\n",
      "augmented text generation. It ﬁrstly highlights\n",
      "the generic paradigm of retrieval-augmented\n",
      "generation, and then it reviews notable ap-\n",
      "proaches according to different tasks including\n",
      "dialogue response generation, machine trans-\n",
      "lation, and other generation tasks. Finally, it\n",
      "points out some promising directions on top of\n",
      "recent methods to facilitate future research.\n",
      "1 Introduction\n",
      "Retrieval-augmented text generation, as a new\n",
      "text generation paradigm that fuses emerging deep\n",
      "learning technology and traditional retrieval tech-\n",
      "nology, has achieved state-of-the-art (SOTA) per-\n",
      "formance in many NLP tasks and attracted the at-\n",
      "tention of the computational linguistics community\n",
      "(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n",
      "2021). Compared with generation-based counter-\n",
      "part, this new paradigm has some remarkable ad-\n",
      "vantages: 1) The knowledge is not necessary to be\n",
      "implicitly stored in model parameters, but is explic-\n",
      "itly acquired in a plug-and-play manner, leading\n",
      "to great scalibility; 2) Instead of generating from\n",
      "scratch, the paradigm generating text from some re-\n",
      "trieved human-written reference, which potentially\n",
      "alleviates the difﬁculty of text generation.\n",
      "This paper aims to review many representative\n",
      "approaches for retrieval-augmented text generation\n",
      "tasks including dialogue response generation (We-\n",
      "ston et al., 2018), machine translation (Gu et al.,\n",
      "2018) and others (Hashimoto et al., 2018). We\n",
      "\u0003All authors contributed equally.ﬁrstly present the generic paradigm of retrieval-\n",
      "augmented generation as well as three key com-\n",
      "ponents under this paradigm, which are retrieval\n",
      "sources, retrieval metrics and generation models.\n",
      "Then, we introduce notable methods about\n",
      "retrieval-augmented generation, which are orga-\n",
      "nized with respect to different tasks. Speciﬁcally,\n",
      "on the dialogue response generation task, exem-\n",
      "plar/template retrieval as an intermediate step has\n",
      "been shown beneﬁcial to informative response gen-\n",
      "eration (Weston et al., 2018; Wu et al., 2019; Cai\n",
      "et al., 2019a,b). In addition, there has been growing\n",
      "interest in knowledge-grounded generation explor-\n",
      "ing different forms of knowledge such as knowl-\n",
      "edge bases and external documents (Dinan et al.,\n",
      "2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n",
      "2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n",
      "2021). On the machine translation task, we summa-\n",
      "rize the early work on how the retrieved sentences\n",
      "(called translation memory) are used to improve\n",
      "statistical machine translation (SMT) (Koehn et al.,\n",
      "2003) models (Simard and Isabelle, 2009; Koehn\n",
      "and Senellart, 2010) and in particular, we inten-\n",
      "sively highlight several popular methods to inte-\n",
      "grating translation memory to NMT models (Gu\n",
      "et al., 2018; Zhang et al., 2018; Xu et al., 2020;\n",
      "He et al., 2021). We also review the applications\n",
      "of retrieval-augmented generation in other genera-\n",
      "tion tasks such as abstractive summarization (Peng\n",
      "et al., 2019), code generation (Hashimoto et al.,\n",
      "2018), paraphrase (Kazemnejad et al., 2020; Su\n",
      "et al., 2021b), and knowledge-intensive generation\n",
      "(Lewis et al., 2020b). Finally, we also point out\n",
      "some promising directions on retrieval-augmented\n",
      "generation to push forward the future research.\n",
      "2 Retrieval-Augmented Paradigm\n",
      "In this section, we ﬁrst give a general formulation\n",
      "of retrieval-augmented text generation. Then, we\n",
      "discuss three major components of the retrieval-\n",
      "augmented generation paradigm, including the re-arXiv:2202.01110v2  [cs.CL]  13 Feb 2022Input\n",
      "Sources (Sec. 2.2):Training CorpusExternal DataUnsupervised DataMetrics(Sec. 2.3):Sparse-vector RetrievalDense-vector RetrievalTask-specific RetrievalRetrieval MemoryGeneration ModelSec. 4: Machine TranslationSec. 5: Other TasksData AugmentationAttention MechanismSkeleton & TemplatesInformation RetrievalTasks:Sec. 3: Dialogue GenerationModels (Sec 2.4):OutputFigure 1: The overview of this survey.\n",
      "trieval source, retrieval metric and integration meth-\n",
      "ods.\n",
      "2.1 Formulation\n",
      "Most text generation tasks can be formulated as a\n",
      "mapping from input sequence xto output sequence\n",
      "y:y=f(x). For instance, xandycould be the\n",
      "dialogue history and the corresponding response\n",
      "for dialogue response generation, the text in the\n",
      "source language and the translation in the target\n",
      "language for machine translation, and so on.\n",
      "Recently, some researchers suggest to endow\n",
      "models the capability to access external memory\n",
      "via some information retrieval techniques, so that\n",
      "they can acquire more information in the generation\n",
      "process (Gu et al., 2018; Weston et al., 2018; Cai\n",
      "et al., 2019b). The retrieval-augmented generation\n",
      "can be further formulated as:\n",
      "y=f(x;z) (1)\n",
      "where z=fhxr;yrigis a set of relevant instances\n",
      "retrieved from the original training set or external\n",
      "datasets. The main idea of this paradigm is that yr\n",
      "may beneﬁt the response generation, if xr(oryr)\n",
      "is similar (or relevant) to the input x. It is worth\n",
      "noting that xr=;when unsupervised retrieval\n",
      "sources are used. In general, the retrieval mem-\n",
      "ory can be retrieved from three kinds of sources:\n",
      "the training corpus, external datasets in the same\n",
      "format with the training corpus, and large-scale\n",
      "unsupervised corpus (§2.2). Metrics that evaluate\n",
      "the relevance between text are varied as well, in\n",
      "§2.3 we divided them into three categories: sparse-\n",
      "vector retrieval, dense-vector retrieval, and training-\n",
      "based retrieval. Finally, how to integrate the re-\n",
      "trieval memory to the generation model is also sig-\n",
      "niﬁcant, we also introduce some popular integra-\n",
      "tion approaches in §2.4.2.2 Retrieval Sources\n",
      "Training Corpus Most previous studies search\n",
      "the external memory from its training corpus (Song\n",
      "et al., 2016; Gu et al., 2018; Weston et al., 2018).\n",
      "In the inference time, retrieved examples with high\n",
      "relevant scores could be regarded as extra refer-\n",
      "ences and reduce model’s uncertainty in generation.\n",
      "The main motivation of those works is to to store\n",
      "knowledge not only in the model parameters but\n",
      "also in an explicit and accessible form, making the\n",
      "model be able to re-access it during inference.\n",
      "External Data Some researchers also propose to\n",
      "retrieval relevant samples from external datasets\n",
      "(Su et al., 2021c; Xiao et al., 2021). In these stud-\n",
      "ies, the retrieval pool is different with the training\n",
      "corpus, which can further provide additional infor-\n",
      "mation that are not contained in the training corpus.\n",
      "This is especially beneﬁcial for applications such\n",
      "as domain adaptation and knowledge update. For\n",
      "example, Khandelwal et al. (2020a); Zheng et al.\n",
      "(2021a) employ the in-domain dataset as the exter-\n",
      "nal memory to achieve fast domain adaptation for\n",
      "machine translation.\n",
      "Unsupervised Data One limitation for previous\n",
      "two sources is that the datasets have to be super-\n",
      "vised datasets consisting of aligned input-output\n",
      "pairs. For machine translation, Cai et al. (2021) pro-\n",
      "pose a cross-lingual retriever to directly retrieve tar-\n",
      "get sentence from unsupervised corpus (i.e., mono-\n",
      "lingual corpus in the target language). The main\n",
      "idea is aligning source-side sentences and the corre-\n",
      "sponding target-side translations in a dense vector\n",
      "space, i.e., aligning xandyrwhen xris absent.\n",
      "As a result, the retriever directly connects the dots\n",
      "between the source-side input and target-side trans-\n",
      "lations, enabling monolingual data in the targetlanguage to be used alone as memories.\n",
      "2.3 Retrieval Metrics\n",
      "Sparse-vector Retrieval Given an input se-\n",
      "quence xand a retrieval corpus, retrieval model\n",
      "aims to retrieve a set of relevant examples z=\n",
      "fhxr;yrigfrom the corpus. When a supervised\n",
      "corpus is used,fhxr;yrigis retrieved by measur-\n",
      "ing the similarity between xandxr. For simi-\n",
      "larity measurement, sparse-vector retrieval meth-\n",
      "ods such as TF-IDF and BM25 (Robertson and\n",
      "Zaragoza, 2009) are widely used. They match key-\n",
      "words efﬁciently with an inverted index.\n",
      "Dense-vector Retrieval However, these meth-\n",
      "ods prefer examples with similar surfaces, and may\n",
      "fail to retrieve examples that are only semantically\n",
      "relevant. To alleviate above problem, some stud-\n",
      "ies (Cao and Xiong, 2018) attempt to retrieve in\n",
      "dense-vector space instead of the lexical overlap.\n",
      "Recent work (Lee et al., 2019) makes use of pre-\n",
      "trained language models, which encodes the text to\n",
      "low-dimensional dense vectors via BERT-based en-\n",
      "coders. The retrieval score are computed via inner\n",
      "products between vectors.\n",
      "Task-speciﬁc Retrieval Similarity-based re-\n",
      "trieval is based on a simple heuristic. That is, the\n",
      "more xrresembles with x, the more likely xr\n",
      "andyrwill help the generation. However, the\n",
      "most similar one by universal textual similarity\n",
      "does not necessarily serve the best for downstream\n",
      "models. Ideally, the retrieval metric would be\n",
      "learned from the data in a task-dependent way: we\n",
      "wish to consider a memory only if it can indeed\n",
      "boost the quality of ﬁnal generation. To this end,\n",
      "Cai et al. (2021) propose to unify the memory\n",
      "retriever and its downstream generation model\n",
      "into a learnable whole. Such memory retrieval is\n",
      "end-to-end optimized for task-speciﬁc objectives.\n",
      "2.4 Integration\n",
      "Data Augmentation There are several ways to\n",
      "integrate the retrieved external memory in gener-\n",
      "ation. One straightforward way is data augmen-\n",
      "tation , which constructs some augmented inputs\n",
      "by concatenating spans from fhxr;yrigwith the\n",
      "original input x. By training on the augmented\n",
      "inputs, a generation model implicitly leans how\n",
      "to integrate the retrieved information. Despite the\n",
      "simplicity, this kind of methods works efﬁciently\n",
      "in lots of tasks (Song et al., 2016; Weston et al.,\n",
      "2018; Bulte and Tezcan, 2019).Attention Mechanisms Another integration\n",
      "method is based on attention mechanisms\n",
      "(Bahdanau et al., 2014). The main idea of this\n",
      "fashion is adopting additional encoders (in various\n",
      "architectures) to encode retrieved target sentences,\n",
      "and integrate them through attention (Cao and\n",
      "Xiong, 2018; Gu et al., 2018; Bapna and Firat,\n",
      "2019). Since the attention mechanism is becoming\n",
      "(Bahdanau et al., 2014; Vaswani et al., 2017) a\n",
      "key module in lots of NLP models, integrating\n",
      "retrieved memory through attention becomes a\n",
      "very nature and efﬁcient way.\n",
      "Skeleton Extraction In the previous two meth-\n",
      "ods, the downstream generation model learns how\n",
      "to ﬁlter out irrelevant or even harmful informa-\n",
      "tion from the retrieved examples implicitly. There\n",
      "also exist some works that try to explicitly extract\n",
      "useful information, i.e., skeleton extraction , from\n",
      "the retrieved memory (Cai et al., 2019a; Wu et al.,\n",
      "2019; Cai et al., 2019b). For example, one skeleton\n",
      "should be a part of a whole utterance with irrelevant\n",
      "content masked, and the generation model only in-\n",
      "tegrate this skeleton in the generation process.\n",
      "3 Dialogue Response Generation\n",
      "Background Dialogue systems can be grouped\n",
      "into two categories: chit-chat systems and task-\n",
      "oriented systems. While task-oriented dialogue\n",
      "systems are designed to accomplish speciﬁc user\n",
      "tasks such as air tickets booking, chit-chat dialogue\n",
      "systems aim at giving a meaningful and ﬂuent re-\n",
      "sponse for any dialogue history in the open domain.\n",
      "Dialogue response generation in chit-chat dialogue\n",
      "system is challenging partly due to the diversity\n",
      "of possible responses to a single dialogue history\n",
      "(i.e., the one-to-many problem). The dialogue his-\n",
      "tory alone cannot decide a meaningful and speciﬁc\n",
      "response. Also, external knowledge that is not\n",
      "present in the dialogue history are often necessary\n",
      "for avoiding safe but boring responses. We focus\n",
      "on recent efforts tackling the challenges to develop\n",
      "chit-chat dialogue systems.\n",
      "Most modern chit-chat dialogue systems can\n",
      "be categorized into two classes, namely, retrieval-\n",
      "based models and generation-based models. The\n",
      "retrieval-based models (Ji et al., 2014; Hu et al.,\n",
      "2014) directly copy an existing response from cu-\n",
      "rated dialogue corpora (i.e., the retrieval pool)\n",
      "when receiving a response request. The retrieved\n",
      "responses are often informative and grammatical\n",
      "as they are collected from real-world conversa-tions and possibly post-edited by a human. How-\n",
      "ever, such systems perform poorly when a given\n",
      "dialogue history is substantially different from\n",
      "those in the retrieval pool. On the other hand,\n",
      "the generation-based models (Shang et al., 2015;\n",
      "Vinyals and Le, 2015; Li et al., 2016a) generate\n",
      "a new utterance from scratch. Those generation-\n",
      "based models have better generalization capacity\n",
      "when handling unseen dialogue contexts. Never-\n",
      "theless, the generated utterances are inclined to be\n",
      "dull and non-informative (e.g., “I don’t know”, “I\n",
      "think so”, “Me too” etc.) (Li et al., 2016a).\n",
      "Shallow Integration As discussed, retrieval-\n",
      "based models may give informative but inappro-\n",
      "priate responses while generation-based models\n",
      "often do the opposite. It is desirable to combine the\n",
      "best of both worlds. Early work (Qiu et al., 2017)\n",
      "attempts to re-rank the output from both models.\n",
      "For a deep integration, Song et al. (2016) and Yang\n",
      "et al. (2019) extend the standard SEQ2SEQencoder-\n",
      "decoder model (Bahdanau et al., 2014) with an ex-\n",
      "tra encoder for encoding the retrieval result. The\n",
      "output of the extra encoder, along with the output\n",
      "from the original encoder for dialogue history, is\n",
      "used to feed the decoder. Weston et al. (2018) use\n",
      "a single encoder that takes the concatenation of\n",
      "the original dialogue history and the retrieved as\n",
      "input. Wu et al. (2019) note that the retrieved infor-\n",
      "mation should be used in awareness of the context\n",
      "difference, and further proposed to construct an\n",
      "edit vector by explicitly encoding the lexical differ-\n",
      "ences between the input dialogue history and the\n",
      "retrieved dialogue history. Pandey et al. (2018) fur-\n",
      "ther propose to weight different training instances\n",
      "by context similarity.\n",
      "Deep Integration To prevent the inﬂow of er-\n",
      "roneous information, Cai et al. (2019a) propose\n",
      "a general framework that ﬁrst extracts a skeleton\n",
      "from the retrieved response and then generates the\n",
      "response based on the extracted skeleton. This\n",
      "framework is also adopted for stylistic response\n",
      "generation (Su et al., 2021c). Gupta et al. (2021)\n",
      "suggest to use the semantic structure of an exem-\n",
      "plar response, instead of the tokens of the exem-\n",
      "plar response, to guide generation. Despite their\n",
      "differences, a common issue is that the genera-\n",
      "tion model easily learns to ignore the retrieved re-\n",
      "sponse entirely and collapses to a vanilla seq2seq\n",
      "model. This happens with improper training in-\n",
      "stances. Due to the one-to-many nature, it hap-pens frequently that a retrieved response (extracted\n",
      "skeleton) is suitable for responding to the query,\n",
      "but inconsistent with the current target response.\n",
      "Earlier studies (Weston et al., 2018; Wu et al.,\n",
      "2019; Cai et al., 2019a) alleviate the above prob-\n",
      "lems by putting hard constraints on the data (e.g.,\n",
      "discarding data with low similarity of the retrieved\n",
      "response and the target response), which, however,\n",
      "greatly reduces the amount of usable data. Cai\n",
      "et al. (2019b) employ a random mechanism for\n",
      "generating the skeletons used for training, which\n",
      "extract skeletons from the corresponding responses\n",
      "with some deliberate disturbance. Paranjape et al.\n",
      "(2021) propose to model the retriever after the pos-\n",
      "terior distribution of retrieval given the input and\n",
      "the target output and train it jointly with the stan-\n",
      "dard retriever and the generator by maximizing the\n",
      "evidence lower bound (ELBo) in expectation over\n",
      "retrieval.\n",
      "Knowledge-Enhanced Generation The afore-\n",
      "mentioned work demonstrates that retrieval-based\n",
      "dialogue systems can be used for building bet-\n",
      "ter generation-based models. In general, this is\n",
      "done by conditioning the generation on some re-\n",
      "trieved responses. More traditionally, to infuse\n",
      "the response with external knowledge, the retrieval\n",
      "pool is not necessarily a dialogue corpus. In fact,\n",
      "knowledge-grounded dialogue response generation\n",
      "exploring different forms of knowledge such as\n",
      "knowledge bases and external documents (Dinan\n",
      "et al., 2018; Zhou et al., 2018; Lian et al., 2019;\n",
      "Li et al., 2019; Qin et al., 2019; Wu et al., 2021;\n",
      "Zhang et al., 2021; Komeili et al., 2021) has been\n",
      "actively explored.\n",
      "Limitations We note that there are three major\n",
      "limitations in existing work for dialogue response\n",
      "generation. First, current methods only use one\n",
      "retrieved response for generation. It can be more\n",
      "beneﬁcial to combine multiple retrieval responses.\n",
      "However, this can be difﬁcult due to the one-to-\n",
      "many nature of dialogue response generation. Sec-\n",
      "ond, current methods use universal relevance score\n",
      "for retrieval. It can be more effective if we can\n",
      "use more customized retrieval metric especially\n",
      "for controlled dialogue response generation (e.g.,\n",
      "persona, emotion, etc). Third, the retrieval pool\n",
      "of existing methods is limited to dialogue corpora\n",
      "(context-response pairs) or documents. It might\n",
      "be useful to enlarge the retrieval pool by including\n",
      "more corpora in other domains or in other modali-ties. As discussed, there leaves plenty of possible\n",
      "directions to explore in the future.\n",
      "4 Machine Translation\n",
      "Retrieval augmented translation originates from hu-\n",
      "man translation scenarios (Somers, 2003). When\n",
      "translating ^yfrom an input source sentence x, a hu-\n",
      "man translator typically involves a search engine to\n",
      "retrieve similar sentences fhxr;yrigfrom a bilin-\n",
      "gual database. Such a technique called translation\n",
      "memory is helpful to improve the translation qual-\n",
      "ity and efﬁciency for human translators (Dillon\n",
      "and Fraser, 2006). As the development of ma-\n",
      "chine translation techniques, there is a surge of\n",
      "interests in improving machine translation models\n",
      "with translation memory. In the rest of this section,\n",
      "we will review translation memory for both statisti-\n",
      "cal machine translation (SMT) and neural machine\n",
      "translation (NMT).\n",
      "4.1 Translation Memory in SMT\n",
      "Generally, SMT includes three key components in\n",
      "a pipeline manner such as phrase table extraction,\n",
      "parameter tuning and decoding (Koehn et al., 2003;\n",
      "Chiang, 2007). As a result, many efforts have been\n",
      "made to make use of translation memory (TM) on\n",
      "top of each component.\n",
      "Constrained Decoding with TM Constrained\n",
      "decoding is the most straightforward way to in-\n",
      "tegrating TM into SMT (Smith and Clark, 2009;\n",
      "Koehn and Senellart, 2010; Zhechev and Van Gen-\n",
      "abith, 2010; Ma et al., 2011). Its basic idea is\n",
      "to reuse the useful segments in yrwhile trans-\n",
      "late other segments by SMT. Speciﬁcally, the ap-\n",
      "proach consists of three steps: 1) identify the un-\n",
      "matched segments in both xrandxthrough the\n",
      "edit-distance algorithm; 2) identify the unmatched\n",
      "segments in yr, each of which is aligned to one\n",
      "unmatched segment in xrby a word alignment\n",
      "algorithm; 3) decode each unmatched segment in\n",
      "xby SMT and then use the result to replace its\n",
      "corresponding unmatched segment in yr. Li et al.\n",
      "(2016b) further extend this approach from sentence\n",
      "level to phrase level. The advantage in constrained\n",
      "decoding is that it does not require to change the\n",
      "translation model (including phrase table and pa-\n",
      "rameters) and can be applied in a plug-and-play\n",
      "way. This approach is successful when xis highly\n",
      "similar to xr; otherwise its performance is de-\n",
      "graded largely, because it explicitly isolates TMmatching and SMT decoding and reuses the results\n",
      "inxror not in a deterministic way.\n",
      "Phrase Table Aggregation with TM There are\n",
      "also notable efforts to augment the phrase table\n",
      "for SMT by extracting translation rules from the\n",
      "retrieved bilingual sentences fhxr;yrig. Then\n",
      "they re-tune the parameters for the SMT model\n",
      "which makes use of translation knowledge from\n",
      "fhxr;yrigin a implicit way when translating x.\n",
      "For example, Biçici and Dymetman (2008); Simard\n",
      "and Isabelle (2009) directly combine the extracted\n",
      "translation rules into the phrase table in a shallow\n",
      "combination way. They introduce an additional fea-\n",
      "ture to indicate that whether translation rule is from\n",
      "fhxr;yrigor not and then train all feature weights\n",
      "with MERT (Och, 2003). One characteristic of\n",
      "these work is that a translation rule extracted from\n",
      "fhxr;yrigwhich can not exactly match any seg-\n",
      "ments in xis useless even if it may contain some\n",
      "useful words in its target side. To remedy this ob-\n",
      "servation, Wang et al. (2013, 2014) resort to a deep\n",
      "combination way to using the extracted translation\n",
      "rules. For each rule in the phrase table, it designs\n",
      "a generative model to reward the rules which are\n",
      "similar to those extracted from fhxr;yrig. Then\n",
      "this generative model is used as a feature in the log-\n",
      "linear based SMT model whose weight is tuned\n",
      "together with other features by MERT. In addition,\n",
      "Li et al. (2014) employ a similar way to reward\n",
      "the rules but it relies on a discriminative model\n",
      "which is easy to integrate potential features from\n",
      "fhxr;yrig.\n",
      "Parameter Tuning with TM Unlike the above\n",
      "two research lines, Liu et al. (2012, 2014) make use\n",
      "of translation memory only in tuning parameters.\n",
      "To be speciﬁc, when translating an input sentence\n",
      "x, they ﬁrstly retrieve many similar bilingual sen-\n",
      "tencesfhxr;yrig, and then tune the parameters on\n",
      "top of the retrieved sentences as well as a given de-\n",
      "velopment dataset in a sentence-wise manner, i.e.,\n",
      "it performs an independent tuning for each input\n",
      "sentence. To improve the efﬁciency of each tuning\n",
      "step, it propose a local update on top of fhxr;yrig\n",
      "from a baseline model.\n",
      "Despite the successes of translation memory in\n",
      "SMT, there are still some limitations for the above\n",
      "three kinds of methods. Firstly, all these methods\n",
      "employ fuzzy score for retrieval which is highly de-\n",
      "pendent on word matching and thus can not recall\n",
      "such examples which are similar in word seman-tics but different in surface form. Secondly, these\n",
      "methods integrate the retrieved examples into a\n",
      "module of SMT in the ways which can not make\n",
      "full use of the knowledge in retrieved examples.\n",
      "For example, the integration ways in the ﬁrst two\n",
      "kinds (constrained decoding and phrase table ag-\n",
      "gregation) are heuristic and not optimized towards\n",
      "translation quality; the parameter tuning method\n",
      "ﬁne-tunes few parameters for log-linear based SMT\n",
      "which are not enough to preserve sufﬁcient knowl-\n",
      "edge from retrieved examples. Thirdly, since SMT\n",
      "performs in a pipeline manner, it is intractable to\n",
      "jointly optimize retrieval metrics as well as SMT\n",
      "models. Consequently, all these methods adopt an\n",
      "off-the-shelf metric for retrieval, leading to sub-\n",
      "optimal performance.\n",
      "4.2 Translation Memory in NMT\n",
      "Translation memory has been widely explored in\n",
      "Neural Machine Translation (NMT). Depending\n",
      "on when retrieval is involved, we can categorize\n",
      "previous works into two classes: 1) an NMT model\n",
      "leans how to cooperate with the retrieval model in\n",
      "the training phase; 2) an NMT model is only aware\n",
      "of the retrieved data in the inference phase.\n",
      "Inference Phase The key point of literature in\n",
      "this line is to reward some target words based on\n",
      "words in yrin the inference process. Thus, a de-\n",
      "cision can be made based on both the distribution\n",
      "of generation model and the additional reward of\n",
      "retrieval model. Some previous works propose to\n",
      "reward target words based on the sentence-level\n",
      "similarity between xandxr, and the word align-\n",
      "ment between xrandyr. Given the input sentence\n",
      "x, Zhang et al. (2018) try to assign target words\n",
      "in^ywith higher rewards, when they appear in yr\n",
      "and the aligned source words are in both xrand\n",
      "x. He et al. (2019) follow a similar framework\n",
      "and consider the position information of those tar-\n",
      "get words when rewarding. Those works reward\n",
      "the target words in an explicit way, however, the\n",
      "one-sentence-one-model approach (Li et al., 2016c;\n",
      "Turchi et al., 2017) propose to reward target word\n",
      "implicitly. For each testing input x, their approach\n",
      "will ﬁrst ﬁnetune the translation model on retrieved\n",
      "memoryfhxr;yrigand then translate x.\n",
      "Others try to reward target words based on token-\n",
      "level similarity score. Most works in this line are\n",
      "based on the dense retriever (Khandelwal et al.,\n",
      "2020a), e.g., faiss. Khandelwal et al. (2020a) build\n",
      "a key-value datastore, where key h(xr;yr\n",
      "<t)is thehidden state at each time step when translating yr\n",
      "fromxr, and value is its golden-truth target word\n",
      "yr\n",
      "t. Therefore, in the inference time, they can use\n",
      "theh(x;^y<t)as query and reward target words\n",
      "with similar hidden representations in the datas-\n",
      "tore. Although this method achieves signiﬁcant\n",
      "performance gain, one drawback of it is the high la-\n",
      "tency. To address this issue, Meng et al. (2021) use\n",
      "some heuristics, e.g., pre-ﬁltering, to avoid search-\n",
      "ing on the entire datastore. The reward score of\n",
      "previous works is got from some non-parametric\n",
      "approaches, however, Zheng et al. (2021a) propose\n",
      "a light-weight network to learn the reward score.\n",
      "Since dense retrieval has the potential of cross-\n",
      "lingual retrieval, Zheng et al. (2021b) use a similar\n",
      "approach to achieve unsupervised domain adapta-\n",
      "tion, where a main change is to create the datastore\n",
      "based on synthetic sources sentence and the real\n",
      "target sentences.\n",
      "Training Phase Different from those model-\n",
      "agnostic approaches, previous works in this line\n",
      "aim to train the generation model to learn how\n",
      "to cooperate with the retrieval model. It is also\n",
      "worth noting that most works in this line adopt\n",
      "the sentence-level retrieval, when integrating the\n",
      "retrieval information in the training process. To\n",
      "achieve its goal, Bulte and Tezcan (2019) and\n",
      "Hossain et al. (2020) propose a data augmenta-\n",
      "tion method to integrate the retrieved information,\n",
      "where xis concatenated with yrbefore feeding\n",
      "into the model . Following the data augmentation\n",
      "approach, Xu et al. (2020) propose more matching\n",
      "methods to determine including which retrieved\n",
      "example in the source is better.\n",
      "There also exist some works that propose new\n",
      "architectures to integrate the retrieval information.\n",
      "Under the RNN-based framework, Cao and Xiong\n",
      "(2018) and Gu et al. (2018) use the gating and at-\n",
      "tention mechanism to incorporate the retrieved tar-\n",
      "get sentences. When Transformer (Vaswani et al.,\n",
      "2017) becomes the backbone of NMT, some works\n",
      "also use additional transformer encoders to en-\n",
      "code retrieved target sentences, and integrate them\n",
      "through attention mechanism (Bapna and Firat,\n",
      "2019; Cao et al., 2019). Xia et al. (2019) repre-\n",
      "sent the retrieved target sentences in a different\n",
      "data structure, i.e., a graph structure, and integrate\n",
      "it through attention mechanism. He et al. (2021)\n",
      "propose a light-weight method to encode the re-\n",
      "trieved target sentences and leverage the alignment\n",
      "information to ﬁlter out irrelevant information. Dif-ferent from previous works that rely on bilingual\n",
      "memories, Cai et al. (2021) propose a framework\n",
      "that can retrieve the most similar target sentence in\n",
      "a monolingual dataset, using a source sentence as\n",
      "query.\n",
      "Limitations In the section of SMT, we have\n",
      "showed some limitations of the retrieval augmented\n",
      "approaches. There also exist some limitations in\n",
      "the line of NMT. First, the information used for\n",
      "deriving reward scores is limited. The similarity\n",
      "between an input and retrieved examples is the\n",
      "primary feature to derive reward scores. How-\n",
      "ever, some information, e.g., frequencies of words\n",
      "and context, may also be beneﬁcial for integrating\n",
      "the translation memory. Second, it remains to be\n",
      "an open question that when should we use the re-\n",
      "trieved information and when not. In the inference\n",
      "phase, approaches tend to integrate the translation\n",
      "memory excessively, e.g., at each time step, which\n",
      "not only reduces the translation efﬁciency but may\n",
      "also dampen the ﬂuency of generated results.\n",
      "5 Other Tasks\n",
      "In addition to dialogue system and machine trans-\n",
      "lation, retrieval-augmented generation techniques\n",
      "have shown to be beneﬁcial in many other tasks. In\n",
      "the following, we highlight several key tasks that\n",
      "apply retrieval-augmented generation approaches.1\n",
      "Language Modelling It has been shown that\n",
      "properly leveraging information from retrieval\n",
      "memory could improve the performance of large\n",
      "pre-trained language model. To build a more accu-\n",
      "rate language model, Khandelwal et al. (2020b) pro-\n",
      "pose to incorporate a soft memory module into the\n",
      "system. Speciﬁcally, an index is built by caching\n",
      "the hidden states of the training corpus. Then, the\n",
      "language model accesses the index via k-NN search\n",
      "and displays a greatly improved performance. As\n",
      "another example, Guu et al. (2020) propose a new\n",
      "paradigm that applies retrieval-augmented tech-\n",
      "nique into the pre-training of generative language\n",
      "model. During learning, they train a neural se-\n",
      "lector that dynamically samples a relevant text to\n",
      "guide the reconstruction of a corrupted input se-\n",
      "quence. In this way, the pre-trained model deliv-\n",
      "ers better results by explicitly grounding on the\n",
      "retrieval memory. Lewis et al. (2020a) combine\n",
      "language model pre-training with a paraphrasing\n",
      "1Here, we focus on tasks other than question answering.\n",
      "We refer readers interested in QA to Chen and Yih (2020).approach. During learning, an input sequence to\n",
      "the model is ﬁrst corrupted. In the meantime, a set\n",
      "of multi-lingual texts are retrieved based on which\n",
      "the model learns to reconstruct the original input\n",
      "sequence. Recently, Borgeaud et al. (2021) pro-\n",
      "pose RETRO , a large pre-trained language model\n",
      "enhanced with retrieved documents, and obtained\n",
      "comparable performances with GPT-3 using 25 \u0002\n",
      "fewer parameters.\n",
      "Summarization Text summarization is another\n",
      "research area that beneﬁts from retrieval-\n",
      "augmented text generation. Peng et al. (2019)\n",
      "propose an adaptive decoding framework which\n",
      "ﬁrst retrieves an exemplar document given the\n",
      "source document. Then, the summarization of the\n",
      "source document is derived through an adaptive\n",
      "generation process based on the retrieved template.\n",
      "Different from Peng et al. (2019), Cao et al.\n",
      "(2018) and Hossain et al. (2020) introduce an\n",
      "intermediate re-ranking stage into the generation\n",
      "pipeline. Speciﬁcally, before generating the\n",
      "document summary, the retrieval documents are\n",
      "ﬁrst re-ranked based on their similarity scores\n",
      "with respect to the source document. Then, the\n",
      "document summarization is produced by re-writing\n",
      "the selected templates.\n",
      "Paraphrase Generation To address the lack of\n",
      "quality as well as diversity in the generation of para-\n",
      "phrases, Kazemnejad et al. (2020) propose a gen-\n",
      "eration framework which ﬁrst retrieves a sentence\n",
      "that is similar to input sentence. Then, based on\n",
      "the retrieved sentence, a neural editor produces the\n",
      "resulting paraphrased sentence. Chen et al. (2019)\n",
      "investigate a different aspect of paraphrasing, i.e.\n",
      "how to control the linguistic syntax displayed in\n",
      "the generated text. To achieve this goal, Chen et al.\n",
      "(2019) propose to ﬁrst extract a sentential exem-\n",
      "plar that serves as the syntax template. A neural\n",
      "model then generates the paraphrase with desired\n",
      "linguistic syntax following the retrieved exemplar.\n",
      "Text Style Transfer To improve the quality of\n",
      "generated text, Li et al. (2018) propose a retrieval-\n",
      "augmented framework which ﬁrst retrieves texts\n",
      "that are similar to the input based on lexical-level\n",
      "similarity. Then, the retrieved tokens that are irrel-\n",
      "evant to the source are deleted, and the output is\n",
      "derived from the edited template. Xiao et al. (2021)\n",
      "also adopte this framework by incorporating re-\n",
      "trieval information from two sources (i.e. sparse\n",
      "and dense memories) and obtained an improvedmodel performance.\n",
      "Data-to-Text Generation Recently, retrieval-\n",
      "augmented generation has been adapted to the task\n",
      "of data-to-text generation. To bridge the gap be-\n",
      "tween the structured data and natural language\n",
      "text, Su et al. (2021a) propose a novel retrieval-\n",
      "augmented framework. Speciﬁcally, given the\n",
      "source data, a set of candidate texts are ﬁrst re-\n",
      "trieved from a large unlabelled corpus. Then, a\n",
      "neural selector is applied to measure the similari-\n",
      "ties between the source data and candidate texts,\n",
      "and extract a set of more ﬁne-grained prototypes\n",
      "from the candidates. Lastly, a generation model\n",
      "takes the prototypes as input to produce the text\n",
      "that describes the given structured data.\n",
      "While retrieval-augmented generation has been\n",
      "widely explored in the NLP community, we sug-\n",
      "gest that future research could extend this approach\n",
      "to tasks that involve data from multiple modali-\n",
      "ties. For instance, with recent advancements in\n",
      "image-text retrieval (Jia et al., 2021; Radford et al.,\n",
      "2021), the structural gap between images and texts\n",
      "is largely bridged. Some early studies (Zhang et al.,\n",
      "2020) have shown that information retrieved from\n",
      "images could improve the performance of neural\n",
      "machine translation model. Naturally, such meth-\n",
      "ods could be extended to other multi-modal tasks,\n",
      "such as image captioning (Karpathy and Li, 2015).\n",
      "A similar idea could also be applied to tasks be-\n",
      "yond images, such as speech-to-text transcription\n",
      "(Gales and Young, 2007).\n",
      "6 Future Directions\n",
      "Despite the current success of retrieval augmented\n",
      "text generation, there is still a long way to go as\n",
      "discussed in previous sections. We highlight some\n",
      "directions to facilitate the future research as fol-\n",
      "lows:\n",
      "Retrieval Sensitivity The performance of re-\n",
      "trieval augmented text generation is very sensitive\n",
      "to the retrieval quality, i.e., the similarity between\n",
      "the query and the retrieved examples. Currently, re-\n",
      "trieval augmented text generation models perform\n",
      "well when the retrieved examples are very simi-\n",
      "lar to the query. However, they are even worse\n",
      "than the generation models without retrieval when\n",
      "the retrieval examples are less similar. Therefore,\n",
      "it would be important to exploit new methods to\n",
      "address such an issue on similarity.Retrieval Efﬁciency Generally, if one enlarges\n",
      "the retrieval memory to some extent, it would be\n",
      "possible to retrieve an example which is very simi-\n",
      "lar to the query.Unfortunately, the downside is that\n",
      "the overall inference for the retrieval augmented\n",
      "generation models is less efﬁcient due the consid-\n",
      "erable retrieval overhead. In this sense, it is urgent\n",
      "to consider some methods to trade off the retrieval\n",
      "memory size and retrieval efﬁciency, for example,\n",
      "data compression for the retrieval memory.\n",
      "Local vs. Global Optimization Theoretically, it\n",
      "seems promising to jointly learn retrieval metrics\n",
      "and generation models. However, in practice, there\n",
      "is an essential gap about the retrieval metric be-\n",
      "tween the training and inference phrases. In the\n",
      "training phase, the loss is locally back-propagated\n",
      "to only a few retrieved examples while in the infer-\n",
      "ence phase the metric is globally conducted among\n",
      "all examples in the memory. It would be interesting\n",
      "to narrow such a gap when learning a better metric\n",
      "for generation tasks.\n",
      "Multi-Modalities With recent advancement in\n",
      "image-text retrieval, directly associating images\n",
      "with relevant text becomes possible. This urges\n",
      "researchers to investigate the possibility of retrieval-\n",
      "based text generation in tasks that involve data from\n",
      "different modalities. One typical task is image\n",
      "captioning. Beyond images, other tasks like speech-\n",
      "to-text transcription could potentially beneﬁt from\n",
      "retrieval-based generation methods as well.\n",
      "Diverse & Controllable Retrieval Most of the\n",
      "existing approaches adopt a universal metric for\n",
      "retrieval, such as lexical similarities of sentences.\n",
      "Future work should explore how to use customized\n",
      "metrics for retrieval. This can be beneﬁcial for\n",
      "more controlled text generation. For example, in-\n",
      "stances with emotions and styles may be more de-\n",
      "sirable in the personalized dialogue generation, par-\n",
      "allel data that contains speciﬁc terminologies is\n",
      "more helpful in machine translation, and so on. On\n",
      "the other hand, using a universal metric for retrieval\n",
      "may lead to the lack of diversity of the retrieval re-\n",
      "sults. Collecting a diverse set of retrieval results\n",
      "can improve the coverage of useful information.\n",
      "Thus, considering multiple different metrics for re-\n",
      "trieval may lead to generation with higher quality\n",
      "in the future.7 Conclusion\n",
      "In this paper, we surveyed recent approaches for\n",
      "retrieval-augmented text generation. We reviewed\n",
      "and summarized the development of different com-\n",
      "ponents of retrieval-augmented text generation in-\n",
      "cluding retrieval metrics, retrieval sources, and in-\n",
      "tegration paradigms. We gave in-depth discussions\n",
      "when retrieval-augmented text generation comes to\n",
      "different applications including dialogue response\n",
      "generation, machine translation, and other genera-\n",
      "tion tasks. We also pointed out some future direc-\n",
      "tions for retrieval-augmented text generation.\n",
      "References\n",
      "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\n",
      "gio. 2014. Neural machine translation by jointly\n",
      "learning to align and translate. arXiv preprint\n",
      "arXiv:1409.0473 .\n",
      "Ankur Bapna and Orhan Firat. 2019. Non-parametric\n",
      "adaptation for neural machine translation. In Pro-\n",
      "ceedings of the 2019 Conference of the North Amer-\n",
      "ican Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies, Vol-\n",
      "ume 1 (Long and Short Papers) , pages 1921–1931.\n",
      "Ergun Biçici and Marc Dymetman. 2008. Dynamic\n",
      "translation memory: Using statistical machine trans-\n",
      "lation to improve translation memory fuzzy matches.\n",
      "InInternational Conference on Intelligent Text Pro-\n",
      "cessing and Computational Linguistics , pages 454–\n",
      "465. Springer.\n",
      "Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-\n",
      "mann, Trevor Cai, Eliza Rutherford, Katie Millican,\n",
      "George van den Driessche, Jean-Baptiste Lespiau,\n",
      "Bogdan Damoc, Aidan Clark, Diego de Las Casas,\n",
      "Aurelia Guy, Jacob Menick, Roman Ring, Tom Hen-\n",
      "nigan, Saffron Huang, Loren Maggiore, Chris Jones,\n",
      "Albin Cassirer, Andy Brock, Michela Paganini, Ge-\n",
      "offrey Irving, Oriol Vinyals, Simon Osindero, Karen\n",
      "Simonyan, Jack W. Rae, Erich Elsen, and Laurent\n",
      "Sifre. 2021. Improving language models by retriev-\n",
      "ing from trillions of tokens. CoRR , abs/2112.04426.\n",
      "Bram Bulte and Arda Tezcan. 2019. Neural fuzzy re-\n",
      "pair: Integrating fuzzy matches into neural machine\n",
      "translation. In Proceedings of the 57th Annual Meet-\n",
      "ing of the Association for Computational Linguistics ,\n",
      "pages 1800–1809.\n",
      "Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xi-\n",
      "aojiang Liu, Wai Lam, and Shuming Shi. 2019a.\n",
      "Skeleton-to-response: Dialogue generation guided\n",
      "by retrieval memory. In Proceedings of the 2019\n",
      "Conference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Human\n",
      "Language Technologies, Volume 1 (Long and Short\n",
      "Papers) , pages 1219–1228.Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiao-\n",
      "jiang Liu, and Shuming Shi. 2019b. Retrieval-\n",
      "guided dialogue response generation via a matching-\n",
      "to-generation framework. In Proceedings of the\n",
      "2019 Conference on Empirical Methods in Natu-\n",
      "ral Language Processing and the 9th International\n",
      "Joint Conference on Natural Language Processing\n",
      "(EMNLP-IJCNLP) , pages 1866–1875.\n",
      "Deng Cai, Yan Wang, Huayang Li, Wai Lam, and\n",
      "Lemao Liu. 2021. Neural machine translation with\n",
      "monolingual translation memory. In Proceedings of\n",
      "the 59th Annual Meeting of the Association for Com-\n",
      "putational Linguistics and the 11th International\n",
      "Joint Conference on Natural Language Processing\n",
      "(Volume 1: Long Papers) , pages 7307–7318, Online.\n",
      "Association for Computational Linguistics.\n",
      "Qian Cao, Shaohui Kuang, and Deyi Xiong. 2019.\n",
      "Learning to reuse translations: Guiding neural ma-\n",
      "chine translation with examples. arXiv preprint\n",
      "arXiv:1911.10732 .\n",
      "Qian Cao and Deyi Xiong. 2018. Encoding gated\n",
      "translation memory into neural machine translation.\n",
      "InProceedings of the 2018 Conference on Empiri-\n",
      "cal Methods in Natural Language Processing , pages\n",
      "3042–3047.\n",
      "Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei.\n",
      "2018. Retrieve, rerank and rewrite: Soft template\n",
      "based neural summarization. In Proceedings of the\n",
      "56th Annual Meeting of the Association for Com-\n",
      "putational Linguistics, ACL 2018, Melbourne, Aus-\n",
      "tralia, July 15-20, 2018, Volume 1: Long Papers ,\n",
      "pages 152–161. Association for Computational Lin-\n",
      "guistics.\n",
      "Danqi Chen and Wen-tau Yih. 2020. Open-domain\n",
      "question answering. In Proceedings of the 58th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics: Tutorial Abstracts , pages 34–37, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Mingda Chen, Qingming Tang, Sam Wiseman, and\n",
      "Kevin Gimpel. 2019. Controllable paraphrase gen-\n",
      "eration with a syntactic exemplar. In Proceedings of\n",
      "the 57th Conference of the Association for Compu-\n",
      "tational Linguistics, ACL 2019, Florence, Italy, July\n",
      "28- August 2, 2019, Volume 1: Long Papers , pages\n",
      "5972–5984. Association for Computational Linguis-\n",
      "tics.\n",
      "David Chiang. 2007. Hierarchical phrase-based trans-\n",
      "lation. computational linguistics , 33(2):201–228.\n",
      "Sarah Dillon and Janet Fraser. 2006. Translators and\n",
      "tm: An investigation of translators’ perceptions of\n",
      "translation memory adoption. Machine Translation ,\n",
      "20(2):67–79.\n",
      "Emily Dinan, Stephen Roller, Kurt Shuster, Angela\n",
      "Fan, Michael Auli, and Jason Weston. 2018. Wizard\n",
      "of wikipedia: Knowledge-powered conversational\n",
      "agents. arXiv preprint arXiv:1811.01241 .Mark J. F. Gales and Steve J. Young. 2007. The applica-\n",
      "tion of hidden markov models in speech recognition.\n",
      "Found. Trends Signal Process. , 1(3):195–304.\n",
      "Jiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-\n",
      "tor OK Li. 2018. Search engine guided neural ma-\n",
      "chine translation. In Proceedings of the AAAI Con-\n",
      "ference on Artiﬁcial Intelligence , volume 32.\n",
      "Prakhar Gupta, Jeffrey Bigham, Yulia Tsvetkov, and\n",
      "Amy Pavel. 2021. Controlling dialogue generation\n",
      "with semantic exemplars. In Proceedings of the\n",
      "2021 Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics: Hu-\n",
      "man Language Technologies , pages 3018–3029, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\n",
      "pat, and Ming-Wei Chang. 2020. REALM: retrieval-\n",
      "augmented language model pre-training. CoRR ,\n",
      "abs/2002.08909.\n",
      "Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren,\n",
      "and Percy S Liang. 2018. A retrieve-and-edit frame-\n",
      "work for predicting structured outputs. In Advances\n",
      "in Neural Information Processing Systems , pages\n",
      "10052–10062.\n",
      "Qiuxiang He, Guoping Huang, Qu Cui, Li Li, and\n",
      "Lemao Liu. 2021. Fast and accurate neural machine\n",
      "translation with translation memory. In Proceed-\n",
      "ings of the 59th Annual Meeting of the Association\n",
      "for Computational Linguistics and the 11th Interna-\n",
      "tional Joint Conference on Natural Language Pro-\n",
      "cessing (Volume 1: Long Papers) , pages 3170–3180.\n",
      "Qiuxiang He, Guoping Huang, Lemao Liu, and Li Li.\n",
      "2019. Word position aware translation memory for\n",
      "neural machine translation. In CCF International\n",
      "Conference on Natural Language Processing and\n",
      "Chinese Computing , pages 367–379. Springer.\n",
      "Nabil Hossain, Marjan Ghazvininejad, and Luke Zettle-\n",
      "moyer. 2020. Simple and effective retrieve-edit-\n",
      "rerank text generation. In Proceedings of the 58th\n",
      "Annual Meeting of the Association for Computa-\n",
      "tional Linguistics , pages 2532–2538.\n",
      "Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai\n",
      "Chen. 2014. Convolutional neural network architec-\n",
      "tures for matching natural language sentences. In\n",
      "NIPS , pages 2042–2050.\n",
      "Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An\n",
      "information retrieval approach to short text conver-\n",
      "sation. arXiv preprint arXiv:1408.6988 .\n",
      "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\n",
      "Parekh, Hieu Pham, Quoc V . Le, Yun-Hsuan Sung,\n",
      "Zhen Li, and Tom Duerig. 2021. Scaling up visual\n",
      "and vision-language representation learning with\n",
      "noisy text supervision. In Proceedings of the 38th In-\n",
      "ternational Conference on Machine Learning, ICML\n",
      "2021, 18-24 July 2021, Virtual Event , volume 139 of\n",
      "Proceedings of Machine Learning Research , pages\n",
      "4904–4916. PMLR.Andrej Karpathy and Fei-Fei Li. 2015. Deep visual-\n",
      "semantic alignments for generating image descrip-\n",
      "tions. In IEEE Conference on Computer Vision and\n",
      "Pattern Recognition, CVPR 2015, Boston, MA, USA,\n",
      "June 7-12, 2015 , pages 3128–3137. IEEE Computer\n",
      "Society.\n",
      "Amirhossein Kazemnejad, Mohammadreza Salehi, and\n",
      "Mahdieh Soleymani Baghshah. 2020. Paraphrase\n",
      "generation by learning how to edit from samples. In\n",
      "Proceedings of the 58th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics , pages 6010–\n",
      "6021, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\n",
      "Zettlemoyer, and Mike Lewis. 2020a. Near-\n",
      "est neighbor machine translation. arXiv preprint\n",
      "arXiv:2010.00710 .\n",
      "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\n",
      "Zettlemoyer, and Mike Lewis. 2020b. Generaliza-\n",
      "tion through memorization: Nearest neighbor lan-\n",
      "guage models. In 8th International Conference on\n",
      "Learning Representations, ICLR 2020, Addis Ababa,\n",
      "Ethiopia, April 26-30, 2020 . OpenReview.net.\n",
      "Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.\n",
      "Statistical phrase-based translation. In Proceedings\n",
      "of the 2003 Human Language Technology Confer-\n",
      "ence of the North American Chapter of the Associa-\n",
      "tion for Computational Linguistics , pages 127–133.\n",
      "Philipp Koehn and Jean Senellart. 2010. Convergence\n",
      "of translation memory and statistical machine trans-\n",
      "lation. In Proceedings of AMTA Workshop on MT\n",
      "Research and the Translation Industry , pages 21–31.\n",
      "Mojtaba Komeili, Kurt Shuster, and Jason Weston.\n",
      "2021. Internet-augmented dialogue generation.\n",
      "arXiv preprint arXiv:2107.07566 .\n",
      "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n",
      "2019. Latent retrieval for weakly supervised\n",
      "open domain question answering. arXiv preprint\n",
      "arXiv:1906.00300 .\n",
      "Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\n",
      "men Aghajanyan, Sida Wang, and Luke Zettlemoyer.\n",
      "2020a. Pre-training via paraphrasing. In Advances\n",
      "in Neural Information Processing Systems 33: An-\n",
      "nual Conference on Neural Information Processing\n",
      "Systems 2020, NeurIPS 2020, December 6-12, 2020,\n",
      "virtual .\n",
      "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\n",
      "Petroni, Vladimir Karpukhin, Naman Goyal, Hein-\n",
      "rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\n",
      "täschel, et al. 2020b. Retrieval-augmented gen-\n",
      "eration for knowledge-intensive nlp tasks. arXiv\n",
      "preprint arXiv:2005.11401 .\n",
      "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\n",
      "and Bill Dolan. 2016a. A diversity-promoting ob-\n",
      "jective function for neural conversation models. In\n",
      "NAACL , pages 110–119.Juncen Li, Robin Jia, He He, and Percy Liang. 2018.\n",
      "Delete, retrieve, generate: a simple approach to sen-\n",
      "timent and style transfer. In Proceedings of the 2018\n",
      "Conference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Human\n",
      "Language Technologies, NAACL-HLT 2018, New\n",
      "Orleans, Louisiana, USA, June 1-6, 2018, Volume\n",
      "1 (Long Papers) , pages 1865–1874. Association for\n",
      "Computational Linguistics.\n",
      "Liangyou Li, Andy Way, and Qun Liu. 2014. A\n",
      "discriminative framework of integrating translation\n",
      "memory features into smt. In Proceedings of the\n",
      "11th Conference of the Association for Machine\n",
      "Translation in the Americas , volume 1, pages 249–\n",
      "260.\n",
      "Liangyou Li, Andy Way, and Qun Liu. 2016b. Phrase-\n",
      "level combination of smt and tm using constrained\n",
      "word lattice. Association for Computational Lin-\n",
      "guistics (ACL).\n",
      "Xiaoqing Li, Jiajun Zhang, and Chengqing Zong.\n",
      "2016c. One sentence one model for neural machine\n",
      "translation. arXiv preprint arXiv:1609.06490 .\n",
      "Zekang Li, Cheng Niu, Fandong Meng, Yang Feng,\n",
      "Qian Li, and Jie Zhou. 2019. Incremental trans-\n",
      "former with deliberation decoder for document\n",
      "grounded conversations. In Proceedings of the 57th\n",
      "Annual Meeting of the Association for Computa-\n",
      "tional Linguistics , pages 12–21.\n",
      "Rongzhong Lian, Min Xie, Fan Wang, Jinhua Peng,\n",
      "and Hua Wu. 2019. Learning to select knowledge\n",
      "for response generation in dialog systems. arXiv\n",
      "preprint arXiv:1902.04911 .\n",
      "Lemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao,\n",
      "Mo Yu, and Conghui Zhu. 2012. Locally training\n",
      "the log-linear model for smt. In Proceedings of the\n",
      "2012 Joint Conference on Empirical Methods in Nat-\n",
      "ural Language Processing and Computational Natu-\n",
      "ral Language Learning , pages 402–411.\n",
      "Lemao Liu, Tiejun Zhao, Taro Watanabe, Hailong Cao,\n",
      "and Conghui Zhu. 2014. Discriminative training for\n",
      "log-linear based smt: Global or local methods. ACM\n",
      "Transactions on Asian Language Information Pro-\n",
      "cessing (TALIP) , 13(4):1–25.\n",
      "Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-\n",
      "abith. 2011. Consistent translation using discrim-\n",
      "inative learning-a translation memory-inspired ap-\n",
      "proach. In Proceedings of the 49th Annual Meet-\n",
      "ing of the Association for Computational Linguistics:\n",
      "Human Language Technologies , pages 1239–1248.\n",
      "Yuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xi-\n",
      "aofei Sun, Tianwei Zhang, and Jiwei Li. 2021.\n",
      "Fast nearest neighbor machine translation. arXiv\n",
      "preprint arXiv:2105.14528 .\n",
      "Franz Josef Och. 2003. Minimum error rate training in\n",
      "statistical machine translation. In Proceedings of the41st Annual Meeting of the Association for Compu-\n",
      "tational Linguistics , pages 160–167, Sapporo, Japan.\n",
      "Association for Computational Linguistics.\n",
      "Gaurav Pandey, Danish Contractor, Vineet Kumar, and\n",
      "Sachindra Joshi. 2018. Exemplar encoder-decoder\n",
      "for neural conversation generation. In ACL, pages\n",
      "1329–1338.\n",
      "Ashwin Paranjape, Omar Khattab, Christopher Potts,\n",
      "Matei Zaharia, and Christopher D Manning. 2021.\n",
      "Hindsight: Posterior-guided training of retrievers for\n",
      "improved open-ended generation. arXiv preprint\n",
      "arXiv:2110.07752 .\n",
      "Hao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan\n",
      "Dhingra, and Das Dipanjan. 2019. Text generation\n",
      "with exemplar-based adaptive decoding. In Proceed-\n",
      "ings of the Conference of the North American Chap-\n",
      "ter of the Association for Computational Linguistics:\n",
      "Human Language Technologies .\n",
      "Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong\n",
      "Liu, Xiang Gao, William B Dolan, Yejin Choi, and\n",
      "Jianfeng Gao. 2019. Conversing by reading: Con-\n",
      "tentful neural conversation with on-demand machine\n",
      "reading. In Proceedings of the 57th Annual Meet-\n",
      "ing of the Association for Computational Linguistics ,\n",
      "pages 5427–5436.\n",
      "Minghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan\n",
      "Chen, Weipeng Zhao, Haiqing Chen, Jun Huang,\n",
      "and Wei Chu. 2017. Alime chat: A sequence to se-\n",
      "quence and rerank based chatbot engine. In ACL,\n",
      "pages 498–503.\n",
      "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\n",
      "Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\n",
      "Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\n",
      "Gretchen Krueger, and Ilya Sutskever. 2021. Learn-\n",
      "ing transferable visual models from natural lan-\n",
      "guage supervision. In Proceedings of the 38th In-\n",
      "ternational Conference on Machine Learning, ICML\n",
      "2021, 18-24 July 2021, Virtual Event , volume 139 of\n",
      "Proceedings of Machine Learning Research , pages\n",
      "8748–8763. PMLR.\n",
      "Stephen Robertson and Hugo Zaragoza. 2009. The\n",
      "probabilistic relevance framework: BM25 and be-\n",
      "yond . Now Publishers Inc.\n",
      "Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-\n",
      "ral responding machine for short-text conversation.\n",
      "InACL, pages 1577–1586.\n",
      "Michel Simard and Pierre Isabelle. 2009. Phrase-based\n",
      "machine translation in a computer-assisted transla-\n",
      "tion environment. Proceedings of the Twelfth Ma-\n",
      "chine Translation Summit (MT Summit XII) , pages\n",
      "120–127.\n",
      "James Smith and Stephen Clark. 2009. Ebmt for smt:\n",
      "a new ebmt-smt hybrid. In Proceedings of the 3rd\n",
      "International Workshop on Example-Based Machine\n",
      "Translation , pages 3–10. Citeseer.Harold Somers. 2003. Translation memory systems.\n",
      "Benjamins Translation Library , 35:31–48.\n",
      "Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and\n",
      "Ming Zhang. 2016. Two are better than one: An en-\n",
      "semble of retrieval-and generation-based dialog sys-\n",
      "tems. arXiv preprint arXiv:1610.07149 .\n",
      "Yixuan Su, Zaiqiao Meng, Simon Baker, and Nigel Col-\n",
      "lier. 2021a. Few-shot table-to-text generation with\n",
      "prototype memory. In Findings of the Association\n",
      "for Computational Linguistics: EMNLP 2021, Vir-\n",
      "tual Event / Punta Cana, Dominican Republic, 16-\n",
      "20 November, 2021 , pages 910–917. Association for\n",
      "Computational Linguistics.\n",
      "Yixuan Su, David Vandyke, Simon Baker, Yan Wang,\n",
      "and Nigel Collier. 2021b. Keep the primary, rewrite\n",
      "the secondary: A two-stage approach for paraphrase\n",
      "generation. In Findings of the Association for Com-\n",
      "putational Linguistics: ACL-IJCNLP 2021 , pages\n",
      "560–569, Online. Association for Computational\n",
      "Linguistics.\n",
      "Yixuan Su, Yan Wang, Deng Cai, Simon Baker, Anna\n",
      "Korhonen, and Nigel Collier. 2021c. PROTOTYPE-\n",
      "TO-STYLE: dialogue generation with style-aware\n",
      "editing on retrieval memory. IEEE ACM Trans. Au-\n",
      "dio Speech Lang. Process. , 29:2152–2161.\n",
      "Marco Turchi, Matteo Negri, M Farajian, and Marcello\n",
      "Federico. 2017. Continuous learning from human\n",
      "post-edits for neural machine translation.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\n",
      "Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
      "you need. In Advances in neural information pro-\n",
      "cessing systems , pages 5998–6008.\n",
      "Oriol Vinyals and Quoc Le. 2015. A neural conversa-\n",
      "tional model. In ICML (Deep Learning Workshop) .\n",
      "Kun Wang, Chengqing Zong, and Keh-Yih Su. 2013.\n",
      "Integrating translation memory into phrase-based\n",
      "machine translation during decoding. In Proceed-\n",
      "ings of the 51st Annual Meeting of the Association\n",
      "for Computational Linguistics (Volume 1: Long Pa-\n",
      "pers) , pages 11–21.\n",
      "Kun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\n",
      "Dynamically integrating cross-domain translation\n",
      "memory into phrase-based machine translation dur-\n",
      "ing decoding. In Proceedings of COLING 2014,\n",
      "the 25th International Conference on Computational\n",
      "Linguistics: Technical Papers , pages 398–408.\n",
      "Jason Weston, Emily Dinan, and Alexander Miller.\n",
      "2018. Retrieve and reﬁne: Improved sequence gen-\n",
      "eration models for dialogue. In Proceedings of the\n",
      "2018 EMNLP Workshop SCAI: The 2nd Interna-\n",
      "tional Workshop on Search-Oriented Conversational\n",
      "AI, pages 87–92.Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\n",
      "jun Li, and Ming Zhou. 2019. Response generation\n",
      "by context-aware prototype editing. In Proceedings\n",
      "of the AAAI Conference on Artiﬁcial Intelligence ,\n",
      "volume 33, pages 7281–7288.\n",
      "Zeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\n",
      "Xiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\n",
      "Jianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\n",
      "et al. 2021. A controllable model of grounded re-\n",
      "sponse generation. In Proceedings of the AAAI Con-\n",
      "ference on Artiﬁcial Intelligence , volume 35, pages\n",
      "14085–14093.\n",
      "Mengzhou Xia, Guoping Huang, Lemao Liu, and\n",
      "Shuming Shi. 2019. Graph based translation mem-\n",
      "ory for neural machine translation. In Proceedings\n",
      "of the AAAI Conference on Artiﬁcial Intelligence ,\n",
      "volume 33, pages 7297–7304.\n",
      "Fei Xiao, Liang Pang, Yanyan Lan, Yan Wang, Huawei\n",
      "Shen, and Xueqi Cheng. 2021. Transductive learn-\n",
      "ing for unsupervised text style transfer. In Proceed-\n",
      "ings of the 2021 Conference on Empirical Methods\n",
      "in Natural Language Processing, EMNLP 2021, Vir-\n",
      "tual Event / Punta Cana, Dominican Republic, 7-11\n",
      "November, 2021 , pages 2510–2521. Association for\n",
      "Computational Linguistics.\n",
      "Jitao Xu, Josep M Crego, and Jean Senellart. 2020.\n",
      "Boosting neural machine translation with similar\n",
      "translations. In Proceedings of the 58th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 1580–1590.\n",
      "Liu Yang, Junjie Hu, Minghui Qiu, Chen Qu, Jian-\n",
      "feng Gao, W Bruce Croft, Xiaodong Liu, Yelong\n",
      "Shen, and Jingjing Liu. 2019. A hybrid retrieval-\n",
      "generation neural conversation model. In Proceed-\n",
      "ings of the 28th ACM international conference on in-\n",
      "formation and knowledge management , pages 1341–\n",
      "1350.\n",
      "Jingyi Zhang, Masao Utiyama, Eiichiro Sumita, Gra-\n",
      "ham Neubig, and Satoshi Nakamura. 2018. Guiding\n",
      "neural machine translation with retrieved translation\n",
      "pieces. In Proceedings of the 2018 Conference of the\n",
      "North American Chapter of the Association for Com-\n",
      "putational Linguistics: Human Language Technolo-\n",
      "gies, Volume 1 (Long Papers) , pages 1325–1335.\n",
      "Yizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris\n",
      "Brockett, Michel Galley, Jianfeng Gao, and Bill\n",
      "Dolan. 2021. Joint retrieval and generation train-\n",
      "ing for grounded text generation. arXiv preprint\n",
      "arXiv:2105.06597 .\n",
      "Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao\n",
      "Utiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao.\n",
      "2020. Neural machine translation with universal\n",
      "visual representation. In 8th International Confer-\n",
      "ence on Learning Representations, ICLR 2020, Ad-\n",
      "dis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-\n",
      "view.net.Ventsislav Zhechev and Josef Van Genabith. 2010.\n",
      "Seeding statistical machine translation with trans-\n",
      "lation memory output through tree-based structural\n",
      "alignment. In Proceedings of the 4th Workshop\n",
      "on Syntax and Structure in Statistical Translation ,\n",
      "pages 43–51.\n",
      "Xin Zheng, Zhirui Zhang, Junliang Guo, Shujian\n",
      "Huang, Boxing Chen, Weihua Luo, and Jiajun Chen.\n",
      "2021a. Adaptive nearest neighbor machine transla-\n",
      "tion. arXiv preprint arXiv:2105.13022 .\n",
      "Xin Zheng, Zhirui Zhang, Shujian Huang, Boxing\n",
      "Chen, Jun Xie, Weihua Luo, and Jiajun Chen. 2021b.\n",
      "Non-parametric unsupervised domain adaptation for\n",
      "neural machine translation. In Findings of the As-\n",
      "sociation for Computational Linguistics: EMNLP\n",
      "2021 , pages 4234–4241.\n",
      "Kangyan Zhou, Shrimai Prabhumoye, and Alan W\n",
      "Black. 2018. A dataset for document grounded con-\n",
      "versations. arXiv preprint arXiv:1809.07358 .\n",
      "==================================================\n",
      "\n",
      "Document 3:\n",
      "Benchmarking Large Language Models in Retrieval-Augmented Generation\n",
      "Jiawei Chen1,3, Hongyu Lin1,*, Xianpei Han1,2,*, Le Sun1,2\n",
      "1Chinese Information Processing Laboratory2State Key Laboratory of Computer Science\n",
      "Institute of Software, Chinese Academy of Sciences, Beijing, China\n",
      "3University of Chinese Academy of Sciences, Beijing, China\n",
      "{jiawei2020,hongyu,xianpei,sunle }@iscas.ac.cn\n",
      "Abstract\n",
      "Retrieval-Augmented Generation (RAG) is a promising ap-\n",
      "proach for mitigating the hallucination of large language\n",
      "models (LLMs). However, existing research lacks rigorous\n",
      "evaluation of the impact of retrieval-augmented generation\n",
      "on different large language models, which make it challeng-\n",
      "ing to identify the potential bottlenecks in the capabilities\n",
      "of RAG for different LLMs. In this paper, we systemati-\n",
      "cally investigate the impact of Retrieval-Augmented Gener-\n",
      "ation on large language models. We analyze the performance\n",
      "of different large language models in 4 fundamental abili-\n",
      "ties required for RAG, including noise robustness, negative\n",
      "rejection, information integration, and counterfactual robust-\n",
      "ness. To this end, we establish Retrieval-Augmented Genera-\n",
      "tion Benchmark (RGB), a new corpus for RAG evaluation in\n",
      "both English and Chinese. RGB divides the instances within\n",
      "the benchmark into 4 separate testbeds based on the afore-\n",
      "mentioned fundamental abilities required to resolve the case.\n",
      "Then we evaluate 6 representative LLMs on RGB to diag-\n",
      "nose the challenges of current LLMs when applying RAG.\n",
      "Evaluation reveals that while LLMs exhibit a certain degree\n",
      "of noise robustness, they still struggle significantly in terms of\n",
      "negative rejection, information integration, and dealing with\n",
      "false information. The aforementioned assessment outcomes\n",
      "indicate that there is still a considerable journey ahead to ef-\n",
      "fectively apply RAG to LLMs.\n",
      "Introduction\n",
      "Recently, there have been impressive advancements in large\n",
      "language models (LLMs) like ChatGPT (OpenAI 2022),\n",
      "LLaMA-2 (Touvron et al. 2023), and ChatGLM (THUDM\n",
      "2023a). Although these models have shown remarkable gen-\n",
      "eral abilities (Bang et al. 2023; Guo et al. 2023), they still\n",
      "suffer severely from challenges including factual halluci-\n",
      "nation (Cao et al. 2020; Raunak, Menezes, and Junczys-\n",
      "Dowmunt 2021; Ji et al. 2023), knowledge out-dating (He,\n",
      "Zhang, and Roth 2022), and the lack of domain-specific ex-\n",
      "pertise (Li et al. 2023c; Shen et al. 2023).\n",
      "Incorporating external knowledge via information re-\n",
      "trieval, i.e., Retrieval-Augmented Generation (RAG), has\n",
      "been regarded as a promising way to resolve the above chal-\n",
      "lenges. (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.\n",
      "2022; Izacard et al. 2022). With the help of external knowl-\n",
      "*Corresponding authors.\n",
      "Noise Robustness Negative Rejection\n",
      "Who was awarded the 2022 Nobel prize in \n",
      "literature?\n",
      "The Nobel Prize in Literature for 2022 is \n",
      "awarded to the French author Annie Ernaux, \n",
      "“for the courage and clinical acuity …\n",
      "The Nobel Prize in Literature for 2021 is \n",
      "awarded to the novelist Abdulrazak Gurnah, \n",
      "born in Zanzibar and active in …\n",
      "Annie ErnauxQuestion\n",
      "External documents contain noises\n",
      "Retrieval Augmented \n",
      "GenerationWho was awarded the 2022 Nobel prize in \n",
      "literature?\n",
      "The Nobel Prize in Literature for 2021 is \n",
      "awarded to the novelist Abdulrazak Gurnah, \n",
      "born in Zanzibar and active in …\n",
      "The 2020 Nobel Laureate in Literature, \n",
      "poet Louise Glück, has written both poetry \n",
      "and essays about poetry. Since her…\n",
      "I can not answer the question because of the \n",
      "insufficient information in documentsQuestion\n",
      "External documents are all noises\n",
      "Information Integration\n",
      "When were the ChatGPT app for iOS and \n",
      "ChatGPT apilaunched?\n",
      "On May 18th, 2023, OpenAI introduced its \n",
      "own ChatGPT app for iOS…\n",
      "That changed on March 1, when OpenAI \n",
      "announced the release of API access to \n",
      "ChatGPT and Whisper,…\n",
      "May 18 and March 1 .Question\n",
      "External documents contain all answers\n",
      "Retrieval Augmented \n",
      "GenerationCounterfactual Robustness\n",
      "Which city hosted the Olympic games in \n",
      "2004?\n",
      "The 2004 Olympic Games returned home to \n",
      "New York , birthplace of the … \n",
      "After leading all voting rounds, New York\n",
      "easily defeated Rome in the fifth and \n",
      "final vote …\n",
      "There are factual errors in the provided \n",
      "documents. The answer should be Athens . Question\n",
      "Counterfactual external documents\n",
      "Retrieval Augmented \n",
      "GenerationRetrieval Augmented \n",
      "GenerationFigure 1: Illustration of 4 kinds of abilities required for\n",
      "retrieval-augmented generation of LLMs.\n",
      "edge, LLMs can generate more accurate and reliable re-\n",
      "sponses. The most common method is to use a search engine\n",
      "as a retriever such as New Bing. Due to the vast amount of\n",
      "information available on the Internet, using a search engine\n",
      "can provide more real-time information.\n",
      "However, Retrieval-Augmented Generation brings not\n",
      "only positive effects to LLMs (Liu, Zhang, and Liang 2023;\n",
      "Maynez et al. 2020). On one hand, there is a significant\n",
      "amount of noise information even fake news in the content\n",
      "available on the Internet, which poses challenges for search\n",
      "engines in accurately retrieving desirable knowledge. On the\n",
      "other hand, LLMs suffer from unreliable generation chal-\n",
      "lenge. LLMs can be misled by incorrect information con-\n",
      "tained in the context (Bian et al. 2023) and also suffer from\n",
      "hallucination during the generation (Adlakha et al. 2023),\n",
      "resulting in generating content that goes beyond external in-\n",
      "formation. These challenges result in LLMs being unable toarXiv:2309.01431v1  [cs.CL]  4 Sep 2023consistently generate reliable and accurate responses. Un-\n",
      "fortunately, currently there lacks of comprehensive under-\n",
      "standing on how these factors can influence RAG, and how\n",
      "could each model survives from these drawbacks and im-\n",
      "provement their performance via information retrieval. As a\n",
      "result, there is a pressing need for a comprehensive evalua-\n",
      "tion of LLMs on their ability to effectively utilize retrieved\n",
      "information, as well as their ability to withstand the various\n",
      "drawbacks present in information retrieval.\n",
      "To this end, this paper conducts a comprehensive evalua-\n",
      "tion of RAG for current LLMs. Specifically, we create a new\n",
      "Retrieval-Augmented Generation Benchmark, namely RGB,\n",
      "in both English and Chinese. In order to ensure that the in-\n",
      "ternal knowledge of LLMs does not introduce bias into the\n",
      "evaluation results, RGB chooses to aggregate the latest news\n",
      "information and constructs queries based on the news infor-\n",
      "mation. Then, based on these queries, we use Search API to\n",
      "fetch relevant documents and select most relevant snippets\n",
      "from the content as external retrieved documents. Finally,\n",
      "based on different compositions of query and document-set\n",
      "pairs, we expand the corpus and divided it into 4 testbeds to\n",
      "evaluate the following basic abilities of LLMs according to\n",
      "the common challenges in RAG, as shown in Figure 1:\n",
      "•Noise Robustness , which means a model can extract use-\n",
      "ful information from noisy documents. In this paper, we\n",
      "define noisy documents as those that are relevant to the\n",
      "question but do not contain any information of the an-\n",
      "swer. For the instance in Figure 1, the noisy documents\n",
      "related to the question “Who was awarded the 2022 No-\n",
      "bel Prize in Literature” include reports about the 2021\n",
      "Nobel Prize in Literature. To this end, the testbed for\n",
      "noise robustness contains instances whose external doc-\n",
      "uments contain a certain number of noisy documents\n",
      "based on the desired noise ratio.\n",
      "•Negative Rejection , which means that a model should\n",
      "reject to answer the question when the required knowl-\n",
      "edge is not present in any retrieved document. The\n",
      "testbed for negative rejection contains instances whose\n",
      "external documents are only with noisy documents.\n",
      "LLMs are expected to indicate “insufficient information”\n",
      "or other rejection signals.\n",
      "•Information Integration , which evaluates whether the\n",
      "model can answer complex questions that require inte-\n",
      "grating information from multiple documents. For the\n",
      "instance in Figure 1, for the question “When were the\n",
      "ChatGPT app for iOS and ChatGPT api launched?”, the\n",
      "LLMs are expected to provide information on the launch\n",
      "dates for both the ChatGPT app on iOS and the ChatGPT\n",
      "API. The testbed for information integration contains in-\n",
      "stances that can only be answered using multiple external\n",
      "documents.\n",
      "•Counterfactual Robustness , which evaluates whether\n",
      "the model can identify risks of known factual errors in the\n",
      "retrieved documents when the LLMs are given warnings\n",
      "about potential risks in the retrieved information through\n",
      "instruction. The testbed for counterfactual robustness in-\n",
      "cludes instances that can be answered directly by the\n",
      "LLMs, but the external documents contain factual errors.Based on RGB, we conduct evaluation on 6 state-of-\n",
      "the-art large language models including ChatGPT (Ope-\n",
      "nAI 2022), ChatGLM-6B (THUDM 2023a), ChatGLM2-\n",
      "6B (THUDM 2023b), Vicuna-7b (Chiang et al. 2023),\n",
      "Qwen-7B-Chat (QwenLM 2023), BELLE-7B (Yunjie Ji\n",
      "2023). We found that even though RAG can improve the re-\n",
      "sponse accuracy of LLMs, they still suffer from the above-\n",
      "mentioned challenges significantly. Specifically, we found\n",
      "that even though LLMs demonstrate some level of noise ro-\n",
      "bustness, they tend to confuse similar information and fre-\n",
      "quently generate inaccurate answers when relevant informa-\n",
      "tion exists. For example, when faced with a question about\n",
      "the 2022 Nobel Prize in Literature, if there are noisy docu-\n",
      "ments about the 2021 Nobel Prize in Literature in external\n",
      "documents, LLMs may become confused and provide inac-\n",
      "curate answers. Besides, LLMs frequently fail to reject an-\n",
      "swering and generate incorrect answers when none of the\n",
      "external documents contain relevant information. Further-\n",
      "more, LLMs lack the ability to summarize from multiple\n",
      "documents, and therefore if multiple documents are needed\n",
      "to answer a question, LLMs often fail to provide accurate\n",
      "answer. Finally, we found that even when the LLMs contain\n",
      "the required knowledge and are given warnings about po-\n",
      "tential risks in the retrieved information through instruction,\n",
      "they still tend to trust and prioritize the retrieved information\n",
      "over their own existing knowledge. The experimental results\n",
      "mentioned above highlight the need for further resolution of\n",
      "important issues in the existing RAG method. Therefore, it\n",
      "is crucial to exercise caution and carefully design its usage.\n",
      "Generally speaking, the contributions of this paper are1:\n",
      "• We proposed to evaluate four capabilities for retrieval-\n",
      "augmented generation of LLMs and created the\n",
      "Retrieval-Augmented Generation Benchmark in both En-\n",
      "glish and Chinese. To best of our knowledge, it is the first\n",
      "benchmark designed to assess these four capabilities for\n",
      "retrieval-augmented generation of LLMs.\n",
      "• We evaluated the existing LLMs using RGB and found\n",
      "the limitations of them in the four different abilities.\n",
      "• We analyzed the responses of LLMs in RGB and identi-\n",
      "fied their current shortcomings as well as suggested di-\n",
      "rections for improvement.\n",
      "Related work\n",
      "Retrieval-augmented models The knowledge stored in\n",
      "large language models is commonly out-of-date (He, Zhang,\n",
      "and Roth 2022) and they also sometimes generate hallu-\n",
      "cination (Cao et al. 2020; Raunak, Menezes, and Junczys-\n",
      "Dowmunt 2021; Ji et al. 2023) i.e., they may generate ir-\n",
      "relevant or factually incorrect contents. By using external\n",
      "knowledge as guidance, retrieval-augmented models can\n",
      "generate more accurate and reliable responses (Guu et al.\n",
      "2020; Lewis et al. 2020; Borgeaud et al. 2022; Izacard\n",
      "et al. 2022; Shi et al. 2023; Ren et al. 2023). Retrieval-\n",
      "augmented models have achieved remarkable results in var-\n",
      "ious tasks such as open-domain QA (Izacard and Grave\n",
      "1We will release the code and RGB of this paper in https:\n",
      "//github.com/chen700564/RGB.News CollectionThe 2022 Nobel Prize for Physiology and Medicine was awarded on \n",
      "Monday to Swedish scientist Svante Pä ä bo for sequencing the \n",
      "genome of the Neanderthal.Data adjustment \n",
      "and filtering by \n",
      "Human{\n",
      "“Question”: “Who was awarded the 2022 \n",
      "Nobel Prize for Physiology and Medicine ?”,\n",
      "“Answer”: ['Svante Pä ä bo ','Svante Paabo ’]\n",
      "}\n",
      "Data generation by \n",
      "ChatGPTRetrieve using \n",
      "search engineRerank by dense \n",
      "retrieval model\n",
      "We simulate the process of a user querying and obtaining \n",
      "information. Suppose the user retrieves a current event news, \n",
      "speculate the event that the user is concerned about and the \n",
      "question that he/she may want to know, and generate the key \n",
      "information corresponding to the answer to the question. …\n",
      "…\n",
      "News: The 2022 Nobel Prize for Physiology and Medicine was …Related event: 2022 Nobel Prize for Physiology and Medicine\n",
      "Question: Who was awarded the 2022 Nobel Prize for Physiology \n",
      "and Medicine?\n",
      "Key information: Svante Pääbo and Svante Paabo\n",
      "gpt-3.5-turbo apiQuery: Who was awarded the 2022 Nobel Prize for Physiology and \n",
      "Medicine?”,{\"link\": \"https://www.nobelprize.org/prizes/medicine/\", \"title\": \n",
      "\"The Nobel Prize in Physiology or Medicine 2022\", \"snippet\": \"The \n",
      "Nobel Assembly...\"}, ...\n",
      "Google Search APIChun2ChunkWho was awarded the 2022 Nobel \n",
      "Prize for Physiology and Medicine?”,Dense retrieval modelTop1 Chunk Top30 Chunk Top2 Chunk ……\n",
      "……Figure 2: The process of data generation. Firstly, we use\n",
      "models to extract (event, question, answer) from news ar-\n",
      "ticles. Next, we utilize search engines to retrieve relevant\n",
      "web pages. Finally, a dense retrieval model is employed to\n",
      "re-rank the content of these web pages.\n",
      "2021; Trivedi et al. 2023; Li et al. 2023a), dialogue (Cai\n",
      "et al. 2019a,b; Peng et al. 2023), domain-specific ques-\n",
      "tion answering (Cui et al. 2023) and code generation (Zhou\n",
      "et al. 2023b). Recently, with the development of large mod-\n",
      "els, a series of retrieval-enhanced tools and products have\n",
      "gained widespread attention, such as ChatGPT retrieval plu-\n",
      "gin, Langchain, New Bing, etc. However, in real-world sce-\n",
      "narios, the retrieved text inevitably contains noise. There-\n",
      "fore, in this paper we conducted a systematic evaluation and\n",
      "analysis of retrieval-augmented generation in LLMs.\n",
      "Evaluation of LLMs Evaluating LLMs has received sig-\n",
      "nificant attention due to their remarkable general capabil-\n",
      "ity (Chang et al. 2023). It enables us to gain a deeper under-\n",
      "standing of the specific abilities and limitations of LLMs,\n",
      "while also providing valuable guidance for future research.\n",
      "In the past, benchmarks such as GLUE (Wang et al. 2019b)\n",
      "and SuperCLUE (Wang et al. 2019a) primarily focused on\n",
      "evaluating NLP tasks, particularly in natural language un-\n",
      "derstanding. However, these evaluations often fail to fully\n",
      "capture the capabilities of LLMs. MMLU (Hendrycks et al.\n",
      "2021) was then proposed to measure the knowledge acquired\n",
      "by language models when pre-training. Recently, with the\n",
      "development of LLMs, a series of general evaluation bench-\n",
      "marks have emerged, such as AGIEval (Zhong et al. 2023),\n",
      "C-Eval (Huang et al. 2023), AlpacaEval (Li et al. 2023b),OpenLLM Leaderboard (Edward Beeching 2023), etc. In\n",
      "addition to general abilities, there are also specific bench-\n",
      "marks that focus on evaluating the capabilities of models.\n",
      "For example, CValues (Xu et al. 2023a) focuses on the safety\n",
      "and responsibility of LLMs, M3Exam (Zhang et al. 2023)\n",
      "focuses on human exam and ToolBench (Qin et al. 2023)\n",
      "evaluates how well LLMs use external tools. Recently, Ad-\n",
      "lakha et al. (2023) evaluate the RAG of LLMs in exist QA\n",
      "dataset. Different from their work, we focus on 4 required\n",
      "abilities of RAG and create Retrieval-Augmented Genera-\n",
      "tion Benchmark to evaluate the LLMs.\n",
      "Retrieval-Augmented Generation Benchmark\n",
      "In this section, we first introduce the specific retrieval-\n",
      "augmented generation abilities we aim to evaluate in LLMs.\n",
      "Next, we outline the process of constructing the RAG bench-\n",
      "mark for evaluation. Lastly, we present the evaluation met-\n",
      "rics.\n",
      "Required abilities of RAG\n",
      "External knowledge is the key to resolving the problems\n",
      "of LLMs such as hallucination and outdated knowledge,\n",
      "which can make LLMs generate more accurate and reliable\n",
      "responses through retrieval-augmented generation (RAG).\n",
      "However, LLMs cannot always response as expected with\n",
      "RAG. For one thing, there are numerous irrelevant docu-\n",
      "ments and false information on the Internet. Incorporating\n",
      "these external documents into LLMs could have a detrimen-\n",
      "tal effect. For anthoer, LLMs suffer from the unreliable gen-\n",
      "eration challenge. The generation of LLMs is often unpre-\n",
      "dictable, and we cannot guarantee that they will utilize the\n",
      "useful information entailed in the external documents. Ad-\n",
      "ditionally, LLMs can easily be misled by incorrect infor-\n",
      "mation in the document. To this end, we build Retrieval-\n",
      "Augmented Generation Benchmark (RGB) to evaluate the\n",
      "retrieval-augmented generation of LLMs, and we concern\n",
      "about 4 specific abilities:\n",
      "Noise Robustness is the robustness of LLMs in noisy\n",
      "documents. As retrievers are not perfect, the external knowl-\n",
      "edge they retrieve often contains a significant amount of\n",
      "noise, i.e., documents which are relevant to the question but\n",
      "do not contain any information about the answer. To effec-\n",
      "tively answer user questions, LLMs must be able to extract\n",
      "the necessary information from documents despite there are\n",
      "noisy documents.\n",
      "Negative Rejection is a measure of whether LLMs can\n",
      "decline to answer a question when none of the contexts pro-\n",
      "vide useful information. In real-world situations, the search\n",
      "engine often fails to retrieve documents containing the an-\n",
      "swers. In these cases, it is important for the model to have\n",
      "the capability to reject recognition and avoid generating mis-\n",
      "leading content.\n",
      "Information Integration is a capacity to integrate an-\n",
      "swers from multiple documents. In many cases, the an-\n",
      "swer to a question may be contained in multiple documents.\n",
      "For example, for the question ”Who are the champions of\n",
      "the U.S. Open 2022 men’s and women’s singles?” , the two\n",
      "champions may be mentioned in different documents. In or-der to provide better answers to complex questions, it is nec-\n",
      "essary for LLMs to have the ability to integrate information.\n",
      "Counterfactual Robustness refers to a capacity to han-\n",
      "dle errors in external knowledge. In the real world, there is\n",
      "an abundance of false information on the internet. Please\n",
      "note that we only evaluate the situation that LLMs are given\n",
      "warnings about potential risks in the retrieved information\n",
      "through instruction.\n",
      "In real-world scenarios, it is not possible to obtain per-\n",
      "fect documents with all the necessary external knowledge.\n",
      "Therefore, evaluating these four abilities of the model be-\n",
      "comes essential in order to measure the RAG of LLMs.\n",
      "Data construction\n",
      "Inspired by previous benchmarks for LLMs, RGB utilizes\n",
      "a question-answering format for evaluation. We evaluate the\n",
      "LLMs by judging the retrieval-augmented responses of them\n",
      "to the questions. To simulate real-world scenarios, we con-\n",
      "struct question and answer data using actual news articles.\n",
      "Due to the abundance of knowledge contained within the\n",
      "LLMs there is a potential for bias when measuring the first\n",
      "three abilities. To mitigate this, the instances of RGB are\n",
      "constructed by latest news articles. Additionally, we retrieve\n",
      "external documents from Internet through search engines.\n",
      "Finally, we expand the corpus and divided it into 4 testbeds\n",
      "to evaluate the above basic abilities of LLMs. The overall\n",
      "procedure of our data construction is illustrated in Figure 2.\n",
      "QA instances generation. We first collect latest news ar-\n",
      "ticles and use prompts to have ChatGPT generate events,\n",
      "questions, and answers for each articles. For example, as\n",
      "shown in the Figure 2, for a report about “The 2022 Nobel\n",
      "Prize”, ChatGPT will generate corresponding event, ques-\n",
      "tion and provide key information for answering it. By gen-\n",
      "erating events, the model is able to preliminarily filter out\n",
      "news articles that do not contain any events. After genera-\n",
      "tion, we manually check the answer and filter out data that\n",
      "is difficult to retrieve through search engines.\n",
      "Retrieve using search engine. For each query, we use\n",
      "Google’s API to fetch 10 relevant web pages and extract\n",
      "corresponding snippets of text from them. Simultaneously,\n",
      "we read these web pages and convert their textual content\n",
      "into text chunks with a maximum length of 300 tokens. Us-\n",
      "ing an existing dense retrieval model2, we select the top-30\n",
      "text chunks that match the query most effectively. These re-\n",
      "trieved text chunks, along with the snippets provided by the\n",
      "search API, will serve as our external documents. These doc-\n",
      "uments will be divided into positive documents and negative\n",
      "documents based on whether they contain the answer.\n",
      "Testbeds construction for each ability. We expand the\n",
      "corpus and divided it into 4 testbeds to evaluate the above\n",
      "basic abilities of LLMs. To evaluate the noise robustness,\n",
      "we sample varying numbers of negative documents ac-\n",
      "cording to the desired ratio of noises. For negative rejec-\n",
      "tion, all the external documents are sampled from negative\n",
      "documents. For the information integration ability, we fur-\n",
      "2https://huggingface.co/sentence-transformers/all-mpnet-base-\n",
      "v2 for English; https://huggingface.co/moka-ai/m3e-base for\n",
      "Chinese.\n",
      "System instruction\n",
      "You are an accurate and reliable AI assistant that can \n",
      "answer questions with the help of external documents. \n",
      "Please note that external documents may contain noisy \n",
      "or factually incorrect information. If the information in \n",
      "the document contains the correct answer, you will give \n",
      "an accurate answer. If the information in the document \n",
      "does not contain the answer, you will generate ’I can not \n",
      "answer the question because of the insufficient \n",
      "information in documents.‘ If there are inconsistencies \n",
      "with the facts in some of the documents, please generate \n",
      "the response 'There are factual errors in the provided \n",
      "documents.' and provide the correct answer.\n",
      "User input Instruction\n",
      "Document: \\n{DOCS} \\n\\nQuestion: \\n{QUERY}System instruction\n",
      "你是一个准确和可靠的人工智能助手，\n",
      "能够借助外部文档回答问题，请注意\n",
      "外部文档可能存在噪声事实性错误。\n",
      "如果文档中的信息包含了正确答案，\n",
      "你将进行准确的回答。如果文档中的\n",
      "信息不包含答案，你将生成“文档信\n",
      "息不足，因此我无法基于提供的文档\n",
      "回答该问题。”如果部分文档中存在\n",
      "与事实不一致的错误，请先生成“提\n",
      "供文档的文档存在事实性错误。”，\n",
      "并生成正确答案。\n",
      "User input Instruction\n",
      "文档：\\n{DOCS} \\n\\n问题：\\n{QUERY}English ChineseFigure 3: The instructions used in our experiments, which\n",
      "include a system instruction followed by a user input instruc-\n",
      "tion. The “ {DOCS }” and “ {QUERY }” will be replaced by\n",
      "the external documents and the question.\n",
      "ther construct data based on the above generated questions.\n",
      "This involves expanding or rewriting these questions so that\n",
      "their answers encompass multiple aspects. For example, the\n",
      "question “Who won the MVP of Super Bowl 2023?” can\n",
      "be rewrite as “Who won the MVPs of Super Bowl 2022\n",
      "and 2023?”. Consequently, answering such questions re-\n",
      "quires utilizing information from various documents. Dif-\n",
      "ferent from the first three abilities, the data of counterfactual\n",
      "robustness is constructed solely based on the internal knowl-\n",
      "edge of the model. Based on the aforementioned generated\n",
      "questions mentioned above, we adopt ChatGPT to automat-\n",
      "ically generate its known knowledge. Specifically, we use\n",
      "prompts to allow the model to generate both questions and\n",
      "answers that are already known. For example, based on the\n",
      "question “Who was awarded the 2022 Nobel Prize for Phys-\n",
      "iology and Medicine?”, the model will generate the known\n",
      "question “Who was awarded the 2021 Nobel Prize in Lit-\n",
      "erature?” and answer “ Abdulrazak Gurnah ”. We then man-\n",
      "ually verified the generated answers, and retrieve relevant\n",
      "documents as described above. In order to make documents\n",
      "contain factual errors, we manually modify the answers and\n",
      "replace the corresponding parts in the document.\n",
      "Finally, we collect totally 600 base questions in RGB,\n",
      "and 200 additional questions for the information integration\n",
      "ability and 200 additional questions for counterfactual ro-\n",
      "bustness ability. Half of the instances are in English, and the\n",
      "other half are in Chinese.\n",
      "Evaluation metrics\n",
      "The core of this benchmark is to evaluate whether LLMs can\n",
      "utilize the provided external documents to acquire knowl-\n",
      "edge and generate reasonable answers. We evaluate the re-\n",
      "sponses of LLMs in order to measure above-mentioned four\n",
      "abilities of them.\n",
      "Accuracy is used to measure noise robustness and infor-\n",
      "mation integration We employ an exact matching approach\n",
      "where if the generated text contains an exact match to the\n",
      "answer, it is considered as a correct answer.\n",
      "Rejection rate is used to measure negative rejection.\n",
      "When only noisy documents are provided, LLMs should\n",
      "output the specific content – ”I can not answer the question\n",
      "because of the insufficient information in documents.” (WeEnglish Chinese\n",
      "Noise Ratio 0 0.2 0.4 0.6 0.8 0 0.2 0.4 0.6 0.8\n",
      "ChatGPT (OpenAI 2022) 96.33 94.67 94.00 90.00 76.00 95.67 94.67 91.00 87.67 70.67\n",
      "ChatGLM-6B (THUDM 2023a) 93.67 90.67 89.33 84.67 70.67 94.33 90.67 89.00 82.33 69.00\n",
      "ChatGLM2-6B (THUDM 2023b) 91.33 89.67 83.00 77.33 57.33 86.67 82.33 76.67 72.33 54.00\n",
      "Vicuna-7B-v1.3 (Chiang et al. 2023) 87.67 83.33 86.00 82.33 60.33 85.67 82.67 77.00 69.33 49.67\n",
      "Qwen-7B-Chat (QwenLM 2023) 94.33 91.67 91.00 87.67 73.67 94.00 92.33 88.00 84.33 68.67\n",
      "BELLE-7B-2M (Yunjie Ji 2023) 83.33 81.00 79.00 71.33 64.67 92.00 88.67 85.33 78.33 67.68\n",
      "Table 1: The experimental result of noise robustness measured by accuracy (%) under different noise ratios. We can see that the\n",
      "increasing noise rate poses a challenge for RAG in LLMs.\n",
      "Long-distance information. Evidence uncertainty. Concept confusion.\n",
      "Question Who did Iga Swiatek defeat to win the Qatar Open 2022? What is the name of Apple’s headset? What was Tesla’s revenue in Q1 2022?\n",
      "Answer Anett Kontaveit Vision Pro 18.76 billion\n",
      "DocumentsPositive document\n",
      "In February, Swiatek entered into the Qatar Open ...\n",
      "In the final, she won ... Anett Kontaveit ...\n",
      "Negative document\n",
      "This time, she defeated Ons Jabeur 6-2, 7-6(5) to win\n",
      "the 2022 US Open, ...Positive document\n",
      "Apple (AAPL.O) on Monday unveiled a costly\n",
      "augmented-reality headset called the Vision Pro ...\n",
      "Negative document\n",
      "... is what Gurman believes will be called\n",
      "Apple Reality Pro. ...Positive document\n",
      "Tesla, Inc. (TSLA) reported Q1 FY 2022 earnings results\n",
      "... detailed revenues of $ 18.76 billion ...\n",
      "Negative document\n",
      "...first-quarter earnings for 2022 ...\n",
      "...Automotive revenue reached $16.86 billion...\n",
      "ResponsesIga Swiatek defeated Ons Jabeur in the second round\n",
      "of the Qatar Open 2022 to win the tournament.According to the document, the name of Apple’s\n",
      "headset is Apple Reality Pro.According to the financial results provided in the article,\n",
      "Tesla’s revenue in Q1 2022 was $16.86 billion.\n",
      "Table 2: Error cases of noise robustness, and only one positive document and one negative document are shown. The responses\n",
      "are generated by ChatGLM2-6B. The blue text indicates the matching parts between the document and the question or answer,\n",
      "while the red text highlights the non-matching parts.\n",
      "use instructions to inform the model.). If the model gener-\n",
      "ates this content, it indicates a successful rejection.\n",
      "Error detection rate measures whether the model can\n",
      "detect the factual errors in the documents for counterfactual\n",
      "robustness. When the provided documents contain factual\n",
      "errors, the model should output the specific content – ”There\n",
      "are factual errors in the provided documents.” (We use in-\n",
      "structions to inform the model.). If the model generates this\n",
      "content, it indicates that the model has detected erroneous\n",
      "information in the document.\n",
      "Error correction rate measures whether the model can\n",
      "provide the correct answer after identifying errors for coun-\n",
      "terfactual robustness. The model is asked to generate the cor-\n",
      "rect answer after identifying the factual errors. If the model\n",
      "generates the correct answer, it indicates that the model is\n",
      "capable of correcting errors in the document.\n",
      "The formula for the metrics is\n",
      "ACC =#tt\n",
      "#nums(1)\n",
      "where #tt is the number of correct responses and #nums is\n",
      "the number of instances to be evaluated.\n",
      "Considering that the model may not fully adhere to in-\n",
      "structions, for rejection rate and error detection rate, we\n",
      "also use ChatGPT to conduct additional evaluation of the\n",
      "answers. Specifically, we assess the model’s responses by\n",
      "using instructions and demonstrations to determine if they\n",
      "can reflect information that is not present in the document or\n",
      "identify any factual errors.Experiments\n",
      "In this section, we evaluate the performance of various\n",
      "LLMs, analyze and discuss the results, summarizing the\n",
      "main challenges that existing LLMs encounter when using\n",
      "external knowledge.\n",
      "Settings\n",
      "Task formats. Due to contextual limitations, we provide 5\n",
      "external documents for each question. In our experiments\n",
      "on noise robustness, we evaluate scenarios with noise ra-\n",
      "tios ranging from 0 to 0.8. To comprehensively evaluate the\n",
      "overall capabilities, we have adopted a unified instruction\n",
      "for each language, as shown in Figure 3. The experiments\n",
      "were conducted using an NVIDIA GeForce RTX 3090.\n",
      "Models We conduct evaluation on 6 state-of-the-art large\n",
      "language models which can generate both English and\n",
      "Chinese including ChatGPT3, ChatGPT (OpenAI 2022),\n",
      "ChatGLM-6B (THUDM 2023a), ChatGLM2-6B (THUDM\n",
      "2023b), Vicuna-7b-v1.3 (Chiang et al. 2023), Qwen-7B-\n",
      "Chat (QwenLM 2023), BELLE-7B-2M (Yunjie Ji 2023).\n",
      "Results on Noise Robustness\n",
      "We evaluated the accuracy based on the different noise ratios\n",
      "in external documents, and the results are shown in Table 1.\n",
      "We can see that:\n",
      "(1) RAG can effect improve the responses of LLMs.\n",
      "LLMs have shown strong performance even in the presence\n",
      "of noise, indicating that RAG is a promising way for LLMs\n",
      "to generate accurate and reliable responses.\n",
      "3We use gpt-3.5-turbo api in the experiments.(2) The increasing noise rate poses a challenge for\n",
      "RAG in LLMs. Specifically, when the noise ratio exceeds\n",
      "80%, the accuracy of the model decreases significantly. For\n",
      "example, the performance of ChatGPT has decreased from\n",
      "96.33% to 76.00%, while the performance of ChatGLM2-\n",
      "6B has decreased from 91.33% to 57.33%.\n",
      "Error Analysis. To better comprehend the negative im-\n",
      "pact of noise on model generation, we examined the incor-\n",
      "rect answers and found that these errors typically originate\n",
      "from three reasons, as shown in Table 2.\n",
      "(1) Long-distance information. When dealing with an\n",
      "external document, the model often faces difficulty in iden-\n",
      "tifying the correct answer when the information related to\n",
      "the question is distant from the information related to the an-\n",
      "swer. This scenario is quite common as longer texts are fre-\n",
      "quently encountered on the internet. In such cases, it is typ-\n",
      "ical for the question’s information to be initially presented\n",
      "at the start of the document and subsequently referred to us-\n",
      "ing pronouns. In Table 2, the question information (”Qatar\n",
      "Open 2022”) is only mentioned once at the beginning and is\n",
      "far from where the answer text ”Anett Kontaveit” appears.\n",
      "This situation may cause LLMs to depend on information\n",
      "from other documents and create false impressions, i.e., hal-\n",
      "lucination.\n",
      "(2) Evidence uncertainty. Before highly anticipated\n",
      "events, like the release of new Apple products or the an-\n",
      "nouncement of the Oscars, there is often a significant\n",
      "amount of speculative information circulating on the inter-\n",
      "net. Although the relevant documents explicitly state that\n",
      "it is uncertain or speculative content, they can still impact\n",
      "on the retrieval-augmented generation of LLMs. In Table 2,\n",
      "when the noise ratio increases, the content of erroneous\n",
      "documents is all about some people’s predictions about the\n",
      "name of the headset (“Apple Reality Pro”). Even if there is a\n",
      "correct answer (“Vision Pro”) in the relevant documents, the\n",
      "model can still be misled by uncertain evidences.\n",
      "(3) Concept confusion. The concepts in external docu-\n",
      "ments may be similar to, but different from, the concepts in\n",
      "the question. This can cause confusion for LLMs and make\n",
      "LLMs generate incorrect answers. In Table 2, the model an-\n",
      "swer focuses on the concept “automotive revenue” in the\n",
      "document rather than “revenue” in the question.\n",
      "Based on the analysis above, we have identified certain\n",
      "limitations in LLMs regarding retrieval-augmented genera-\n",
      "tion. To effectively handle the vast amount of noise present\n",
      "on the internet, further detailed enhancements are required\n",
      "for the model such as long documents modeling and precise\n",
      "concept comprehension.\n",
      "Results on Negative Rejection testbed\n",
      "We evaluated the rejection rate when only noise documents\n",
      "were provided. The results are shown in Table 3. In addi-\n",
      "tion to evaluating the rejection rate through exact matching\n",
      "(Rej in Table 3), we also utilize ChatGPT to determine if\n",
      "the responses from the LLMs contain any rejection informa-\n",
      "tion (Rej∗in Table 3). We can see that: Negative Rejection\n",
      "poses a challenge for RAG in LLMs. The highest rejection\n",
      "rates for LLMs in English and Chinese were only 45% andLanguages English Chinese\n",
      "Rej Rej∗Rej Rej∗\n",
      "ChatGPT 24.67 45.00 5.33 43.33\n",
      "ChatGLM-6B 9.00 25.00 6.33 17.00\n",
      "ChatGLM2-6B 10.33 41.33 6.33 36.33\n",
      "Vicuna-7B-v1.3 17.00 33.33 3.37 24.67\n",
      "Qwen-7B-Chat 31.00 35.67 8.67 25.33\n",
      "BELLE-7B-2M 5.67 32.33 5.33 13.67\n",
      "Table 3: The result of negative rejection. Rej means the re-\n",
      "jection rate (%) and Rej∗means the rejection rate evaluated\n",
      "by ChatGPT. We can see that negative rejection poses a chal-\n",
      "lenge for RAG in LLMs.\n",
      "Question Answer Response\n",
      "who will direct\n",
      "Irredeemable film?Jeymes\n",
      "SamuelThe information provided in the document\n",
      "is that Fox has attached ”The Big Short”\n",
      "writer-director Adam McKay to its movie\n",
      "adaptation of ”Irredeemable” from Boom!\n",
      "Comics. Therefore, the answer is Adam McKay.\n",
      "Which country won the\n",
      "most medals at the\n",
      "2022 Winter Olympics?NorwayBased on the information provided in the\n",
      "document, the country that won the most medals\n",
      "at the 2022 Winter Olympics is Germany.\n",
      "Germany has won a total of 31 medals, including 14\n",
      "gold medals, 14 bronze medals, and 31 silver medals.\n",
      "Table 4: Error cases of negative rejection generated by\n",
      "ChatGLM2-6B. The red text highlights the error answers.\n",
      "43.33%, respectively. This suggests that LLMs can be easily\n",
      "misled by noisy documents, leading to incorrect answers.\n",
      "In addition, through comparing Rej and Rej∗, we found\n",
      "that LLMs fail to strictly follow instructions, and they often\n",
      "generate unpredictable responses, which make it hard to use\n",
      "them as state triggers (such as for recognizing rejection).\n",
      "We conduct case studies in Table 4. The first error is\n",
      "because of Evidence uncertainty . Although the document\n",
      "only mentions contact with “Adam McKay” and does not\n",
      "explicitly state that he is the director of the movie, the\n",
      "model still concludes that he holds this role. The first er-\n",
      "ror is because of Concept confusion . The information pro-\n",
      "vided in the answer pertains to “the 2018 Winter Olympics”\n",
      "instead of “the 2022 Olympics” mentioned in the question.\n",
      "Retrieval-augmented generation poses a greater challenge of\n",
      "negative rejection compared to answer directly as it presents\n",
      "relevant documents that could potentially mislead the LLMs\n",
      "and result in incorrect responses. In future developments, it\n",
      "will be crucial for LLMs to enhance their ability to accu-\n",
      "rately match questions with the appropriate documents.\n",
      "Results on Information Integration testbed\n",
      "We evaluated the accuracy based on the different noise ratios\n",
      "in external documents, and the results are shown in Table 5.\n",
      "When comparing the model to Table 1, we observed that\n",
      "it has a weak information integration ability, which in turn\n",
      "affects its noise robustness. We can see that:\n",
      "(1) Information integration poses a challenge for RAG\n",
      "in LLMs. Even without noise, the highest accuracy of LLMs\n",
      "can only reach 60% and 67% for English and Chinese,\n",
      "respectively. After adding noise, the highest accuracy de-English Chinese\n",
      "Noise Ratio 0 0.2 0.4 0 0.2 0.4\n",
      "ChatGPT 55 51 34 63 58 47\n",
      "ChatGLM-6B 45 36 35 60 53 52\n",
      "ChatGLM2-6B 34 32 21 44 43 32\n",
      "Vicuna-7B-v1.3 60 53 43 43 36 25\n",
      "Qwen-7B-Chat 55 50 37 67 56 55\n",
      "BELLE-7B-2M 40 34 24 49 41 38\n",
      "Table 5: The experimental result of information integration\n",
      "measured by accuracy (%) under different noise ratios. We\n",
      "can see that information integration poses a challenge for\n",
      "RAG in LLMs.\n",
      "Question Answer Response Errors\n",
      "What groupings are France and\n",
      "Germany in World Cup 2022?Group D\n",
      "Group EFrance and Germany are in\n",
      "Group D of the 2022 FIFA\n",
      "World Cup.Merging\n",
      "Error\n",
      "Who were the MVP of Super\n",
      "Bowl 2022 and 2023?Cooper Kupp\n",
      "Patrick\n",
      "MahomesAccording to the provided\n",
      "document, the MVP of Super\n",
      "Bowl LVI was Cooper Kupp\n",
      "of the Los Angeles Rams.Ignoring\n",
      "Error\n",
      "What films won the 2022 and\n",
      "2023 Academy Awards for\n",
      "Best Picture?CODA\n",
      "Everything\n",
      "Everywhere\n",
      "All at OnceThe film CODA won the\n",
      "award for Best Picture at the\n",
      "95th Academy Awards\n",
      "ceremony held on 2023.Misalignment\n",
      "Error\n",
      "Table 6: Error cases of information integration, the re-\n",
      "sponses are generated by ChatGLM2-6B. The blue and red\n",
      "texts represent the answers to two sub-questions.\n",
      "creases to 43% and 55%. These results suggest that LLMs\n",
      "struggle with integrating information effectively and are not\n",
      "well-suited for directly answering complex questions.\n",
      "(2) Complex questions are more challenging for RAG\n",
      "with noisy documents. Performance decline becomes sig-\n",
      "nificant when the noise ratio is 0.4, but for simple problems,\n",
      "a significant decline occurs only at a noise ratio of 0.8. This\n",
      "indicates that complex problems are more vulnerable to in-\n",
      "terference from noise. We speculate that this is because solv-\n",
      "ing complex problems requires integrating information from\n",
      "multiple documents, and this information can be considered\n",
      "as noise to each other, making it harder for the model to ex-\n",
      "tract relevant information from the documents.\n",
      "Error Analysis. We conducted an error analysis on\n",
      "ChatGLM2-6B (noise ratio is 0). Apart from the similar er-\n",
      "rors founded in the noise robustness experiment (38% of the\n",
      "total), there are also three types of unique errors. We have\n",
      "presented these cases in Table 6.\n",
      "(1) Merging Error (28% of the total). The model some-\n",
      "times merges the answers of the two sub-questions, resulting\n",
      "in an error. It mistakenly uses the answer from one question\n",
      "to address both two questions. At this point, the model will\n",
      "disregard any documents related to one sub-question. For\n",
      "example, in Table 6, it incorrectly states that Group D is the\n",
      "World Cup group for both France and Germany, while in fact\n",
      "Germany is actually assigned to Group E.\n",
      "(2) Ignoring Error (28% of the total). Sometimes, the\n",
      "model may ignore one of the sub-questions and only answer\n",
      "the other. This error occurs when the model lacks a complete\n",
      "understanding of the problem and fails to recognize that it\n",
      "consists of multiple sub-problems. As a result, the modelAcc Acc doc ED ED∗CR\n",
      "ChatGPT-zh 91 17 1 3 33.33\n",
      "Qwen-7B-Chat-zh 77 12 5 4 25.00\n",
      "ChatGPT-en 89 9 8 7 57.14\n",
      "Table 7: The result of counterfactual robustness. ACC is the\n",
      "accuracy (%) of LLMs without external documents. ACC doc\n",
      "is the accuracy (%) of LLMs with counterfactual documents.\n",
      "ED and ED∗are error detection rates evaluated by exact\n",
      "matching and ChatGPT, respectively. CR is the error cor-\n",
      "rection rate.\n",
      "only considers relevant documents for one sub-problem in\n",
      "order to generate an answer, disregarding the question posed\n",
      "by another sub-problem. For example, in Table 6, the model\n",
      "only provides the answer for the MVP of Super Bowl 2022\n",
      "and does not consider 2023.\n",
      "(3) Misalignment Error (6% of the total). Sometimes,\n",
      "the model incorrectly identifies the documents for one sub-\n",
      "question as the documents for another sub-question, leading\n",
      "to misaligned answers. For example, in Table 6, the third an-\n",
      "swer has two errors: an ignoring error and a misalignment er-\n",
      "ror. Firstly, the model only mentioned the Best Picture of the\n",
      "2023 (95th) Academy Awards, completely disregarding the\n",
      "2022 awards. Additionally, it incorrectly stated that “CODA”\n",
      "is the Best Picture of 2023 when it was actually awarded as\n",
      "the Best Picture in 2022.\n",
      "The errors mentioned above are primarily caused by the\n",
      "limited understanding of complex questions, which hinders\n",
      "the ability to effectively utilize information from different\n",
      "sub-problems. The key lies in improving the model’s rea-\n",
      "soning capability. One possible solution is to use a chain-of-\n",
      "thought approach to break down complex problems (Zhou\n",
      "et al. 2023a; Xu et al. 2023b; Drozdov et al. 2023). How-\n",
      "ever, these methods slow down the inference speed and can-\n",
      "not provide timely responses.\n",
      "Results on Counterfactual Robustness testbed\n",
      "In order to ensure that LLMs possess relevant knowledge,\n",
      "we assess their performance by directly asking them ques-\n",
      "tions. We only consider LLMs that have an accuracy rate of\n",
      "over 70%. The results are shown in Table 7. We present the\n",
      "following metrics: accuracy without any documents, accu-\n",
      "racy with counterfactual documents, error detection rates,\n",
      "and error correction rates. We can see that It is hard for\n",
      "LLMs to identify and correct factual errors in the docu-\n",
      "ments. This suggests that the model can be easily misled by\n",
      "documents containing incorrect facts.\n",
      "It is important to note that retrieval-augmented generation\n",
      "is not designed to automatically address factual errors within\n",
      "a given context, as this contradicts the underlying assump-\n",
      "tion that the model lacks knowledge and relies on retrieved\n",
      "documents for additional information. However, this issue is\n",
      "crucial in practical applications due to the abundance of fake\n",
      "news on the internet. Existing LLMs do not have a safeguard\n",
      "to handle inaccurate responses caused by misinformation. In\n",
      "fact, they heavily depend on the information they retrieve.\n",
      "Even when LLMs contain the internal knowledge about thequestions, they often trust false information that is retrieved.\n",
      "This presents significant a challenge for the future develop-\n",
      "ment of RAG in LLMs.\n",
      "Conclusion\n",
      "In this paper, we evaluated four abilities of retrieval-\n",
      "augmented generation in LLMs: noise robustness, nega-\n",
      "tive rejection, information integration, and counterfactual\n",
      "robustness. To conduct the evaluation, we built Retrieval-\n",
      "Augmented Generation Benchmark (RGB). The instances of\n",
      "RGB are generated from latest news articles and the external\n",
      "documents obtained from search engines. The experimental\n",
      "results suggest that current LLMs have limitations in the 4\n",
      "abilities. This indicates that there is still a significant amount\n",
      "of work needed to effectively apply RAG to LLMs. To en-\n",
      "sure accurate and reliable responses from LLMs, it is crucial\n",
      "to exercise caution and carefully design for RAG.\n",
      "References\n",
      "Adlakha, V .; BehnamGhader, P.; Lu, X. H.; Meade, N.; and\n",
      "Reddy, S. 2023. Evaluating Correctness and Faithfulness\n",
      "of Instruction-Following Models for Question Answering.\n",
      "arXiv:2307.16877.\n",
      "Bang, Y .; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie,\n",
      "B.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; Do, Q. V .; Xu,\n",
      "Y .; and Fung, P. 2023. A Multitask, Multilingual, Multi-\n",
      "modal Evaluation of ChatGPT on Reasoning, Hallucination,\n",
      "and Interactivity. arXiv:2302.04023.\n",
      "Bian, N.; Liu, P.; Han, X.; Lin, H.; Lu, Y .; He, B.; and\n",
      "Sun, L. 2023. A Drop of Ink Makes a Million Think: The\n",
      "Spread of False Information in Large Language Models.\n",
      "arXiv:2305.04812.\n",
      "Borgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Ruther-\n",
      "ford, E.; Millican, K.; van den Driessche, G.; Lespiau, J.-B.;\n",
      "Damoc, B.; Clark, A.; de Las Casas, D.; Guy, A.; Menick, J.;\n",
      "Ring, R.; Hennigan, T.; Huang, S.; Maggiore, L.; Jones, C.;\n",
      "Cassirer, A.; Brock, A.; Paganini, M.; Irving, G.; Vinyals,\n",
      "O.; Osindero, S.; Simonyan, K.; Rae, J. W.; Elsen, E.; and\n",
      "Sifre, L. 2022. Improving language models by retrieving\n",
      "from trillions of tokens. arXiv:2112.04426.\n",
      "Cai, D.; Wang, Y .; Bi, W.; Tu, Z.; Liu, X.; Lam, W.; and\n",
      "Shi, S. 2019a. Skeleton-to-Response: Dialogue Genera-\n",
      "tion Guided by Retrieval Memory. In Proceedings of the\n",
      "2019 Conference of the North American Chapter of the As-\n",
      "sociation for Computational Linguistics: Human Language\n",
      "Technologies, Volume 1 (Long and Short Papers) , 1219–\n",
      "1228. Minneapolis, Minnesota: Association for Computa-\n",
      "tional Linguistics.\n",
      "Cai, D.; Wang, Y .; Bi, W.; Tu, Z.; Liu, X.; and Shi, S.\n",
      "2019b. Retrieval-guided Dialogue Response Generation via\n",
      "a Matching-to-Generation Framework. In Proceedings of\n",
      "the 2019 Conference on Empirical Methods in Natural Lan-\n",
      "guage Processing and the 9th International Joint Confer-\n",
      "ence on Natural Language Processing (EMNLP-IJCNLP) ,\n",
      "1866–1875. Hong Kong, China: Association for Computa-\n",
      "tional Linguistics.Cao, M.; Dong, Y .; Wu, J.; and Cheung, J. C. K. 2020. Fac-\n",
      "tual Error Correction for Abstractive Summarization Mod-\n",
      "els. In Proceedings of the 2020 Conference on Empirical\n",
      "Methods in Natural Language Processing (EMNLP) , 6251–\n",
      "6258. Online: Association for Computational Linguistics.\n",
      "Chang, Y .; Wang, X.; Wang, J.; Wu, Y .; Yang, L.; Zhu,\n",
      "K.; Chen, H.; Yi, X.; Wang, C.; Wang, Y .; Ye, W.;\n",
      "Zhang, Y .; Chang, Y .; Yu, P. S.; Yang, Q.; and Xie, X.\n",
      "2023. A Survey on Evaluation of Large Language Models.\n",
      "arXiv:2307.03109.\n",
      "Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\n",
      "Zheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,\n",
      "I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\n",
      "Impressing GPT-4 with 90%* ChatGPT Quality.\n",
      "Cui, J.; Li, Z.; Yan, Y .; Chen, B.; and Yuan, L. 2023. Chat-\n",
      "Law: Open-Source Legal Large Language Model with Inte-\n",
      "grated External Knowledge Bases. arXiv:2306.16092.\n",
      "Drozdov, A.; Sch ¨arli, N.; Aky ¨urek, E.; Scales, N.; Song,\n",
      "X.; Chen, X.; Bousquet, O.; and Zhou, D. 2023. Compo-\n",
      "sitional Semantic Parsing with Large Language Models. In\n",
      "The Eleventh International Conference on Learning Repre-\n",
      "sentations .\n",
      "Edward Beeching, N. H. S. H. N. L. N. R. O. S. L. T.\n",
      "T. W., Cl ´ementine Fourrier. 2023. Open LLM Leader-\n",
      "board. https://huggingface.co/spaces/HuggingFaceH4/\n",
      "open llmleaderboard.\n",
      "Guo, B.; Zhang, X.; Wang, Z.; Jiang, M.; Nie, J.; Ding, Y .;\n",
      "Yue, J.; and Wu, Y . 2023. How Close is ChatGPT to Hu-\n",
      "man Experts? Comparison Corpus, Evaluation, and Detec-\n",
      "tion. arXiv:2301.07597.\n",
      "Guu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.-W.\n",
      "2020. REALM: Retrieval-Augmented Language Model Pre-\n",
      "Training. In Proceedings of the 37th International Confer-\n",
      "ence on Machine Learning , ICML’20. JMLR.org.\n",
      "He, H.; Zhang, H.; and Roth, D. 2022. Rethinking\n",
      "with Retrieval: Faithful Large Language Model Inference.\n",
      "arXiv:2301.00303.\n",
      "Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\n",
      "Song, D.; and Steinhardt, J. 2021. Measuring Massive Mul-\n",
      "titask Language Understanding. In International Conference\n",
      "on Learning Representations .\n",
      "Huang, Y .; Bai, Y .; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.;\n",
      "Liu, J.; Lv, C.; Zhang, Y .; Lei, J.; Fu, Y .; Sun, M.; and He,\n",
      "J. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese\n",
      "Evaluation Suite for Foundation Models. arXiv preprint\n",
      "arXiv:2305.08322 .\n",
      "Izacard, G.; and Grave, E. 2021. Leveraging Passage Re-\n",
      "trieval with Generative Models for Open Domain Ques-\n",
      "tion Answering. In Proceedings of the 16th Conference of\n",
      "the European Chapter of the Association for Computational\n",
      "Linguistics: Main Volume , 874–880. Online: Association for\n",
      "Computational Linguistics.\n",
      "Izacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni,\n",
      "F.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and\n",
      "Grave, E. 2022. Atlas: Few-shot Learning with Retrieval\n",
      "Augmented Language Models. arXiv:2208.03299.Ji, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii, E.;\n",
      "Bang, Y . J.; Madotto, A.; and Fung, P. 2023. Survey of Hal-\n",
      "lucination in Natural Language Generation. ACM Comput.\n",
      "Surv. , 55(12).\n",
      "Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V .;\n",
      "Goyal, N.; K ¨uttler, H.; Lewis, M.; Yih, W.-t.; Rockt ¨aschel,\n",
      "T.; Riedel, S.; and Kiela, D. 2020. Retrieval-Augmented\n",
      "Generation for Knowledge-Intensive NLP Tasks. In Pro-\n",
      "ceedings of the 34th International Conference on Neural\n",
      "Information Processing Systems , NIPS’20. Red Hook, NY ,\n",
      "USA: Curran Associates Inc. ISBN 9781713829546.\n",
      "Li, D.; Rawat, A. S.; Zaheer, M.; Wang, X.; Lukasik, M.;\n",
      "Veit, A.; Yu, F.; and Kumar, S. 2023a. Large Language\n",
      "Models with Controllable Working Memory. In Findings of\n",
      "the Association for Computational Linguistics: ACL 2023 ,\n",
      "1774–1793. Toronto, Canada: Association for Computa-\n",
      "tional Linguistics.\n",
      "Li, X.; Zhang, T.; Dubois, Y .; Taori, R.; Gulrajani, I.;\n",
      "Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023b. Al-\n",
      "pacaEval: An Automatic Evaluator of Instruction-following\n",
      "Models. https://github.com/tatsu-lab/alpaca eval.\n",
      "Li, X.; Zhu, X.; Ma, Z.; Liu, X.; and Shah, S. 2023c. Are\n",
      "ChatGPT and GPT-4 General-Purpose Solvers for Financial\n",
      "Text Analytics? An Examination on Several Typical Tasks.\n",
      "arXiv:2305.05862.\n",
      "Liu, N. F.; Zhang, T.; and Liang, P. 2023. Evaluating Verifi-\n",
      "ability in Generative Search Engines. arXiv:2304.09848.\n",
      "Maynez, J.; Narayan, S.; Bohnet, B.; and McDonald, R.\n",
      "2020. On Faithfulness and Factuality in Abstractive Sum-\n",
      "marization. In Proceedings of the 58th Annual Meeting of\n",
      "the Association for Computational Linguistics , 1906–1919.\n",
      "Online: Association for Computational Linguistics.\n",
      "OpenAI. 2022. Chatgpt: Optimizing language models for\n",
      "dialogue. https://openai.com/blog/chatgpt.\n",
      "Peng, B.; Galley, M.; He, P.; Cheng, H.; Xie, Y .; Hu, Y .;\n",
      "Huang, Q.; Liden, L.; Yu, Z.; Chen, W.; and Gao, J. 2023.\n",
      "Check Your Facts and Try Again: Improving Large Lan-\n",
      "guage Models with External Knowledge and Automated\n",
      "Feedback. arXiv:2302.12813.\n",
      "Qin, Y .; Liang, S.; Ye, Y .; Zhu, K.; Yan, L.; Lu, Y .; Lin, Y .;\n",
      "Cong, X.; Tang, X.; Qian, B.; Zhao, S.; Tian, R.; Xie, R.;\n",
      "Zhou, J.; Gerstein, M.; Li, D.; Liu, Z.; and Sun, M. 2023.\n",
      "ToolLLM: Facilitating Large Language Models to Master\n",
      "16000+ Real-world APIs. arXiv:2307.16789.\n",
      "QwenLM. 2023. Qwen-7B. https://github.com/QwenLM/\n",
      "Qwen-7B.\n",
      "Raunak, V .; Menezes, A.; and Junczys-Dowmunt, M. 2021.\n",
      "The Curious Case of Hallucinations in Neural Machine\n",
      "Translation. In Proceedings of the 2021 Conference of the\n",
      "North American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies , 1172–\n",
      "1183. Online: Association for Computational Linguistics.\n",
      "Ren, R.; Wang, Y .; Qu, Y .; Zhao, W. X.; Liu, J.; Tian, H.;\n",
      "Wu, H.; Wen, J.-R.; and Wang, H. 2023. Investigating the\n",
      "Factual Knowledge Boundary of Large Language Models\n",
      "with Retrieval Augmentation. arXiv:2307.11019.Shen, X.; Chen, Z.; Backes, M.; and Zhang, Y . 2023. In\n",
      "ChatGPT We Trust? Measuring and Characterizing the Re-\n",
      "liability of ChatGPT. arXiv:2304.08979.\n",
      "Shi, W.; Min, S.; Yasunaga, M.; Seo, M.; James, R.;\n",
      "Lewis, M.; Zettlemoyer, L.; and tau Yih, W. 2023. RE-\n",
      "PLUG: Retrieval-Augmented Black-Box Language Models.\n",
      "arXiv:2301.12652.\n",
      "THUDM. 2023a. ChatGLM-6B. https://github.com/\n",
      "THUDM/ChatGLM-6B.\n",
      "THUDM. 2023b. ChatGLM2-6B. https://github.com/\n",
      "THUDM/ChatGLM2-6B.\n",
      "Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\n",
      "Babaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\n",
      "S.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucu-\n",
      "rull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.;\n",
      "Gao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hosseini,\n",
      "S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa, M.;\n",
      "Kloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.;\n",
      "Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\n",
      "X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poul-\n",
      "ton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.;\n",
      "Silva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang,\n",
      "B.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.;\n",
      "Zarov, I.; Zhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Ro-\n",
      "driguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023.\n",
      "Llama 2: Open Foundation and Fine-Tuned Chat Models.\n",
      "arXiv:2307.09288.\n",
      "Trivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal,\n",
      "A. 2023. Interleaving Retrieval with Chain-of-Thought Rea-\n",
      "soning for Knowledge-Intensive Multi-Step Questions. In\n",
      "Proceedings of the 61st Annual Meeting of the Associa-\n",
      "tion for Computational Linguistics (Volume 1: Long Papers) ,\n",
      "10014–10037. Toronto, Canada: Association for Computa-\n",
      "tional Linguistics.\n",
      "Wang, A.; Pruksachatkun, Y .; Nangia, N.; Singh, A.;\n",
      "Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019a. Su-\n",
      "perGLUE: A Stickier Benchmark for General-Purpose Lan-\n",
      "guage Understanding Systems . Red Hook, NY , USA: Curran\n",
      "Associates Inc.\n",
      "Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\n",
      "Bowman, S. R. 2019b. GLUE: A Multi-Task Benchmark\n",
      "and Analysis Platform for Natural Language Understanding.\n",
      "InInternational Conference on Learning Representations .\n",
      "Xu, G.; Liu, J.; Yan, M.; Xu, H.; Si, J.; Zhou, Z.; Yi, P.;\n",
      "Gao, X.; Sang, J.; Zhang, R.; Zhang, J.; Peng, C.; Huang, F.;\n",
      "and Zhou, J. 2023a. CValues: Measuring the Values of Chi-\n",
      "nese Large Language Models from Safety to Responsibility.\n",
      "arXiv:2307.09705.\n",
      "Xu, S.; Pang, L.; Shen, H.; Cheng, X.; and Chua, T.-\n",
      "S. 2023b. Search-in-the-Chain: Towards Accurate, Credi-\n",
      "ble and Traceable Large Language Models for Knowledge-\n",
      "intensive Tasks. arXiv:2304.14732.\n",
      "Yunjie Ji, Y . G. Y . P. Q. N. B. M. X. L., Yong Deng. 2023.\n",
      "BELLE: Bloom-Enhanced Large Language model Engine.\n",
      "https://github.com/LianjiaTech/BELLE.Zhang, W.; Aljunied, S. M.; Gao, C.; Chia, Y . K.; and Bing,\n",
      "L. 2023. M3Exam: A Multilingual, Multimodal, Multilevel\n",
      "Benchmark for Examining Large Language Models.\n",
      "Zhong, W.; Cui, R.; Guo, Y .; Liang, Y .; Lu, S.; Wang,\n",
      "Y .; Saied, A.; Chen, W.; and Duan, N. 2023. AGIEval:\n",
      "A Human-Centric Benchmark for Evaluating Foundation\n",
      "Models. arXiv:2304.06364.\n",
      "Zhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\n",
      "X.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q. V .; and\n",
      "Chi, E. H. 2023a. Least-to-Most Prompting Enables Com-\n",
      "plex Reasoning in Large Language Models. In The Eleventh\n",
      "International Conference on Learning Representations .\n",
      "Zhou, S.; Alon, U.; Xu, F. F.; Jiang, Z.; and Neubig, G.\n",
      "2023b. DocPrompting: Generating Code by Retrieving the\n",
      "Docs. In The Eleventh International Conference on Learn-\n",
      "ing Representations .\n",
      "==================================================\n",
      "\n",
      "Document 4:\n",
      "Generation-Augmented Retrieval for Open-Domain Question Answering\n",
      "Yuning Mao1\u0003, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\n",
      "Jianfeng Gao3, Jiawei Han1, Weizhu Chen2\n",
      "1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\n",
      "1fyuningm2, hanjg@illinois.edu\n",
      "2;3fpenhe, xiaodl, yeshe, jfgao,wzchen g@microsoft.com\n",
      "Abstract\n",
      "We propose Generation-Augmented Retrieval\n",
      "(GAR) for answering open-domain questions,\n",
      "which augments a query through text genera-\n",
      "tion of heuristically discovered relevant con-\n",
      "texts without external resources as supervi-\n",
      "sion. We demonstrate that the generated con-\n",
      "texts substantially enrich the semantics of the\n",
      "queries and G ARwith sparse representations\n",
      "(BM25) achieves comparable or better per-\n",
      "formance than state-of-the-art dense retrieval\n",
      "methods such as DPR (Karpukhin et al., 2020).\n",
      "We show that generating diverse contexts for a\n",
      "query is beneﬁcial as fusing their results con-\n",
      "sistently yields better retrieval accuracy. More-\n",
      "over, as sparse and dense representations are\n",
      "often complementary, G ARcan be easily com-\n",
      "bined with DPR to achieve even better per-\n",
      "formance. G ARachieves state-of-the-art per-\n",
      "formance on Natural Questions and TriviaQA\n",
      "datasets under the extractive QA setup when\n",
      "equipped with an extractive reader, and con-\n",
      "sistently outperforms other retrieval methods\n",
      "when the same generative reader is used.1\n",
      "1 Introduction\n",
      "Open-domain question answering (OpenQA) aims\n",
      "to answer factoid questions without a pre-speciﬁed\n",
      "domain and has numerous real-world applications.\n",
      "In OpenQA, a large collection of documents ( e.g.,\n",
      "Wikipedia) are often used to seek information per-\n",
      "taining to the questions. One of the most com-\n",
      "mon approaches uses a retriever-reader architecture\n",
      "(Chen et al., 2017), which ﬁrst retrieves a small sub-\n",
      "set of documents using the question as the query\n",
      "and then reads the retrieved documents to extract\n",
      "(or generate) an answer. The retriever is crucial as it\n",
      "is infeasible to examine every piece of information\n",
      "in the entire document collection ( e.g., millions\n",
      "of Wikipedia passages) and the retrieval accuracy\n",
      "bounds the performance of the (extractive) reader.\n",
      "\u0003Work was done during internship at Microsoft Azure AI.\n",
      "1Our code and retrieval results are available at https:\n",
      "//github.com/morningmoni/GAR .Early OpenQA systems (Chen et al., 2017)\n",
      "use classic retrieval methods such as TF-IDF and\n",
      "BM25 with sparse representations. Sparse methods\n",
      "are lightweight and efﬁcient, but unable to per-\n",
      "form semantic matching and fail to retrieve rele-\n",
      "vant passages without lexical overlap. More re-\n",
      "cently, methods based on dense representations\n",
      "(Guu et al., 2020; Karpukhin et al., 2020) learn to\n",
      "embed queries and passages into a latent vector\n",
      "space, in which text similarity beyond lexical over-\n",
      "lap can be measured. Dense retrieval methods can\n",
      "retrieve semantically relevant but lexically differ-\n",
      "ent passages and often achieve better performance\n",
      "than sparse methods. However, the dense mod-\n",
      "els are more computationally expensive and suffer\n",
      "from information loss as they condense the entire\n",
      "text sequence into a ﬁxed-size vector that does not\n",
      "guarantee exact matching (Luan et al., 2020).\n",
      "There have been some recent studies on query re-\n",
      "formulation with text generation for other retrieval\n",
      "tasks, which, for example, rewrite the queries to\n",
      "context-independent (Yu et al., 2020; Lin et al.,\n",
      "2020; Vakulenko et al., 2020) or well-formed (Liu\n",
      "et al., 2019) ones. However, these methods re-\n",
      "quire either task-speciﬁc data ( e.g., conversational\n",
      "contexts, ill-formed queries) or external resources\n",
      "such as paraphrase data (Zaiem and Sadat, 2019;\n",
      "Wang et al., 2020) that cannot or do not trans-\n",
      "fer well to OpenQA. Also, some rely on time-\n",
      "consuming training process like reinforcement\n",
      "learning (RL) (Nogueira and Cho, 2017; Liu et al.,\n",
      "2019; Wang et al., 2020) that is not efﬁcient enough\n",
      "for OpenQA (more discussions in Sec. 2).\n",
      "In this paper, we propose Generation-\n",
      "Augmented Retrieval ( GAR), which augments\n",
      "a query through text generation of a pre-trained\n",
      "language model (PLM). Different from prior\n",
      "studies that reformulate queries, GARdoes not\n",
      "require external resources or downstream feedback\n",
      "via RL as supervision, because it does not rewrite\n",
      "the query but expands it with heuristically discov-arXiv:2009.08553v4  [cs.CL]  6 Aug 2021ered relevant contexts, which are fetched from\n",
      "PLMs and provide richer background information\n",
      "(Table 2). For example, by prompting a PLM\n",
      "to generate the title of a relevant passage given\n",
      "a query and appending the generated title to the\n",
      "query, it becomes easier to retrieve that relevant\n",
      "passage. Intuitively, the generated contexts\n",
      "explicitly express the search intent not presented\n",
      "in the original query. As a result, GARwith\n",
      "sparse representations achieves comparable or\n",
      "even better performance than state-of-the-art\n",
      "approaches (Karpukhin et al., 2020; Guu et al.,\n",
      "2020) with dense representations of the original\n",
      "queries, while being more lightweight and efﬁcient\n",
      "in terms of both training and inference (including\n",
      "the cost of the generation model) (Sec. 6.4).\n",
      "Speciﬁcally, we expand the query (question) by\n",
      "adding relevant contexts as follows. We conduct\n",
      "seq2seq learning with the question as the input\n",
      "and various freely accessible in-domain contexts as\n",
      "the output such as the answer, the sentence where\n",
      "the answer belongs to , and the title of a passage\n",
      "that contains the answer . We then append the gen-\n",
      "erated contexts to the question as the generation-\n",
      "augmented query for retrieval. We demonstrate\n",
      "that using multiple contexts from diverse gener-\n",
      "ation targets is beneﬁcial as fusing the retrieval\n",
      "results of different generation-augmented queries\n",
      "consistently yields better retrieval accuracy.\n",
      "We conduct extensive experiments on the Nat-\n",
      "ural Questions (NQ) (Kwiatkowski et al., 2019)\n",
      "and TriviaQA (Trivia) (Joshi et al., 2017) datasets.\n",
      "The results reveal four major advantages of GAR:\n",
      "(1)GAR, combined with BM25, achieves signif-\n",
      "icant gains over the same BM25 model that uses\n",
      "the original queries or existing unsupervised query\n",
      "expansion (QE) methods. (2) GARwith sparse rep-\n",
      "resentations (BM25) achieves comparable or even\n",
      "better performance than the current state-of-the-art\n",
      "retrieval methods, such as DPR (Karpukhin et al.,\n",
      "2020), that use dense representations. (3) Since\n",
      "GARuses sparse representations to measure lexical\n",
      "overlap2, it is complementary to dense representa-\n",
      "tions: by fusing the retrieval results of GARand\n",
      "DPR (denoted as GAR+), we obtain consistently\n",
      "better performance than either method used individ-\n",
      "ually. (4) GARoutperforms DPR in the end-to-end\n",
      "QA performance (EM) when the same extractive\n",
      "reader is used: EM=41.8 (43.8 for GAR+) on NQ\n",
      "2Strictly speaking, GARwith sparse representations han-\n",
      "dles semantics before retrieval by enriching the queries, while\n",
      "maintaining the advantage of exact matching.and 62.7 on Trivia, creating new state-of-the-art re-\n",
      "sults for extractive OpenQA. GARalso outperforms\n",
      "other retrieval methods under the generative setup\n",
      "when the same generative reader is used: EM=38.1\n",
      "(45.3 for G AR+) on NQ and 62.2 on Trivia.\n",
      "Contributions . (1) We propose Generation-\n",
      "Augmented Retrieval ( GAR), which augments\n",
      "queries with heuristically discovered relevant con-\n",
      "texts through text generation without external su-\n",
      "pervision or time-consuming downstream feedback.\n",
      "(2) We show that using generation-augmented\n",
      "queries achieves signiﬁcantly better retrieval and\n",
      "QA results than using the original queries or ex-\n",
      "isting unsupervised QE methods. (3) We show\n",
      "thatGAR, combined with a simple BM25 model,\n",
      "achieves new state-of-the-art performance on two\n",
      "benchmark datasets in extractive OpenQA and com-\n",
      "petitive results in the generative setting.\n",
      "2 Related Work\n",
      "Conventional Query Expansion .GARshares\n",
      "some merits with query expansion (QE) meth-\n",
      "ods based on pseudo relevance feedback (Rocchio,\n",
      "1971; Abdul-Jaleel et al., 2004; Lv and Zhai, 2010)\n",
      "in that they both expand the queries with relevant\n",
      "contexts (terms) without the use of external super-\n",
      "vision. GARis superior as it expands the queries\n",
      "with knowledge stored in the PLMs rather than\n",
      "the retrieved passages and its expanded terms are\n",
      "learned through text generation.\n",
      "Recent Query Reformulation . There are recent\n",
      "or concurrent studies (Nogueira and Cho, 2017;\n",
      "Zaiem and Sadat, 2019; Yu et al., 2020; Vaku-\n",
      "lenko et al., 2020; Lin et al., 2020) that reformu-\n",
      "late queries with generation models for other re-\n",
      "trieval tasks. However, these studies are not eas-\n",
      "ily applicable or efﬁcient enough for OpenQA be-\n",
      "cause: (1) They require external resources such as\n",
      "paraphrase data (Zaiem and Sadat, 2019), search\n",
      "sessions (Yu et al., 2020), or conversational con-\n",
      "texts (Lin et al., 2020; Vakulenko et al., 2020)\n",
      "to form the reformulated queries, which are not\n",
      "available or showed inferior domain-transfer per-\n",
      "formance in OpenQA (Zaiem and Sadat, 2019);\n",
      "(2) They involve time-consuming training process\n",
      "such as RL. For example, Nogueira and Cho (2017)\n",
      "reported a training time of 8 to 10 days as it uses\n",
      "retrieval performance in the reward function and\n",
      "conducts retrieval at each iteration. In contrast,\n",
      "GARuses freely accessible in-domain contexts like\n",
      "passage titles as the generation targets and standardseq2seq learning, which, despite its simplicity, is\n",
      "not only more efﬁcient but effective for OpenQA.\n",
      "Retrieval for OpenQA . Existing sparse retrieval\n",
      "methods for OpenQA (Chen et al., 2017) solely rely\n",
      "on the information of the questions. GARextends\n",
      "to contexts relevant to the questions by extracting\n",
      "information inside PLMs and helps sparse meth-\n",
      "ods achieve comparable or better performance than\n",
      "dense methods (Guu et al., 2020; Karpukhin et al.,\n",
      "2020), while enjoying the simplicity and efﬁciency\n",
      "of sparse representations. GARcan also be used\n",
      "with dense representations to seek for even better\n",
      "performance, which we leave as future work.\n",
      "Generative QA . Generative QA generates answers\n",
      "through seq2seq learning instead of extracting an-\n",
      "swer spans. Recent studies on generative OpenQA\n",
      "(Lewis et al., 2020a; Min et al., 2020; Izacard and\n",
      "Grave, 2020) are orthogonal to GARin that they\n",
      "focus on improving the reading stage and directly\n",
      "reuse DPR (Karpukhin et al., 2020) as the retriever.\n",
      "Unlike generative QA, the goal of GARis not to\n",
      "generate perfect answers to the questions but perti-\n",
      "nent contexts that are helpful for retrieval. Another\n",
      "line in generative QA learns to generate answers\n",
      "without relevant passages as the evidence but solely\n",
      "the question itself using PLMs (Roberts et al., 2020;\n",
      "Brown et al., 2020). GARfurther conﬁrms that one\n",
      "can extract factual knowledge from PLMs, which\n",
      "is not limited to the answers as in prior studies but\n",
      "also other relevant contexts.\n",
      "3 Generation-Augmented Retrieval\n",
      "3.1 Task Formulation\n",
      "OpenQA aims to answer factoid questions with-\n",
      "out pre-speciﬁed domains. We assume that a large\n",
      "collection of documents C(i.e., Wikipedia) are\n",
      "given as the resource to answer the questions and\n",
      "a retriever-reader architecture is used to tackle the\n",
      "task, where the retriever retrieves a small subset\n",
      "of the documents D\u001aCand the reader reads the\n",
      "documents Dto extract (or generate) an answer.\n",
      "Our goal is to improve the effectiveness and efﬁ-\n",
      "ciency of the retriever and consequently improve\n",
      "the performance of the reader.\n",
      "3.2 Generation of Query Contexts\n",
      "InGAR, queries are augmented with various heuris-\n",
      "tically discovered relevant contexts in order to re-\n",
      "trieve more relevant passages in terms of both quan-\n",
      "tity and quality. For the task of OpenQA where the\n",
      "query is a question, we take the following threefreely accessible contexts as the generation targets.\n",
      "We show in Sec. 6.2 that having multiple gener-\n",
      "ation targets is helpful in that fusing their results\n",
      "consistently brings better retrieval accuracy.\n",
      "Context 1: The default target (answer) . The de-\n",
      "fault target is the label in the task of interest, which\n",
      "is the answer in OpenQA. The answer to the ques-\n",
      "tion is apparently useful for the retrieval of relevant\n",
      "passages that contain the answer itself. As shown\n",
      "in previous work (Roberts et al., 2020; Brown et al.,\n",
      "2020), PLMs are able to answer certain questions\n",
      "solely by taking the questions as input ( i.e., closed-\n",
      "book QA). Instead of using the generated answers\n",
      "directly as in closed-book QA, GARtreats them\n",
      "as contexts of the question for retrieval. The ad-\n",
      "vantage is that even if the generated answers are\n",
      "partially correct (or even incorrect), they may still\n",
      "beneﬁt retrieval as long as they are relevant to the\n",
      "passages that contain the correct answers ( e.g., co-\n",
      "occur with the correct answers).\n",
      "Context 2: Sentence containing the default tar-\n",
      "get. The sentence in a passage that contains the\n",
      "answer is used as another generation target. Sim-\n",
      "ilar to using answers as the generation target, the\n",
      "generated sentences are still beneﬁcial for retriev-\n",
      "ing relevant passages even if they do not contain\n",
      "the answers, as their semantics is highly related to\n",
      "the questions/answers (examples in Sec. 6.1). One\n",
      "can take the relevant sentences in the ground-truth\n",
      "passages (if any) or those in the positive passages\n",
      "of a retriever as the reference, depending on the\n",
      "trade-off between reference quality and diversity.\n",
      "Context 3: Title of passage containing the de-\n",
      "fault target . One can also use the titles of rele-\n",
      "vant passages as the generation target if available.\n",
      "Speciﬁcally, we retrieve Wikipedia passages using\n",
      "BM25 with the question as the query, and take the\n",
      "page titles of positive passages that contain the an-\n",
      "swers as the generation target. We observe that\n",
      "the page titles of positive passages are often entity\n",
      "names of interest, and sometimes (but not always)\n",
      "the answers to the questions. Intuitively, if GAR\n",
      "learns which Wikipedia pages the question is re-\n",
      "lated to, the queries augmented by the generated\n",
      "titles would naturally have a better chance of re-\n",
      "trieving those relevant passages.\n",
      "While it is likely that some of the generated\n",
      "query contexts involve unfaithful or nonfactual in-\n",
      "formation due to hallucination in text generation\n",
      "(Mao et al., 2020) and introduce noise during re-\n",
      "trieval, they are beneﬁcial rather than harmful over-all, as our experiments show that GARimprove\n",
      "both retrieval and QA performance over BM25 sig-\n",
      "niﬁcantly. Also, since we generate 3 different (com-\n",
      "plementary) query contexts and fuse their retrieval\n",
      "results, the distraction of hallucinated content is\n",
      "further alleviated.\n",
      "3.3 Retrieval with Generation-Augmented\n",
      "Queries\n",
      "After generating the contexts of a query, we append\n",
      "them to the query to form a generation-augmented\n",
      "query .3We observe that conducting retrieval with\n",
      "the generated contexts ( e.g., answers) alone as\n",
      "queries instead of concatenation is ineffective be-\n",
      "cause (1) some of the generated answers are rather\n",
      "irrelevant, and (2) a query consisting of the correct\n",
      "answer alone (without the question) may retrieve\n",
      "false positive passages with unrelated contexts that\n",
      "happen to contain the answer. Such low-quality\n",
      "passages may lead to potential issues in the follow-\n",
      "ing passage reading stage.\n",
      "If there are multiple query contexts, we conduct\n",
      "retrieval using queries with different generated con-\n",
      "texts separately and then fuse their results. The per-\n",
      "formance of one-time retrieval with all the contexts\n",
      "appended is slightly but not signiﬁcantly worse.\n",
      "For simplicity, we fuse the retrieval results in a\n",
      "straightforward way: an equal number of passages\n",
      "are taken from the top-retrieved passages of each\n",
      "source. One may also use weighted or more so-\n",
      "phisticated fusion strategies such as reciprocal rank\n",
      "fusion (Cormack et al., 2009), the results of which\n",
      "are slightly better according to our experiments.4\n",
      "Next, one can use any off-the-shelf retriever for\n",
      "passage retrieval. Here, we use a simple BM25\n",
      "model to demonstrate that GARwith sparse repre-\n",
      "sentations can already achieve comparable or better\n",
      "performance than state-of-the-art dense methods\n",
      "while being more lightweight and efﬁcient (includ-\n",
      "ing the cost of the generation model), closing the\n",
      "gap between sparse and dense retrieval methods.\n",
      "4 OpenQA with G AR\n",
      "To further verify the effectiveness of GAR, we\n",
      "equip it with both extractive and generative read-\n",
      "ers for end-to-end QA evaluation. We follow the\n",
      "3One may create a title ﬁeld during document indexing\n",
      "and conduct multi-ﬁeld retrieval but here we append the titles\n",
      "to the questions as other query contexts for generalizability.\n",
      "4We use the fusion tools at https://github.com/\n",
      "joaopalotti/trectools .reader design of the major baselines for a fair com-\n",
      "parison, while virtually any existing QA reader can\n",
      "be used with G AR.\n",
      "4.1 Extractive Reader\n",
      "For the extractive setup, we largely follow the de-\n",
      "sign of the extractive reader in DPR (Karpukhin\n",
      "et al., 2020). Let D= [d1; d2; :::; d k]denote the list\n",
      "of retrieved passages with passage relevance scores\n",
      "D. LetSi= [s1; s2; :::; s N]denote the top Ntext\n",
      "spans in passage diranked by span relevance scores\n",
      "Si. Brieﬂy, the DPR reader uses BERT-base (De-\n",
      "vlin et al., 2019) for representation learning, where\n",
      "it estimates the passage relevance score Dkfor\n",
      "each retrieved passage dkbased on the [CLS] to-\n",
      "kens of all retrieved passages D, and assigns span\n",
      "relevance scores Sifor each candidate span based\n",
      "on the representations of its start and end tokens.\n",
      "Finally, the span with the highest span relevance\n",
      "score from the passage with the highest passage rel-\n",
      "evance score is chosen as the answer. We refer the\n",
      "readers to Karpukhin et al. (2020) for more details.\n",
      "Passage-level Span Voting . Many extractive QA\n",
      "methods (Chen et al., 2017; Min et al., 2019b; Guu\n",
      "et al., 2020; Karpukhin et al., 2020) measure the\n",
      "probability of span extraction in different retrieved\n",
      "passages independently, despite that their collec-\n",
      "tive signals may provide more evidence in deter-\n",
      "mining the correct answer. We propose a simple\n",
      "yet effective passage-level span voting mechanism,\n",
      "which aggregates the predictions of the spans in\n",
      "the same surface form from different retrieved pas-\n",
      "sages. Intuitively, if a text span is considered as the\n",
      "answer multiple times in different passages, it is\n",
      "more likely to be the correct answer. Speciﬁcally,\n",
      "GARcalculates a normalized score p(Si[j])for the\n",
      "j-th span in passage diduring inference as follows:\n",
      "p(Si[j]) = softmax (D)[i]\u0002softmax (Si)[j].GAR\n",
      "then aggregates the scores of the spans with the\n",
      "same surface string among all the retrieved pas-\n",
      "sages as the collective passage-level score.5\n",
      "4.2 Generative Reader\n",
      "For the generative setup, we use a seq2seq frame-\n",
      "work where the input is the concatenation of the\n",
      "question and top-retrieved passages and the target\n",
      "output is the desired answer. Such generative read-\n",
      "ers are adopted in recent methods such as SpanSe-\n",
      "5We ﬁnd that the number of spans used for normalization\n",
      "in each passage does not have signiﬁcant impact on the ﬁnal\n",
      "performance (we take N= 5) and using the raw or normalized\n",
      "strings for aggregation also perform similarly.qGen (Min et al., 2020) and Longformer (Belt-\n",
      "agy et al., 2020). Speciﬁcally, we use BART-large\n",
      "(Lewis et al., 2019) as the generative reader, which\n",
      "concatenates the question and top-retrieved pas-\n",
      "sages up to its length limit (1,024 tokens, 7.8 pas-\n",
      "sages on average). Generative GARis directly com-\n",
      "parable with SpanSeqGen (Min et al., 2020) that\n",
      "uses the retrieval results of DPR but not comparable\n",
      "with Fusion-in-Decoder (FID) (Izacard and Grave,\n",
      "2020) since it encodes 100 passages rather than\n",
      "1,024 tokens and involves more model parameters.\n",
      "5 Experiment Setup\n",
      "5.1 Datasets\n",
      "We conduct experiments on the open-domain ver-\n",
      "sion of two popular QA benchmarks: Natural Ques-\n",
      "tions (NQ) (Kwiatkowski et al., 2019) and Trivi-\n",
      "aQA (Trivia) (Joshi et al., 2017). The statistics of\n",
      "the datasets are listed in Table 1.\n",
      "Dataset Train / Val / Test Q-len A-len #-A\n",
      "NQ 79,168 / 8,757 / 3,610 12.5 5.2 1.2\n",
      "Trivia 78,785 / 8,837 / 11,313 20.2 5.5 13.7\n",
      "Table 1: Dataset statistics that show the number of sam-\n",
      "ples per data split, the average question (answer) length,\n",
      "and the number of answers for each question.\n",
      "5.2 Evaluation Metrics\n",
      "Following prior studies (Karpukhin et al., 2020),\n",
      "we use top-k retrieval accuracy to evaluate the per-\n",
      "formance of the retriever and the Exact Match (EM)\n",
      "score to measure the performance of the reader.\n",
      "Top-k retrieval accuracy is deﬁned as the pro-\n",
      "portion of questions for which the top-k retrieved\n",
      "passages contain at least one answer span, which\n",
      "is an upper bound of how many questions are “an-\n",
      "swerable” by an extractive reader.\n",
      "Exact Match (EM) is the proportion of the pre-\n",
      "dicted answer spans being exactly the same as (one\n",
      "of) the ground-truth answer(s), after string normal-\n",
      "ization such as article and punctuation removal.\n",
      "5.3 Compared Methods\n",
      "For passage retrieval, we mainly compare with\n",
      "BM25 and DPR, which represent the most used\n",
      "state-of-the-art methods of sparse and dense re-\n",
      "trieval for OpenQA, respectively. For query ex-\n",
      "pansion, we re-emphasize that GARis the ﬁrst QE\n",
      "approach designed for OpenQA and most of the\n",
      "recent approaches are not applicable or efﬁcientenough for OpenQA since they have task-speciﬁc\n",
      "objectives, require external supervision that was\n",
      "shown to transfer poorly to OpenQA, or take many\n",
      "days to train (Sec. 2). We thus compare with a clas-\n",
      "sic unsupervised QE method RM3 (Abdul-Jaleel\n",
      "et al., 2004) that does not need external resources\n",
      "for a fair comparison. For passage reading, we\n",
      "compare with both extractive (Min et al., 2019a;\n",
      "Asai et al., 2019; Lee et al., 2019; Min et al., 2019b;\n",
      "Guu et al., 2020; Karpukhin et al., 2020) and gen-\n",
      "erative (Brown et al., 2020; Roberts et al., 2020;\n",
      "Min et al., 2020; Lewis et al., 2020a; Izacard and\n",
      "Grave, 2020) methods when equipping GARwith\n",
      "the corresponding reader.\n",
      "5.4 Implementation Details\n",
      "Retriever . We use Anserini (Yang et al., 2017) for\n",
      "text retrieval of BM25 and GARwith its default\n",
      "parameters. We conduct grid search for the QE\n",
      "baseline RM3 (Abdul-Jaleel et al., 2004).\n",
      "Generator . We use BART-large (Lewis et al.,\n",
      "2019) to generate query contexts in GAR. When\n",
      "there are multiple desired targets (such as multi-\n",
      "ple answers or titles), we concatenate them with\n",
      "[SEP] tokens as the reference and remove the [SEP]\n",
      "tokens in the generation-augmented queries. For\n",
      "Trivia, in particular, we use the value ﬁeld as the\n",
      "generation target of answer and observe better per-\n",
      "formance. We take the checkpoint with the best\n",
      "ROUGE-1 F1 score on the validation set, while\n",
      "observing that the retrieval accuracy of GARis rel-\n",
      "atively stable to the checkpoint selection since we\n",
      "do not directly use the generated contexts but treat\n",
      "them as augmentation of queries for retrieval.\n",
      "Reader . Extractive GARuses the reader of DPR\n",
      "with largely the same hyperparameters, which is\n",
      "initialized with BERT-base (Devlin et al., 2019)\n",
      "and takes 100 (500) retrieved passages during train-\n",
      "ing (inference). Generative GARconcatenates the\n",
      "question and top-10 retrieved passages, and takes\n",
      "at most 1,024 tokens as input. Greedy decoding is\n",
      "adopted for all generation models, which appears to\n",
      "perform similarly to (more expensive) beam search.\n",
      "6 Experiment Results\n",
      "We evaluate the effectiveness of GARin three\n",
      "stages: generation of query contexts (Sec. 6.1),\n",
      "retrieval of relevant passages (Sec. 6.2), and pas-\n",
      "sage reading for OpenQA (Sec. 6.3). Ablation\n",
      "studies are mostly shown on the NQ dataset to un-\n",
      "derstand the drawbacks of GARsince it achievesQuestion : when did bat out of hell get released?\n",
      "Answer :September 1977 fSeptember 1977g\n",
      "Sentence : Bat Out of Hell is the second studio album and the major - label debut by American rock singer Meat\n",
      "Loaf ... released in September 1977 on Cleveland International / Epic Records.\n",
      "fThe album was released in September 1977 on Cleveland International / Epic Records. g\n",
      "Title :Bat Out of Hell fBat Out of Hellg\n",
      "Question : who sings does he love me with reba?\n",
      "Answer :Brooks & Dunn fLinda Davisg\n",
      "Sentence :Linda Kaye Davis ( born November 26, 1962 ) is an American country music singer.\n",
      "f“ Does He Love You ” is a song written by Sandy Knox and Billy Stritch, and recorded as a duet by American\n",
      "country music artists Reba McEntire and Linda Davis. g\n",
      "Title :Does He Love Me [SEP] Does He Love Me (Reba McEntire song) [SEP] I Do (Reba McEntire album)\n",
      "fLinda Davis [SEP] Greatest Hits V olume Two (Reba McEntire album) [SEP] Does He Love You g\n",
      "Question : what is the name of wonder womans mother?\n",
      "Answer :Mother Magda fQueen Hippolytag\n",
      "Sentence : In the Amazonian myths, she is the daughter of the Amazon queen Sifrat and the male dwarf Shuri,\n",
      "and is the mother of Wonder Woman. fWonder Woman’s origin story relates that she was sculpted from clay\n",
      "by her mother Queen Hippolyta and given life by Aphrodite. g\n",
      "Title :Wonder Woman [SEP] Diana Prince [SEP] Wonder Woman (2011 TV pilot)\n",
      "fWonder Woman [SEP] Orana (comics) [SEP] Wonder Woman (TV series) g\n",
      "Table 2: Examples of generated query contexts .Relevant andirrelevant contexts are shown in green and\n",
      "red. Ground-truth references are shown in the fbracesg. The issue of generating wrong answers is alleviated by\n",
      "generating other contexts highly related to the question/answer.\n",
      "better performance on Trivia.\n",
      "6.1 Query Context Generation\n",
      "Automatic Evaluation . To evaluate the quality\n",
      "of the generated query contexts, we ﬁrst measure\n",
      "their lexical overlap with the ground-truth query\n",
      "contexts. As suggested by the nontrivial ROUGE\n",
      "scores in Table 3, GARdoes learn to generate\n",
      "meaningful query contexts that could help the re-\n",
      "trieval stage. We next measure the lexical overlap\n",
      "between the query and the ground-truth passage.\n",
      "The ROUGE-1/2/L F1 scores between the original\n",
      "query and ground-truth passage are 6.00/2.36/5.01,\n",
      "and those for the generation-augmented query are\n",
      "7.05/2.84/5.62 (answer), 13.21/6.99/10.27 (sen-\n",
      "tence), 7.13/2.85/5.76 (title) on NQ, respectively.\n",
      "Such results further demonstrate that the generated\n",
      "query contexts signiﬁcantly increase the word over-\n",
      "lap between the queries and the positive passages,\n",
      "and thus are likely to improve retrieval results.6\n",
      "Case Studies . In Table 2, we show several ex-\n",
      "amples of the generated query contexts and their\n",
      "ground-truth references. In the ﬁrst example, the\n",
      "correct album release date appears in both the gen-\n",
      "erated answer and the generated sentence, and the\n",
      "generated title is the same as the Wikipedia page\n",
      "6We use F1 instead of recall to avoid the unfair favor of\n",
      "(longer) generation-augmented query.Context ROUGE-1 ROUGE-2 ROUGE-L\n",
      "Answer 33.51 20.54 33.30\n",
      "Sentence 37.14 24.71 33.91\n",
      "Title 43.20 32.11 39.67\n",
      "Table 3: ROUGE F1 scores of the generated query\n",
      "contexts on the validation set of the NQ dataset.\n",
      "title of the album. In the last two examples, the\n",
      "generated answers are wrong but fortunately, the\n",
      "generated sentences contain the correct answer and\n",
      "(or) other relevant information and the generated\n",
      "titles are highly related to the question as well,\n",
      "which shows that different query contexts are com-\n",
      "plementary to each other and the noise during query\n",
      "context generation is thus reduced.\n",
      "6.2 Generation-Augmented Retrieval\n",
      "Comparison w. the state-of-the-art . We next\n",
      "evaluate the effectiveness of GARfor retrieval.\n",
      "In Table 4, we show the top-k retrieval accuracy\n",
      "of BM25, BM25 with query expansion (+RM3)\n",
      "(Abdul-Jaleel et al., 2004), DPR (Karpukhin et al.,\n",
      "2020), G AR, and G AR+(GAR+DPR).\n",
      "On the NQ dataset, while BM25 clearly under-\n",
      "performs DPR regardless of the number of retrieved\n",
      "passages, the gap between GARand DPR is signiﬁ-\n",
      "cantly smaller and negligible when k\u0015100. When\n",
      "k\u0015500,GARis slightly better than DPR despiteMethodNQ Trivia\n",
      "Top-5 Top-20 Top-100 Top-500 Top-1000 Top-5 Top-20 Top-100 Top-500 Top-1000\n",
      "BM25 (ours) 43.6 62.9 78.1 85.5 87.8 67.7 77.3 83.9 87.9 88.9\n",
      "BM25 +RM3 44.6 64.2 79.6 86.8 88.9 67.0 77.1 83.8 87.7 88.9\n",
      "DPR 68.3 80.1 86.1 90.3 91.2 72.7 80.2 84.8 - -\n",
      "GAR 60.9 74.4 85.3 90.3 91.7 73.1 80.4 85.7 88.9 89.7\n",
      "GAR+70.7 81.6 88.9 92.0 93.2 76.0 82.1 86.6 - -\n",
      "Table 4: Top-k retrieval accuracy on the test sets . The baselines are evaluated by ourselves and better than\n",
      "reported in Karpukhin et al. (2020). G ARhelps BM25 to achieve comparable or better performance than DPR.\n",
      "Best and second best methods are bold and underlined , respectively.\n",
      "that it simply uses BM25 for retrieval. In con-\n",
      "trast, the classic QE method RM3, while showing\n",
      "marginal improvement over the vanilla BM25, does\n",
      "not achieve comparable performance with GARor\n",
      "DPR. By fusing the results of GARand DPR in\n",
      "the same way as described in Sec. 3.3, we further\n",
      "obtain consistently higher performance than both\n",
      "methods, with top-100 accuracy 88.9% and top-\n",
      "1000 accuracy 93.2%.\n",
      "On the Trivia dataset, the results are even more\n",
      "encouraging – GARachieves consistently better\n",
      "retrieval accuracy than DPR when k\u00155. On\n",
      "the other hand, the difference between BM25 and\n",
      "BM25 +RM3 is negligible, which suggests that\n",
      "naively considering top-ranked passages as relevant\n",
      "(i.e., pseudo relevance feedback) for QE does not\n",
      "always work for OpenQA. Results on more cutoffs\n",
      "ofkcan be found in App. A.\n",
      "Effectiveness of diverse query contexts . In\n",
      "Fig. 1, we show the performance of GARwhen\n",
      "different query contexts are used to augment the\n",
      "queries. Although the individual performance\n",
      "when using each query context is somewhat similar,\n",
      "fusing their retrieved passages consistently leads\n",
      "to better performance, conﬁrming that different\n",
      "generation-augmented queries are complementary\n",
      "to each other (recall examples in Table 2).\n",
      "Performance breakdown by question type . In\n",
      "Table 5, we show the top-100 accuracy of the com-\n",
      "pared retrieval methods per question type on the\n",
      "NQ test set. Again, GARoutperforms BM25 on all\n",
      "types of questions signiﬁcantly and GAR+achieves\n",
      "the best performance across the board, which fur-\n",
      "ther veriﬁes the effectiveness of G AR.\n",
      "6.3 Passage Reading with G AR\n",
      "Comparison w. the state-of-the-art . We show\n",
      "the comparison of end-to-end QA performance of\n",
      "extractive and generative methods in Table 6. Ex-\n",
      "tractive GARachieves state-of-the-art performance\n",
      "1 5 10 20 50 100 200 300 500 1000\n",
      "k: # of retrieved passages30405060708090Top-k Accuracy (%)\n",
      "Answer+Sentence+Title\n",
      "Answer+Sentence\n",
      "Answer+Title\n",
      "Answer\n",
      "Title\n",
      "SentenceFigure 1: Top-k retrieval accuracy on the test\n",
      "set of NQ when fusing retrieval results of different\n",
      "generation-augmented queries.\n",
      "Type Percentage BM25 DPR G AR GAR+\n",
      "Who 37.5% 82.1 88.0 87.5 90.8\n",
      "When 19.0% 73.1 86.9 83.8 88.6\n",
      "What 15.0% 76.5 82.6 81.5 86.0\n",
      "Where 10.9% 77.4 89.1 87.0 90.8\n",
      "Other 9.1% 79.3 78.1 81.8 84.2\n",
      "How 5.0% 78.2 83.8 83.2 85.5\n",
      "Which 3.3% 89.0 90.7 94.1 94.9\n",
      "Why 0.3% 90.0 90.0 90.0 90.0\n",
      "Table 5: Top-100 retrieval accuracy breakdown of\n",
      "question type on NQ . Best and second best methods\n",
      "in each category are bold and underlined , respectively.\n",
      "among extractive methods on both NQ and Trivia\n",
      "datasets, despite that it is more lightweight and\n",
      "computationally efﬁcient. Generative GARoutper-\n",
      "forms most of the generative methods on Trivia but\n",
      "does not perform as well on NQ, which is some-\n",
      "what expected and consistent with the performance\n",
      "at the retrieval stage, as the generative reader only\n",
      "takes a few passages as input and GARdoes not\n",
      "outperform dense retrieval methods on NQ when k\n",
      "is very small. However, combining GARwith DPR\n",
      "achieves signiﬁcantly better performance than bothMethod NQ TriviaExtractiveHard EM (Min et al., 2019a) 28.1 50.9 -\n",
      "Path Retriever (Asai et al., 2019) 32.6 - -\n",
      "ORQA (Lee et al., 2019) 33.3 45.0 -\n",
      "Graph Retriever (Min et al., 2019b) 34.5 56.0 -\n",
      "REALM (Guu et al., 2020) 40.4 - -\n",
      "DPR (Karpukhin et al., 2020) 41.5 57.9 -\n",
      "BM25 (ours) 37.7 60.1 -\n",
      "GAR 41.8 62.7 74.8\n",
      "GAR+43.8 - -GenerativeGPT-3 (Brown et al., 2020) 29.9 - 71.2\n",
      "T5 (Roberts et al., 2020) 36.6 60.5 -\n",
      "SpanSeqGen (Min et al., 2020) 42.2 - -\n",
      "RAG (Lewis et al., 2020a) 44.5 56.1 68.0\n",
      "FID (Izacard and Grave, 2020) 51.4 67.6 80.1\n",
      "BM25 (ours) 35.3 58.6 -\n",
      "GAR 38.1 62.2 -\n",
      "GAR+45.3 - -\n",
      "Table 6: End-to-end comparison with the state-of-\n",
      "the-art methods in EM . For Trivia, the left column\n",
      "denotes the open-domain test set and the right is the\n",
      "hidden Wikipedia test set on the public leaderboard.\n",
      "methods or baselines that use DPR as input such as\n",
      "SpanSeqGen (Min et al., 2020) and RAG (Lewis\n",
      "et al., 2020a). Also, GARoutperforms BM25 sig-\n",
      "niﬁcantly under both extractive and generative se-\n",
      "tups, which again shows the effectiveness of the\n",
      "generated query contexts, even if they are heuristi-\n",
      "cally discovered without any external supervision.\n",
      "The best performing generative method FID\n",
      "(Izacard and Grave, 2020) is not directly compara-\n",
      "ble as it takes more (100) passages as input. As an\n",
      "indirect comparison, GARperforms better than FID\n",
      "when FID encodes 10 passages (cf. Fig. 2 in Izac-\n",
      "ard and Grave (2020)). Moreover, since FID relies\n",
      "on the retrieval results of DPR as well, we believe\n",
      "that it is a low-hanging fruit to replace its input\n",
      "with GARorGAR+and further boost the perfor-\n",
      "mance.7We also observe that, perhaps surprisingly,\n",
      "extractive BM25 performs reasonably well, espe-\n",
      "cially on the Trivia dataset, outperforming many\n",
      "recent state-of-the-art methods.8Generative BM25\n",
      "also performs competitively in our experiments.\n",
      "Model Generalizability . Recent studies (Lewis\n",
      "et al., 2020b) show that there are signiﬁcant ques-\n",
      "tion and answer overlaps between the training and\n",
      "test sets of popular OpenQA datasets. Speciﬁcally,\n",
      "60% to 70% test-time answers also appear in the\n",
      "7This claim is later veriﬁed by the best systems in the\n",
      "NeurIPS 2020 EfﬁcientQA competition (Min et al., 2021).\n",
      "8We ﬁnd that taking 500 passages during reader inference\n",
      "instead of 100 as in Karpukhin et al. (2020) improves the\n",
      "performance of BM25 but not DPR.training set and roughly 30% test-set questions\n",
      "have a near-duplicate paraphrase in the training\n",
      "set. Such observations suggest that many questions\n",
      "might have been answered by simple question or\n",
      "answer memorization. To further examine model\n",
      "generalizability, we study the per-category perfor-\n",
      "mance of different methods using the annotations\n",
      "in Lewis et al. (2020b).\n",
      "Method TotalQuestion\n",
      "OverlapAnswer\n",
      "Overlap\n",
      "OnlyNo\n",
      "Overlap\n",
      "DPR 41.3 69.4 34.6 19.3\n",
      "GAR+(E) 43.8 66.7 38.1 23.9\n",
      "BART 26.5 67.6 10.2 0.8\n",
      "RAG 44.5 70.7 34.9 24.8\n",
      "GAR+(G) 45.3 67.9 38.1 27.0\n",
      "Table 7: EM scores with question-answer overlap\n",
      "category breakdown on NQ. (E) and (G) denote ex-\n",
      "tractive and generative readers, respectively. Results of\n",
      "baseline methods are taken from Lewis et al. (2020b).\n",
      "The observations on Trivia are similar and omitted.\n",
      "As listed in Table 7, for the No Overlap category,\n",
      "GAR+(E) outperforms DPR on the extractive setup\n",
      "andGAR+(G) outperforms RAG on the generative\n",
      "setup, which indicates that better end-to-end model\n",
      "generalizability can be achieved by adding GAR\n",
      "for retrieval. GAR+also achieves the best EM un-\n",
      "der the Answer Overlap Only category. In addition,\n",
      "we observe that a closed-book BART model that\n",
      "only takes the question as input performs much\n",
      "worse than additionally taking top-retrieved pas-\n",
      "sages, i.e.,GAR+(G), especially on the questions\n",
      "that require generalizability. Notably, all methods\n",
      "perform signiﬁcantly better on the Question Over-\n",
      "lapcategory, which suggests that the high Total\n",
      "EM is mostly contributed by question memoriza-\n",
      "tion. That said, GAR+appears to be less dependent\n",
      "on question memorization given its lower EM for\n",
      "this category.9\n",
      "6.4 Efﬁciency of G AR\n",
      "GARis efﬁcient and scalable since it uses sparse\n",
      "representations for retrieval and does not in-\n",
      "volve time-consuming training process such as\n",
      "RL (Nogueira and Cho, 2017; Liu et al., 2019).\n",
      "The only overhead of GARis on the generation of\n",
      "query contexts and the retrieval with generation-\n",
      "9The same ablation study is also conducted on the retrieval\n",
      "stage and similar results are observed. More detailed discus-\n",
      "sions can be found in App. A.Training Indexing Retrieval\n",
      "DPR 24h w. 8 GPUs 17.3h w. 8 GPUs 30 min w. 1 GPU\n",
      "GAR 3\u00186h w. 1 GPU 0.5h w. 35 CPUs 5 min w. 35 CPUs\n",
      "Table 8: Comparison of computational cost between\n",
      "DPR and G ARat different stages. The training time\n",
      "of G ARis for one generation target but different gener-\n",
      "ators can be trained in parallel.\n",
      "augmented (thus longer) queries, whose computa-\n",
      "tional complexity is signiﬁcantly lower than other\n",
      "methods with comparable retrieval accuracy.\n",
      "We use Nvidia V100 GPUs and Intel Xeon Plat-\n",
      "inum 8168 CPUs in our experiments. As listed in\n",
      "Table 8, the training time of GARis 3 to 6 hours\n",
      "on 1 GPU depending on the generation target. As\n",
      "a comparison, REALM (Guu et al., 2020) uses\n",
      "64 TPUs to train for 200k steps during pre-training\n",
      "alone and DPR (Karpukhin et al., 2020) takes about\n",
      "24 hours to train with 8 GPUs. To build the indices\n",
      "of Wikipedia passages, GARonly takes around 30\n",
      "min with 35 CPUs, while DPR takes 8.8 hours\n",
      "on 8 GPUs to generate dense representations and\n",
      "another 8.5 hours to build the FAISS index (John-\n",
      "son et al., 2017). For retrieval, GARtakes about\n",
      "1 min to generate one query context with 1 GPU,\n",
      "1 min to retrieve 1,000 passages for the NQ test\n",
      "set with answer/title-augmented queries and 2 min\n",
      "with sentence-augmented queries using 35 CPUs.\n",
      "In contrast, DPR takes about 30 min on 1 GPU.\n",
      "7 Conclusion\n",
      "In this work, we propose Generation-Augmented\n",
      "Retrieval and demonstrate that the relevant contexts\n",
      "generated by PLMs without external supervision\n",
      "can signiﬁcantly enrich query semantics and im-\n",
      "prove retrieval accuracy. Remarkably, GARwith\n",
      "sparse representations performs similarly or better\n",
      "than state-of-the-art methods based on the dense\n",
      "representations of the original queries. GARcan\n",
      "also be easily combined with dense representa-\n",
      "tions to produce even better results. Furthermore,\n",
      "GARachieves state-of-the-art end-to-end perfor-\n",
      "mance on extractive OpenQA and competitive per-\n",
      "formance under the generative setup.\n",
      "8 Future Extensions\n",
      "Potential improvements . There is still much\n",
      "space to explore and improve for GARin future\n",
      "work. For query context generation, one can ex-\n",
      "plore multi-task learning to further reduce computa-\n",
      "tional cost and examine whether different contextscan mutually enhance each other when generated\n",
      "by the same generator. One may also sample multi-\n",
      "ple contexts instead of greedy decoding to enrich a\n",
      "query. For retrieval, one can adopt more advanced\n",
      "fusion techniques based on both the ranking and\n",
      "score of the passages. As the generator and re-\n",
      "triever are largely independent now, it is also inter-\n",
      "esting to study how to jointly or iteratively optimize\n",
      "generation and retrieval such that the generator is\n",
      "aware of the retriever and generates query contexts\n",
      "more beneﬁcial for the retrieval stage. Last but not\n",
      "least, it is very likely that better results can be ob-\n",
      "tained by more extensive hyper-parameter tuning.\n",
      "Applicability to other tasks . Beyond OpenQA,\n",
      "GARalso has great potentials for other tasks that\n",
      "involve text matching such as conversation utter-\n",
      "ance selection (Lowe et al., 2015; Dinan et al.,\n",
      "2020) or information retrieval (Nguyen et al., 2016;\n",
      "Craswell et al., 2020). The default generation tar-\n",
      "get is always available for supervised tasks. For\n",
      "example, for conversation utterance selection one\n",
      "can use the reference utterance as the default target\n",
      "and then match the concatenation of the conversa-\n",
      "tion history and the generated utterance with the\n",
      "provided utterance candidates. For article search,\n",
      "the default target could be (part of) the ground-truth\n",
      "article itself. Other generation targets are more task-\n",
      "speciﬁc and can be designed as long as they can\n",
      "be fetched from the latent knowledge inside PLMs\n",
      "and are helpful for further text retrieval (matching).\n",
      "Note that by augmenting (expanding) the queries\n",
      "with heuristically discovered relevant contexts ex-\n",
      "tracted from PLMs instead of reformulating them,\n",
      "GARbypasses the need for external supervision to\n",
      "form the original-reformulated query pairs.\n",
      "Acknowledgments\n",
      "We thank Vladimir Karpukhin, Sewon Min, Gau-\n",
      "tier Izacard, Wenda Qiu, Revanth Reddy, and Hao\n",
      "Cheng for helpful discussions. We thank the anony-\n",
      "mous reviewers for valuable comments.\n",
      "References\n",
      "Nasreen Abdul-Jaleel, James Allan, W Bruce Croft,\n",
      "Fernando Diaz, Leah Larkey, Xiaoyan Li, Mark D\n",
      "Smucker, and Courtney Wade. 2004. Umass at trec\n",
      "2004: Novelty and hard. Computer Science Depart-\n",
      "ment Faculty Publication Series , page 189.\n",
      "Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\n",
      "Richard Socher, and Caiming Xiong. 2019. Learn-\n",
      "ing to retrieve reasoning paths over wikipediagraph for question answering. arXiv preprint\n",
      "arXiv:1911.10470 .\n",
      "Iz Beltagy, Matthew E Peters, and Arman Cohan.\n",
      "2020. Longformer: The long-document transformer.\n",
      "arXiv preprint arXiv:2004.05150 .\n",
      "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, et al. 2020. Language models are few-shot\n",
      "learners. arXiv preprint arXiv:2005.14165 .\n",
      "Danqi Chen, Adam Fisch, Jason Weston, and Antoine\n",
      "Bordes. 2017. Reading Wikipedia to answer open-\n",
      "domain questions. In Proceedings of the 55th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 1870–\n",
      "1879, Vancouver, Canada. Association for Computa-\n",
      "tional Linguistics.\n",
      "Gordon V Cormack, Charles LA Clarke, and Stefan\n",
      "Buettcher. 2009. Reciprocal rank fusion outper-\n",
      "forms condorcet and individual rank learning meth-\n",
      "ods. In Proceedings of the 32nd international ACM\n",
      "SIGIR conference on Research and development in\n",
      "information retrieval , pages 758–759.\n",
      "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\n",
      "Campos, and Ellen M V oorhees. 2020. Overview\n",
      "of the trec 2019 deep learning track. arXiv preprint\n",
      "arXiv:2003.07820 .\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2019. BERT: Pre-training of\n",
      "deep bidirectional transformers for language under-\n",
      "standing. In Proceedings of the 2019 Conference\n",
      "of the North American Chapter of the Association\n",
      "for Computational Linguistics: Human Language\n",
      "Technologies, Volume 1 (Long and Short Papers) ,\n",
      "pages 4171–4186, Minneapolis, Minnesota. Associ-\n",
      "ation for Computational Linguistics.\n",
      "Emily Dinan, Varvara Logacheva, Valentin Malykh,\n",
      "Alexander Miller, Kurt Shuster, Jack Urbanek,\n",
      "Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan\n",
      "Lowe, et al. 2020. The second conversational in-\n",
      "telligence challenge (convai2). In The NeurIPS’18\n",
      "Competition , pages 187–208. Springer.\n",
      "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\n",
      "pat, and Ming-Wei Chang. 2020. Realm: Retrieval-\n",
      "augmented language model pre-training. arXiv\n",
      "preprint arXiv:2002.08909 .\n",
      "Gautier Izacard and Edouard Grave. 2020. Lever-\n",
      "aging passage retrieval with generative models for\n",
      "open domain question answering. arXiv preprint\n",
      "arXiv:2007.01282 .\n",
      "Jeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\n",
      "Billion-scale similarity search with gpus. arXiv\n",
      "preprint arXiv:1702.08734 .Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\n",
      "Zettlemoyer. 2017. TriviaQA: A large scale dis-\n",
      "tantly supervised challenge dataset for reading com-\n",
      "prehension. In Proceedings of the 55th Annual Meet-\n",
      "ing of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers) , pages 1601–1611, Van-\n",
      "couver, Canada. Association for Computational Lin-\n",
      "guistics.\n",
      "Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\n",
      "Wu, Sergey Edunov, Danqi Chen, and Wen-\n",
      "tau Yih. 2020. Dense passage retrieval for\n",
      "open-domain question answering. arXiv preprint\n",
      "arXiv:2004.04906 .\n",
      "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\n",
      "ﬁeld, Michael Collins, Ankur Parikh, Chris Al-\n",
      "berti, Danielle Epstein, Illia Polosukhin, Jacob De-\n",
      "vlin, Kenton Lee, Kristina Toutanova, Llion Jones,\n",
      "Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\n",
      "Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\n",
      "Natural questions: A benchmark for question an-\n",
      "swering research. Transactions of the Association\n",
      "for Computational Linguistics , 7:452–466.\n",
      "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n",
      "2019. Latent retrieval for weakly supervised open\n",
      "domain question answering. In Proceedings of the\n",
      "57th Annual Meeting of the Association for Com-\n",
      "putational Linguistics , pages 6086–6096, Florence,\n",
      "Italy. Association for Computational Linguistics.\n",
      "Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\n",
      "jan Ghazvininejad, Abdelrahman Mohamed, Omer\n",
      "Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\n",
      "Bart: Denoising sequence-to-sequence pre-training\n",
      "for natural language generation, translation, and\n",
      "comprehension. arXiv preprint arXiv:1910.13461 .\n",
      "Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\n",
      "Petroni, Vladimir Karpukhin, Naman Goyal, Hein-\n",
      "rich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\n",
      "Rockt ¨aschel, et al. 2020a. Retrieval-augmented gen-\n",
      "eration for knowledge-intensive nlp tasks. arXiv\n",
      "preprint arXiv:2005.11401 .\n",
      "Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n",
      "2020b. Question and answer test-train overlap in\n",
      "open-domain question answering datasets. arXiv\n",
      "preprint arXiv:2008.02637 .\n",
      "Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\n",
      "Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\n",
      "Jimmy Lin. 2020. Query reformulation using query\n",
      "history for passage retrieval in conversational search.\n",
      "arXiv preprint arXiv:2005.02230 .\n",
      "Ye Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\n",
      "Philip S Yu. 2019. Generative question reﬁnement\n",
      "with deep reinforcement learning in retrieval-based\n",
      "qa system. In Proceedings of the 28th ACM Inter-\n",
      "national Conference on Information and Knowledge\n",
      "Management , pages 1643–1652.Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle\n",
      "Pineau. 2015. The ubuntu dialogue corpus: A large\n",
      "dataset for research in unstructured multi-turn dia-\n",
      "logue systems. arXiv preprint arXiv:1506.08909 .\n",
      "Yi Luan, Jacob Eisenstein, Kristina Toutanova, and\n",
      "Michael Collins. 2020. Sparse, dense, and at-\n",
      "tentional representations for text retrieval. arXiv\n",
      "preprint arXiv:2005.00181 .\n",
      "Yuanhua Lv and ChengXiang Zhai. 2010. Positional\n",
      "relevance model for pseudo-relevance feedback. In\n",
      "Proceedings of the 33rd international ACM SIGIR\n",
      "conference on Research and development in infor-\n",
      "mation retrieval , pages 579–586.\n",
      "Yuning Mao, Xiang Ren, Heng Ji, and Jiawei Han.\n",
      "2020. Constrained abstractive summarization: Pre-\n",
      "serving factual consistency with constrained genera-\n",
      "tion. arXiv preprint arXiv:2010.12723 .\n",
      "Sewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi\n",
      "Chen, Eunsol Choi, Michael Collins, Kelvin Guu,\n",
      "Hannaneh Hajishirzi, Kenton Lee, Jennimaria Palo-\n",
      "maki, et al. 2021. Neurips 2020 efﬁcientqa compe-\n",
      "tition: Systems, analyses and lessons learned. arXiv\n",
      "preprint arXiv:2101.00133 .\n",
      "Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and\n",
      "Luke Zettlemoyer. 2019a. A discrete hard EM ap-\n",
      "proach for weakly supervised question answering.\n",
      "InProceedings of the 2019 Conference on Empirical\n",
      "Methods in Natural Language Processing and the\n",
      "9th International Joint Conference on Natural Lan-\n",
      "guage Processing (EMNLP-IJCNLP) , pages 2851–\n",
      "2864, Hong Kong, China. Association for Computa-\n",
      "tional Linguistics.\n",
      "Sewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\n",
      "naneh Hajishirzi. 2019b. Knowledge guided text re-\n",
      "trieval and reading for open domain question answer-\n",
      "ing. arXiv preprint arXiv:1911.03868 .\n",
      "Sewon Min, Julian Michael, Hannaneh Hajishirzi, and\n",
      "Luke Zettlemoyer. 2020. Ambigqa: Answering\n",
      "ambiguous open-domain questions. arXiv preprint\n",
      "arXiv:2004.10645 .\n",
      "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\n",
      "Saurabh Tiwary, Rangan Majumder, and Li Deng.\n",
      "2016. Ms marco: A human-generated machine read-\n",
      "ing comprehension dataset.\n",
      "Rodrigo Nogueira and Kyunghyun Cho. 2017. Task-\n",
      "oriented query reformulation with reinforcement\n",
      "learning. In Proceedings of the 2017 Conference on\n",
      "Empirical Methods in Natural Language Processing ,\n",
      "pages 574–583, Copenhagen, Denmark. Association\n",
      "for Computational Linguistics.\n",
      "Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.\n",
      "How much knowledge can you pack into the pa-\n",
      "rameters of a language model? arXiv preprint\n",
      "arXiv:2002.08910 .Joseph Rocchio. 1971. Relevance feedback in in-\n",
      "formation retrieval. The Smart retrieval system-\n",
      "experiments in automatic document processing ,\n",
      "pages 313–323.\n",
      "Svitlana Vakulenko, Shayne Longpre, Zhucheng Tu,\n",
      "and Raviteja Anantha. 2020. Question rewriting for\n",
      "conversational question answering. arXiv preprint\n",
      "arXiv:2004.14652 .\n",
      "Xiao Wang, Craig Macdonald, and Iadh Ounis. 2020.\n",
      "Deep reinforced query reformulation for informa-\n",
      "tion retrieval. arXiv preprint arXiv:2007.07987 .\n",
      "Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:\n",
      "Enabling the use of lucene for information retrieval\n",
      "research. In Proceedings of the 40th International\n",
      "ACM SIGIR Conference on Research and Develop-\n",
      "ment in Information Retrieval , pages 1253–1256.\n",
      "Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong,\n",
      "Paul Bennett, Jianfeng Gao, and Zhiyuan Liu. 2020.\n",
      "Few-shot generative conversational query rewriting.\n",
      "arXiv preprint arXiv:2006.05009 .\n",
      "Salah Zaiem and Fatiha Sadat. 2019. Sequence to se-\n",
      "quence learning for query expansion. In Proceed-\n",
      "ings of the AAAI Conference on Artiﬁcial Intelli-\n",
      "gence, Student Abstract Track , volume 33, pages\n",
      "10075–10076.A More Analysis of Retrieval\n",
      "Performance\n",
      "We show the detailed results of top-k retrieval accu-\n",
      "racy of the compared methods in Figs. 2 and 3.\n",
      "GARperforms comparably or better than DPR\n",
      "when k\u0015100on NQ and k\u00155on Trivia.\n",
      "1 5 10 20 50 100 200 300 500 1000\n",
      "k: # of retrieved passages2030405060708090Top-k Accuracy (%)\n",
      "GAR +DPR\n",
      "DPR\n",
      "GAR\n",
      "BM25 +RM3\n",
      "BM25\n",
      "Figure 2: Top-k retrieval accuracy of sparse and\n",
      "dense methods on the test set of NQ. GARimproves\n",
      "BM25 and achieves comparable or better performance\n",
      "than DPR when k\u0015100.\n",
      "1 5 10 20 50 100\n",
      "k: # of retrieved passages5055606570758085Top-k Accuracy (%)\n",
      "GAR +DPR\n",
      "DPR\n",
      "GAR\n",
      "BM25 +RM3\n",
      "BM25\n",
      "Figure 3: Top-k retrieval accuracy on the Trivia test\n",
      "set.GARachieves better results than DPR when k\u00155.\n",
      "We show in Table 9 the retrieval accuracy break-\n",
      "down using the question-answer overlap categories.\n",
      "The most signiﬁcant gap between BM25 and other\n",
      "methods is on the Question Overlap category,\n",
      "which coincides with the fact that BM25 is un-\n",
      "able to conduct question paraphrasing (semantic\n",
      "matching). GARhelps BM25 to bridge the gap by\n",
      "providing the query contexts and even outperform\n",
      "DPR in this category. Moreover, GARconsistently\n",
      "improves over BM25 on other categories and GAR+\n",
      "outperforms DPR as well.Method TotalQuestion\n",
      "OverlapAnswer\n",
      "Overlap\n",
      "OnlyNo\n",
      "Overlap\n",
      "BM25 78.8 81.2 85.1 70.6\n",
      "DPR 86.1 93.2 89.5 76.8\n",
      "GAR 85.3 94.1 87.9 73.7\n",
      "GAR+88.9 96.3 91.7 79.8\n",
      "Table 9: Top-100 retrieval accuracy by question-\n",
      "answer overlap categories on the NQ test set.\n",
      "==================================================\n",
      "\n",
      "Document 5:\n",
      "In-Context Retrieval-Augmented Language Models\n",
      "Ori Ram∗Yoav Levine∗Itay Dalmedigos Dor Muhlgay\n",
      "Amnon Shashua Kevin Leyton-Brown Yoav Shoham\n",
      "AI21 Labs\n",
      "{orir,yoavl,itayd,dorm,amnons,kevinlb,yoavs}@ai21.com\n",
      "Abstract\n",
      "Retrieval-Augmented Language Modeling\n",
      "(RALM) methods, which condition a lan-\n",
      "guage model (LM) on relevant documents\n",
      "from a grounding corpus during generation,\n",
      "were shown to significantly improve lan-\n",
      "guage modeling performance. In addition,\n",
      "they can mitigate the problem of factually\n",
      "inaccurate text generation and provide natu-\n",
      "ral source attribution mechanism. Existing\n",
      "RALM approaches focus on modifying the\n",
      "LM architecture in order to facilitate the in-\n",
      "corporation of external information, signifi-\n",
      "cantly complicating deployment. This paper\n",
      "considers a simple alternative, which we dub\n",
      "In-Context RALM : leaving the LM architec-\n",
      "ture unchanged and prepending grounding\n",
      "documents to the input, without any further\n",
      "training of the LM . We show that In-Context\n",
      "RALM that builds on off-the-shelf general\n",
      "purpose retrievers provides surprisingly large\n",
      "LM gains across model sizes and diverse cor-\n",
      "pora. We also demonstrate that the document\n",
      "retrieval and ranking mechanism can be spe-\n",
      "cialized to the RALM setting to further boost\n",
      "performance. We conclude that In-Context\n",
      "RALM has considerable potential to increase\n",
      "the prevalence of LM grounding, particularly\n",
      "in settings where a pretrained LM must be\n",
      "used without modification or even via API\n",
      "access.1\n",
      "1 Introduction\n",
      "Recent advances in language modeling (LM) have\n",
      "dramatically increased the usefulness of machine-\n",
      "generated text across a wide range of use-cases\n",
      "and domains (Brown et al., 2020). However, the\n",
      "mainstream paradigm of generating text with LMs\n",
      "bears inherent limitations in access to external\n",
      "knowledge. First, LMs are not coupled with any\n",
      "∗Equal contribution.\n",
      "1Our code is available at https://github.com/\n",
      "AI21Labs/in-context-ralm\n",
      "Perplexity\n",
      "10.015.020.025.030.0\n",
      "GPT-2 345M (M) GPT-2 1.5B (XL)No Retrieval In-Context RALM (BM25)\n",
      "In-Context RALM (Predictive Reranking)Figure 1: Our framework, dubbed In-Context\n",
      "RALM , provides large language modeling gains on\n",
      "the test set of WikiText-103, without modifying the\n",
      "LM. Adapting the use of a BM25 retriever (Robert-\n",
      "son and Zaragoza, 2009) to the LM task (§5) yields\n",
      "significant gains, and choosing the grounding doc-\n",
      "uments via our new class of Predictive Rerankers\n",
      "(§6) provides a further boost. See Table 1 for the\n",
      "full results on five diverse corpora.\n",
      "source attribution, and must be trained in order\n",
      "to incorporate up-to-date information that was not\n",
      "seen during training. More importantly, they tend\n",
      "to produce factual inaccuracies and errors (Lin\n",
      "et al., 2022; Maynez et al., 2020; Huang et al.,\n",
      "2020). This problem is present in any LM gen-\n",
      "eration scenario, and is exacerbated when gener-\n",
      "ation is made in uncommon domains or private\n",
      "data. A promising approach for addressing the\n",
      "above is Retrieval-Augmented Language Modeling\n",
      "(RALM), grounding the LM during generation by\n",
      "conditioning on relevant documents retrieved from\n",
      "an external knowledge source. RALM systems in-\n",
      "clude two high level components: (i) document se-\n",
      "lection , selecting the set of documents upon which\n",
      "to condition; and (ii) document reading , determin-\n",
      "ing how to incorporate the selected documents into\n",
      "the LM generation process.\n",
      "Leading RALM systems introduced recentlyarXiv:2302.00083v3  [cs.CL]  1 Aug 2023Language \n",
      "Model World Cup 2022 was the \n",
      "last with 32 teams, \n",
      "before the increase to Retriever FIFA World Cup 2026 will \n",
      "expand to 48 teams. \n",
      "World Cup 2022 was the \n",
      "last with 32 teams, before \n",
      "the increase to 48 in the 2026 \n",
      "tournament. \n",
      "Figure 2: An example of In-Context RALM : we simply prepend the retrieved document before the input prefix.\n",
      "tend to be focused on altering the language model\n",
      "architecture (Khandelwal et al., 2020; Borgeaud\n",
      "et al., 2022; Zhong et al., 2022; Levine et al., 2022c;\n",
      "Li et al., 2022). Notably, Borgeaud et al. (2022) in-\n",
      "troduced RETRO, featuring document reading via\n",
      "nontrivial modifications that require further train-\n",
      "ing to the LM architecture, while using an off-the-\n",
      "shelf frozen BERT retriever for document selec-\n",
      "tion. Although the paper’s experimental findings\n",
      "showed impressive performance gains, the need for\n",
      "changes in architecture and dedicated retraining\n",
      "has hindered the wide adoption of such models.\n",
      "In this paper, we show that a very simple doc-\n",
      "ument reading mechanism can have a large im-\n",
      "pact, and that substantial gains can also be made\n",
      "by adapting the document selection mechanism to\n",
      "the task of language modeling. Thus, we show that\n",
      "many of the benefits of RALM can be achieved\n",
      "while working with off-the-shelf LMs, even via\n",
      "API access. Specifically, we consider a simple but\n",
      "powerful RALM framework, dubbed In-Context\n",
      "RALM (presented in Section 3), which employs a\n",
      "zero-effort document reading mechanism: we sim-\n",
      "ply prepend the selected documents to the LM’s\n",
      "input text (Figure 2).\n",
      "Section 4 describes our experimental setup. To\n",
      "show the wide applicability of our framework, we\n",
      "performed LM experiments on a suite of five di-\n",
      "verse corpora: WikiText-103 (Merity et al., 2016),\n",
      "RealNews (Zellers et al., 2019), and three datasets\n",
      "from The Pile (Gao et al., 2021): ArXiv, Stack\n",
      "Exchange and FreeLaw. We use open-source LMs\n",
      "ranging from 110M to 66B parameters (from the\n",
      "GPT-2, GPT-Neo, OPT and LLaMA model fami-\n",
      "lies).\n",
      "In Section 5 we evaluate the application of off-\n",
      "the-shelf retrievers to our framework. In this\n",
      "minimal-effort setting, we found that In-Context\n",
      "RALM led to LM performance gains equivalent to\n",
      "increasing the LM’s number of parameters by 2–\n",
      "3×across all of the text corpora we examined. In\n",
      "Section 6 we investigate methods for adapting doc-ument ranking to the LM task, a relatively under-\n",
      "explored RALM degree of freedom. Our adapta-\n",
      "tion methods range from using a small LM to per-\n",
      "form zero-shot ranking of the retrieved documents,\n",
      "up to training a dedicated bidirectional reranker\n",
      "by employing self-supervision from the LM signal .\n",
      "These methods lead to further gains in the LM task\n",
      "corresponding to an additional size increase of 2×\n",
      "in the LM architecture. As a concrete example of\n",
      "the gains, a 345M parameter GPT-2 enhanced by\n",
      "In-Context RALM outperforms a 762M parame-\n",
      "ter GPT-2 when employing an off-the-shelf BM25\n",
      "retriever (Robertson and Zaragoza, 2009), and out-\n",
      "performs a 1.5B parameter GPT-2 when employing\n",
      "our trained LM-oriented reranker (see Figure 1).\n",
      "For large model sizes, our method is even more\n",
      "effective: In-Context RALM with an off-the-shelf\n",
      "retriever improved the performance of a 6.7B pa-\n",
      "rameter OPT model to match that of a 66B param-\n",
      "eter parameter OPT model (see Figure 4).\n",
      "In Section 7 we demonstrate the applicability\n",
      "of In-Context RALM to downstream open-domain\n",
      "questions answering (ODQA) tasks.\n",
      "In a concurrent work, Shi et al. (2023) also sug-\n",
      "gest to augment off-the-shelf LMs with retrieved\n",
      "texts by prepending them to the input. Their re-\n",
      "sults are based on training a dedicated retriever for\n",
      "language modeling. In contrast, we focus on the\n",
      "gains achievable in using off-the-shelf retrievers\n",
      "for this task. We show strong gains of this simpler\n",
      "setting by investigating: (1) which off-the-shelf\n",
      "retriever is best suited for language modeling, (2)\n",
      "the frequency of retrieval operations, and (3) the\n",
      "optimal query length. In addition, we boost the off-\n",
      "the-shelf retrieval performance by introducing two\n",
      "reranking methods that demonstrate further gains\n",
      "in perplexity.\n",
      "We believe that In-Context RALM can play two\n",
      "important roles in making RALM systems more\n",
      "powerful and more prevalent. First, given its simple\n",
      "reading mechanism, In-Context RALM can serve\n",
      "as a clean probe for developing document retrievalmethods that are specialized for the LM task. These\n",
      "in turn can be used to improve both In-Context\n",
      "RALM and other more elaborate RALM methods\n",
      "that currently leverage general purpose retrievers.\n",
      "Second, due to its compatibility with off-the-shelf\n",
      "LMs, In-Context RALM can help drive wider de-\n",
      "ployment of RALM systems.\n",
      "2 Related Work\n",
      "RALM approaches can be roughly divided into two\n",
      "families of models: (i) nearest-neighbor language\n",
      "models (also called kNN-LM), and (ii) retrieve\n",
      "and read models . Our work belongs to the second\n",
      "family, but is distinct in that it involves no further\n",
      "training of the LM.\n",
      "Nearest Neighbor Language Models ThekNN-\n",
      "LM approach was first introduced in Khandel-\n",
      "wal et al. (2020). The authors suggest a simple\n",
      "inference-time model that interpolates between two\n",
      "next-token distributions: one induced by the LM\n",
      "itself, and one induced by the kneighbors from the\n",
      "retrieval corpus that are closest to the query token in\n",
      "the LM embedding space. Zhong et al. (2022) sug-\n",
      "gest a framework for training these models. While\n",
      "they showed significant gains from kNN-LM, the\n",
      "approach requires storing the representations for\n",
      "each token in the corpus , an expensive requirement\n",
      "even for a small corpus like Wikipedia. Although\n",
      "numerous approaches have been suggested for al-\n",
      "leviating this issue (He et al., 2021; Alon et al.,\n",
      "2022), scaling any of them to large corpora remains\n",
      "an open challenge.\n",
      "Retrieve and Read Models This family of\n",
      "RALMs creates a clear division between document\n",
      "selection anddocument reading components. All\n",
      "prior work involves training the LM. We begin by\n",
      "describing works that use this approach for tack-\n",
      "ling downstream tasks, and then mention works ori-\n",
      "ented towards RALM. Lewis et al. (2020) and Izac-\n",
      "ard and Grave (2021) fine tuned encoder–decoder\n",
      "architectures for downstream knowledge-intensive\n",
      "tasks. Izacard et al. (2022b) explored different\n",
      "ways of pretraining such models, while Levine\n",
      "et al. (2022c) pretrained an autoregressive LM on\n",
      "clusters of nearest neighbors in sentence embed-\n",
      "ding space. Levine et al. (2022a) showed competi-\n",
      "tive open domain question-answering performance\n",
      "by prompt-tuning a frozen LM as a reader. Guu\n",
      "et al. (2020) pretrained REALM, a retrieval aug-\n",
      "mented bidirectional, masked LM, later fine-tunedfor open-domain question answering. The work\n",
      "closest to this paper—with a focus on the language\n",
      "modeling task—is RETRO (Borgeaud et al., 2022),\n",
      "which modifies an autoregressive LM to attend to\n",
      "relevant documents via chunked cross-attention,\n",
      "thus introducing new parameters to the model. Our\n",
      "In-Context RALM differs from prior work in this\n",
      "family of models in two key aspects:\n",
      "•We use off-the-shelf LMs for document read-\n",
      "ingwithout any further training of the LM .\n",
      "•We focus on how to choose documents for\n",
      "improved LM performance .\n",
      "3 Our Framework\n",
      "3.1 In-Context RALM\n",
      "Language models define probability distributions\n",
      "over sequences of tokens. Given such a sequence\n",
      "x1, ..., x n, the standard way to model its probabil-\n",
      "ity is via next-token prediction: p(x1, ..., x n) =Qn\n",
      "i=1p(xi|x<i), where x<i:=x1, ..., x i−1is the\n",
      "sequence of tokens preceding xi, also referred to\n",
      "as its prefix . This autoregressive model is usu-\n",
      "ally implemented via a learned transformer net-\n",
      "work (Vaswani et al., 2017) parameterized by the\n",
      "set of parameters θ:\n",
      "p(x1, ..., x n) =nY\n",
      "i=1pθ(xi|x<i), (1)\n",
      "where the conditional probabilities are modeled\n",
      "by employing a causal self-attention mask (Rad-\n",
      "ford et al., 2018). Notably, leading LMs such\n",
      "as GPT-2 (Radford et al., 2019), GPT-3 (Brown\n",
      "et al., 2020), OPT (Zhang et al., 2022) or Jurassic-\n",
      "1 (Lieber et al., 2021) follow this simple parame-\n",
      "terization.\n",
      "Retrieval augmented language models (RALMs)\n",
      "add an operation that retrieves one or more docu-\n",
      "ments from an external corpus C, and condition the\n",
      "above LM predictions on these documents. Specifi-\n",
      "cally, for predicting xi, the retrieval operation from\n",
      "Cdepends on its prefix: RC(x<i), so the most\n",
      "general RALM decomposition is: p(x1, ..., x n) =Qn\n",
      "i=1p(xi|x<i,RC(x<i)). In order to condition\n",
      "the LM generation on the retrieved document, pre-\n",
      "vious RALM approaches used specialized architec-\n",
      "tures or algorithms (see §2). Inspired by the suc-\n",
      "cess of In-Context Learning (Brown et al., 2020;\n",
      "Dong et al., 2023), In-Context RALM refers to the\n",
      "following specific, simple method of concatenatingthe retrieved documents2within the Transformer’s\n",
      "input prior to the prefix (see Figure 2), which does\n",
      "not involve altering the LM weights θ:\n",
      "p(x1, ..., x n) =\n",
      "nY\n",
      "i=1pθ(xi|[RC(x<i);x<i]),(2)\n",
      "where [a;b]denotes the concatenation of strings a\n",
      "andb.\n",
      "Since common Transformer-based LM imple-\n",
      "mentations support limited length input sequences,\n",
      "when the concatenation of the document and the\n",
      "input sequence exceed this limit we remove to-\n",
      "kens from the beginning of xuntil the overall input\n",
      "length equals that allowed by the model. Because\n",
      "our retrieved documents are passages of limited\n",
      "length, we always have enough context left from x\n",
      "(see §4.3).\n",
      "3.2 RALM Design Choices\n",
      "We detail below two practical design choices often\n",
      "made in RALM systems. In §5, we investigate the\n",
      "effect of these in the setting of In-Context RALM.\n",
      "Retrieval Stride While in the above formulation\n",
      "a retrieval operation can occur at each generation\n",
      "step, we might want to perform retrieval only once\n",
      "every s >1tokens due to the cost of calling the\n",
      "retriever, and the need to replace the documents in\n",
      "the LM prefix during generation. We refer to sas\n",
      "theretrieval stride . This gives rise to the follow-\n",
      "ing In-Context RALM formulation (which reduces\n",
      "back to Eq. (2) for s= 1):\n",
      "p(x1, ..., x n) =\n",
      "ns−1Y\n",
      "j=0sY\n",
      "i=1pθ\u0000\n",
      "xs·j+i|\u0002\n",
      "RC(x≤s·j);x<(s·j+i)\u0003\u0001\n",
      ",\n",
      "(3)\n",
      "where ns=n/sis the number of retrieval strides.\n",
      "Notably, in this framework the runtime costs of\n",
      "each retrieval operation is composed of (a) apply-\n",
      "ing the retriever itself, and (b) recomputing the\n",
      "embeddings of the prefix. In §5.2 we show that us-\n",
      "ing smaller retrieval strides, i.e., retrieving as often\n",
      "as possible, is superior to using larger ones (though\n",
      "In-Context RALM with larger strides already pro-\n",
      "vides large gains over vanilla LM). Thus, choosing\n",
      "the retrieval stride is ultimately a tradeoff between\n",
      "runtime and performance.\n",
      "2We always use a single document , but it is conceptually\n",
      "simple to support multiple documents as well.Retrieval Query Length While the retrieval\n",
      "query above in principle depends on all prefix to-\n",
      "kensx≤s·j, the information at the very end of the\n",
      "prefix is typically the most relevant to the generated\n",
      "tokens. If the retrieval query is too long then this in-\n",
      "formation can be diluted. To avoid this, we restrict\n",
      "the retrieval query at stride jto the last ℓtokens\n",
      "of the prefix, i.e., we use qs,ℓ\n",
      "j:=xs·j−ℓ+1, ..., x s·j.\n",
      "We refer to ℓas the retrieval query length . Note that\n",
      "prior RALM work couples the retrieval stride sand\n",
      "the retrieval query length ℓ(Borgeaud et al., 2022).\n",
      "In §5, we show that enforcing s=ℓdegrades LM\n",
      "performance. Integrating these hyper-parameters\n",
      "into the In-Context RALM formulation gives\n",
      "p(x1, ..., x n) =\n",
      "ns−1Y\n",
      "j=0sY\n",
      "i=1pθ\u0010\n",
      "xs·j+i|h\n",
      "RC(qs,ℓ\n",
      "j);x<(s·j+i)i\u0011\n",
      ".\n",
      "(4)\n",
      "4 Experimental Details\n",
      "We now describe our experimental setup, including\n",
      "all models we use and their implementation details.\n",
      "4.1 Datasets\n",
      "We evaluated the effectiveness of In-Context\n",
      "RALM across five diverse language modeling\n",
      "datasets and two common open-domain question\n",
      "answering datasets.\n",
      "Language Modeling The first LM dataset is\n",
      "WikiText-103 (Merity et al., 2016), which has been\n",
      "extensively used to evaluate RALMs (Khandelwal\n",
      "et al., 2020; He et al., 2021; Borgeaud et al., 2022;\n",
      "Alon et al., 2022; Zhong et al., 2022). Second, we\n",
      "chose three datasets spanning diverse subjects from\n",
      "The Pile (Gao et al., 2021): ArXiv ,Stack Exchange\n",
      "andFreeLaw . Finally, we also investigated Real-\n",
      "News (Zellers et al., 2019), since The Pile lacks a\n",
      "corpus focused only on news (which is by nature a\n",
      "knowledge-intensive domain).\n",
      "Open-Domain Question Answering In order\n",
      "to evaluate In-Context RALM on downstream\n",
      "tasks as well, we use the Natural Questions (NQ;\n",
      "Kwiatkowski et al. 2019) and TriviaQA (Joshi et al.,\n",
      "2017) open-domain question answering datasets.\n",
      "4.2 Models\n",
      "Language Models We performed our experi-\n",
      "ments using the four models of GPT-2 (110M–\n",
      "1.5B; Radford et al. 2019), three models of GPT-\n",
      "Neo and GPT-J (1.3B–6B; Black et al. 2021; Wangand Komatsuzaki 2021), eight models of OPT\n",
      "(125M–66B; Zhang et al. 2022) and three mod-\n",
      "els of LLaMA (7B–33B; Touvron et al. 2023). All\n",
      "models are open source and publicly available.3\n",
      "We elected to study these particular models for\n",
      "the following reasons. The first four (GPT-2) mod-\n",
      "els were trained on WebText (Radford et al., 2019),\n",
      "with Wikipedia documents excluded from their\n",
      "training datasets. We were thus able to evaluate our\n",
      "method’s “zero-shot” performance when retrieving\n",
      "from a novel corpus (for WikiText-103). The rest of\n",
      "the models brought two further benefits. First, they\n",
      "allowed us to investigate how our methods scale\n",
      "to models larger than GPT-2. Second, the fact that\n",
      "Wikipedia was part of their training data allowed us\n",
      "to investigate the usefulness of In-Context RALM\n",
      "for corpora seen during training. The helpfulness\n",
      "of such retrieval has been demonstrated for previ-\n",
      "ous RALM methods (Khandelwal et al., 2020) and\n",
      "has also been justified theoretically by Levine et al.\n",
      "(2022c).\n",
      "We ran all models with a maximum sequence\n",
      "length of 1,024, even though GPT-Neo, OPT and\n",
      "LLaMA models support a sequence length of\n",
      "2,048.4\n",
      "Retrievers We experimented with both sparse\n",
      "(word-based) and dense (neural) retrievers. We\n",
      "used BM25 (Robertson and Zaragoza, 2009) as our\n",
      "sparse model. For dense models, we experimented\n",
      "with (i) a frozen BERT-base (Devlin et al., 2019)\n",
      "followed by mean pooling, similar to Borgeaud\n",
      "et al. (2022); and (ii) the Contriever (Izacard et al.,\n",
      "2022a) and Spider (Ram et al., 2022) models,\n",
      "which are dense retrievers that were trained in un-\n",
      "supervised manners.\n",
      "Reranking When training rerankers (Sec-\n",
      "tion 6.2), we initialized from RoBERTa-base (Liu\n",
      "et al., 2019).\n",
      "4.3 Implementation Details\n",
      "We implemented our code base using the Trans-\n",
      "formers library (Wolf et al., 2020). We based\n",
      "our dense retrieval code on the DPR repository\n",
      "(Karpukhin et al., 2020).\n",
      "3All models are available for use use via https://\n",
      "huggingface.co/\n",
      "4In preliminary experiments, we observed similar improve-\n",
      "ments from In-Context RALM when using a sequence length\n",
      "of 2,048. We used a sequence length of 1,024 in order to\n",
      "facilitate a direct comparison between all models.\n",
      "Perplexity\n",
      "10203040\n",
      "GPT-2 117M (S) GPT-2 1.5B (XL)No Retrieval BERT Contriever Spider BM25Figure 3: The performance of four off-the-shelf\n",
      "retrievers used for In-Context RALM on the de-\n",
      "velopment set of WikiText-103. All RALMs are\n",
      "run with s= 4(i.e., retrieval is applied every four\n",
      "tokens). For each RALM, we report the result of\n",
      "the best query length ℓ(see Figures 6, 9, 10).\n",
      "Retrieval Corpora For WikiText-103 and\n",
      "ODQA datasets, we used the Wikipedia corpus\n",
      "from Dec. 20, 2018, standardized by Karpukhin\n",
      "et al. (2020) using the preprocessing from Chen\n",
      "et al. (2017). To avoid contamination, we found\n",
      "and removed all 120 articles of the development\n",
      "and test set of WikiText-103 from the corpus. For\n",
      "the remaining datasets, we used their training\n",
      "data as the retrieval corpus. Similar to Karpukhin\n",
      "et al. (2020), our retrieval corpora consist of\n",
      "non-overlapping passages of 100 words (which\n",
      "translate to less than 150 tokens for the vast\n",
      "majority of passages). Thus, we truncate our\n",
      "retrieved passages at 256 tokens when input to the\n",
      "models, but they are usually much smaller.\n",
      "Retrieval For sparse retrieval, we used the Py-\n",
      "serini library (Lin et al., 2021). For dense retrieval,\n",
      "we applied exact search using FAISS (Johnson\n",
      "et al., 2021).\n",
      "5 The Effectiveness of In-Context RALM\n",
      "with Off-the-Shelf Retrievers\n",
      "We now empirically show that despite its simple\n",
      "document reading mechanism, In-Context RALM\n",
      "leads to substantial LM gains across our diverse\n",
      "evaluation suite. We begin in this section by inves-\n",
      "tigating the effectiveness of off-the-shelf retrievers\n",
      "for In-Context RALM; we go on in §6 to show\n",
      "that further LM gains can be made by tailoring\n",
      "document ranking functions to the LM task.\n",
      "The experiments in this section provided us\n",
      "with a recommended configuration for applying In-Model Retrieval RerankingWikiText-103 RealNews ArXiv Stack Exch. FreeLaw\n",
      "word ppl token ppl token ppl token ppl token ppl\n",
      "GPT-2 S– – 37.5 21.3 12.0 12.8 13.0\n",
      "BM25 §5 – 29.6 16.1 10.9 11.3 9.6\n",
      "BM25 Zero-shot §6.1 28.6 15.5 10.1 10.6 8.8\n",
      "BM25 Predictive §6.2 26.8 – – – –\n",
      "GPT-2 M– – 26.3 15.7 9.3 8.8 9.6\n",
      "BM25 §5 – 21.5 12.4 8.6 8.1 7.4\n",
      "BM25 Zero-shot §6.1 20.8 12.0 8.0 7.7 6.9\n",
      "BM25 Predictive §6.2 19.7 – – – –\n",
      "GPT-2 L– – 22.0 13.6 8.4 8.5 8.7\n",
      "BM25 §5 – 18.1 10.9 7.8 7.8 6.8\n",
      "BM25 Zero-shot §6.1 17.6 10.6 7.3 7.4 6.4\n",
      "BM25 Predictive §6.2 16.6 – – – –\n",
      "GPT-2 XL– – 20.0 12.4 7.8 8.0 8.0\n",
      "BM25 §5 – 16.6 10.1 7.2 7.4 6.4\n",
      "BM25 Zero-shot §6.1 16.1 9.8 6.8 7.1 6.0\n",
      "BM25 Predictive §6.2 15.4 – – – –\n",
      "Table 1: Perplexity on the test set of WikiText-103, RealNews and three datasets from the Pile. For\n",
      "each LM, we report: (a) its performance without retrieval, (b) its performance when fed the top-scored\n",
      "passage by BM25 (§5), and (c) its performance when applied on the top-scored passage of each of our two\n",
      "suggested rerankers (§6). All models share the same vocabulary, thus token-level perplexity ( token ppl )\n",
      "numbers are comparable. For WikiText we follow prior work and report word-level perplexity ( word ppl ).\n",
      "Model RetrievalWikiText-103\n",
      "word ppl\n",
      "LLaMA-7B- 9.9\n",
      "BM25, §5 8.8\n",
      "LLaMA-13B- 8.5\n",
      "BM25, §5 7.6\n",
      "LLaMA-33B- 6.3\n",
      "BM25, §5 6.1\n",
      "Table 2: The performance of models from the\n",
      "LLaMA family, measured by word-level perplexity\n",
      "on the test set of WikiText-103.\n",
      "Context RALM: applying a sparse BM25 retriever\n",
      "that receives ℓ= 32 query tokens and is applied\n",
      "as frequently as possible. Practically, we retrieve\n",
      "every s= 4 tokens ( ℓandsare defined in §3).\n",
      "Table 1 shows for the GPT-2 models that across\n",
      "all the examined corpora, employing In-Context\n",
      "RALM with an off-the-shelf retriever improved\n",
      "LM perplexity to a sufficient extent that it matched\n",
      "that of a 2–3×larger model. Figure 4 and Tables 2\n",
      "and 5 show that this trend holds across model sizes\n",
      "up to 66B parameters, for both WikiText-103 andRealNews.\n",
      "5.1 BM25 Outperforms Off-the-Shelf Neural\n",
      "Retrievers in Language Modeling\n",
      "We experimented with different off-the-shelf gen-\n",
      "eral purpose retrievers, and found that the sparse\n",
      "(lexical) BM25 retriever (Robertson and Zaragoza,\n",
      "2009) outperformed three popular dense (neu-\n",
      "ral) retrievers: the self-supervised retrievers Con-\n",
      "triever (Izacard et al., 2022a) and Spider (Ram et al.,\n",
      "2022), as well as a retriever based on the average\n",
      "pooling of BERT embeddings that was used in\n",
      "the RETRO system (Borgeaud et al., 2022). We\n",
      "conducted a minimal hyper-parameter search on\n",
      "the query length ℓfor each of the retrievers, and\n",
      "found that ℓ= 32 was optimal for BM25 (Fig-\n",
      "ure 6), and ℓ= 64 worked best for dense retrievers\n",
      "(Figures 9, 10).\n",
      "Figure 3 compares the performance gains of In-\n",
      "Context RALM with these four general-purpose re-\n",
      "trievers. The BM25 retriever clearly outperformed\n",
      "all dense retrievers. This outcome is consistent\n",
      "with prior work showing that BM25 outperforms\n",
      "neural retrievers across a wide array of tasks, when\n",
      "applied in zero-shot settings (Thakur et al., 2021).\n",
      "This result renders In-Context RALM even moreWikiText-103Perplexity\n",
      "10.020.030.040.0\n",
      "OPT-125M OPT-350M OPT-1.3B OPT-2.7B OPT-6.7B OPT-13B OPT-30B OPT-66BNo Retrieval In-Context RALM (BM25)\n",
      "RealNewsPerplexity\n",
      "3.08.013.018.0\n",
      "OPT-125M OPT-350M OPT-1.3B OPT-2.7B OPT-6.7B OPT-13B OPT-30B OPT-66BNo Retrieval In-Context RALM (BM25)Figure 4: Results of OPT models (Zhang et al., 2022) on the test set of WikiText-103 (word-level\n",
      "perplexity) and the development set of RealNews (token-level perplexity). In-Context RALM models use\n",
      "a BM25 retriever with s= 4(i.e., the retriever is called every four tokens) and ℓ= 32 (i.e., the retriever\n",
      "query is comprised of the last 32 tokens of the prefix). In-Context RALM with an off-the-shelf retriever\n",
      "improved the performance of a 6.7B parameter OPT model to match that of a 66B parameter OPT model.\n",
      "appealing since applying a BM25 retriever is sig-\n",
      "nificantly cheaper than the neural alternatives.\n",
      "5.2 Frequent Retrieval Improves Language\n",
      "Modeling\n",
      "We investigated the effect of varying the retrieval\n",
      "stride s(i.e., the number of tokens between consec-\n",
      "utive retrieval operations). Figure 5 shows that LM\n",
      "performance improved as the retrieval operation\n",
      "became more frequent. This supports the intuition\n",
      "that retrieved documents become more relevant the\n",
      "closer the retrieval query becomes to the gener-\n",
      "ated tokens. Of course, each retrieval operation\n",
      "imposes a runtime cost. To balance performance\n",
      "and runtime, we used s= 4 in our experiments.\n",
      "For comparison, RETRO employed a retrieval fre-\n",
      "quency of s= 64 (Borgeaud et al., 2022), which\n",
      "leads to large degradation in perplexity. Intuitively,\n",
      "retrieving with high frequency (low retrieval stride)\n",
      "allows to ground the LM in higher resolution.5.3 A Contextualization vs. Recency Tradeoff\n",
      "in Query Length\n",
      "We also investigated the effect of varying ℓ, the\n",
      "length of the retrieval query for BM25. Figure 6\n",
      "reveals an interesting tradeoff and a sweet spot\n",
      "around a query length of 32tokens. Similar ex-\n",
      "periments for dense retrievers are given in App. A.\n",
      "We conjecture that when the retriever query is too\n",
      "short, it does not include enough of the input con-\n",
      "text, decreasing the retrieved document’s relevance.\n",
      "Conversely, excessively growing the retriever query\n",
      "deemphasizes the tokens at the very end of the pre-\n",
      "fix, diluting the query’s relevance to the LM task.\n",
      "6 Improving In-Context RALM with\n",
      "LM-Oriented Reranking\n",
      "Since In-Context RALM uses a fixed document\n",
      "reading component by definition, it is natural to\n",
      "ask whether performance can be improved by spe-\n",
      "cializing its document retrieval mechanism to the\n",
      "LM task. Indeed, there is considerable scope for\n",
      "improvement: the previous section considered con-\n",
      "ditioning the model only on the first document re-Retrieval Stride ( 𝑠)Perplexity\n",
      "10.020.030.040.0\n",
      "1 2 4 8 16 32 64GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)Figure 5: An analysis of perplexity as a function\n",
      "ofs, the retrieval stride ,i.e., the number of tokens\n",
      "between consecutive retrieval operations, on the\n",
      "development set of WikiText-103. Throughout the\n",
      "paper, we use s= 4 to balance perplexity and\n",
      "runtime.\n",
      "Retrieval Query Length (ℓ)Perplexity\n",
      "10.015.020.025.030.035.0\n",
      "16 32 64GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)\n",
      "Figure 6: An analysis of perplexity as a function\n",
      "ofthe number of tokens in the query ℓfor BM25\n",
      "on the development set of WikiText-103. In the\n",
      "appendix, we show similar trade-offs for dense\n",
      "retrievers within WikiText-103. Throughout the\n",
      "paper, we use a query length of ℓ= 32 tokens.\n",
      "trieved by the BM25 retriever. This permits very\n",
      "limited semantic understanding of the query, since\n",
      "BM25 is based only on the bag of words signal.\n",
      "Moreover, it offers no way to accord different de-\n",
      "grees of importance to different retrieval query to-\n",
      "kens, such as recognizing that later query tokens\n",
      "are more relevant to the generated text.\n",
      "In this section, we focus on choosing which doc-\n",
      "ument to present to the model, by reranking the\n",
      "top-kdocuments returned by the BM25 retriever.5\n",
      "We use Figure 7 as motivation: it shows the large\n",
      "potential for improvement among the top- 16docu-\n",
      "ments returned by the BM25 retriever. We act upon\n",
      "5In both §6.1 and §6.2 we use k= 16 .\n",
      "Perplexity\n",
      "10.020.030.040.0\n",
      "No Retrieval BM25 (Top-1) Oracle: BM25 (Top-16)GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)Figure 7: Potential for gains from reranking: per-\n",
      "plexity improvement (on the development set of\n",
      "WikiText-103) from an oracle that takes the best\n",
      "of the top-16 documents retrieved by BM25 rather\n",
      "than the first.\n",
      "this motivation by using two rerankers. Specifi-\n",
      "cally, in §6.1 we show performance gains across\n",
      "our evaluation suite obtained by using an LM to\n",
      "perform zero-shot reranking of the top- kBM25\n",
      "retrieved documents (results in third row for each\n",
      "of the models in Table 1). Then, in §6.2 we show\n",
      "that training a specialized bidirectional reranker\n",
      "of the top- kBM25 retrieved documents in a self-\n",
      "supervised manner via the LM signal can provide\n",
      "further LM gains (results in forth row for each of\n",
      "the models in Table 1).\n",
      "6.1 LMs as Zero-Shot Rerankers\n",
      "First, we used off-the-shelf language models as\n",
      "document rerankers for the In-Context RALM set-\n",
      "ting. Formally, for a query qconsisting of the\n",
      "lastℓtokens in the prefix of the LM input x, let\n",
      "{d1, ..., d k}be the top- kdocuments returned by\n",
      "BM25. For retrieval iteration j, let the text for\n",
      "generation be y:=xs·j+1, ..., x s·j+s. Ideally, we\n",
      "would like to find the document di∗that maximizes\n",
      "the probability of the text for generation, i.e.,\n",
      "i∗= arg max\n",
      "i∈[k]pθ(y|[di;x≤s·j]). (5)\n",
      "However, at test time we do not have access to\n",
      "the tokens of y. Instead, we used the last pre-\n",
      "fixtokens (which areavailable at test time), de-\n",
      "noted by y′, for reranking. Formally, let s′be\n",
      "a hyper-parameter that determines the number of\n",
      "the prefix tokens by which to rerank. We define\n",
      "y′:=xs·j−s′+1, ..., x s·j(i.e., the stride of length s′\n",
      "that precedes y) and choose the document dˆisuchModelReranking\n",
      "ModelWikiText-103 RealNews\n",
      "word ppl token ppl\n",
      "GPT-2 345M (M)GPT-2 110M (S) 20.8 12.1\n",
      "GPT-2 345M (M) 20.8 12.0\n",
      "GPT-2 762M (L)GPT-2 110M (S) 17.7 10.7\n",
      "GPT-2 762M (L) 17.6 10.6\n",
      "GPT-2 1.5B (XL)GPT-2 110M (S) 16.2 9.9\n",
      "GPT-2 1.5B (XL) 16.1 9.8\n",
      "Table 3: Perplexity for zero-shot reranking (§6.1) where the reranking models is smaller than the LM, or\n",
      "the LM itself. Reranking is performed on the top 16 documents retrieved by BM25. Using a GPT-2 110M\n",
      "(S) instead of a larger language model as a reranker leads to only a minor degradation.\n",
      "that\n",
      "ˆi= arg max\n",
      "i∈[k]pϕ(y′|\u0002\n",
      "di;x≤(s·j−s′)\u0003\n",
      ").(6)\n",
      "The main motivation is that since BM25 is a lexical\n",
      "retriever, we want to incorporate a semantic signal\n",
      "induced by the LM. Also, this reranking shares con-\n",
      "ceptual similarities with the reranking framework\n",
      "of Sachan et al. (2022) for open-domain question\n",
      "answering, where y′(i.e., the last prefix tokens) can\n",
      "be thought of as their “question”.\n",
      "Note that our zero-shot reranking does not re-\n",
      "quire that the LM used for reranking is the same\n",
      "model as the LM used for generation ( i.e., the LM\n",
      "in Eq. (6), parameterized by ϕ, does not need to be\n",
      "the LM in Eq. (2), parameterized by θ). This ob-\n",
      "servation unlocks the possibility of reranking with\n",
      "smaller (and thus faster) models, which is impor-\n",
      "tant for two main reasons: (i) Reranking kdocu-\n",
      "ments requires kforward passes; and (ii) it allows\n",
      "our methods to be used in cases where the actual\n",
      "LM’s log probabilities are not available (for exam-\n",
      "ple, when the LM is accessed through an API).6\n",
      "Results A minimal hyper-parameter search on\n",
      "the development set of WikiText-103 revealed that\n",
      "the optimal query length is s′= 16 ,7so we proceed\n",
      "with this value going forward. Table 1 shows the\n",
      "results of letting the LM perform zero-shot rerank-\n",
      "ing on the top-16 documents retrieved by BM25\n",
      "(third row for each of the models). It is evident\n",
      "that reranking yielded consistently better results\n",
      "than simply taking the first result returned by the\n",
      "retriever.\n",
      "6Note we do not require that the two models share the\n",
      "same vocabulary.\n",
      "7We experimented with s′∈ {4,8,16,32}.Table 3 shows that a small LM (GPT-2 117M)\n",
      "can be used to rerank the documents for all larger\n",
      "GPT-2 models, with roughly the same performance\n",
      "as having each LM perform reranking for itself,\n",
      "supporting the applicability of this method for LMs\n",
      "that are only accessible via an API.\n",
      "6.2 Training LM-dedicated Rerankers\n",
      "Next, we trained a reranker to choose one of the\n",
      "top-kdocuments retrieved by BM25. We refer to\n",
      "this approach as Predictive Reranking , since the\n",
      "reranker learns to choose which document will help\n",
      "in “predicting” the upcoming text. For this process,\n",
      "we assume availability of training data from the\n",
      "target corpus. Our reranker is a classifier that gets\n",
      "a prefix x≤s·jand a document di(fori∈[k]), and\n",
      "produces a scalar f(x≤s·j, di)that should resemble\n",
      "the relevance of diforthe continuation ofx≤s·j.\n",
      "We then normalize these relevance scores:\n",
      "prank(di|x≤s·j) =exp(f(x≤s·j, di))Pk\n",
      "i′=1exp(f(x≤s·j, di′)),(7)\n",
      "and choose the document dˆisuch that\n",
      "ˆi= arg max\n",
      "i∈[k]prank(di|x≤s·j). (8)\n",
      "Collecting Training Examples To train our pre-\n",
      "dictive reranker, we collected training examples\n",
      "as follows. Let x≤s·jbe a prefix we sample from\n",
      "the training data, and y:=xs·j+1, ..., x s·j+sbe the\n",
      "text for generation upcoming in its next stride. We\n",
      "run BM25 on the query qs,ℓ\n",
      "jderived from x≤s·j\n",
      "(see §3.2) and get kdocuments {d1, ..., d k}. For\n",
      "each document di, we then run the LM to compute\n",
      "pθ(y|[di;x≤s·j])similar to Eq. (4).Number of documentsEM\n",
      "0.020.040.060.080.0\n",
      "0 1 2 3 4TriviaQA: LLaMa-7B TriviaQA: LLaMa-13B\n",
      "NQ: LLaMa-7B NQ: LLaMa-13BFigure 8: Zero-shot performance of In-Context\n",
      "RALM on the development set of Natural Ques-\n",
      "tions and TriviaQA, when varying the number of\n",
      "documents (retrieved by DPR) shown in-context.\n",
      "Training Our reranker was a fine-tuned\n",
      "RoBERTa-base (Liu et al., 2019) that trained for\n",
      "10,000 steps with a peak learning rate of 10−5and\n",
      "a batch size of 32. Overall, we created 300,000\n",
      "examples from the training set of WikiText-103 as\n",
      "explained above. The loss function we use to train\n",
      "the reranker follows previous work (Guu et al.,\n",
      "2020; Lewis et al., 2020):\n",
      "−logkX\n",
      "i=1prank(di|x≤s·j)·pθ(y|[di;x≤s·j]).(9)\n",
      "Note that unlike those works, we train only the\n",
      "reranker ( prank), keeping the LM weights θfrozen.\n",
      "Results Table 1 shows the result of our predictive\n",
      "reranker, trained on WikiText-103. Specifically, we\n",
      "trained it with data produced by GPT-2 110M (S),\n",
      "and tested its effectiveness for all GPT-2 models.\n",
      "We observed significant gains obtained from Predic-\n",
      "tive Reranking. For example, the perplexity of GPT-\n",
      "2 110M (S) improved from 29.6 to 26.8, and that of\n",
      "GPT-2 1.5B (XL) improved from 16.6 to 15.4. This\n",
      "trend held for the other two models as well. Overall,\n",
      "these results demonstrate that training a reranker\n",
      "with domain-specific data was more effective than\n",
      "zero-shot reranking (Section 6.1). Note that these\n",
      "results—while impressive—still leave room for fur-\n",
      "ther improvements, compared to the top-16 BM25\n",
      "oracle results (see Figure 7). Moreover, the oracle\n",
      "results themselves can be improved by retrieving\n",
      "k > 16documents via a BM25 retriever, or by\n",
      "training stronger retrievers dedicated to the RALM\n",
      "task. We leave this direction for future work.Model Retrieval NQ TriviaQA\n",
      "LLaMA-7B- 10.3 47.5\n",
      "DPR 28.0 56.0\n",
      "LLaMA-13B- 12.0 54.8\n",
      "DPR 31.0 60.1\n",
      "LLaMA-33B- 13.7 58.3\n",
      "DPR 32.3 62.7\n",
      "Table 4: Zero-shot results of In-Context RALM on\n",
      "the test set of Natural Questions and TriviaQA mea-\n",
      "sured by exact match. In the open-book setting, we\n",
      "include the top two documents returned by DPR.\n",
      "7 In-Context RALM for Open-Domain\n",
      "Question Answering\n",
      "So far, we evaluated our framework on language\n",
      "modeling benchmarks. To test its efficacy in addi-\n",
      "tional scenarios, and specifically downstream tasks,\n",
      "we now turn to evaluate In-Context RALM on open-\n",
      "domain question answering (ODQA; Chen et al.\n",
      "2017). This experiment is intended to verify, in\n",
      "a controlled environment, that LMs can leverage\n",
      "retrieved documents without further training and\n",
      "without any training examples . Specifically, we\n",
      "use the LLaMA family (Touvron et al., 2023) with\n",
      "andwithout In-Context RALM (often referred to\n",
      "in ODQA literature as open-book and closed-book\n",
      "settings, respectively). In contrast to most prior\n",
      "work on ODQA ( e.g., Izacard and Grave 2021; Fa-\n",
      "jcik et al. 2021; Izacard et al. 2022b; Levine et al.\n",
      "2022b), our “reader” ( i.e., the model that gets the\n",
      "question along with its corresponding retrieved doc-\n",
      "uments, and returns the answer) is simply a frozen\n",
      "large LM: notpretrained, fine-tuned or prompted\n",
      "to be retrieval-augmented. For the closed-book set-\n",
      "ting, we utilize the prompt of Touvron et al. (2023).\n",
      "For the open-book setting, we extend this prompt\n",
      "to include retrieved documents (see App. C). We\n",
      "use DPR (Karpukhin et al., 2020) as our retriever.\n",
      "Varying the Number of Documents To inves-\n",
      "tigate the the effect of the number of documents\n",
      "shown to the model, we performed a minimal anal-\n",
      "ysis on the development set of NQ and TriviaQA.\n",
      "Figure 8 demonstrates that showing documents in-\n",
      "context significantly improves the model’s perfor-\n",
      "mance. In addition, most of the gain can be ob-\n",
      "tained by using only two documents (or even a\n",
      "single one in some cases).Results Table 4 gives the results of In-Context\n",
      "RALM on the test set of Natural Questions and\n",
      "TriviaQA. Motivated by our previous findings,\n",
      "we used two retrieved documents. It is evident\n",
      "that showing the model relevant documents sig-\n",
      "nificantly boosted its performance. For example,\n",
      "adding retrieved documents improved LLaMA-\n",
      "13B in the zero-shot setting by more than 18 points\n",
      "on NQ (from 12.0% to 31.0%) and more than 5\n",
      "points on TriviaQA (from 54.8% to 60.1%).\n",
      "8 Discussion\n",
      "Retrieval from external sources has become a com-\n",
      "mon practice in knowledge-intensive tasks (such\n",
      "as factual question answering, fact checking, and\n",
      "more; Petroni et al. 2021). In parallel, recent break-\n",
      "throughs in LM generation capabilities has led to\n",
      "LMs that can generate useful long texts. How-\n",
      "ever, factual inaccuracies remain a common way in\n",
      "which machine-generated text can fall short, and\n",
      "lack of direct provenance makes it hard to trust\n",
      "machine generated text. This makes language mod-\n",
      "eling both a promising and an urgent new applica-\n",
      "tion area for knowledge grounding, and motivates\n",
      "promoting RALM approaches. Prior research has\n",
      "already investigated RALM, of course, but it is\n",
      "not yet widely deployed. One likely reason is that\n",
      "existing approaches rely upon fine-tuning the LM,\n",
      "which is typically difficult and costly, and is even\n",
      "impossible for LMs accessible only via an API.\n",
      "This paper presented the framework of In-\n",
      "Context RALM , enabling frozen, off-the-shelf LMs\n",
      "to benefit from retrieval. We demonstrated that\n",
      "substantial performance gains can be achieved by\n",
      "using general purpose retrievers, and showed that\n",
      "additional gains can be achieved by tailoring the\n",
      "document selection to the LM setting. A recent\n",
      "work by Muhlgay et al. (2023) demonstrates that\n",
      "In-Context RALM is indeed able to improve the\n",
      "factuality of large LMs.\n",
      "Several directions for further improvement re-\n",
      "main for future work. First, this paper considers\n",
      "only the case of prepending a single external docu-\n",
      "ment to the context; adding more documents could\n",
      "drive further gains (for example, using the frame-\n",
      "work of Ratner et al. 2022). Second, we retrieved\n",
      "documents every fixed interval of stokens, but see\n",
      "potential for large latency and cost gains by retriev-\n",
      "ing more sparsely, such as only when a specialized\n",
      "model predicts that retrieval is needed.\n",
      "We release the code used in this work, for thecommunity to use and improve over. We hope it\n",
      "will drive further research of RALM, which will\n",
      "enable its wider adoption.\n",
      "Acknowledgements\n",
      "We would like to thank the reviewers and the Ac-\n",
      "tion Editor for their valuable feedback.\n",
      "References\n",
      "Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta,\n",
      "Dan Roth, and Graham Neubig. 2022. Neuro-\n",
      "symbolic language modeling with automaton-\n",
      "augmented retrieval. In ICML .\n",
      "Sid Black, Leo Gao, Phil Wang, Connor Leahy,\n",
      "and Stella Biderman. 2021. GPT-Neo: Large\n",
      "Scale Autoregressive Language Modeling with\n",
      "Mesh-Tensorflow.\n",
      "Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-\n",
      "mann, Trevor Cai, Eliza Rutherford, Katie Mil-\n",
      "lican, George Bm Van Den Driessche, Jean-\n",
      "Baptiste Lespiau, Bogdan Damoc, Aidan Clark,\n",
      "Diego De Las Casas, Aurelia Guy, Jacob Menick,\n",
      "Roman Ring, Tom Hennigan, Saffron Huang,\n",
      "Loren Maggiore, Chris Jones, Albin Cassirer,\n",
      "Andy Brock, Michela Paganini, Geoffrey Irv-\n",
      "ing, Oriol Vinyals, Simon Osindero, Karen Si-\n",
      "monyan, Jack Rae, Erich Elsen, and Laurent\n",
      "Sifre. 2022. Improving language models by re-\n",
      "trieving from trillions of tokens. In ICML .\n",
      "Tom B. Brown, Benjamin Mann, Nick Ryder,\n",
      "Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\n",
      "wal, Arvind Neelakantan, Pranav Shyam, Girish\n",
      "Sastry, Amanda Askell, Sandhini Agarwal, Ariel\n",
      "Herbert-V oss, Gretchen Krueger, Tom Henighan,\n",
      "Rewon Child, Aditya Ramesh, Daniel Ziegler,\n",
      "Jeffrey Wu, Clemens Winter, Christopher Hesse,\n",
      "Mark Chen, Eric Sigler, Mateusz Litwin, Scott\n",
      "Gray, Benjamin Chess, Jack Clark, Christopher\n",
      "Berner, Sam McCandlish, Alec Radford, Ilya\n",
      "Sutskever, and Dario Amodei. 2020. Language\n",
      "models are few-shot learners. In Advances in\n",
      "Neural Information Processing Systems .\n",
      "Danqi Chen, Adam Fisch, Jason Weston, and An-\n",
      "toine Bordes. 2017. Reading Wikipedia to an-\n",
      "swer open-domain questions. In Proceedings\n",
      "of the 55th Annual Meeting of the Association\n",
      "for Computational Linguistics (Volume 1: LongPapers) , pages 1870–1879, Vancouver, Canada.\n",
      "Association for Computational Linguistics.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2019. BERT: Pre-training\n",
      "of deep bidirectional transformers for language\n",
      "understanding. In Proceedings of the 2019 Con-\n",
      "ference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Hu-\n",
      "man Language Technologies, Volume 1 (Long\n",
      "and Short Papers) , pages 4171–4186, Minneapo-\n",
      "lis, Minnesota. Association for Computational\n",
      "Linguistics.\n",
      "Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\n",
      "ong Wu, Baobao Chang, Xu Sun, Jingjing Xu,\n",
      "Lei Li, and Zhifang Sui. 2023. A survey on\n",
      "in-context learning.\n",
      "Martin Fajcik, Martin Docekal, Karel Ondrej, and\n",
      "Pavel Smrz. 2021. R2-D2: A modular baseline\n",
      "for open-domain question answering. In Find-\n",
      "ings of the Association for Computational Lin-\n",
      "guistics: EMNLP 2021 , pages 854–870, Punta\n",
      "Cana, Dominican Republic. Association for\n",
      "Computational Linguistics.\n",
      "Leo Gao, Stella Biderman, Sid Black, Laurence\n",
      "Golding, Travis Hoppe, Charles Foster, Jason\n",
      "Phang, Horace He, Anish Thite, Noa Nabeshima,\n",
      "Shawn Presser, and Connor Leahy. 2021. The\n",
      "pile: An 800gb dataset of diverse text for lan-\n",
      "guage modeling.\n",
      "Kelvin Guu, Kenton Lee, Zora Tung, Panupong\n",
      "Pasupat, and Ming-Wei Chang. 2020. REALM:\n",
      "Retrieval-augmented language model pre-\n",
      "training. In ICML .\n",
      "Junxian He, Graham Neubig, and Taylor Berg-\n",
      "Kirkpatrick. 2021. Efficient nearest neighbor\n",
      "language models. In Proceedings of the 2021\n",
      "Conference on Empirical Methods in Natural\n",
      "Language Processing , pages 5703–5714, Online\n",
      "and Punta Cana, Dominican Republic. Associa-\n",
      "tion for Computational Linguistics.\n",
      "Minlie Huang, Xiaoyan Zhu, and Jianfeng Gao.\n",
      "2020. Challenges in building intelligent open-\n",
      "domain dialog systems. ACM Trans. Inf. Syst. ,\n",
      "38(3).\n",
      "Gautier Izacard, Mathilde Caron, Lucas Hosseini,\n",
      "Sebastian Riedel, Piotr Bojanowski, ArmandJoulin, and Edouard Grave. 2022a. Unsu-\n",
      "pervised dense information retrieval with con-\n",
      "trastive learning. Transactions on Machine\n",
      "Learning Research .\n",
      "Gautier Izacard and Edouard Grave. 2021. Lever-\n",
      "aging passage retrieval with generative models\n",
      "for open domain question answering. In Pro-\n",
      "ceedings of the 16th Conference of the European\n",
      "Chapter of the Association for Computational\n",
      "Linguistics: Main Volume , pages 874–880, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\n",
      "cas Hosseini, Fabio Petroni, Timo Schick, Jane\n",
      "Dwivedi-Yu, Armand Joulin, Sebastian Riedel,\n",
      "and Edouard Grave. 2022b. Atlas: Few-shot\n",
      "learning with retrieval augmented language mod-\n",
      "els.\n",
      "Jeff Johnson, Matthijs Douze, and Hervé Jégou.\n",
      "2021. Billion-scale similarity search with GPUs.\n",
      "IEEE Transactions on Big Data , 7(3):535–547.\n",
      "Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\n",
      "Zettlemoyer. 2017. TriviaQA: A large scale\n",
      "distantly supervised challenge dataset for read-\n",
      "ing comprehension. In Proceedings of the 55th\n",
      "Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (Volume 1: Long Papers) ,\n",
      "pages 1601–1611, Vancouver, Canada. Associa-\n",
      "tion for Computational Linguistics.\n",
      "Vladimir Karpukhin, Barlas Oguz, Sewon Min,\n",
      "Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\n",
      "Chen, and Wen-tau Yih. 2020. Dense passage re-\n",
      "trieval for open-domain question answering. In\n",
      "Proceedings of the 2020 Conference on Empir-\n",
      "ical Methods in Natural Language Processing\n",
      "(EMNLP) , pages 6769–6781, Online. Associa-\n",
      "tion for Computational Linguistics.\n",
      "Urvashi Khandelwal, Omer Levy, Dan Juraf-\n",
      "sky, Luke Zettlemoyer, and Mike Lewis. 2020.\n",
      "Generalization through memorization: Nearest\n",
      "neighbor language models. In International\n",
      "Conference on Learning Representations .\n",
      "Tom Kwiatkowski, Jennimaria Palomaki, Olivia\n",
      "Redfield, Michael Collins, Ankur Parikh, Chris\n",
      "Alberti, Danielle Epstein, Illia Polosukhin, Jacob\n",
      "Devlin, Kenton Lee, Kristina Toutanova, Llion\n",
      "Jones, Matthew Kelcey, Ming-Wei Chang, An-\n",
      "drew M. Dai, Jakob Uszkoreit, Quoc Le, andSlav Petrov. 2019. Natural questions: A bench-\n",
      "mark for question answering research. Trans-\n",
      "actions of the Association for Computational\n",
      "Linguistics , 7:452–466.\n",
      "Yoav Levine, Itay Dalmedigos, Ori Ram, Yoel\n",
      "Zeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin,\n",
      "Opher Lieber, Barak Lenz, Shai Shalev-Shwartz,\n",
      "Amnon Shashua, Kevin Leyton-Brown, and\n",
      "Yoav Shoham. 2022a. Standing on the shoul-\n",
      "ders of giant frozen language models.\n",
      "Yoav Levine, Ori Ram, Daniel Jannai, Barak Lenz,\n",
      "Shai Shalev-Shwartz, Amnon Shashua, Kevin\n",
      "Leyton-Brown, and Yoav Shoham. 2022b. Huge\n",
      "frozen language models as readers for open-\n",
      "domain question answering. In ICML 2022\n",
      "Workshop on Knowledge Retrieval and Lan-\n",
      "guage Models .\n",
      "Yoav Levine, Noam Wies, Daniel Jannai, Dan\n",
      "Navon, Yedid Hoshen, and Amnon Shashua.\n",
      "2022c. The inductive bias of in-context learn-\n",
      "ing: Rethinking pretraining example design. In\n",
      "International Conference on Learning Represen-\n",
      "tations .\n",
      "Patrick Lewis, Ethan Perez, Aleksandra Piktus,\n",
      "Fabio Petroni, Vladimir Karpukhin, Naman\n",
      "Goyal, Heinrich Küttler, Mike Lewis, Wen-tau\n",
      "Yih, Tim Rocktäschel, Sebastian Riedel, and\n",
      "Douwe Kiela. 2020. Retrieval-augmented gen-\n",
      "eration for knowledge-intensive nlp tasks. In\n",
      "Advances in Neural Information Processing Sys-\n",
      "tems, pages 9459–9474.\n",
      "Zonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022.\n",
      "Decoupled context processing for context aug-\n",
      "mented language modeling. In Advances in Neu-\n",
      "ral Information Processing Systems .\n",
      "Opher Lieber, Or Sharir, Barak Lenz, and Yoav\n",
      "Shoham. 2021. Jurassic-1: Technical details and\n",
      "evaluation.\n",
      "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin,\n",
      "Jheng-Hong Yang, Ronak Pradeep, and Rodrigo\n",
      "Nogueira. 2021. Pyserini: A python toolkit for\n",
      "reproducible information retrieval research with\n",
      "sparse and dense representations. In Proceed-\n",
      "ings of the 44th International ACM SIGIR Con-\n",
      "ference on Research and Development in Infor-\n",
      "mation Retrieval , SIGIR ’21, page 2356–2362,\n",
      "New York, NY , USA. Association for Comput-\n",
      "ing Machinery.Stephanie Lin, Jacob Hilton, and Owain Evans.\n",
      "2022. TruthfulQA: Measuring how models\n",
      "mimic human falsehoods. In Proceedings of the\n",
      "60th Annual Meeting of the Association for Com-\n",
      "putational Linguistics (Volume 1: Long Papers) ,\n",
      "pages 3214–3252, Dublin, Ireland. Association\n",
      "for Computational Linguistics.\n",
      "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\n",
      "Mandar Joshi, Danqi Chen, Omer Levy, Mike\n",
      "Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n",
      "2019. RoBERTa: A robustly optimized bert\n",
      "pretraining approach.\n",
      "Joshua Maynez, Shashi Narayan, Bernd Bohnet,\n",
      "and Ryan McDonald. 2020. On faithfulness and\n",
      "factuality in abstractive summarization. In Pro-\n",
      "ceedings of the 58th Annual Meeting of the As-\n",
      "sociation for Computational Linguistics , pages\n",
      "1906–1919, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Stephen Merity, Caiming Xiong, James Bradbury,\n",
      "and Richard Socher. 2016. Pointer sentinel mix-\n",
      "ture models.\n",
      "Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\n",
      "Nir Ratner, Yonatan Belinkov, Omri Abend,\n",
      "Kevin Leyton-Brown, Amnon Shashua, and\n",
      "Yoav Shoham. 2023. Generating benchmarks\n",
      "for factuality evaluation of language models.\n",
      "Fabio Petroni, Aleksandra Piktus, Angela Fan,\n",
      "Patrick Lewis, Majid Yazdani, Nicola De Cao,\n",
      "James Thorne, Yacine Jernite, Vladimir\n",
      "Karpukhin, Jean Maillard, Vassilis Plachouras,\n",
      "Tim Rocktäschel, and Sebastian Riedel. 2021.\n",
      "KILT: a benchmark for knowledge intensive lan-\n",
      "guage tasks. In Proceedings of the 2021 Con-\n",
      "ference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Hu-\n",
      "man Language Technologies , pages 2523–2544,\n",
      "Online. Association for Computational Linguis-\n",
      "tics.\n",
      "Alec Radford, Karthik Narasimhan, Tim Salimans,\n",
      "and Ilya Sutskever. 2018. Improving language\n",
      "understanding by generative pre-training.\n",
      "Alec Radford, Jeff Wu, Rewon Child, David Luan,\n",
      "Dario Amodei, and Ilya Sutskever. 2019. Lan-\n",
      "guage models are unsupervised multitask learn-\n",
      "ers.Ori Ram, Gal Shachaf, Omer Levy, Jonathan Be-\n",
      "rant, and Amir Globerson. 2022. Learning to re-\n",
      "trieve passages without supervision. In Proceed-\n",
      "ings of the 2022 Conference of the North Amer-\n",
      "ican Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technolo-\n",
      "gies, pages 2687–2700, Seattle, United States.\n",
      "Association for Computational Linguistics.\n",
      "Nir Ratner, Yoav Levine, Yonatan Belinkov,\n",
      "Ori Ram, Omri Abend, Ehud Karpas, Am-\n",
      "non Shashua, Kevin Leyton-Brown, and Yoav\n",
      "Shoham. 2022. Parallel context windows im-\n",
      "prove in-context learning of large language mod-\n",
      "els.\n",
      "Stephen Robertson and Hugo Zaragoza. 2009. The\n",
      "probabilistic relevance framework: BM25 and\n",
      "beyond. Found. Trends Inf. Retr. , 3(4):333–389.\n",
      "Devendra Sachan, Mike Lewis, Mandar Joshi, Ar-\n",
      "men Aghajanyan, Wen-tau Yih, Joelle Pineau,\n",
      "and Luke Zettlemoyer. 2022. Improving passage\n",
      "retrieval with zero-shot question generation. In\n",
      "Proceedings of the 2022 Conference on Empir-\n",
      "ical Methods in Natural Language Processing ,\n",
      "pages 3781–3797, Abu Dhabi, United Arab Emi-\n",
      "rates. Association for Computational Linguis-\n",
      "tics.\n",
      "Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-\n",
      "joon Seo, Rich James, Mike Lewis, Luke Zettle-\n",
      "moyer, and Wen tau Yih. 2023. REPLUG:\n",
      "Retrieval-augmented black-box language mod-\n",
      "els.\n",
      "Nandan Thakur, Nils Reimers, Andreas Rücklé,\n",
      "Abhishek Srivastava, and Iryna Gurevych. 2021.\n",
      "BEIR: A heterogeneous benchmark for zero-shot\n",
      "evaluation of information retrieval models. In\n",
      "Proceedings of the Neural Information Process-\n",
      "ing Systems Track on Datasets and Benchmarks ,\n",
      "volume 1.\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard,\n",
      "Xavier Martinet, Marie-Anne Lachaux, Timo-\n",
      "thée Lacroix, Baptiste Rozière, Naman Goyal,\n",
      "Eric Hambro, Faisal Azhar, Aurelien Rodriguez,\n",
      "Armand Joulin, Edouard Grave, and Guillaume\n",
      "Lample. 2023. LLaMA: Open and efficient foun-\n",
      "dation language models.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar,\n",
      "Jakob Uszkoreit, Llion Jones, Aidan Gomez,Ł ukasz Kaiser, and Illia Polosukhin. 2017. At-\n",
      "tention is all you need. In Advances in Neural\n",
      "Information Processing Systems 30 , pages 5998–\n",
      "6008.\n",
      "Ben Wang and Aran Komatsuzaki. 2021. GPT-J-\n",
      "6B: A 6 Billion Parameter Autoregressive Lan-\n",
      "guage Model.\n",
      "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\n",
      "Chaumond, Clement Delangue, Anthony Moi,\n",
      "Pierric Cistac, Tim Rault, Remi Louf, Morgan\n",
      "Funtowicz, Joe Davison, Sam Shleifer, Patrick\n",
      "von Platen, Clara Ma, Yacine Jernite, Julien Plu,\n",
      "Canwen Xu, Teven Le Scao, Sylvain Gugger,\n",
      "Mariama Drame, Quentin Lhoest, and Alexan-\n",
      "der Rush. 2020. Transformers: State-of-the-art\n",
      "natural language processing. In Proceedings of\n",
      "the 2020 Conference on Empirical Methods in\n",
      "Natural Language Processing: System Demon-\n",
      "strations , pages 38–45, Online. Association for\n",
      "Computational Linguistics.\n",
      "Rowan Zellers, Ari Holtzman, Hannah Rashkin,\n",
      "Yonatan Bisk, Ali Farhadi, Franziska Roesner,\n",
      "and Yejin Choi. 2019. Defending against neural\n",
      "fake news. In Advances in Neural Information\n",
      "Processing Systems , volume 32. Curran Asso-\n",
      "ciates, Inc.\n",
      "Susan Zhang, Stephen Roller, Naman Goyal, Mikel\n",
      "Artetxe, Moya Chen, Shuohui Chen, Christopher\n",
      "Dewan, Mona Diab, Xian Li, Xi Victoria Lin,\n",
      "Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt\n",
      "Shuster, Daniel Simig, Punit Singh Koura, An-\n",
      "jali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\n",
      "2022. OPT: Open pre-trained transformer lan-\n",
      "guage models.\n",
      "Zexuan Zhong, Tao Lei, and Danqi Chen. 2022.\n",
      "Training language models with memory augmen-\n",
      "tation. In Proceedings of the 2022 Conference\n",
      "on Empirical Methods in Natural Language Pro-\n",
      "cessing , pages 5657–5673, Abu Dhabi, United\n",
      "Arab Emirates. Association for Computational\n",
      "Linguistics.\n",
      "A Query Length Ablations\n",
      "Figure 9 and Figure 10 show ablations on the opti-\n",
      "mal query length ℓfor off-the-shelf dense retrievers\n",
      "(BERT and Contriever respectively). We omit the\n",
      "results of Spider as they are almost identical to\n",
      "those of Contriever. Consistently, using ℓ= 64(tokens) is optimal. This is in contrast to similar\n",
      "experiments we conducted for BM25 ( cf. Figure 6),\n",
      "where ℓ= 32 is optimal.\n",
      "B GPT-Neo Results\n",
      "Table 5 gives the results of applying In-Context\n",
      "RALM to the models from the GPT-Neo model\n",
      "family on WikiText-103 and RealNews.\n",
      "C Open-Domain Question Answering\n",
      "Experiments: Further Details\n",
      "Closed-Book Setting For the closed-book set-\n",
      "ting, we adopt the prompt of Touvron et al. (2023):\n",
      "Answer these questions:\n",
      "Q: Who got the first nobel\n",
      "prize in physics?\n",
      "A:\n",
      "Open-Book Setting For the open-book setting,\n",
      "we extend the above prompt as follows:\n",
      "Nobel Prize\n",
      "A group including 42\n",
      "Swedish writers, artists,\n",
      "and literary critics\n",
      "protested against this\n",
      "decision, having expected\n",
      "Leo Tolstoy to be awarded.\n",
      "Some, including Burton\n",
      "Feldman, have criticised\n",
      "this prize because they...\n",
      "Nobel Prize in Physiology\n",
      "or Medicine\n",
      "In the last half century\n",
      "there has been an\n",
      "increasing tendency\n",
      "for scientists to work\n",
      "as teams, resulting in\n",
      "controversial exclusions.\n",
      "Alfred Nobel was born\n",
      "on 21 October 1833 in\n",
      "Stockholm, Sweden, into\n",
      "a family of engineers...\n",
      "Based on these texts,\n",
      "answer these questions:\n",
      "Q: Who got the first nobel\n",
      "prize in physics?\n",
      "A:Model RetrievalWiki-103 RealNews\n",
      "word ppl token ppl\n",
      "GPT-Neo 1.3B- 17.5 12.3\n",
      "BM25, §5 14.6 9.9\n",
      "GPT-Neo 2.7B- 15.1 11.0\n",
      "BM25, §5 12.8 9.0\n",
      "GPT-J 6B- 11.6 9.2\n",
      "BM25, §5 10.0 7.7\n",
      "Table 5: The performance of models from the GPT-\n",
      "Neo family, measured by word-level perplexity on\n",
      "the test set of WikiText-103 and token-level per-\n",
      "plexity on the development set of RealNews.\n",
      "Retrieval Query Length (ℓ)Perplexity\n",
      "15.025.035.045.0\n",
      "16 32 64 128 256GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)\n",
      "Figure 9: An analysis of perplexity as a function\n",
      "ofthe number of tokens in the query for an off-\n",
      "the-shelf BERT retriever on the development set of\n",
      "WikiText-103.\n",
      "Retrieval Query Length (ℓ)Perplexity\n",
      "10.020.030.040.0\n",
      "16 32 64 128 256GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)\n",
      "Figure 10: An analysis of perplexity as a function\n",
      "ofthe number of tokens in the query for Contriever\n",
      "on the development set of WikiText-103.\n",
      "==================================================\n",
      "\n",
      "Document 6:\n",
      "MuRAG: Multimodal Retrieval-Augmented Generator\n",
      "for Open Question Answering over Images and Text\n",
      "Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen\n",
      "Google Research\n",
      "{wenhuchen,hexiang,patverga,wcohen}@google.com\n",
      "Abstract\n",
      "While language Models store a massive\n",
      "amount of world knowledge implicitly in their\n",
      "parameters, even very large models often fail\n",
      "to encode information about rare entities and\n",
      "events, while incurring huge computational\n",
      "costs. Recently, retrieval-augmented models,\n",
      "such as REALM, RAG, and RETRO, have\n",
      "incorporated world knowledge into language\n",
      "generation by leveraging an external non-\n",
      "parametric index and have demonstrated im-\n",
      "pressive performance with constrained model\n",
      "sizes. However, these methods are restricted\n",
      "to retrieving only textual knowledge, neglect-\n",
      "ing the ubiquitous amount of knowledge in\n",
      "other modalities like images – much of which\n",
      "contains information not covered by any text.\n",
      "To address this limitation, we propose the\n",
      "ﬁrst Multimodal Retrieval-Augmented Trans-\n",
      "former (MuRAG), which accesses an external\n",
      "non-parametric multimodal memory to aug-\n",
      "ment language generation. MuRAG is pre-\n",
      "trained with a mixture of large-scale image-\n",
      "text and text-only corpora using a joint con-\n",
      "trastive and generative loss. We perform ex-\n",
      "periments on two different datasets that re-\n",
      "quire retrieving and reasoning over both im-\n",
      "ages and text to answer a given query: We-\n",
      "bQA, and MultimodalQA. Our results show\n",
      "that MuRAG achieves state-of-the-art accu-\n",
      "racy, outperforming existing models by 10-\n",
      "20% absolute on both datasets and under both\n",
      "distractor and full-wiki settings.\n",
      "1 Introduction\n",
      "Pre-trained language models like GPT-3 (Brown\n",
      "et al., 2020), PaLM (Chowdhery et al., 2022), etc\n",
      "have been shown to capture a massive amount\n",
      "of world knowledge implicitly in their parame-\n",
      "ters. However, using such large models incurs an\n",
      "extremely high computation cost. As an alterna-\n",
      "tive to a singular monolithic transformer, retrieval-\n",
      "augmented architectures like KNN-LM (Khandel-\n",
      "wal et al., 2019), REALM (Guu et al., 2020),\n",
      "Figure 1: Visual information-seeking queries : These\n",
      "queries are unanswerable with text-only retrieval and\n",
      "require retrieving and reasoning over images.\n",
      "RAG (Lewis et al., 2020), FiD (Izacard and Grave,\n",
      "2021), and RETRO (Borgeaud et al., 2021) have\n",
      "been proposed to decouple world knowledge from\n",
      "the model’s parameters. More speciﬁcally, these\n",
      "models are trained to access an external mem-\n",
      "ory to enhance the model’s predictions. Such\n",
      "retrieval-augmented architectures have multiple\n",
      "beneﬁcial properties including: decreased model\n",
      "size (Borgeaud et al., 2021), better attribution/-\n",
      "explanation for model predictions (Lewis et al.,\n",
      "2020), and adaptability to new information with-\n",
      "out retraining (Verga et al., 2021). However, pre-\n",
      "vious retrieval-augmented models are limited to\n",
      "memories that contain only text or structured data\n",
      "and hence cannot make use of the massive amount\n",
      "of multimodal knowledge available on the web—\n",
      "much of which contains information only available\n",
      "in non-text modalities.\n",
      "Figure 1, shows several information-seeking\n",
      "queries that require retrieving and reasoning over\n",
      "visual knowledge. Here, a user ﬁrst poses a ques-\n",
      "tion such as “What can be found on the White\n",
      "House balconies at Christmas” . The system then\n",
      "retrieves relevant items from its memory, for exam-arXiv:2210.02928v2  [cs.CL]  20 Oct 2022ple, the ﬁrst image of Figure 1 with the caption\n",
      "“White House during Christmas” , which it uses to\n",
      "produce the answer “wreaths and garlands” . Ex-\n",
      "isting text retrieval-augmented models would strug-\n",
      "gle with such queries because, in many cases, they\n",
      "would simply not have access to the answer as some\n",
      "knowledge does not exist in text form. That, cou-\n",
      "pled with the abundance of multimodal knowledge\n",
      "that exists, leads to the conclusion that retrieval-\n",
      "augmented models should ultimately be developed\n",
      "to retrieve and reason over multiple modalities.\n",
      "Figure 2: Model Overview : retrieval-and-predict pro-\n",
      "cess of MuRAG on downstream datasets.\n",
      "In this paper, we are speciﬁcally interested in\n",
      "endowing pre-trained language models with a non-\n",
      "parametric multimodal memory containing images,\n",
      "text, or image-text pairs. To accomplish this, we\n",
      "ﬁrst combine pre-trained T5 (Raffel et al., 2020)\n",
      "and ViT (Dosovitskiy et al., 2020) models to build\n",
      "a backbone encoder (Figure 3), which encodes\n",
      "image-text pairs, image-only, and text-only inputs\n",
      "into a multimodal representation. MuRAG uses the\n",
      "backbone encoder to embed items into an external\n",
      "memory as well as queries to retrieve multimodal\n",
      "knowledge from that memory. These retrievals\n",
      "then augment a language model to generate more\n",
      "visually-grounded outputs.\n",
      "We pre-train MuRAG with a mixture of\n",
      "image-text and text-only datasets including\n",
      "LAION (Schuhmann et al., 2021), Conceptual-\n",
      "Caption (Sharma et al., 2018), VQA (An-\n",
      "tol et al., 2015) and Probably-Asked-Questions\n",
      "(PAQ) (Lewis et al., 2021). More speciﬁcally, we\n",
      "reformulate these datasets in a retrieve-and-predict\n",
      "format. Here, the model’s input is an image along\n",
      "with a text prompt. The model then retrieves from\n",
      "a memory containing captions and passages, which\n",
      "it uses to generate a target token sequence. The\n",
      "model is trained with both a contrastive and a gen-erative loss; this teaches the model to discriminate\n",
      "relevant from irrelevant memory entries, and guides\n",
      "the model to leverage the multimodal knowledge\n",
      "into generation.\n",
      "Unlike the pre-training stage, during ﬁne-\n",
      "tuning Figure 2 the model’s input is a question,\n",
      "and the memory contains a collection of captioned\n",
      "images and text snippets. We ﬁne-tune MuRAG\n",
      "on the downstream datasets with a contrastive and\n",
      "generative loss similar to pre-training. To avoid ex-\n",
      "cessive computation cost, we develop a two-stage\n",
      "training pipeline to ﬁrst train with small in-batch\n",
      "memory, and then with a statically encoded and\n",
      "indexed large global memory.\n",
      "Our experiments show that MuRAG achieves\n",
      "state-of-the-art performance on two different open-\n",
      "multimodal-QA datasets, both of which require\n",
      "retrieving images and text from a large corpus to\n",
      "answer factoid questions: WebQA (Chang et al.,\n",
      "2022) and MultimodalQA (Talmor et al., 2021). On\n",
      "both datasets, we outperform sophisticated base-\n",
      "lines (Li et al., 2020; Radford et al., 2021; Zhang\n",
      "et al., 2021) by 10-20% accuracy under both dis-\n",
      "tractor (from 40+ candidates) and full-wiki settings\n",
      "(from 1M candidates). We also perform a compre-\n",
      "hensive study to ablate different components of the\n",
      "pre-training to see their contributions. These em-\n",
      "pirical results demonstrate the effectiveness of our\n",
      "proposed models to integrate multimodal knowl-\n",
      "edge into pre-trained generation models and pave\n",
      "the way to uniﬁed retrieval-augmented frameworks.\n",
      "2 Related Work\n",
      "Retrieval Augmented Models Retrieval aug-\n",
      "mented models are hybrid models containing\n",
      "both parameterized sequence models and a non-\n",
      "parametric memory, infusing world knowledge into\n",
      "existing language models. Among them, KNN-\n",
      "LM (Khandelwal et al., 2019) was ﬁrst proposed\n",
      "to retrieve instances from a text training corpus to\n",
      "help language modeling. Later, RETRO (Borgeaud\n",
      "et al., 2021) was proposed to scale up the text cor-\n",
      "pus to trillions of tokens, enabling the model to\n",
      "achieve similar perplexity to GPT-3 (Brown et al.,\n",
      "2020) with 25x fewer model parameters. Another\n",
      "family of models, such as REALM (Guu et al.,\n",
      "2020), RAG (Lewis et al., 2020), and FiD (Izacard\n",
      "and Grave, 2021), integrate Wikipedia passages as\n",
      "a datastore to beneﬁt downstream knowledge in-\n",
      "tensive tasks ( e.g.Question Answering). REALM\n",
      "is an encoder-only model trained with masked lan-guage modeling, while RAG and FiD adopt an\n",
      "encoder-decoder model with a generative language\n",
      "modeling objective. Compared to them, MuRAG\n",
      "is the ﬁrst retrieval-augmented model that is ca-\n",
      "pable of using knowledge presented in multiple\n",
      "modalities ( i.e.visual and textual knowledge data),\n",
      "whereas all prior methods are restricted to using\n",
      "text-only knowledge.\n",
      "Multimodal Transformers Multimodal trans-\n",
      "formers have demonstrated strong performances\n",
      "in learning cross-modal representation that are gen-\n",
      "erally beneﬁcial on downstream vision and lan-\n",
      "guage tasks, such as image-text retrieval (Karpa-\n",
      "thy and Fei-Fei, 2015), image captioning (Chen\n",
      "et al., 2015), and VQA (Antol et al., 2015). These\n",
      "methods typically learn a joint transformer model\n",
      "on top of unimodal visual and textual backbones,\n",
      "via fusing deep features from each modality. The\n",
      "early version of multimodal transformers (Lu et al.,\n",
      "2019; Chen et al., 2020; Li et al., 2020) usually\n",
      "learns a Transformer on pre-extracted unimodal\n",
      "features for contextualization, which makes it im-\n",
      "possible to adjust those unimodal features to the\n",
      "target tasks. Recently, SimVLM (Wang et al., 2022)\n",
      "and COCA (Yu et al., 2022) proposed end-to-end\n",
      "training for both deep multimodal transformers and\n",
      "unimodal featurization networks and demonstrated\n",
      "strong performance in both multimodal and uni-\n",
      "modal downstream tasks. The multimodal memory\n",
      "encoder of MuRAG is broadly similar to SimVLM\n",
      "and CoCa, but has a different focus to encode and\n",
      "retrieve multimodal knowledge ( i.e.images and\n",
      "texts) to augment language generation models.\n",
      "Multimodal Question Answering The problem\n",
      "of multimodal question answering has been ex-\n",
      "tensively studied. VQA was the ﬁrst proposed to\n",
      "answer questions from visual-only inputs. Later,\n",
      "OK-VQA (Marino et al., 2019) enlarged VQA’s\n",
      "scope to annotate questions requiring both image\n",
      "and implicit textual/common-sense knowledge to\n",
      "answer. More recently, MuMuQA (Reddy et al.,\n",
      "2021), ManyModelQA (Hannan et al., 2020) and\n",
      "MIMOQA (Singh et al., 2021) provide questions\n",
      "which require reasoning over images and explicitly\n",
      "provided text snippets. However, these datasets\n",
      "are restricted to dealing with given text and images\n",
      "without requiring any retrieval from the web: they\n",
      "are analogous to machine-reading approaches to\n",
      "QA from text like SQuAD, rather than open-book\n",
      "QA. To study the more realistic open multimodal\n",
      "QA task, WebQA (Chang et al., 2022) and Multi-modalQA (Talmor et al., 2021) have been proposed\n",
      "to evaluate answers to open queries which require\n",
      "retrieving and reasoning over a large-scale web\n",
      "multimodal corpus. Our model uses these datasets\n",
      "to study open-world multimodal question answer-\n",
      "ing, obtaining state-of-the-art results.\n",
      "3 Model\n",
      "3.1 Backbone Encoder\n",
      "Figure 3: Backbone encoder: ViT encodes image\n",
      "patches into a sequence of vectors eI, while word em-\n",
      "bedding converts text tokens into another sequence of\n",
      "vectors eT. These vectors are concatenated to form\n",
      "f\u0012(e)and fed to a decoder for text generation.\n",
      "MuRAG is built on top of a simpler model we\n",
      "call a “backbone” model, which is pre-trained to\n",
      "encode image-text pairs such that they are suitable\n",
      "for both answer generation and retrieval. The back-\n",
      "bone model’s encoder is used as a component of\n",
      "the MuRAG model. The backbone model is built\n",
      "with a pre-trained visual Transformer (Dosovitskiy\n",
      "et al., 2020) and a T5 text Transformer (Raffel et al.,\n",
      "2020), and consists of a multimodal encoder f\u0012and\n",
      "decoder g\u0012. The encoder takes as input a sequence\n",
      "of image-text pairs, where either the image or the\n",
      "text component can be empty to accommodate text-\n",
      "only and image-only cases.\n",
      "As depicted in Figure 3, the encoder can take a\n",
      "sequence of images and text. For image input, we\n",
      "ﬁrst split each into 16x16 patches and feed them\n",
      "to a ViT (Dosovitskiy et al., 2020) transformer to\n",
      "generate a sequence of visual embedding denoted\n",
      "aseI2RLi\u0002D, where Liis the length of the im-\n",
      "age tokens. For text input, we use word embedding\n",
      "to produce another sequence of textual embedding\n",
      "eT2RLt\u0002D. Forkimages and ntext inputs, we\n",
      "concatenate all their embeddings in the input or-\n",
      "der as e= [e1\n",
      "I;e1\n",
      "T;\u0001\u0001\u0001;ek\n",
      "I;en\n",
      "T]2R(kLt+nLi)\u0002D,\n",
      "which is fed to another bi-directional transformer\n",
      "f\u0012initialized from T5. We enable cross-attentionbetween the two modalities to produce a fused rep-\n",
      "resentation, denoted as f\u0012(e)2R(kLt+nLi)\u0002D.\n",
      "We add a [CLS] token to obtain a pooled repre-\n",
      "sentation f\u0012(e)[CLS]2RDfor dense retrieval.\n",
      "3.2 MuRAG\n",
      "We build MuRAG (shown in Figure 4) on top of\n",
      "the backbone model. During the retriever stage,\n",
      "MuRAG takes a query qof any modality as in-\n",
      "put and retrieves from a memory Mof image-text\n",
      "pairs. Speciﬁcally, we apply the backbone encoder\n",
      "f\u0012to encode a query q, and use maximum inner\n",
      "product search (MIPS (Guo et al., 2020)) over all of\n",
      "the memory candidates m2M to ﬁnd the Top-K\n",
      "nearest neighbors TopK(Mjq) = [m1;\u0001\u0001\u0001; mk].\n",
      "Formally, we deﬁne TopK(Mjq)as follows:\n",
      "TopK(Mjq) =TopK\n",
      "m2Mf\u0012(q)[CLS]\u0001f\u0012(m)[CLS]\n",
      "During the reader stage, the retrievals (the raw im-\n",
      "age patches) are combined with the query qas\n",
      "an augmented input [m1;\u0001\u0001\u0001; mk; q], which is fed\n",
      "to the backbone encoder f\u0012to produce retrieval-\n",
      "augmented encoding. The decoder model g\u0012uses\n",
      "attention over this representation to generate tex-\n",
      "tual outputs y=y1;\u0001\u0001\u0001; yntoken by token.\n",
      "p(yijyi\u00001) =g\u0012(yijf\u0012(TopK(Mjq);q);y1:i\u00001)\n",
      "where yis decoded from a given vocabulary V.\n",
      "3.3 Pre-training\n",
      "The pre-training implementation is depicted in the\n",
      "upper portion of Figure 4, where the input query\n",
      "is an image xIplus a text prompt xp. The exter-\n",
      "nal memoryMcontains textual-only entries mT.\n",
      "The Top-K retrievals mT\n",
      "1;\u0001\u0001\u0001; mT\n",
      "kare leveraged to\n",
      "generate the textual output. To avoid the excessive\n",
      "computation cost of backpropagation over the mas-\n",
      "sive external memory, we adopt an in-batch mem-\n",
      "oryMB, dynamically constructed from the input\n",
      "examples in a batch. The small in-batch memory\n",
      "enables MuRAG to continuously update the mem-\n",
      "ory encoder efﬁciently similar to TOME (de Jong\n",
      "et al., 2022) and QAMAT (Chen et al., 2022).\n",
      "Dataset The pre-training corpus consists of\n",
      "LAION (Schuhmann et al., 2021), Conceptual-\n",
      "Caption-12M+3M (CC) (Sharma et al., 2018;\n",
      "Changpinyo et al., 2021), VQA (Antol et al., 2015)\n",
      "and PAQ (Lewis et al., 2021) Table 1. LAION is\n",
      "a publicly-released image-text dataset containingcrawled image-text pairs ﬁltered by CLIP (Rad-\n",
      "ford et al., 2021). We apply rules to ﬁlter LAION\n",
      "from 400M to 200M by removing text with HTTP-\n",
      "URLs or image width/height beyond 1000 pixels.\n",
      "CC contains 15M (image, anonymized alt-text)\n",
      "pairs crawled from the web but ﬁltered more ex-\n",
      "tensively to maintain high alignment quality. VQA\n",
      "contains annotated QA pairs aligned to MSCOCO\n",
      "images. We further add captions to each image\n",
      "from MSCOCO-Captioning (Lin et al., 2014) to\n",
      "create (Image, Caption, QA) triples. PAQ is a text-\n",
      "only dataset containing 65M machine-generated\n",
      "QA pairs along with their source Wikipedia pas-\n",
      "sage.\n",
      "Dataset #Size Format Source\n",
      "CC 15M (Image, Caption) Crawled\n",
      "LAION 200M (Image, Alt-Text) Crawled\n",
      "PAQ 65M (Passage, QA) Generated\n",
      "VQA 400K (Image, Caption, QA) Annotated\n",
      "Table 1: Pre-training Dataset Statistics\n",
      "For LAION and CC, we use the input image as\n",
      "xI, and ‘generate caption:’ as the text prompt xp.\n",
      "For VQA, we use the input image as xIand the\n",
      "question as the prompt xp. For PAQ, we use an\n",
      "empty array as the input image and the question\n",
      "as the prompt. The in-batch memory MBis con-\n",
      "structed by stacking the captions associated with\n",
      "the input images in LAION/CC/VQA and the pas-\n",
      "sages associated with the questions in PAQ. Each\n",
      "textual memory entry is denoted as mT. The de-\n",
      "coder is optimized to generate either a caption or\n",
      "an answer, depending on the source dataset. Since\n",
      "the four dataset sizes are highly unbalanced, we\n",
      "use ﬁxed mixture sampling ratios to balance their\n",
      "presence during pre-training.\n",
      "We train the model with a joint loss L=Lgen+\n",
      "Lconas follows:\n",
      "Lcon=\u0000logexp(f\u0012(xI; xp)\u0001f\u0012(mT))P\n",
      "m2MBexp(f\u0012(xI; xp)\u0001f\u0012(mT))\n",
      "Lgen=\u0000logg\u0012(yjf\u0012(Mp;xI;xp))\n",
      "Mp=(\n",
      "TopK(MBjxI; xp)If(xI; xp)2PAQ/VQA\n",
      "Ø If (xI; xp)2LAION/CC\n",
      "where Mpis the retrieved augmentation: if the\n",
      "input query is from PAQ/VQA, we use the retrieved\n",
      "memory entries, otherwise, we use null. The reason\n",
      "for setting it to null for LAION/CC is to avoid a\n",
      "trivial solution when the generation target (caption)\n",
      "also exactly appears in the memory.\n",
      "The contrastive loss Lconis minimized to dis-\n",
      "criminate between the positive query-memory pairsFigure 4: Model Architecture: the model accesses an external memory to obtain multimodal knowledge contained\n",
      "in images or text snippets, which is used to augment the generation. The upper part deﬁnes the pre-training\n",
      "implementation, while the lower part deﬁnes ﬁne-tuning implementation.\n",
      "and all other query-memory pairs from the mem-\n",
      "ory. The pairwise matching score is computed as\n",
      "the dot product between query f\u0012(xI;xp)[CLS] and\n",
      "candidates f\u0012(mT)[CLS]. This objective enables\n",
      "the model to retrieve the most relevant knowledge\n",
      "from the memory. The generative loss Lgenis min-\n",
      "imized to generate target tokens yconditioned on\n",
      "the retrieval-augmented representation. This ob-\n",
      "jective enables the model to combine information\n",
      "across different modalities for text generation.\n",
      "3.4 Fine-tuning\n",
      "We ﬁnetune MuRAG to align with the expected\n",
      "inputs of the downstream datasets which require an-\n",
      "swering text questions by retrieving image-caption\n",
      "pairs or text snippets from the external knowledge\n",
      "datastore. As depicted in the lower part of Figure 4,\n",
      "the input query for the downstream task is a text\n",
      "question xq, and the memory Mcontaining (im-\n",
      "age, text) pairs (mI; mT).1The Top-K retrievals\n",
      "f(mI\n",
      "1; mT\n",
      "1);\u0001\u0001\u0001;(mI\n",
      "k; mT\n",
      "k)gare leveraged to gen-\n",
      "erate the answer a. To minimize the computation\n",
      "cost, we develop a two-stage pipeline to optimize\n",
      "with an in-batch memory and then resume with\n",
      "ﬁxed retrieval from global memory.\n",
      "In-Batch Training In this stage, we aim to mini-\n",
      "mize the joint loss function L=Lcon+Lgenbased\n",
      "1We set the image to a zero array if the memory entry is a\n",
      "text snippet.on the in-batch memory MBas follows:\n",
      "Lcon=\u0000logexp(f\u0012(xq)\u0001f\u0012(mI;mT))P\n",
      "m2MBexp(f\u0012(xq)\u0001f\u0012(mI;mT))\n",
      "Lgen=\u0000logg\u0012(yjf\u0012(TopK(MBjxq);xq))\n",
      "The in-batch memory MBis constructed in the\n",
      "following way: the k-th example in the dataset is\n",
      "represented as (xq;k; yk;fmI\n",
      "i; mI\n",
      "igk;f\u0016mI\n",
      "j;\u0016mT\n",
      "jgk),\n",
      "where mrepresents the positive (image, text)\n",
      "source, and \u0016mrepresents the hard negative\n",
      "(image, text) source provided by the dataset2.\n",
      "For a batch with Bexamples, we assemble\n",
      "all the associated positive and negative knowl-\n",
      "edge source as our in-batch memory MB=\n",
      "ffmI\n",
      "i; mI\n",
      "ig1;f\u0016mI\n",
      "j;\u0016mT\n",
      "jg1;\u0001\u0001\u0001;f\u0016mI\n",
      "j;\u0016mT\n",
      "jgBg.\n",
      "Fixed-Retrieval Training After in-batch train-\n",
      "ing, we encode all available cross-modal pairs, and\n",
      "index these encodings for fast MIPS retrieval. We\n",
      "then apply the trained retriever to search over the\n",
      "full multimodal corpus Mto obtain the global top-\n",
      "K retrievals TopK(Mjxq)and continue to opti-\n",
      "mizeLgen. During this training phase, the stored\n",
      "encodings are not updated. During inference time,\n",
      "we use ﬁxed encodings to generate the answers.\n",
      "2These hard negatives are mined through Bing Search API\n",
      "and Wikipedia page, refer to (Chang et al., 2022) for details.4 Experiments\n",
      "4.1 Implementation Details\n",
      "The backbone model uses T5-base (Raffel et al.,\n",
      "2020) and a ViT-large model (Dosovitskiy et al.,\n",
      "2020) as described in Table 2. We adopt the\n",
      "sentence-piece model from T5 with a vocabulary\n",
      "size of 32128. The ViT model was pre-trained\n",
      "on the JFT dataset. We resize every image into\n",
      "224x224 pixels and split them into a sequence of\n",
      "16x16 patches. The output of ViT is a sequence\n",
      "of 1024-dimension vectors, which are projected\n",
      "to 768-dimension for consistency with T5 model.\n",
      "MuRAG reuses the model as retriever and reader,\n",
      "thus the full model size is 527M parameters.\n",
      "Model #Enc #Dec Hidden Heads Params\n",
      "ViT-large 24 0 1024 16 307M\n",
      "T5-base 12 12 768 12 220M\n",
      "Table 2: The model size and conﬁgurations, with\n",
      "#Enc/#Dec denoting encoder/decoder layers.\n",
      "Our model is implemented in JAX (Bradbury\n",
      "et al., 2018), based on the T5X codebase (Roberts\n",
      "et al., 2022). During pre-training, we ﬁrst train the\n",
      "model on LAION for 1M steps, and then continue\n",
      "training on CC/PAQ/VQA with 1:1:1 sample ratio\n",
      "for another 200K steps. We optimize the model\n",
      "with Adafactor (Shazeer and Stern, 2018). For both\n",
      "stages, we adopt a constant learning rate of 5e-4\n",
      "and a batch size of 4096. The models are trained\n",
      "on 64 Cloud v4 TPUs (Jouppi et al., 2020).\n",
      "We then ﬁne-tune MuRAG on WebQA and Mul-\n",
      "timodalQA with a constant learning rate of 3e-4\n",
      "for 20K steps. The checkpoint with the highest\n",
      "validation score is run on the test set. We use a\n",
      "batch size of 64 and set TopK=4 for both in-batch\n",
      "training and ﬁxed-retrieval training. We noticed\n",
      "that increasing Top-K further does not yield further\n",
      "improvement. We use a beam size of 2 to search\n",
      "for the best hypothesis for both datasets (increasing\n",
      "it further doesn’t yield better performance).\n",
      "4.2 Datasets\n",
      "For evaluation, we choose two multimodal QA\n",
      "datasets: WebQA (Chang et al., 2022) and Mul-\n",
      "timodalQA (Talmor et al., 2021) and demonstrate\n",
      "their statistics in Table 3.\n",
      "WebQA This dataset contains multi-hop, multi-\n",
      "modal question-answer pairs where all questions\n",
      "are knowledge-seeking queries. The queries re-\n",
      "quire 1-2 images or 1-2 text snippets to answer.Dataset Train Dev Test\n",
      "Image/Text Image/Text Image/Text\n",
      "WebQA 18K/17K 2.5K/2.4K 3.4K/4K\n",
      "MultimodalQA 2.1K/7.4K 230/721 -\n",
      "Table 3: Overall Statistics of downstream dataset.\n",
      "Each query in WebQA is associated with a set of\n",
      "visual/text distractors (hard negatives). The an-\n",
      "swers in WebQA are normally complete sentences\n",
      "to better assess the model’s generation capabil-\n",
      "ity. Two evaluation setups are used, namely dis-\n",
      "tractor and full-wiki. Under the distractor setup,\n",
      "the model needs to retrieve from these hard neg-\n",
      "atives + positives to answer the question. Under\n",
      "the full-wiki setup, the model needs to search over\n",
      "1.1M text and visual sources from Wikipedia to an-\n",
      "swer the question. For evaluation, WebQA uses\n",
      "BARTScore (Yuan et al., 2021) to measure the\n",
      "ﬂuency between the generation and the reference,\n",
      "and keyword accuracy score to measure the cor-\n",
      "rectness/truthfulness of the generation. These two\n",
      "scores are multiplied to calculate the overall score.\n",
      "MultimodalQA-Subset This dataset contains\n",
      "human-annotated multimodal questions over differ-\n",
      "ent modalities including tables, text, and images.\n",
      "Wikipedia tables are used as anchors to connect dif-\n",
      "ferent modalities. The authors ﬁrst use the template\n",
      "to generate questions and then ask crowd-workers\n",
      "to ﬁlter and paraphrase the generated questions.\n",
      "Since tables are outside the scope of our paper, we\n",
      "focus on the subset of queries requiring only text\n",
      "and image information. Speciﬁcally, we choose the\n",
      "questions with types of ‘TextQ’ and ‘ImageQ’ to\n",
      "construct the subset. The query requires 1 image\n",
      "or 1 text snippet to answer. Each query in Multi-\n",
      "modalQA is also associated with visual and text dis-\n",
      "tractors (hard negatives). Similarly, two evaluation\n",
      "setups are used as before. Under a full-wiki setup,\n",
      "MultimodalQA uses a database containing 500K\n",
      "text and visual sources. The evaluation scores are\n",
      "based on Exact Match and F1.\n",
      "4.3 Baselines\n",
      "For WebQA and MultimodalQA, we mainly\n",
      "compare different variants of pre-trained vision-\n",
      "language models.\n",
      "VLP In WebQA, VLP-like models (Zhou et al.,\n",
      "2020) like Oscar (Li et al., 2020) and VinvL (Zhang\n",
      "et al., 2021) are used as the standard baselines.\n",
      "These models were pre-trained on Conceptual3M (Sharma et al., 2018) with a masked language\n",
      "objective. During ﬁne-tuning, the VLP model takes\n",
      "a set of token inputs <[CLS], si, [SEP], Q, [SEP]>\n",
      "ﬁrst to select the most plausible source si, and then\n",
      "feedsiin the form of <[CLS], S,Q,A, [SEP]>\n",
      "to autoregressively decode answer Awith masked\n",
      "language model prediction.\n",
      "AutoRouting In MultimodalQA, this method\n",
      "ﬁrst applies a question type classiﬁer to detect the\n",
      "modality of the question (either a passage or an\n",
      "image), and then routes the question to its sub-\n",
      "model. The method uses RoBERTa-large (Roberts\n",
      "et al., 2022) for text-questions and VilBERT (Lu\n",
      "et al., 2019) with features extracted from Faster-\n",
      "RCNN (Ren et al., 2015) for image questions.\n",
      "CLIP (K) CLIP (Radford et al., 2021) is used for\n",
      "full-wiki retrieval. Speciﬁcally, the baselines sys-\n",
      "tems adopt CLIP to encode queries and all the im-\n",
      "age/text candidates separately into vectors and then\n",
      "run approximated nearest neighbor searches to ﬁnd\n",
      "a set of K potential candidates. After the coarse-\n",
      "level retrieval without cross-attention, it adopts a\n",
      "reranker to further narrow down to the 1-2 candi-\n",
      "dates to feed as input Sto the QA model.\n",
      "4.4 Experimental Results\n",
      "We demonstrate WebQA’s results in Table 4. All\n",
      "results reported are the medium score from three\n",
      "runs with different random seeds, and the variance\n",
      "of the Overall score is within 0.2%. We can observe\n",
      "that MuRAG can signiﬁcantly outperform VLP\n",
      "with different backends including Oscar, ResNet,\n",
      "and VinVL. In retrieval performance, our model\n",
      "outperforms VLP by 15% in the full-wiki setting.\n",
      "For Fluency, our model outperforms VLP by 12%\n",
      "under the distractor setting and 14% under the full-\n",
      "wiki setting. For Accuracy, our model manages\n",
      "to achieve 16% under the distractor setting and\n",
      "even 20% the under the full-wiki setting. These\n",
      "improvements reﬂect the high ﬂuency and accuracy\n",
      "of MuRAG’s generation, and the improvement is\n",
      "more pronounced for full wiki.\n",
      "We show the MultimodalQA results in Table 5.\n",
      "We can see that MuRAG is also able to vastly\n",
      "outperform the routing-based multimodality QA\n",
      "model. For text questions, our model improves\n",
      "over AutoRouting by 10+% EM under both set-\n",
      "tings. For image questions, the gap becomes more\n",
      "signiﬁcant, with 20+% improvement under both\n",
      "settings. Similarly, we ﬁnd that our model is more\n",
      "capable of handling full-wiki corpus.Evaluation Distractor\n",
      "Metrics Retr FL Accuracy Overall\n",
      "Question-Only - 34.9 22.2 13.4\n",
      "VLP (Oscar) 68.9 42.6 36.7 22.6\n",
      "VLP + ResNeXt 69.0 43.0 37.0 23.0\n",
      "VLP + VinVL 70.9 44.2 38.9 24.1\n",
      "MuRAG 74.6 55.7 54.6 36.1\n",
      "Evaluation Full-Wiki\n",
      "CLIP (2) + VLP 11.9 34.2 24.1 14.6\n",
      "CLIP (20) + VLP 24.0 36.1 27.2 16.1\n",
      "MuRAG 39.7 50.7 47.8 31.5\n",
      "Table 4: WebQA ofﬁcial test-set results indicated\n",
      "on leaderboard3as of May 2022. Retr denotes\n",
      "the retrieval-F1 score. FL refers to ﬂuency metric\n",
      "BARTSCcore, and Accuracy refers to keyword match-\n",
      "ing F1 score, they are combined as Overall.\n",
      "Evaluation Distractor\n",
      "MetricsText Image All\n",
      "EM F1 EM F1 EM\n",
      "Question-Only 15.4 18.4 11.0 15.6 13.8\n",
      "AutoRouting 49.5 56.9 37.8 37.8 46.6\n",
      "MuRAG 60.8 67.5 58.2 58.2 60.2\n",
      "Evaluation Full-Wiki\n",
      "MetricsText Image All\n",
      "EM F1 EM F1 EM\n",
      "CLIP (10) +\n",
      "AutoRouting35.6 40.2 32.5 32.5 34.7\n",
      "MuRAG 49.7 56.1 56.5 56.5 51.4\n",
      "Table 5: Multimodal dev-set results on the subset.\n",
      "4.5 Ablation Study\n",
      "Here we ablate the properties of MuRAG to better\n",
      "understand our experimental results.\n",
      "Pre-training Corpus In order to study the contri-\n",
      "butions of different pre-training corpora, we investi-\n",
      "gated several pre-training corpus combinations. We\n",
      "report their ﬁne-tuned results on WebQA test set\n",
      "in Table 6. As can be seen, without any pre-training,\n",
      "our model only achieves an overall score of 23.5,\n",
      "which lags behind the baseline models. After pre-\n",
      "training on different singular datasets, MuRAG is\n",
      "able to achieve better performance than the base-\n",
      "lines. Among the individual datasets, LAION is\n",
      "shown to yield the highest score, and adding CC,\n",
      "PAQ, and VQA to the pre-training corpus set one\n",
      "by one produces steady improvements.\n",
      "Two-Stage Fine-tuning In order to study the ne-\n",
      "cessity of the two-stage ﬁne-tuning, we perform an\n",
      "ablation study to see the impact of the two stages.\n",
      "We display our results in Table 7. (Only In-Batch)Pre-train Dataset FL Accuracy Overall\n",
      "None 42.5 36.1 23.5\n",
      "CC 46.4 41.3 25.6\n",
      "LAION 47.8 44.8 28.3\n",
      "VQA 47.0 44.4 27.4\n",
      "PAQ 46.8 42.8 27.0\n",
      "LAION+CC 49.5 47.4 30.7\n",
      "LAION+CC+PAQ 53.7 51.8 34.4\n",
      "LAION+CC+PAQ+VQA 55.7 54.6 36.1\n",
      "Table 6: Ablation Study for different pre-training cor-\n",
      "pus, score under distractor setting.\n",
      "Model WebQA Multimodal\n",
      "MuRAG (Only In-Batch) 29.4 49.6\n",
      "MuRAG (Only Fixed-Retrieval) 25.8 40.7\n",
      "MuRAG (Two Stage) 31.5 51.4\n",
      "Table 7: Ablation Study for different ﬁne-tuning stages\n",
      "to see their contributions. WebQA uses the overall\n",
      "score, and MultimodalQA refers to EM-all score.\n",
      "Evaluation Model Correct Wrong\n",
      "DistractorMuRAG (Text) 80% 20%\n",
      "MuRAG (Image) 64% 36%\n",
      "Full-WikiMuRAG (Text) 72% 28%\n",
      "MuRAG (Image) 54% 46%\n",
      "Table 8: The human evaluation results on WebQA\n",
      "dataset separately for image/text queries.\n",
      "refers to the model trained only with in-batch mem-\n",
      "ory are directly used to generate outputs by access-\n",
      "ing the global memory. Without further tuning,\n",
      "the performance will drop by roughly 2% on both\n",
      "datasets. (Only Fixed-Retrieval) refers to using the\n",
      "pre-trained retriever directly to obtain Top-K and\n",
      "then optimize the generative loss. As can be seen,\n",
      "the performance drop is more severe in this case\n",
      "for both datasets. This is understandable due the\n",
      "misalignment between pre-training retrieval is (im-\n",
      "age + text->text) while the ﬁne-tuning retrieval is\n",
      "(text -> image+text). Thus, it is necessary to adapt\n",
      "the MuRAG’s pre-trained retriever to different use\n",
      "cases depending on the downstream datasets.\n",
      "4.6 Human Analysis\n",
      "In order to better understand the model’s perfor-\n",
      "mance, we manually study 200 model outputs and\n",
      "classify them into three categories and show our\n",
      "manual analysis results in Table 8. As can be seen,\n",
      "image queries are much harder than text queries.\n",
      "MuRAG only achieves 64% accuracy for the dis-\n",
      "tractor setting and 54% accuracy for the full-wiki\n",
      "setting, falling signiﬁcantly behind text accuracy.\n",
      "We further categorize the image-query errors\n",
      "Figure 5: Upper left: correct prediction, Upper Right:\n",
      "error due to miscounting, Lower: error due to misrecog-\n",
      "nition (multiple image reasoning). Q refers to the ques-\n",
      "tion, P refers to prediction and R refers to the reference.\n",
      "manually into the categories of Table 9. Counting\n",
      "is the most difﬁcult question type, and constitutes\n",
      "52% of the total errors, while object recognition\n",
      "errors rank second, constituting 29% of errors. In\n",
      "contrast, identifying color, shape, and gender is\n",
      "comparatively easier, with fairly low error rates.\n",
      "We demonstrate some correct and typical error\n",
      "cases in Figure 5 including miscounting and mis-\n",
      "recognizing objects. We observe that these errors\n",
      "are mostly due to several reasons: 1) the question\n",
      "is related to infrequent objects, thus making recog-\n",
      "nition errors, 2) the image scene is highly complex\n",
      "with a large number of objects, thus grounding to a\n",
      "speciﬁc region is difﬁcult, 3) the questions require\n",
      "optical character recognition ability from images.\n",
      "Hence, the bottleneck of MuRAG is still in the\n",
      "visual understanding module.\n",
      "Category Count Object Color Shape Gender\n",
      "Ratio 52% 29.4% 5.8% 5.8% 5.8%\n",
      "Table 9: Error categorization and their ratios on sam-\n",
      "pled WebQA-dev image queries.\n",
      "5 Examples\n",
      "We list more examples in Figure 6 and Figure 7.\n",
      "As can be seen, in the ﬁrst example, the model isgrounded on the oracle image-text pair to make the\n",
      "correct prediction. However, in the second exam-\n",
      "ple, though the model retrieves the wrong image-\n",
      "text pair, it is able to make the correct prediction of\n",
      "‘the angel is holding a dead body’. We conjecture\n",
      "that the model utilizes textual clues to make the pre-\n",
      "diction rather than grounding on the image itself.\n",
      "Such shortcut learning is concerning and needs to\n",
      "be addressed through better learning algorithms.\n",
      "Figure 6: Examples: we demonstrate model retrieval\n",
      "vs. groundtruth and model answer vs. reference.\n",
      "6 Conclusion\n",
      "In this paper, we build the ﬁrst visually-grounded\n",
      "language generator capable of retrieving multi-\n",
      "modal knowledge from a large-scale corpus. Our\n",
      "experiments show the promise of this approach, as\n",
      "it outperforms existing baselines by a large margin.\n",
      "At the same time, the performance on knowledge-\n",
      "seeking queries that require reasoning over images\n",
      "is still signiﬁcantly lower than the performance on\n",
      "queries requiring only text. This indicates that there\n",
      "is still ample room for further improvements and\n",
      "we hope our study can motivate more research on\n",
      "better multimodal retrieval-augmented models.\n",
      "Limitations\n",
      "The current approach has several limitations: 1)\n",
      "since we do not mine hard negatives during pre-\n",
      "training, negatives come from other examples\n",
      "within the same batch. This requires that we set the\n",
      "batch size sufﬁciently large enough to collect hard-\n",
      "enough negatives. This results in the pre-training\n",
      "Figure 7: Examples: we demonstrate model retrieval\n",
      "vs. groundtruth, and model answer vs. reference.\n",
      "requiring a large number of computation resources\n",
      "to reach competitive retrieval abilities. 2) our pre-\n",
      "training corpus’s format (image -> text) is differ-\n",
      "ent from ﬁne-tuning (text -> image+text). This\n",
      "misalignment limits the model’s performance. Fu-\n",
      "ture work should consider how to design a better-\n",
      "aligned pre-training objective to achieve better per-\n",
      "formance. 3) Current visual representation in the\n",
      "reader stage is relatively expensive, i.e. 16x16=196\n",
      "tokens per image, which poses great challenges for\n",
      "the transformer encoder to scale up to large Top-K\n",
      "values due to the quadratic attention complexity.\n",
      "Ethical Statement\n",
      "Our work uses the LAION dataset, a widely-used\n",
      "and publicly available large-scale visual-language\n",
      "corpus crawled from the web. The authors have\n",
      "conducted automatic ﬁltering to greatly reduce\n",
      "harmful content. However, it is not possible to\n",
      "fully remove all of the potential risks from the data\n",
      "given its tremendous size. Being trained on this\n",
      "dataset, we anticipate our model to contain some\n",
      "biases (racial, gender, etc.). During our manual\n",
      "inspection, we saw some such biases, for example,\n",
      "5% of errors are caused by misrecognition of gen-\n",
      "der. However, there are other many other forms of\n",
      "biases that we cannot fully enumerate or observeexplicitly.\n",
      "References\n",
      "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\n",
      "garet Mitchell, Dhruv Batra, C Lawrence Zitnick,\n",
      "and Devi Parikh. 2015. Vqa: Visual question an-\n",
      "swering. In Proceedings of the IEEE international\n",
      "conference on computer vision , pages 2425–2433.\n",
      "Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-\n",
      "mann, Trevor Cai, Eliza Rutherford, Katie Millican,\n",
      "George van den Driessche, Jean-Baptiste Lespiau,\n",
      "Bogdan Damoc, Aidan Clark, et al. 2021. Improv-\n",
      "ing language models by retrieving from trillions of\n",
      "tokens. arXiv preprint arXiv:2112.04426 .\n",
      "James Bradbury, Roy Frostig, Peter Hawkins,\n",
      "Matthew James Johnson, Chris Leary, Dougal\n",
      "Maclaurin, George Necula, Adam Paszke, Jake\n",
      "VanderPlas, Skye Wanderman-Milne, and Qiao\n",
      "Zhang. 2018. JAX: composable transformations of\n",
      "Python+NumPy programs.\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, et al. 2020. Language models are few-shot\n",
      "learners. Advances in neural information processing\n",
      "systems , 33:1877–1901.\n",
      "Yingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\n",
      "hong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.\n",
      "Webqa: Multihop and multimodal qa. The Confer-\n",
      "ence on Computer Vision and Pattern Recognition .\n",
      "Soravit Changpinyo, Piyush Sharma, Nan Ding, and\n",
      "Radu Soricut. 2021. Conceptual 12m: Pushing web-\n",
      "scale image-text pre-training to recognize long-tail\n",
      "visual concepts. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recog-\n",
      "nition , pages 3558–3568.\n",
      "Wenhu Chen, Pat Verga, Michiel de Jong, John Wi-\n",
      "eting, and William Cohen. 2022. Augmenting\n",
      "pre-trained language models with qa-memory for\n",
      "open-domain question answering. arXiv preprint\n",
      "arXiv:2204.04581 .\n",
      "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\n",
      "ishna Vedantam, Saurabh Gupta, Piotr Dollár, and\n",
      "C Lawrence Zitnick. 2015. Microsoft coco captions:\n",
      "Data collection and evaluation server. arXiv preprint\n",
      "arXiv:1504.00325 .\n",
      "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\n",
      "El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\n",
      "Jingjing Liu. 2020. Uniter: Universal image-text\n",
      "representation learning. In European conference on\n",
      "computer vision , pages 104–120. Springer.\n",
      "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\n",
      "Maarten Bosma, Gaurav Mishra, Adam Roberts,\n",
      "Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, et al. 2022. Palm: Scaling\n",
      "language modeling with pathways. arXiv preprint\n",
      "arXiv:2204.02311 .\n",
      "Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\n",
      "ald, Fei Sha, and William Cohen. 2022. Mention\n",
      "memory: incorporating textual knowledge into trans-\n",
      "formers through entity mention attention. ICLR .\n",
      "Alexey Dosovitskiy, Lucas Beyer, Alexander\n",
      "Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\n",
      "Thomas Unterthiner, Mostafa Dehghani, Matthias\n",
      "Minderer, Georg Heigold, Sylvain Gelly, et al. 2020.\n",
      "An image is worth 16x16 words: Transformers\n",
      "for image recognition at scale. In International\n",
      "Conference on Learning Representations .\n",
      "Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\n",
      "David Simcha, Felix Chern, and Sanjiv Kumar. 2020.\n",
      "Accelerating large-scale inference with anisotropic\n",
      "vector quantization. In International Conference on\n",
      "Machine Learning , pages 3887–3896. PMLR.\n",
      "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\n",
      "supat, and Mingwei Chang. 2020. Retrieval aug-\n",
      "mented language model pre-training. In Proceed-\n",
      "ings of the 37th International Conference on Ma-\n",
      "chine Learning , volume 119 of Proceedings of Ma-\n",
      "chine Learning Research , pages 3929–3938. PMLR.\n",
      "Darryl Hannan, Akshay Jain, and Mohit Bansal. 2020.\n",
      "Manymodalqa: Modality disambiguation and qa\n",
      "over diverse inputs. In Proceedings of the AAAI Con-\n",
      "ference on Artiﬁcial Intelligence , volume 34, pages\n",
      "7879–7886.\n",
      "Gautier Izacard and Édouard Grave. 2021. Leveraging\n",
      "passage retrieval with generative models for open\n",
      "domain question answering. In Proceedings of the\n",
      "16th Conference of the European Chapter of the As-\n",
      "sociation for Computational Linguistics: Main Vol-\n",
      "ume, pages 874–880.\n",
      "Norman P Jouppi, Doe Hyun Yoon, George Kurian,\n",
      "Sheng Li, Nishant Patil, James Laudon, Cliff Young,\n",
      "and David Patterson. 2020. A domain-speciﬁc\n",
      "supercomputer for training deep neural networks.\n",
      "Communications of the ACM , 63(7):67–78.\n",
      "Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-\n",
      "semantic alignments for generating image descrip-\n",
      "tions. In Proceedings of the IEEE conference\n",
      "on computer vision and pattern recognition , pages\n",
      "3128–3137.\n",
      "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\n",
      "Zettlemoyer, and Mike Lewis. 2019. Generalization\n",
      "through memorization: Nearest neighbor language\n",
      "models. In International Conference on Learning\n",
      "Representations .\n",
      "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\n",
      "Petroni, Vladimir Karpukhin, Naman Goyal, Hein-\n",
      "rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\n",
      "täschel, et al. 2020. Retrieval-augmented generation\n",
      "for knowledge-intensive nlp tasks. Advances in Neu-\n",
      "ral Information Processing Systems , 33:9459–9474.Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale\n",
      "Minervini, Heinrich Küttler, Aleksandra Piktus, Pon-\n",
      "tus Stenetorp, and Sebastian Riedel. 2021. Paq: 65\n",
      "million probably-asked questions and what you can\n",
      "do with them. Transactions of the Association for\n",
      "Computational Linguistics , 9:1098–1115.\n",
      "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\n",
      "aowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\n",
      "Li Dong, Furu Wei, et al. 2020. Oscar: Object-\n",
      "semantics aligned pre-training for vision-language\n",
      "tasks. In European Conference on Computer Vision ,\n",
      "pages 121–137. Springer.\n",
      "Tsung-Yi Lin, Michael Maire, Serge Belongie, James\n",
      "Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\n",
      "and C Lawrence Zitnick. 2014. Microsoft coco:\n",
      "Common objects in context. In European confer-\n",
      "ence on computer vision , pages 740–755. Springer.\n",
      "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\n",
      "Lee. 2019. Vilbert: Pretraining task-agnostic visi-\n",
      "olinguistic representations for vision-and-language\n",
      "tasks. Advances in neural information processing\n",
      "systems , 32.\n",
      "Kenneth Marino, Mohammad Rastegari, Ali Farhadi,\n",
      "and Roozbeh Mottaghi. 2019. Ok-vqa: A visual\n",
      "question answering benchmark requiring external\n",
      "knowledge. In Proceedings of the IEEE/CVF Con-\n",
      "ference on Computer Vision and Pattern Recogni-\n",
      "tion, pages 3195–3204.\n",
      "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\n",
      "Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\n",
      "Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\n",
      "et al. 2021. Learning transferable visual models\n",
      "from natural language supervision. In International\n",
      "Conference on Machine Learning , pages 8748–8763.\n",
      "PMLR.\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Kather-\n",
      "ine Lee, Sharan Narang, Michael Matena, Yanqi\n",
      "Zhou, Wei Li, and Peter J. Liu. 2020. Exploring\n",
      "the limits of transfer learning with a uniﬁed text-to-\n",
      "text transformer. Journal of Machine Learning Re-\n",
      "search , 21(140):1–67.\n",
      "Revanth Gangi Reddy, Xilin Rui, Manling Li, Xudong\n",
      "Lin, Haoyang Wen, Jaemin Cho, Lifu Huang, Mo-\n",
      "hit Bansal, Avirup Sil, Shih-Fu Chang, et al. 2021.\n",
      "Mumuqa: Multimedia multi-hop news question an-\n",
      "swering via cross-media knowledge extraction and\n",
      "grounding. arXiv preprint arXiv:2112.10728 .\n",
      "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian\n",
      "Sun. 2015. Faster r-cnn: Towards real-time object\n",
      "detection with region proposal networks. Advances\n",
      "in neural information processing systems , 28.\n",
      "Adam Roberts, Hyung Won Chung, Anselm Lev-\n",
      "skaya, Gaurav Mishra, James Bradbury, Daniel An-\n",
      "dor, Sharan Narang, Brian Lester, Colin Gaffney,\n",
      "Afroz Mohiuddin, et al. 2022. Scaling up mod-\n",
      "els and data with t5x and seqio. arXiv preprint\n",
      "arXiv:2203.17189 .Christoph Schuhmann, Richard Vencu, Romain Beau-\n",
      "mont, Robert Kaczmarczyk, Clayton Mullis, Aarush\n",
      "Katta, Theo Coombes, Jenia Jitsev, and Aran Komat-\n",
      "suzaki. 2021. Laion-400m: Open dataset of clip-\n",
      "ﬁltered 400 million image-text pairs. arXiv preprint\n",
      "arXiv:2111.02114 .\n",
      "Piyush Sharma, Nan Ding, Sebastian Goodman, and\n",
      "Radu Soricut. 2018. Conceptual captions: A\n",
      "cleaned, hypernymed, image alt-text dataset for au-\n",
      "tomatic image captioning. In Proceedings of the\n",
      "56th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (Volume 1: Long Papers) , pages\n",
      "2556–2565.\n",
      "Noam Shazeer and Mitchell Stern. 2018. Adafactor:\n",
      "Adaptive learning rates with sublinear memory cost.\n",
      "InInternational Conference on Machine Learning ,\n",
      "pages 4596–4604. PMLR.\n",
      "Hrituraj Singh, Anshul Nasery, Denil Mehta, Aish-\n",
      "warya Agarwal, Jatin Lamba, and Balaji Vasan Srini-\n",
      "vasan. 2021. Mimoqa: Multimodal input multi-\n",
      "modal output question answering. In Proceedings\n",
      "of the 2021 Conference of the North American Chap-\n",
      "ter of the Association for Computational Linguistics:\n",
      "Human Language Technologies , pages 5317–5332.\n",
      "Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\n",
      "Yizhong Wang, Akari Asai, Gabriel Ilharco, Han-\n",
      "naneh Hajishirzi, and Jonathan Berant. 2021. Multi-\n",
      "modalqa: complex question answering over text, ta-\n",
      "bles and images. In ICLR .\n",
      "Pat Verga, Haitian Sun, Livio Baldini Soares, and\n",
      "William Weston Cohen. 2021. Adaptable and inter-\n",
      "pretable neural memory over symbolic knowledge.\n",
      "InProceedings of NAACL-HLT , pages 3678–3691.\n",
      "Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\n",
      "lia Tsvetkov, and Yuan Cao. 2022. Simvlm: Simple\n",
      "visual language model pretraining with weak super-\n",
      "vision. ICLR .\n",
      "Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\n",
      "ung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\n",
      "Coca: Contrastive captioners are image-text founda-\n",
      "tion models. arXiv preprint arXiv:2205.01917 .\n",
      "Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\n",
      "Bartscore: Evaluating generated text as text gener-\n",
      "ation. Advances in Neural Information Processing\n",
      "Systems , 34.\n",
      "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\n",
      "Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\n",
      "feng Gao. 2021. Vinvl: Revisiting visual representa-\n",
      "tions in vision-language models. In Proceedings of\n",
      "the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition , pages 5579–5588.\n",
      "Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong\n",
      "Hu, Jason Corso, and Jianfeng Gao. 2020. Uni-\n",
      "ﬁed vision-language pre-training for image caption-\n",
      "ing and vqa. In Proceedings of the AAAI Conference\n",
      "on Artiﬁcial Intelligence , volume 34, pages 13041–\n",
      "13049.A Pre-training\n",
      "During Pre-trainnig, we found that directly train-\n",
      "ing with a mixture of all four datasets will lead to\n",
      "instability. We experimented with different vari-\n",
      "ants and found that a scheduled pre-training can\n",
      "lead to a stable solution. We propose to ﬁrst pre-\n",
      "train the model on the largest LAION dataset for\n",
      "1M steps, and then continue training on the other\n",
      "three datasets with a ﬁxed sample ratio. We plot\n",
      "the ﬁrst stage of LAION training in Figure 8. We\n",
      "monitor the generation quality (LAION image ->\n",
      "text captioning), and the retrieval quality (image ->\n",
      "4096 in-batch caption retrieval). As can be seen,\n",
      "the LAION pre-training converges after 1M steps,\n",
      "where we ﬁrst warm up and then decrease the learn-\n",
      "ing rate using a scheduler.\n",
      "Figure 8: LAION Pre-training, validation accuracy,\n",
      "generation Cider score and retrieval recall score from\n",
      "the in-batch memory.\n",
      "We further the pre-training on a mixture of the\n",
      "other three datasets. We plot their inference eval-\n",
      "uation scores in Figure 9. We can see that the\n",
      "model is able to achieve very strong performance\n",
      "on these datasets, i.e. higher than 1.2 CiDEron CC12M+3M validation set. The model also\n",
      "achieves strong performance on text-only reading\n",
      "comprehension on PAQ (similar to NQ), i.e. higher\n",
      "than 55% EM score. On the VQA dataset, the\n",
      "model is able to achieve higher than 72% VQA ac-\n",
      "curacy on the validation set. These results demon-\n",
      "strate the efﬁciency and multi-tasking capabilities\n",
      "of the pre-trained model. The overall retrieval\n",
      "accuracy from the multimodal memory consist-\n",
      "ing of captions, and passages are plotted in Fig-\n",
      "ure 10, where the model is able to achieve 85%\n",
      "RECALL@1 from a 4K memory.\n",
      "B Model Conﬁguration\n",
      "We demonstrate the ViT conﬁguration as follows:\n",
      "\" v i t _ c o n f i g \" : {\n",
      "\" model \" : \" ViT \" ,\n",
      "\" p a t c h e s \" : {\n",
      "\" s i z e \" : [ 1 6 , 16]\n",
      "} ,\n",
      "\" h i d d e n _ s i z e \" : 1024 ,\n",
      "\" i m a g e _ s i z e \" : [ 2 2 4 , 2 2 4 ] ,\n",
      "\" num_heads \" : 16 ,\n",
      "\" num_layers \" : 24 ,\n",
      "\" mlp_dim \" : 4096 ,\n",
      "\" r e t u r n _ p o o l e d _ o u t p u t \" : f a l s e ,\n",
      "\" d r o p o u t _ r a t e \" : 0 . 1\n",
      "} ,\n",
      "We demonstrate the T5-EncDec conﬁguration as\n",
      "follows:\n",
      "\" m o d e l _ c o n f i g \" : {\n",
      "\" v o c a b _ s i z e \" : 32128 ,\n",
      "\" h i d d e n _ s i z e \" : 768 ,\n",
      "\" i n t e r m e d i a t e _ d i m \" : 2048 ,\n",
      "\" n u m _ a t t e n t i o n _ h e a d s \" : 12 ,\n",
      "\" memory_key_dim \" : 768 ,\n",
      "\" e n c o d e r _ l a y e r s \" : 12 ,\n",
      "\" d e c o d e r _ l a y e r s \" : 12 ,\n",
      "\" d r o p o u t _ r a t e \" : 0 . 1 ,\n",
      "\" m a x _ d i s t a n c e \" : 128 ,\n",
      "\" num_buckets \" : 32 ,\n",
      "\" s c a l e \" : 1 . 0 ,\n",
      "\" r e t r i e v a l _ w e i g h t \" : 0 . 5 ,\n",
      "}Figure 9: Mixture Pre-training, CiDEr, EM, and VQA\n",
      "accuracy for CC, PAQ, and VQA datasets.\n",
      "Figure 10: Mixture Pre-training retrieval accuracy over\n",
      "CC, PAQ, and VQA datasets.\n",
      "==================================================\n",
      "\n",
      "Document 7:\n",
      "RETRIEVAL-GENERATION SYNERGY AUGMENTED LARGE LANGUAGE MODELS\n",
      "Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, Bing Qin\n",
      "Harbin Institute of Technology, China\n",
      "ABSTRACT\n",
      "Large language models augmented with task-relevant docu-\n",
      "ments have demonstrated impressive performance on knowl-\n",
      "edgeintensive tasks. However, regarding how to obtain effec-\n",
      "tive documents, the existing methods are mainly divided into\n",
      "two categories. One is to retrieve from an external knowledge\n",
      "base, and the other is to utilize large language models to gen-\n",
      "erate documents. We propose an iterative retrieval-generation\n",
      "collaborative framework. It is not only able to leverage both\n",
      "parametric and non-parametric knowledge, but also helps to\n",
      "find the correct reasoning path through retrieval-generation\n",
      "interactions, which is very important for tasks that require\n",
      "multi-step reasoning. We conduct experiments on four ques-\n",
      "tion answering datasets, including single-hop QA and multi-\n",
      "hop QA tasks. Empirical results show that our method signif-\n",
      "icantly improves the reasoning ability of large language mod-\n",
      "els and outperforms previous baselines.\n",
      "Index Terms —large language models, retrieval aug-\n",
      "mented, question answering\n",
      "1. INTRODUCTION\n",
      "Large Language models (LLMs) have demonstrated im-\n",
      "pressive performance on diverse language tasks through in-\n",
      "context learning [1, 2, 3, 4, 5, 6]. However, they still struggle\n",
      "with knowledge-intensive tasks that require access to a large\n",
      "amount of knowledge, such as open-domain question answer-\n",
      "ing [7] and commonsense reasoning [8], since the implicit\n",
      "knowledge preserved in the parameters may be partial and\n",
      "insufficient. As shown in the top of Figure 1, one promising\n",
      "direction is to incorporate non-parametric knowledge to help\n",
      "alleviate this problem with large language models.\n",
      "Recent research shows that retrieving relevant documents\n",
      "from an external datastore [9, 10, 11] or directly generating\n",
      "contextual documents from LLMs [12, 13] both can improve\n",
      "LLMs’ performance on knowledge-intensive tasks. The for-\n",
      "mer, called retrieve-then-read, requires a retriever to retrieve\n",
      "relevant documents. The latter, known as generate-then-read,\n",
      "leverages large language models to generate relevant docu-\n",
      "ments before answering questions. However, as shown in\n",
      "Figure 1, the above two methods are isolated and lack co-\n",
      "ordination with each other. To fill this gap, in this paper, we\n",
      "explore an effective retrieval-generation collaboration frame-\n",
      "Who is the lead singer of Depeche Mode?Depeche Mode currently consists of Dave Gahan (lead vocals, co-songwriting) and Martin Gore (keyboards, guitar, co-lead vocals, primary songwriting)…LLMsQuestionDocumentDave GahanAnswer\n",
      "QuestionLLMsRetrieverLLMsRetrieverDocumentQuestion\n",
      "DocumentQuestion\n",
      "Document(1) Retrieval(2) Generation(3) Retrieval-Generation SynergyFig. 1 : The top is the standard method utilizing LLMs for\n",
      "question answering with relevant documents. The bottom\n",
      "shows three methods to generate relevant documents.\n",
      "work to further improve the ability of large language models\n",
      "to solve knowledge-intensive tasks.\n",
      "In this work, we present ITRG, an ITerative Retrieval-\n",
      "Generation synergy framework to generate relevant doc-\n",
      "uments that simultaneously exploits parametric and non-\n",
      "parametric knowledge. In each iteration, ITRG consists of\n",
      "two important steps: generation augmented retrieval (GAR)\n",
      "and retrieval augmented generation (RAG). In the GAR step,\n",
      "we propose a simple and effective method to expand queries\n",
      "by concatenating pseudo-documents generated from large\n",
      "language models and original questions. And expanded\n",
      "queries improve the accuracy of retrieving relevant docu-\n",
      "ments. In the RAG step, we use large language models to\n",
      "comprehensively understand retrieved documents to generate\n",
      "new documents for answering questions. We repeat these\n",
      "steps until we reach the maximum allowed number of itera-\n",
      "tions. Through multiple retrieval generation collaborations,\n",
      "our method aids in discovering the appropriate reasoning path\n",
      "and providing correct answers to questions.\n",
      "We evaluate the efficacy of our method on 4 question\n",
      "answering datasets, including Natural Questions, TriviaQA,\n",
      "2WikiMultiHopQA, and HotpotQA. Experimental results\n",
      "show that our method performs better than previous baselines\n",
      "on all datasets. In summary, our main contributions can be\n",
      "summarized as follows: (1) We propose ITRG, an iterative\n",
      "retrieval-generation synergy framework using both paramet-\n",
      "ric and non-parametric knowledge. (2) We propose a simple\n",
      "and effective generation-augmented retrieval strategy and\n",
      "two retrieval-augmented generation strategies. (3) Empiri-\n",
      "cal results show that ITRG outperforms previous retrieval-\n",
      "augmented methods.arXiv:2310.05149v1  [cs.CL]  8 Oct 20232. ITERATIVE RETRIEVAL-GENERATION\n",
      "SYNERGY\n",
      "In this section, we first introduce the overall framework, and\n",
      "then introduce the retrieval-generation collaboration frame-\n",
      "work in detail, including generation augmented retrieval and\n",
      "retrieval augmented generation.\n",
      "2.1. Overview\n",
      "We show the framework of ITRG in Figure 2. Given a user\n",
      "question qand a document corpus D={di}|D|\n",
      "i=1(i.e,diis a\n",
      "Wikipedia paragraph.), ITRG repeats generation augmented\n",
      "retrieval (GAR) and retrieval augmented generation (RAG)\n",
      "forTiterations. In the GAR process of iteration t, we con-\n",
      "catenate the output yt−1of the last iteration and question qto\n",
      "form a new query, and then use a dense retriever to retrieve\n",
      "top-kparagraphs. In the first iteration, we only use the ques-\n",
      "tion as the query. In the RAG process of iteration t, based on\n",
      "the question qand the retrieved top- kparagraphs, we exploit\n",
      "large language models to generate new paragraphs to answer\n",
      "questions. Specifically, we propose two methods to generate\n",
      "new paragraphs, which will be introduced in detail in §2.3.\n",
      "2.2. Generation Augmented Retrieval\n",
      "Knowledge-intensive tasks (e.g., open-domain question an-\n",
      "swering) often require access to additional documents. A\n",
      "common approach is to directly employ the question as the\n",
      "query, and then equip a sparse or dense retriever to retrieve\n",
      "relevant documents. In practice, we find that in some cases\n",
      "using the question directly as the query fails to retrieve rel-\n",
      "evant documents because there may exist semantic gaps be-\n",
      "tween them. To alleviate this problem, we propose a simple\n",
      "query expansion method. At the first iteration ( t= 1), we use\n",
      "the original question qas the query. At iteration t(t >1), we\n",
      "concatenate the original question qand the document gener-\n",
      "atedyt−1in the last iteration as the new query qt= [q;yt−1].\n",
      "Then, we utilize a pre-trained dense retriever to retrieve top- k\n",
      "documents, which are denoted as Rt={d}.\n",
      "Given an input question q, the retriever aims to retrieve\n",
      "a small set of documents from a corpus D={di}|D|\n",
      "i=1that\n",
      "are relevant to q. Following prior work [14], we use a dense\n",
      "retriever based on the dual encoder architecture, where an en-\n",
      "coder is used to encode both the input context qand the docu-\n",
      "mentd. Specifically, the encoder maps each document d∈ D\n",
      "to an embedding E(d)by taking the mean pooling of the last\n",
      "hidden representation over the tokens in d. At query time,\n",
      "the same encoder is applied to the input context qto obtain a\n",
      "query embedding E(q). The similarity between the query em-\n",
      "bedding and the document embedding is computed by their\n",
      "cosine similarity: s(d, q) = cos( E(d),E(q)). The top- kdoc-\n",
      "uments that have the highest similarity scores are retrieved.\n",
      "Question: What is the date of birth of Emilie HeghArntzen'smother?Retrieval:Generation:Retrieval:Generation:Retrieval:Generation:Iteration 1Iteration 2\n",
      "Iteration 3infoboxname: Emilie HeghArntzen; caption: HeghArntzenin 2018 ; birth_date: January 1, 1994 ; birth_place: Skien, Norway ; nationality: Norwegian ; Emilie HeghArntzenwas born on January 1, 1994 in Skien, Norway. Her mother is unknown.Camilla Marie Gjersemwas born together with a twin sister, Anne Line, on 6 January 1994 in Hønefoss, Norway. Their mother, PerlinaBangug, is a Filipina from Ilagan, Isabela, and their father, PetterGjersem, a Norwegian from Raufoss. Camilla Gjersemis a law student at the University of Oslo.Hanne Hegh(born 19 January 1960) is a Norwegian handball player. She played 220 matches for the Norwegian national handball team between 1978 and 1992. She is the mother of Emilie HeghArntzen.infoboxname: Hanne Hegh; caption: Hanne Hegh2008 ; nationality: Norwegian ; birth_date: April 27, 1960; birth_place: Oslo, Norway ; Hanne Heghwas born on April 27, 1960 in Oslo, Norway. She is the mother of Emilie HeghArntzen, who was born on January 1, 1994 in Skien, Norway.Fig. 2 : Iterative retrieval-generation synergy framework con-\n",
      "tains two steps in each iteration: (1) generation augmented\n",
      "retrieval (GAR): utilize the output of the previous iteration to\n",
      "expand the query to help retrieve more relevant documents;\n",
      "(2) retrieval augmented generation (RAG): utilize retrieved\n",
      "documents to generate new documents to answer questions.\n",
      "We only show three iterations in this figure for brevity. Solid\n",
      "arrows indicate RAG within an iteration, and dashed arrows\n",
      "indicate GAR between iterations. Purple represents correct\n",
      "and useful information, and red represents wrong or invalid\n",
      "information.\n",
      "2.3. Retrieval Augmented Generation\n",
      "Following previous work [13], for a given question q, we\n",
      "could directly prompt large language models to generate re-\n",
      "lated documents without retrieving them from an external cor-\n",
      "pus. However, we find that if only the parametric knowledge\n",
      "learned by the large model in the pre-training stage is used,\n",
      "the generated documents may be incomplete. Retrieval aug-\n",
      "mented generation (RAG) aims to comprehensively under-\n",
      "stand the retrieved non-parametric knowledge and the para-\n",
      "metric knowledge inside large language models to generate\n",
      "more accurate factual knowledge. Specifically, we propose\n",
      "two strategies, which will be described in detail below.\n",
      "2.3.1. Refine\n",
      "An intuitive idea is to refine the previously generated docu-\n",
      "ment yt−1based on the original question qand the retrieved\n",
      "top-kdocuments at the current iteration step Rtto obtain a\n",
      "new document yt. We call this method refine. Considering\n",
      "that the document retrieved in the last iteration Rt−1has been\n",
      "used to generate the last document yt−1, we refine the previ-\n",
      "ous output yt−1with updated documents Rupdate .\n",
      "Rupdate =Rt−Rt−1, (1)\n",
      "yt=M(prompt ( yt−1, q, R update )), (2)where Rupdate means that these documents are only retrieved\n",
      "in the current iteration, not in the last iteration, Mdenotes a\n",
      "well pre-trained large language model. If Rupdate is an empty\n",
      "set, we do not regenerate a new document and set yt=yt−1.\n",
      "2.3.2. Refresh\n",
      "In order to avoid the negative effect of errors or hallucinations\n",
      "in the previously generated document yt−1, we do not use\n",
      "yt−1, which is used in refine. We refresh the memory and let\n",
      "the large language models directly generate the document yt\n",
      "based on the retrieved document Rtand the original question\n",
      "q. This method is named refresh.\n",
      "yt=M(prompt ( q, R t)) (3)\n",
      "Both refine and refresh are implemented through prompts.\n",
      "We give the prompt corresponding to refresh.\n",
      "Prompt for refresh with all documents\n",
      "In the following task, you should write a document\n",
      "that contains the answer to the question.\n",
      "Passage: {Rt}\n",
      "Question: {q}\n",
      "Document: {yt}\n",
      "3. EXPERIMENTAL SETUP\n",
      "3.1. Datasets\n",
      "We evaluate the effectiveness of ITRG on four open domain\n",
      "question answering datasets, including Natural Questions\n",
      "(NQ) [15], TriviaQA [16], 2WikiMultiHopQA [17] and Hot-\n",
      "potQA [18]. Following previous works [19, 20], we randomly\n",
      "sub-sample 500 examples from each dataset due to the cost\n",
      "of running experiments. We evaluate our method in 0-shot,\n",
      "1-shot and 5-shot settings. The few-shot demonstrations are\n",
      "randomly sampled from the data that is not involved in the\n",
      "evaluation process.\n",
      "3.2. Baselines\n",
      "GPT-3.5 [21] We use text-davinci-002 and text-davinci-003\n",
      "as our baselines. Text-davinci-002 is an InstructGPT model\n",
      "while Text-davinci-003 is trained with reinforcement learn-\n",
      "ing with reward models trained from comparisons by humans.\n",
      "Vanilla LM The vanilla LM baselines prompt an LLM to di-\n",
      "rectly generate an answer following the few-shot in-context\n",
      "learning paradigm [1]. CoT We follow [22] to generate both\n",
      "the chain-of-thought (CoT) reasoning process and the final\n",
      "answer. We only evaluate this method on multi-hop reasoning\n",
      "datasets in 5-shot setting1.Retrieve-then-Read The retrieve-\n",
      "1We also conduct evaluation in 1-shot setting, but the final answer could\n",
      "not be generated according to the corresponding instructionsthen-read baseline consists of a well-pre-trained dense re-\n",
      "triever and a large language model. The retriever retrieves\n",
      "relevant documents for the question, and then the LLM con-\n",
      "ditions on both the question and retrieved documents to gen-\n",
      "erate the answer. Generate-then-Read Generate-then-read\n",
      "baseline first uses few-shot prompts to generate a question-\n",
      "related document, and then concatenates it with the question\n",
      "to regenerate the answer.\n",
      "3.3. Details\n",
      "LLaMA [6] is an open source well trained large language\n",
      "model. Considering the performance and computational cost\n",
      "of the model, we use LLaMA 33B as the backend LLM. We\n",
      "use greedy decoding for both document generation and an-\n",
      "swer generation, and set up to generate 200 tokens and 15\n",
      "tokens respectively. We retrieve the top-5 paragraphs for each\n",
      "query and set the maximum number of iterations Tto 5. We\n",
      "directly use the pre-trained dense retriever [23] and used the\n",
      "December 2018 Wikipedia dump as the retrieval corpus for all\n",
      "datasets. Generated answers are evaluated with the standard\n",
      "exact match metric (EM score): a generated answer is con-\n",
      "sidered correct if it matches any answer of the list of answers\n",
      "after normalization. For this normalization step, we lower-\n",
      "case generated answers and remove articles, punctuation and\n",
      "duplicate whitespaces.\n",
      "4. RESULTS\n",
      "4.1. Main Results\n",
      "Table 1 reports the results on the single-hop question answer-\n",
      "ing datasets. In the 1-shot and 5-shot settings, the perfor-\n",
      "mance of LLaMA-33B based Vanilla LM is very close to\n",
      "that of text-davinci-003. This shows LLaMA-33B is a strong\n",
      "language model, and it is reasonable to choose LLaMA-33B\n",
      "as our backend LLM. Retrieve-then-read and generate-then-\n",
      "read all exceed vanilla LM, verifying that adding relevant\n",
      "external knowledge can improve the reasoning ability of\n",
      "large language models. In addition, we observe that our itera-\n",
      "tive retrieval-generation collaborative method ITRG achieves\n",
      "state-of-the-art performance on both datasets. Specifically,\n",
      "ITRG (refresh) performs better on the NQ dataset, and ITRG\n",
      "(refine) performs better on the TriviaQA dataset.\n",
      "Table 2 presents the results on the multi-hop question an-\n",
      "swering datasets. We observe that LLaMA-33B is still com-\n",
      "parable to text-davinci-003 on the multi-hop question answer-\n",
      "ing datasets. In addition, CoT can answer questions more\n",
      "accurately than vanilla LM by generating reasoning process.\n",
      "Compared with different baseline models, ITRG significantly\n",
      "improves the exact match scores. Specifically, on the 2Wiki-\n",
      "MultiHopQA dataset, the exact match score of ITRG (refresh)\n",
      "in the zero-shot setting is 32.2, which exceeds the perfor-\n",
      "mance of vanilla LM in the 5-shot setting with a score of 31.8.\n",
      "In the 5-shot setting, ITRG (refresh) achieves 38.6 EM scoreTable 1 : Exact match performance on single-hop question answering. All ITRG results are from the last iteration ( T= 5).\n",
      "MethodNatural Questions TriviaQA\n",
      "0-shot 1-shot 5-shot 0-shot 1-shot 5-shot\n",
      "GPT 3.5Text-davinci-002 12.0 24.6 33.0 46.0 74.2 76.0\n",
      "Text-davinci-003 29.4 33.0 33.8 75.8 78.6 77.8\n",
      "LLaMA 33BVanilla LM 27.0 29.4 32.4 74.8 70.8 75.8\n",
      "Retrieve-then-Read 27.8 30.6 29.8 74.6 76.0 76.0\n",
      "Generate-then-Read 28.0 31.4 31.0 73.6 77.2 77.6\n",
      "ITRG (refine) 34.4 34.6 34.8 79.0 79.4 80.6\n",
      "ITRG (refresh) 37.6 38.4 38.0 77.0 78.6 79.4\n",
      "Table 2 : Exact match performance on multi-hop question answering. All ITRG results are from the last iteration ( T= 5).\n",
      "Method2WikiMultiHopQA HotpotQA\n",
      "0-shot 1-shot 5-shot 0-shot 1-shot 5-shot\n",
      "GPT 3.5Text-davinci-002 16.4 27.6 30.8 12.2 20.2 22.2\n",
      "Text-davinci-003 27.2 27.0 29.8 25.0 25.8 26.6\n",
      "LLaMA 33BVanilla LM 24.4 27.6 31.8 22.6 25.0 27.0\n",
      "COT - - 32.2 - - 28.6\n",
      "Retrieve-then-Read 27.4 29.2 32.0 28.4 29.8 30.4\n",
      "Generate-then-Read 30.0 30.4 31.6 25.0 27.0 27.0\n",
      "ITRG (refine) 33.0 33.6 37.0 28.8 29.6 30.6\n",
      "ITRG (refresh) 32.2 36.2 38.6 31.0 32.6 33.4\n",
      "Table 3 : Exact match performance of ITRG (refresh) at dif-\n",
      "ferent iterations in 5-shot setting.\n",
      "Iteration 1 2 3 4 5\n",
      "Natural Questions 34.0 35.2 37.0 37.2 38.0\n",
      "TriviaQA 79.8 79.2 79.8 79.8 79.4\n",
      "2WikiMultiHopQA 34.8 37.4 37.2 38.6 38.6\n",
      "HotpotQA 32.6 32.8 34.0 33.4 33.4\n",
      "and improves by 6.8 points in absolute gains. Compared to\n",
      "vanilla LM, ITRG (refresh) can improve the EM score by 9.4,\n",
      "7.6, and 6.4 points respectively in 0-shot, 1-shot, and 5-shot\n",
      "settings on the Hotpotqa dataset.\n",
      "4.2. Performance at Different Iterations\n",
      "In this section, we analyze the performance of our model and\n",
      "the quality of the generated documents during the iteration\n",
      "process. Specifically, we present the results of ITRG (refresh)\n",
      "at different iterations in 5-shot setting in Table 3. We measure\n",
      "the answer recall of generated documents at different itera-\n",
      "tion steps and present results in Table 4. Table 3 shows that\n",
      "the performance of the model gradually improves with iter-\n",
      "ation. And Table 4 shows that the quality of the generated\n",
      "documents also gradually improves with iteration. These re-\n",
      "sults verify that our iterative retrieval-generation collaborativeTable 4 : Answer recall of generated documents at different\n",
      "iterations with ITRG (refresh).\n",
      "Iteration 1 2 3 4 5\n",
      "Natural Questions 44.0 46.4 48.4 48.8 48.0\n",
      "TriviaQA 18.8 19.0 20.2 19.2 19.2\n",
      "2WikiMultiHopQA 34.2 36.6 35.0 40.0 37.0\n",
      "HotpotQA 34.2 34.8 35.6 33.8 33.6\n",
      "framework is effective and can further enhance the reasoning\n",
      "capabilities of large language models.\n",
      "5. CONCLUSION\n",
      "In this paper, we present ITRG, which is an iterative retrieval-\n",
      "generation synergy framework, containing two important\n",
      "steps: generation-augmented retrieval and retrieval-augmented\n",
      "generation. They form a closed loop, and can improve\n",
      "each other via multiple iterations. We propose a simple\n",
      "and effective generation-augmented retrieval strategy and\n",
      "two retrieval-augmented generation strategies. Empirical re-\n",
      "sults show our approach significantly exceeds several strong\n",
      "baselines, including GPT 3.5, on four open domain ques-\n",
      "tion answering datasets, which indicates that our method can\n",
      "significantly improve the reasoning ability of large language\n",
      "models.6. REFERENCES\n",
      "[1] T. Brown et al. , “Language models are few-shot learners,” Ad-\n",
      "vances in neural information processing systems , vol. 33, pp.\n",
      "1877–1901, 2020.\n",
      "[2] J. Hoffmann et al. , “Training compute-optimal large language\n",
      "models,” 2022.\n",
      "[3] A. Zeng et al. , “Glm-130b: An open bilingual pre-trained\n",
      "model,” arXiv preprint arXiv:2210.02414 , 2022.\n",
      "[4] A. Chowdhery et al. , “Palm: Scaling language modeling with\n",
      "pathways,” arXiv preprint arXiv:2204.02311 , 2022.\n",
      "[5] OpenAI, “Gpt-4 technical report,” 2023.\n",
      "[6] H. Touvron et al. , “Llama: Open and efficient foundation lan-\n",
      "guage models,” 2023.\n",
      "[7] K. Lee, M.-W. Chang, and K. Toutanova, “Latent retrieval\n",
      "for weakly supervised open domain question answering,” in\n",
      "Proceedings of the 57th Annual Meeting of the Association\n",
      "for Computational Linguistics . Florence, Italy: Association\n",
      "for Computational Linguistics, Jul. 2019, pp. 6086–6096.\n",
      "[Online]. Available: https://aclanthology.org/P19-1612\n",
      "[8] R. Zellers, Y . Bisk, R. Schwartz, and Y . Choi, “SWAG:\n",
      "A large-scale adversarial dataset for grounded common-\n",
      "sense inference,” in Proceedings of the 2018 Conference\n",
      "on Empirical Methods in Natural Language Processing .\n",
      "Brussels, Belgium: Association for Computational Lin-\n",
      "guistics, Oct.-Nov. 2018, pp. 93–104. [Online]. Available:\n",
      "https://www.aclweb.org/anthology/D18-1009\n",
      "[9] O. Ram et al. , “In-context retrieval-augmented language mod-\n",
      "els,” arXiv preprint arXiv:2302.00083 , 2023.\n",
      "[10] O. Khattab et al. , “Demonstrate-search-predict: Composing\n",
      "retrieval and language models for knowledge-intensive nlp,”\n",
      "2023.\n",
      "[11] W. Shi et al. , “Replug: Retrieval-augmented black-box lan-\n",
      "guage models,” arXiv preprint arXiv:2301.12652 , 2023.\n",
      "[12] W. Yu et al. , “Generate rather than retrieve: Large language\n",
      "models are strong context generators,” 2023.\n",
      "[13] Z. Sun, X. Wang, Y . Tay, Y . Yang, and D. Zhou, “Recitation-\n",
      "augmented language models,” 2023.\n",
      "[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\n",
      "generative models for open domain question answering,” arXiv\n",
      "preprint arXiv:2007.01282 , 2020.\n",
      "[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\n",
      "question answering research,” Transactions of the Association\n",
      "for Computational Linguistics , vol. 7, pp. 452–466, 2019.\n",
      "[Online]. Available: https://aclanthology.org/Q19-1026\n",
      "[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\n",
      "A large scale distantly supervised challenge dataset for\n",
      "reading comprehension,” in Proceedings of the 55th Annual\n",
      "Meeting of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers) . Vancouver, Canada: Association\n",
      "for Computational Linguistics, Jul. 2017, pp. 1601–1611.\n",
      "[Online]. Available: https://aclanthology.org/P17-1147\n",
      "[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\n",
      "“Constructing a multi-hop QA dataset for comprehensive\n",
      "evaluation of reasoning steps,” in Proceedings of the\n",
      "28th International Conference on Computational Linguistics .Barcelona, Spain (Online): International Committee on Com-\n",
      "putational Linguistics, Dec. 2020, pp. 6609–6625. [Online].\n",
      "Available: https://aclanthology.org/2020.coling-main.580\n",
      "[18] Z. Yang et al. , “HotpotQA: A dataset for diverse, explainable\n",
      "multi-hop question answering,” in Proceedings of the 2018\n",
      "Conference on Empirical Methods in Natural Language\n",
      "Processing . Brussels, Belgium: Association for Computa-\n",
      "tional Linguistics, Oct.-Nov. 2018, pp. 2369–2380. [Online].\n",
      "Available: https://aclanthology.org/D18-1259\n",
      "[19] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabhar-\n",
      "wal, “Interleaving retrieval with chain-of-thought reasoning\n",
      "for knowledge-intensive multi-step questions,” arXiv preprint\n",
      "arXiv:2212.10509 , 2022.\n",
      "[20] Z. Jiang et al. , “Active retrieval augmented generation,” arXiv\n",
      "preprint arXiv:2305.06983 , 2023.\n",
      "[21] L. Ouyang et al. , “Training language models to follow instruc-\n",
      "tions with human feedback,” Advances in Neural Information\n",
      "Processing Systems , vol. 35, pp. 27 730–27 744, 2022.\n",
      "[22] J. Wei et al. , “Chain of thought prompting elicits reasoning\n",
      "in large language models,” arXiv preprint arXiv:2201.11903 ,\n",
      "2022.\n",
      "[23] G. Izacard et al. , “Few-shot learning with retrieval augmented\n",
      "language models,” arXiv preprint arXiv:2208.03299 , 2022.\n",
      "==================================================\n",
      "\n",
      "Document 8:\n",
      "Lift Yourself Up: Retrieval-augmented Text\n",
      "Generation with Self-Memory\n",
      "Xin Cheng1Di Luo2Xiuying Chen3Lemao Liu4Dongyan Zhao1Rui Yan2\n",
      "1Peking University2Remin University of China\n",
      "3KAUST4Tencent AI Lab\n",
      "chengxin1998@stu.pku.edu.cn\n",
      "Abstract\n",
      "With direct access to human-written reference as memory, retrieval-augmented\n",
      "generation has achieved much progress in a wide range of text generation tasks.\n",
      "Since better memory would typically prompt better generation (we deﬁne this as\n",
      "primal problem ). The traditional approach for memory retrieval involves selecting\n",
      "memory that exhibits the highest similarity to the input. However, this method\n",
      "is constrained by the quality of the ﬁxed corpus from which memory is retrieved.\n",
      "In this paper, by exploring the duality of the primal problem: better generation\n",
      "also prompts better memory, we propose a novel framework, Selfmem , which\n",
      "addresses this limitation by iteratively employing a retrieval-augmented generator\n",
      "to create an unbounded memory pool and using a memory selector to choose one\n",
      "output as memory for the subsequent generation round. This enables the model\n",
      "to leverage its own output, referred to as self-memory, for improved generation.\n",
      "We evaluate the effectiveness of Selfmem on three distinct text generation tasks:\n",
      "neural machine translation, abstractive text summarization, and dialogue generation,\n",
      "under two generation paradigms: ﬁne-tuned small model and few-shot LLM. Our\n",
      "approach achieves state-of-the-art results in four directions in JRC-Acquis ,XSum\n",
      "(50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1), demonstrating the potential\n",
      "of self-memory in enhancing retrieval-augmented generation models. Furthermore,\n",
      "we conduct thorough analyses of each component in the Selfmem framework to\n",
      "identify bottlenecks and provide insights for future research.\n",
      "1 Introduction\n",
      "In recent years, retrieval-augmented text generation has garnered increasing interest across vari-\n",
      "ous ﬁelds such as neural machine translation [ 1,22,14], dialogue response generation [ 71,6,42],\n",
      "language modeling [ 21,31,90], and others. This novel generation paradigm initially equips the gener-\n",
      "ation model, either a ﬁne-tuned small model or a large language model (LLM) with in-context learning\n",
      "capability [ 5], with access to an external database (typically the training corpus) via information\n",
      "retrieval techniques [63, 33] and then generates text based on the retrieved memory.\n",
      "In this paradigm, the guiding principle for memory retrieval is to ﬁnd the memory that exhibits the\n",
      "highest similarity to the current input [ 31,84,45]. This aligns with the human intuition that a more\n",
      "similar demonstration sample typically offers more hints. As demonstrated in Figure 1, for a tunable\n",
      "retrieval-augmented translation model, the memory similarity alone exhibits a strong correlation with\n",
      "the ﬁnal translation quality, regardless of other factors that may inﬂuence translation quality (e.g.,\n",
      "polysemy, morphology, and coreference). We deﬁne this as the primal problem :better memory\n",
      "prompts better generation . Consequently, numerous studies have focused on how to retrieve better\n",
      "Preprint. Under review.arXiv:2305.02437v2  [cs.CL]  17 May 2023memory, ranging from sparse retrieval to dense retrieval [ 10,58], from a ﬁxed retriever to a learnable\n",
      "retriever [ 38,8], and from sentence-level memory to more ﬁne-grained token-level memory [ 31,30].\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "Memory Similarity0.20.30.40.50.60.70.80.9Hypothesis BLEU\n",
      "Figure 1: Relation between memory and hy-\n",
      "pothesis on JRC-Acquis En!De dataset.\n",
      "The hypothesis is generated by a retrieval-\n",
      "augmented translator whose memory is re-\n",
      "trieved from the training set. The X-axis\n",
      "represents the similarity between memory\n",
      "and the reference.However, a fundamental limitation exists in all previous\n",
      "works: the memory is retrieved from a ﬁxed corpus and\n",
      "is constrained by the corpus’s quality. Due to the ﬁnite\n",
      "retrieval space, bounded memory signiﬁcantly limits\n",
      "the potential of memory-augmented generation models.\n",
      "In this paper, by exploring the primal problem ’s dual-\n",
      "ity,better generation also prompts better memory ,\n",
      "we propose a novel framework called Selfmem . This\n",
      "framework iteratively employs a retrieval-augmented\n",
      "generator to create an unbounded memory pool and\n",
      "uses a memory selector to choose one output as mem-\n",
      "ory for the subsequent generation round. By combining\n",
      "theprimal anddual problem , a retrieval-augmented\n",
      "generation model can elevate itself using its own out-\n",
      "put, referred to as self-memory. The key insight behind\n",
      "Selfmem is that the text more closely resembling the\n",
      "data distribution during inference is not the training\n",
      "data [76], but the model’s own output.\n",
      "Selfmem consists of two complementary components:\n",
      "a retrieval-augmented generator and a memory selector. The generator operates under two distinct\n",
      "paradigms: ﬁne-tuning a small model or few-shot prompting a LLM. For the former, we supervise the\n",
      "generator with labeled data and retrieved memory, while for the latter, we employ a ﬁxed black-box\n",
      "LLM exclusively for inference alongside retrieved in-context learning samples. We then use the\n",
      "generator’s output to train a memory selector based on a speciﬁc performance metric. By simply\n",
      "replacing the retrieved memory with unbounded generated memory, we achieve higher-quality\n",
      "generation output ( primal problem ), which subsequently serves as memory for the next round after\n",
      "being reﬁned by the memory selector ( dual problem ).\n",
      "To evaluate the efﬁcacy of the Selfmem , we carry out comprehensive experiments in three distinct text\n",
      "generation tasks: neural machine translation, abstractive text summarization, and dialogue generation.\n",
      "We witness substantial enhancements over robust baselines, attaining state-of-the-art outcomes in\n",
      "JRC-Acquis (four directions), XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1). To gain\n",
      "deeper insights into the Selfmem , we meticulously investigate each crucial component and pinpoint\n",
      "the existing system bottleneck to guide future research endeavors.\n",
      "2 Related Work\n",
      "2.1 Retrieval-augmented Text Generation\n",
      "Since the world is not a snapshot once the training corpus is collected, we can never expect an\n",
      "ever-large model to capture everything in its parameters, even for LLMs like GPT-4 [ 57]. Therefore,\n",
      "it is crucial to equip these models with an external memory bank to store additional knowledge or\n",
      "useful demonstration examples for solving various NLP tasks[38, 68, 83].\n",
      "In the translation domain, retrieval techniques have long been employed by the localization industry\n",
      "to enhance human translators’ productivity and consistency even before the advent of machine\n",
      "translation [ 82]. Early works on machine translation primarily focused on utilizing memory for\n",
      "statistical machine translation (SMT) systems [ 70,46]. For neural machine translation (NMT), [ 22]\n",
      "were the ﬁrst to use search engines to retrieve memory from the training set and incorporate it with\n",
      "an external memory network. Subsequent research explored various aspects of retrieval-augmented\n",
      "NMT, such as memory encoding methods [ 80,81,25], joint training of retrievers and generators\n",
      "with monolingual data [ 8], memory granularity [ 30], and memory diversity [ 14]. For few-shot LLM\n",
      "generation, strategies for in-context example selection have been proposed to improve translation\n",
      "quality [ 1]. Furthermore, in-context machine translation has been shown to be effective for on-the-ﬂy\n",
      "adaptation [ 69]. For dialogue response generation tasks, employing exemplar/template retrieval as\n",
      "an intermediate step has proven advantageous for generating informative responses [ 78,79,6,7].\n",
      "In-context learning example retrieval also aids in controllable dialogue [ 42]. Other applications\n",
      "2include abstractive summarization [ 76,11,59], code generation [ 24], paraphrase generation [ 29,73],\n",
      "language modeling [ 31,90], counterfactual data generation [ 18], and question answering [ 32,28,27].\n",
      "2.2 Neural Text Reranking\n",
      "By alleviating the discrepancy between training and inference (i.e., exposure bias) and directly\n",
      "optimizing desired metrics, two-stage reranking methods have facilitated signiﬁcant progress in\n",
      "various text generation tasks. In machine translation, pioneering works by [ 66] and [ 56] introduced\n",
      "and popularized discriminative reranking for SMT. In the context of NMT, research has focused on\n",
      "two primary reranking approaches: generative reranking [ 51,26,77] and discriminative reranking [ 36,\n",
      "64,17]. For syntactic parsing, [ 15] were the ﬁrst to employ a two-stage reranking method to select\n",
      "outputs from a base parser, while [ 12] introduced a maximum entropy reranker. In text summarization,\n",
      "RefSum [ 48] proposed a second-stage summarization framework to address train-test distribution\n",
      "mismatches. SimCLS [ 49] used pairwise Learning To Rank (LTR) to select candidates with the\n",
      "highest matching scores. SummaReranker [ 62] adopted a multi-task mixture-of-experts framework\n",
      "to leverage different metrics capturing various aspects of generated candidates. BRIO [ 50] reused\n",
      "the base model for a second round of ﬁne-tuning with both cross-entropy loss and a candidate-level\n",
      "ranking loss. JGR [67] employed an alternate training paradigm to train the generator and reranker.\n",
      "A key limitation of these reranking methods is that they represent a one-way process, wherein the\n",
      "selected candidates become the system’s ﬁnal output. In contrast, our framework innovatively utilizes\n",
      "the chosen candidates as memory for the subsequent generation round of a retrieval-augmented\n",
      "generator, which can produce better candidates with enhanced memory.\n",
      "3 Methods\n",
      "In this section, we begin with a motivating experiment on generation as memory (§ 3.1). Then, we\n",
      "introduce Selfmem , a framework comprising a retrieval-augmented generator (§ 3.2) and a memory\n",
      "selector (§ 3.3). The complete framework and algorithm are illustrated in Figure 2 and Algorithm 1.\n",
      "3.1 Generation as Memory\n",
      "The primary motivation behind our framework stems from the observation that the memory, which\n",
      "is more similar in distribution to the data during inference, is not the training data (38.89 BLEU,\n",
      "as shown in Table 1). Instead, it is the model’s own output (58.58 BLEU) within the unbounded\n",
      "generation space. One interesting exploration involves directly utilizing the generated output as\n",
      "memory in relation to the primal problem : better memory prompts better generation.\n",
      "Table 1: Experiments on a\n",
      "ﬁxed retrieval-augmented trans-\n",
      "lator with different memory.\n",
      "Memory Hypothesis\n",
      "Retrieval 38.89 58.58\n",
      "Beam 58.58 58.43\n",
      "Reference 100 90.43\n",
      "Random 1.14 49.08We conduct experiments on the JRC-Acquis En!De dataset. The\n",
      "ﬁrst row in Table 1 represents conventional retrieval-augmented\n",
      "training with retrieved memory and achieves a 58.58 BLEU score.\n",
      "However, directly incorporating beam output of this trained model\n",
      "as memory (Beam) back into the generation model does not yield\n",
      "any improvements (row 2), despite its higher similarity to the\n",
      "reference compared to the retrieved ones. We hypothesize two\n",
      "potential reasons for this: (1) the retrieval-augmented generator\n",
      "may not generalize effectively in this context due to the memory\n",
      "distribution shift (from 38.89 to 58.58), and (2) the beam memory\n",
      "does not offer any information gain in comparison to the retrieved\n",
      "memory, even when it exhibits more overlap with the references.\n",
      "To eliminate the ﬁrst hypothesis, we investigate the best and worst scenarios by using the reference\n",
      "as memory (Reference) and randomly sampled sentences as memory (Random). Table 1 illustrates\n",
      "that a retrieval-augmented generator trained with retrieved memory has already learned to effectively\n",
      "utilize memory information in both oracle and random scenarios, with ﬁxed parameters.\n",
      "To evaluate the second conjecture, we ﬁrst deﬁne the token sets of the reference, retrieved memory,\n",
      "and beam memory as R;M, andB, respectively. The overlap token set, denoted by O, is deﬁned\n",
      "as the tokens that overlap with the references in the beam memory but not in the retrieved memory,\n",
      "which is represented as R\\B\u0000R\\M .Ois considered as the additional information provided\n",
      "3Target Distribution Frozen LLM / T rainable LM\n",
      "NLL LossKL Loss\n",
      "YN\n",
      "Y1\n",
      "...\n",
      "Y\n",
      " X\n",
      " Y\n",
      "X\n",
      "candidates\n",
      "source\n",
      "target training\n",
      "memory\n",
      "... ...\n",
      "... ...\n",
      "(a) Retrieval-augmented Generator (b) Memory SelectorRetrievalPredicted Distribution\n",
      "M\n",
      "PrimalDualFigure 2: Overall framework. There are two components in Selfmem , a retrieval-augmented genera-\n",
      "tor (a) and a memory selector (b). For the primal problem, (a) takes source and memory as input to\n",
      "generate candidates for (b). For the dual problem, (b) takes as input source and generated candidates\n",
      "to select memory for (a).\n",
      "by the beam memory. Inspired by the conﬁdence analysis of NMT model [ 53], we compute the set\n",
      "conﬁdence score,  (\u0001), as follows:\n",
      " (\u0001) =1\n",
      "j\u0001jX\n",
      "yi2\u0001p(yijx;y<i) (1)\n",
      "wherep(yijx;y<i)is deﬁned by the generation model.  (\u0001)measures the conﬁdence with which the\n",
      "generation model generates the tokens in the set. The value of  (R)is 0.58, while that of Ois 0.76,\n",
      "indicating that the generator is relatively conﬁdent in generating tokens in O, and therefore does not\n",
      "need to resort to external memory [ 35]. Beam search ranks generated candidates based on p(yjx),\n",
      "where the selected memory falls within the conﬁdence region of the generator and consequently\n",
      "provides no information gain. This observation motivates us to select memory according to metrics\n",
      "other thanp(yjx)in the memory selector (§3.3).\n",
      "3.2 Retrieval-augmented Generator\n",
      "Given a text pair (x;y), wherex=fx1;:::;xjxjgis the source, y=fy1;:::;yjyjgis the target. They\n",
      "could be (document, summary) in summarization, (context, response) in dialogue generation or\n",
      "(source, target) in machine translation. The retrieval-augmented generation would ﬁrst use xto\n",
      "retrieve memory mfrom datastore D. Then the generator G\u0018(x;m), parameterized by \u0018, would take\n",
      "bothxandmas input to generate the target sentence y. In this paper, following standard practice,\n",
      "we choose the training set as D=f(xi;yi)gjDj\n",
      "i=1. For LLM as G\u0018, we use the standard in-context\n",
      "learning format to give (x;y)as demonstration example. For tunable generator G\u0018, we only keep the\n",
      "target side of top-1 retrieval results as memory and we consider two commonly used architectures:\n",
      "Joint-Encoder [23, 76, 38] and Dual-Encoder [80, 8, 14].\n",
      "Joint-Encoder This architecture is the standard encoder-decoder-based model [ 2,74]. The input is\n",
      "the concatenation of xandm. The encoder would ﬁrst map the input into the hidden states H:\n",
      "H=Encoder (x[SEP]m) (2)\n",
      "And the decoder would incorporate Hby attention mechanism and generate tokens in an auto-\n",
      "regressive manner:\n",
      "hi=Decoder (CrossAttn (H);y<i)PG\u0018(\u0001jx;y<i) =Softmax (hi) (3)\n",
      "Dual-Encoder Instead of treating xandmas a long sequence, this architecture has two encoders,\n",
      "one forxand the other for m. Their outputs are sequentially attended by the decoder with dual cross\n",
      "4attention as in [14]:\n",
      "Hx=SourceEncoder (x)Hm=MemoryEncoder (x) (4)\n",
      "hi=Decoder (CrossAttn (Hx;Hm);y<i) (5)\n",
      "We use Transformer [ 74] as the building block for both architectures and optimize G\u0018with NLL loss:\n",
      "Lnll=\u0000jyjX\n",
      "t=1logPG\u0018(ytjx;m;y<t) (6)\n",
      "3.3 Memory Selector\n",
      "The role of memory selector S\u0012(x;c), parameterized by \u0012, is to select one candidate cfrom the\n",
      "candidate pool Cgenerated by G\u0018based on a speciﬁc metric \u0001(\u0001;\u0001). The chosen candidate cis\n",
      "then utilized as memory mfor the subsequent generation round of G\u0018. As discussed in §3.1, using\n",
      "pG\u0018(yjx)as the metric \u0001(\u0001;\u0001)would result in falling into the conﬁdence region of G\u0018, leading to\n",
      "no information gain. Moreover, a larger value of pG\u0018(yjx)does not necessarily guarantee improved\n",
      "generation quality [ 54]. Consequently, we deﬁne \u0001(\u0001;\u0001)as model-free metrics that are widely\n",
      "employed for assessing generation quality, such as BLEU for Neural Machine Translation (NMT)\n",
      "and ROUGE for Summarization. Our memory selector takes the concatenation of the source xand\n",
      "candidatecias input, and produces a multinomial distribution pS\u0012(\u0001jx)overC.\n",
      "In this paper, we focus on the role of the memory selector, S\u0012(x;c), which is parameterized by \u0012.\n",
      "The objective of this selector is to choose a single candidate cfrom the candidate pool C, generated\n",
      "byG\u0018, based on a speciﬁc metric, \u0001(\u0001;\u0001).\n",
      "pS\u0012(cijx) =exp(S\u0012(x[SEP]ci))\n",
      "PjCj\n",
      "j=1exp(S\u0012(x[SEP]cj))(7)\n",
      "In accordance with [ 36], the training goal for S\u0012is to minimize the discrepancy between the S\u0012’s\n",
      "predictions and the scores determined by \u0001(\u0001;\u0001). This divergence is quantiﬁed using the Kullback-\n",
      "Leibler (KL) divergence.\n",
      "Lkl=\u0000jCjX\n",
      "i=1pM(ci)logpS\u0012(cijx)wherepM(ci) =exp(\u0001(ci;y)=\u001c",
      ")\n",
      "PjCj\n",
      "j=1exp(\u0001(cj;y)=\u001c",
      ")(8)\n",
      "\u001c",
      "is the temperature to control the smoothness of the distribution. At inference, the output of the S\u0012\n",
      "isarg max\n",
      "ci2CpS\u0012(cijx).\n",
      "3.4 Combine Generator and Selector\n",
      "We deﬁne two generation modes for G\u0018. The ﬁrst mode, referred to as the hypothesis mode , generates\n",
      "a single output for each input, which is utilized for system evaluation. The second mode, known as\n",
      "thecandidate mode , produces N outputs for a given input, and is employed for training S\u0012as well as\n",
      "memory selection. By integrating two modes together, we present the complete framework of our\n",
      "proposed model, Selfmem , as illustrated in Algorithm 1.\n",
      "4 Experimental Setup\n",
      "4.1 Dataset\n",
      "We assess the performance of Selfmem on three generation tasks, utilizing a total of seven datasets.\n",
      "Translation. We evaluate our framework on JRC-Acquis datasets [ 72], a collection of parallel\n",
      "legislative text of European Union Law. It is the benchmark dataset used in translation memory-\n",
      "augmented NMT task [ 22,80,8,14]. We choose 4 translation directions, namely, Spanish $English\n",
      "(Es$En), German$English (De$En).Summarization. We evaluate on 2 summarization datasets:\n",
      "5Algorithm 1 Selfmem Framework\n",
      "Require: a dataset D, a retriever R, a memory selection metric \u0001(\u0001;\u0001), a retrieval-augmented\n",
      "generatorG\u0018, and a memory selector S\u0012\n",
      "1:retrieve memory MinDwithR\n",
      "2:trainG\u0018withDandM(if not LLM)\n",
      "3:useG\u0018to generate candidate pool CwithMin candidate mode\n",
      "4:trainS\u0012onCwith\u0001(\u0001;\u0001)\n",
      "5:while not converged in the validation set do\n",
      "6:S\u0012selects memory from CasM\n",
      "7:G\u0018generates candidate pool CwithMin candidate mode\n",
      "8:end while\n",
      "9:G\u0018generates the ﬁnal hypothesis with Min hypothesis mode\n",
      "1)XSum [55], extreme summarization, a single-document summarization dataset with highly abstrac-\n",
      "tive articles from British Broadcasting Corporation. 2) BigPatent [65], consisting of 1.3 million\n",
      "records of U.S. patent documents along with human-written abstractive summaries. Dialogue. We\n",
      "experiment on DailyDialog [40], which contains multi-turn dialogs on daily life topics and is used\n",
      "by [13, 3, 88]. The detailed statistics for these datasets can be found in the Appendix.\n",
      "4.2 Implementation Details\n",
      "We utilize the BM25 algorithm [ 63] for retrieval purposes. For all tasks, the candidate generation\n",
      "method consists of beam search with a beam width of 50. The number of iterations is determined\n",
      "by the performance on the validation set. For translation , we follow the approach of [ 81,8,14],\n",
      "employing a randomly initialized Transformer basearchitecture as G\u0018for trainable small model and\n",
      "XGLM [ 44] for LLM in-context learning. Evaluation metrics include BLEU, TER, and chrF++\n",
      "obtained from SACRE BLEU [60]. The memory selector S\u0012utilizes an XLM-R base[16] as back-\n",
      "bone, with BLEU serving as \u0001(\u0001;\u0001).For summarization , we initialize G\u0018with BART base[37]\n",
      "forBigPatent and employ BRIO [ 50] for XSum . The evaluation metric comprises ROUGE (R-\n",
      "1/2/L) [ 43].For dialogue generation , BART baseserves as the backbone for G\u0018. Our dialogue system\n",
      "is evaluated using BLEU (B-1/2) and Distinct (D-1/2) scores [ 39]. For both dialogue and summariza-\n",
      "tion tasks, we adhere to the methods of [ 49,20], adopting RoBERTa base[47] as the backbone for S\u0012.\n",
      "The linear combination of B-1/2 is chosen as \u0001(\u0001;\u0001)for Dialogue Generation, while R-1/2/L is used\n",
      "for Summarization, following [ 67]. For further implementation details, please refer to the Appendix.\n",
      "Our code is open sourced1.\n",
      "5 Experimental Results\n",
      "5.1 Machine Translation\n",
      "We select four translation directions and experiment with two generation paradigms: trainable\n",
      "small models and few-shot prompted LLMs. For trainable models, we explore two architectures\n",
      "(joint and dual, as detailed in §3.2). The baselines comprise two types of translation systems: one\n",
      "being the vanilla sequence-to-sequence model [ 2,74] without memory augmentation, and the other\n",
      "consisting of retrieval-augmented translation models focusing on memory encoding [ 22,80], memory\n",
      "construction [ 86], memory retrieval [ 8], and memory diversity [ 14]. Based on the experimental\n",
      "results2shown in Table 2, Selfmem signiﬁcantly enhances the performance of G\u0018across four\n",
      "translation datasets and two different architectures. This is noteworthy, given that the parameters of\n",
      "theG\u0018remain ﬁxed, with the only variable being the input memory. This ﬁnding is consistent with\n",
      "theprimal problem which posits that improved memory typically leads to better generation results.\n",
      "Thedual problem is revealed in Table 3. Self-memory, which essentially represents the model’s\n",
      "own output, exhibits greater similarity with the ground truth and serves as a more effective memory\n",
      "for generating the ﬁnal output. This observation highlights a key distinction between Selfmem and\n",
      "1https://github.com/hannibal046/selfmemory\n",
      "2As higher BLEU scores in this range do not necessarily guarantee a superior translation system [ 9], we also\n",
      "evaluate our system using TER and chrF++. The results can be found in the Appendix.\n",
      "6Table 2: Results of translation task on JRC-Acquis measured by BLEU. Models denoted by the\n",
      "same symbol ( ?andy) have the same parameters and only differ in memory as input. The bolded\n",
      "numbers show the SOTA performance and the underlined numbers show the second-best result. \u0003\n",
      "denotes the system is signiﬁcantly better than baselines with p-value < 0.05 tested by [34].\n",
      "SystemEs!En En!Es De!En En!De\n",
      "Dev Test Dev Test Dev Test Dev Test\n",
      "None Memory\n",
      "RNNsearch [2] 55.02 59.34 50.54 50.48 50.20 49.74 44.94 43.98\n",
      "Transformer [74] 64.08 64.63 62.02 61.80 60.18 60.16 54.65 55.43\n",
      "Retrieval Memory\n",
      "SEG-NMT [22] 60.28 59.34 57.62 57.27 55.63 55.33 49.26 48.80\n",
      "NMT-pieces [86] 63.97 64.30 61.50 61.56 60.10 60.26 55.54 55.14\n",
      "G-TFM [80] 66.37 66.21 62.50 62.76 61.85 61.72 57.43 56.88\n",
      "MonoNMT [8] 67.73 67.42 64.18 63.86 64.48 64.62 58.77 58.42\n",
      "CMM [14] 67.48 67.76 63.84 64.04 64.22 64.33 58.94 58.69\n",
      "Transformer dual? 66.87 67.12 63.14 63.54 64.09 63.36 58.69 58.06\n",
      "Transformer uniy 67.74 67.32 63.93 64.12 64.50 64.40 58.16 58.58\n",
      "Self-Memory\n",
      "Transformer dual?68.63\u000369.20\u000364.12\u000364.67\u000365.06\u000364.98\u000359.26\u000359.49\u0003\n",
      "Transformer uniy 68.26\u000368.80\u000366.07\u000365.94\u000365.32\u000365.65\u000359.88\u000360.11\u0003\n",
      "Table 3: Comparison between retrieval memory and self-memory. The quality of memory and\n",
      "hypothesis is measured by the n-gram overlap with reference (BLEU). All experiments are conducted\n",
      "with Transformer jointonJRC-Acquis .\n",
      "Retrieval Self\n",
      "memory hypothesis memory hypothesis\n",
      "En-De! 38.89 58.58 57.92 60.11\n",
      " 42.56 64.40 64.32 65.65\n",
      "En-Es! 40.67 64.12 63.57 65.94\n",
      " 43.05 67.32 67.78 68.80\n",
      "previous reranking works [ 36,62]. Reranking aims to select candidates of higher quality than the\n",
      "beam output, whereas in Selfmem , the chosen candidates serve as memory for the retrieval-augmented\n",
      "generator and do not necessarily need to surpass the quality of the beam hypotheses.\n",
      "Table 4: Evaluation results of in-context learning with self-memory.\n",
      "XGLM-1.7B XGLM-4.5B XGLM-7.5B\n",
      "Random kNN Self Random kNN Self Random kNN Self\n",
      "En-De! 11.51 37.87 40.94 17.51 37.60 38.25 18.48 47.82 48.32\n",
      " 27.42 51.00 51.88 30.62 48.12 48.36 33.03 55.65 55.12\n",
      "En-Es! 23.87 46.20 48.56 31.83 48.37 49.17 29.97 53.86 54.32\n",
      " 25.29 51.55 53.13 32.16 48.55 49.22 35.22 57.25 57.56\n",
      "In Table 4, we present the results of LLM with self-memory. We employ XGLM [ 44] as our backbone\n",
      "generator, with three different sizes ranging from 1.7B to 7.5B. We utilize the recommended prompt\n",
      "as described in [ 44]. We select three in-context learning examples and report the average scores\n",
      "from three separate runs, taking into account the sensitivity of example selection in ICL [ 45]. From\n",
      "the table, we ﬁrst observe a general trend where few-shot translation performance improves as the\n",
      "size of the model increases. Furthermore, we ﬁnd that more similar translation demonstrations\n",
      "signiﬁcantly enhance performance across all model sizes (from random, kNN to Self). This suggests\n",
      "that demonstration examples in in-context learning not only act as triggers for model ability but also\n",
      "7adhere to the primal problem , where better demonstration example leads to better generation. Also,\n",
      "by comparing the results in Table 2 and Table 4, we can conclude that the cross-lingual LLM with\n",
      "designed examples still falls short of the supervised baselines in this task.\n",
      "5.2 Summarization\n",
      "In this paper, we compare the performance of our trainable model with those of REINA [ 76],\n",
      "PEGASUS [ 85], and BART [ 37]. The results are presented in Table5. Initially, it can be observed\n",
      "that memory has varying impacts on different datasets. The enhancement brought by memory in the\n",
      "BigPatent dataset is signiﬁcantly larger than that in the XSum dataset. This can be attributed to the\n",
      "inherent characteristics of the BigPatent dataset, which consists of ofﬁcial patent documents that\n",
      "exhibit considerable similarity. Consequently, this greatly improves the summarization quality in\n",
      "accordance with the primal problem . Furthermore, we discovered that self-memory substantially\n",
      "enhances the performance of both BRIO (+1.2 R1) and BART (+18.5 R1), achieving state-of-the-art\n",
      "results on both datasets. We selected these baselines for a fair comparison, as they share the same\n",
      "base generator. Due to space constraints, additional comparisons and the conﬁdence region of the\n",
      "SOTA model can be found in the Appendix.\n",
      "Table 5: Results of summarization task on XSum andBigPatent measured by ROUGE.\n",
      "System Memory R-1 R-2 R-L\n",
      "XSum\n",
      "PEGASUS None 47.2 24.6 39.3\n",
      "BRIO None 49.1 25.6 40.4\n",
      "REINA (PG) Retrieval 48.2 26.0 40.2\n",
      "REINA (B) Retrieval 43.2 21.0 35.5\n",
      "REINA (L) Retrieval 46.5 24.1 38.6\n",
      "BRIO dual? Retrieval 48.6 26.1 40.6\n",
      "BRIO jointy Retrieval 49.5 26.5 41.2\n",
      "BRIO dual? Self 49.2 26.2 40.8\n",
      "BRIO jointy Self 50.3 26.7 41.6System Memory R-1 R-2 R-L\n",
      "BigPatent\n",
      "PEGASUS None 53.6 33.2 43.2\n",
      "BART None 44.4 21.3 31.0\n",
      "REINA (B) Retrieval 59.5 42.6 50.6\n",
      "REINA (L) Retrieval 60.7 43.3 51.3\n",
      "REINA (PG) Retrieval 44.6 21.5 33.3\n",
      "BART dual? Retrieval 57.4 43.3 49.7\n",
      "BART jointy Retrieval 59.6 43.4 51.0\n",
      "BART dual? Self 61.2 44.6 52.3\n",
      "BART jointy Self 62.9 48.1 59.6\n",
      "5.3 Dialogue Generation\n",
      "As demonstrated in Table 6, the self-memory signiﬁcantly enhances the performance of the retrieval-\n",
      "augmented generator for dialogue generation tasks. By optimizing memory using BLEU as \u0001(\u0001;\u0001),\n",
      "the self-memory improves the B-1,2 score over retrieved memory by 3.08 B-1 and 0.6 B-2 on\n",
      "BART joint. Intriguingly, although Selfmem surpasses the baselines in terms of B-1/2, it falls behind in\n",
      "D-1 and D-2, which can be attributed to the trade-off between BLEU score and Distinct score when\n",
      "evaluating a dialogue system [ 89]. To address this issue, we opt for D-1,2 as \u0001(\u0001;\u0001)when optimizing\n",
      "S\u0012, denoted as BART jointy(D). The results in Table 6 highlight the remarkable ﬂexibility of Selfmem\n",
      "by directly optimizing memory to achieve the desired attributes for diverse and informative dialogue.\n",
      "1 2 3 4\n",
      "Iteration60626466Hypothesis BLEU\n",
      "Transformer-joint\n",
      "rank1\n",
      "rank2\n",
      "rank3rank4\n",
      "rank5\n",
      "rank6\n",
      "1 2 3 4\n",
      "Iteration60626466\n",
      "Transformer-dual\n",
      "rank1\n",
      "rank2\n",
      "rank3rank4\n",
      "rank5\n",
      "rank6\n",
      "(a) Hypothesis\n",
      "1 2 3 4 5\n",
      "Iteration455055606570Candidates BLEU (b) Candidates\n",
      "Figure 3: (a) shows generation quality in the iteration process with different S\u0012in both trainable\n",
      "generator architectures. (b) shows candidates quality in the iteration process with an oracle S\u0012.\n",
      "8Table 6: Results of dialogue generation task on DailyDialog measured by B-1/2 and D-1/2.\n",
      "BART joint(D) denotes the metric \u0001(\u0001;\u0001)forS\u0012is the average of D-1 and D-2.\n",
      "System Memory B-1 B-2 D-1 D-2\n",
      "NCM [75] None 33.60 26.80 3.00 12.80\n",
      "iV AE [19] None 30.90 24.90 2.90 25.00\n",
      "PLATO-2 [4] None 34.80 25.12 3.54 25.11\n",
      "DialoFlow [41] None 36.17 27.67 4.56 27.12\n",
      "BART None 20.72 11.36 3.92 19.44\n",
      "BART dual? Retrieval 29.50 21.89 4.74 26.01\n",
      "BART jointy Retrieval 36.72 31.55 6.13 35.65\n",
      "BART dual? Self 33.43 22.85 4.66 26.16\n",
      "BART jointy Self 39.80 32.15 5.84 32.16\n",
      "BART jointy(D) Self 36.92 32.09 9.12 37.05\n",
      "6 Further Analysis\n",
      "To gain a deeper insight into Selfmem , we ﬁrst examine the impact of each key component, namely G\u0018\n",
      "andS\u0012. Subsequently, we perform a detailed token-level analysis of the generated output concerning\n",
      "their frequency in the training set. Experiments are conducted on the JRC-Acquis En!De dataset.\n",
      "TuningS\u0012We explored various S\u0012by direct selection from the candidate pool based on gold\n",
      "rankings. As shown in Figure 3a, both architectures with enhanced S\u0012signiﬁcantly outperform\n",
      "the current SOTA performance (60.11 BLEU). Moreover, we assessed the candidate pool quality\n",
      "during this iterative process using an oracle S\u0012, as displayed in Figure 3b. A clear pattern emerges\n",
      "in this boxplot, revealing improvements in the oracle ,quartile ,average , and minimum scores of\n",
      "the candidate pool. These two experiments jointly clarify the Selfmem ’s underlying intuition: a\n",
      "retrieval-augmented generator proﬁts from superior memory, which can be chosen from its own\n",
      "unbounded output, and subsequently, the generator with improved memory produces a higher-quality\n",
      "candidate pool for the next selection round. Consequently, the model lift itself up.\n",
      "The Least 20% 40% 60% 80% The Most\n",
      "Frequency Rank0.650.700.750.800.850.900.951.00F1Self\n",
      "Retrieval\n",
      "None\n",
      "Figure 4: 1-gram F1 score sorted by\n",
      "training corpus frequency.TuningG\u0018As discussed in §3.1, we demonstrated that a\n",
      "trained retrieval-augmented generator, with ﬁxed parameters,\n",
      "possesses the ability to distinguish between \"good\" and \"bad\"\n",
      "memory. This observation not only justiﬁes our decision to\n",
      "maintain a ﬁxed generator within our framework but also im-\n",
      "plies that the G\u0018is not the current bottleneck of the Selfmem .\n",
      "Frequency Analysis We conduct a comprehensive token-\n",
      "level analysis by computing the 1-gram F1 scores for generated\n",
      "translations and subsequently categorizing the tokens based\n",
      "on their frequency in the training set. The results are depicted\n",
      "in Figure 4. A noticeable pattern emerges, suggesting that the\n",
      "more frequently a model encounters a token during training,\n",
      "the higher the accuracy of the generated output [ 87]. Moreover, our ﬁndings indicate that retrieval-\n",
      "augmented models, particularly those incorporating self-memory augmentation, exhibit superior\n",
      "performance in handling long-tail inputs which are challenges for parametric models [61, 52].\n",
      "7 Conclusion\n",
      "For the ﬁrst time, we investigate the fundamental limitation of bounded memory in the current\n",
      "retrieval-augmented literature. We combine the primal anddual problems together and propose\n",
      "Selfmem , a general framework for retrieval-augmented text generation by uplifting generation model\n",
      "with its own output. We conduct comprehensive experiments across various text generation tasks\n",
      "and different generation paradigms, including trainable small model and few-shot prompted LLM.\n",
      "We surpass strong baselines and improve the state-of-the-art performance in serval datasets. We also\n",
      "9meticulously investigate each crucial component and pinpoint the existing system bottleneck to guide\n",
      "future research endeavors.\n",
      "References\n",
      "[1]Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad.\n",
      "In-context examples selection for machine translation. CoRR , abs/2212.02437, 2022.\n",
      "[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "learning to align and translate. In Yoshua Bengio and Yann LeCun, editors, 3rd International\n",
      "Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\n",
      "Conference Track Proceedings , 2015.\n",
      "[3]Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. PLATO: pre-trained dialogue\n",
      "generation model with discrete latent variable. In Dan Jurafsky, Joyce Chai, Natalie Schluter,\n",
      "and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for\n",
      "Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages 85–96. Association for\n",
      "Computational Linguistics, 2020.\n",
      "[4]Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin\n",
      "Liu, and Xinchao Xu. PLATO-2: Towards building an open-domain chatbot via curriculum\n",
      "learning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 ,\n",
      "pages 2513–2525, Online, August 2021. Association for Computational Linguistics.\n",
      "[5]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\n",
      "Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\n",
      "Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\n",
      "Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\n",
      "Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\n",
      "Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo\n",
      "Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,\n",
      "editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural\n",
      "Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\n",
      "[6]Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, and Shuming Shi.\n",
      "Skeleton-to-response: Dialogue generation guided by retrieval memory. In Jill Burstein, Christy\n",
      "Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American\n",
      "Chapter of the Association for Computational Linguistics: Human Language Technologies,\n",
      "NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) ,\n",
      "pages 1219–1228. Association for Computational Linguistics, 2019.\n",
      "[7]Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, and Shuming Shi. Retrieval-guided\n",
      "dialogue response generation via a matching-to-generation framework. In Kentaro Inui, Jing\n",
      "Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical\n",
      "Methods in Natural Language Processing and the 9th International Joint Conference on Natural\n",
      "Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 , pages\n",
      "1866–1875. Association for Computational Linguistics, 2019.\n",
      "[8]Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation\n",
      "with monolingual translation memory. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto\n",
      "Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational\n",
      "Linguistics and the 11th International Joint Conference on Natural Language Processing,\n",
      "ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pages 7307–\n",
      "7318. Association for Computational Linguistics, 2021.\n",
      "[9]Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in\n",
      "machine translation research. In 11th Conference of the European Chapter of the Association\n",
      "for Computational Linguistics , pages 249–256, Trento, Italy, April 2006. Association for\n",
      "Computational Linguistics.\n",
      "10[10] Qian Cao and Deyi Xiong. Encoding gated translation memory into neural machine translation.\n",
      "In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of\n",
      "the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium,\n",
      "October 31 - November 4, 2018 , pages 3042–3047. Association for Computational Linguistics,\n",
      "2018.\n",
      "[11] Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. Retrieve, rerank and rewrite: Soft template\n",
      "based neural summarization. In Proceedings of the 56th Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers) , pages 152–161, Melbourne, Australia,\n",
      "July 2018. Association for Computational Linguistics.\n",
      "[12] Eugene Charniak and Mark Johnson. Coarse-to-ﬁne n-best parsing and maxent discriminative\n",
      "reranking. In Kevin Knight, Hwee Tou Ng, and Kemal Oﬂazer, editors, ACL 2005, 43rd Annual\n",
      "Meeting of the Association for Computational Linguistics, Proceedings of the Conference,\n",
      "25-30 June 2005, University of Michigan, USA , pages 173–180. The Association for Computer\n",
      "Linguistics, 2005.\n",
      "[13] Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu,\n",
      "Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. Dialogved: A pre-trained\n",
      "latent variable encoder-decoder model for dialog response generation. In Smaranda Muresan,\n",
      "Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of\n",
      "the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,\n",
      "Ireland, May 22-27, 2022 , pages 4852–4864. Association for Computational Linguistics, 2022.\n",
      "[14] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation\n",
      "with contrastive translation memories. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang,\n",
      "editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language\n",
      "Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages\n",
      "3591–3601. Association for Computational Linguistics, 2022.\n",
      "[15] Michael Collins and Terry Koo. Discriminative reranking for natural language parsing. Comput.\n",
      "Linguistics , 31(1):25–70, 2005.\n",
      "[16] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,\n",
      "Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsu-\n",
      "pervised cross-lingual representation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie\n",
      "Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages 8440–8451.\n",
      "Association for Computational Linguistics, 2020.\n",
      "[17] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc’Aurelio Ranzato. Resid-\n",
      "ual energy-based models for text generation. In 8th International Conference on Learning\n",
      "Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020.\n",
      "[18] Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer. CORE: A\n",
      "retrieve-then-edit framework for counterfactual data generation. In Yoav Goldberg, Zornitsa\n",
      "Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics:\n",
      "EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages 2964–2984.\n",
      "Association for Computational Linguistics, 2022.\n",
      "[19] Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen. Implicit deep latent\n",
      "variable models for text generation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun\n",
      "Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\n",
      "guage Processing and the 9th International Joint Conference on Natural Language Processing,\n",
      "EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 , pages 3944–3954. Association\n",
      "for Computational Linguistics, 2019.\n",
      "[20] Jiazhan Feng, Chongyang Tao, Zhen Li, Chang Liu, Tao Shen, and Dongyan Zhao. Reciprocal\n",
      "learning of knowledge retriever and response ranker for knowledge-grounded conversations. In\n",
      "Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun\n",
      "Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio,\n",
      "Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus,\n",
      "11Francis Bond, and Seung-Hoon Na, editors, Proceedings of the 29th International Conference\n",
      "on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17,\n",
      "2022 , pages 389–399. International Committee on Computational Linguistics, 2022.\n",
      "[21] Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language modeﬂs with\n",
      "a continuous cache. In 5th International Conference on Learning Representations, ICLR 2017,\n",
      "Toulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 2017.\n",
      "[22] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O. K. Li. Search engine guided neural\n",
      "machine translation. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\n",
      "the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative\n",
      "Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\n",
      "Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\n",
      "2018 , pages 5133–5140. AAAI Press, 2018.\n",
      "[23] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval\n",
      "augmented language model pre-training. In Proceedings of the 37th International Conference\n",
      "on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings\n",
      "of Machine Learning Research , pages 3929–3938. PMLR, 2020.\n",
      "[24] Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, and Percy Liang. A retrieve-and-edit\n",
      "framework for predicting structured outputs. In Samy Bengio, Hanna M. Wallach, Hugo\n",
      "Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances\n",
      "in Neural Information Processing Systems 31: Annual Conference on Neural Information\n",
      "Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages\n",
      "10073–10083, 2018.\n",
      "[25] Qiuxiang He, Guoping Huang, Qu Cui, Li Li, and Lemao Liu. Fast and accurate neural machine\n",
      "translation with translation memory. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto\n",
      "Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational\n",
      "Linguistics and the 11th International Joint Conference on Natural Language Processing,\n",
      "ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pages 3170–\n",
      "3180. Association for Computational Linguistics, 2021.\n",
      "[26] Kenji Imamura and Eiichiro Sumita. Ensemble and reranking: Using multiple models in\n",
      "the NICT-2 neural machine translation system at WAT2017. In Toshiaki Nakazawa and Isao\n",
      "Goto, editors, Proceedings of the 4th Workshop on Asian Translation, WAT@IJCNLP 2017,\n",
      "Taipei, Taiwan, November 27- December 1, 2017 , pages 127–134. Asian Federation of Natural\n",
      "Language Processing, 2017.\n",
      "[27] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models\n",
      "for open domain question answering. In Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty,\n",
      "editors, Proceedings of the 16th Conference of the European Chapter of the Association for\n",
      "Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021 , pages\n",
      "874–880. Association for Computational Linguistics, 2021.\n",
      "[28] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov,\n",
      "Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.\n",
      "In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020\n",
      "Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,\n",
      "November 16-20, 2020 , pages 6769–6781. Association for Computational Linguistics, 2020.\n",
      "[29] Amirhossein Kazemnejad, Mohammadreza Salehi, and Mahdieh Soleymani Baghshah. Para-\n",
      "phrase generation by learning how to edit from samples. In Dan Jurafsky, Joyce Chai, Natalie\n",
      "Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages 6010–6021.\n",
      "Association for Computational Linguistics, 2020.\n",
      "[30] Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Nearest\n",
      "neighbor machine translation. In 9th International Conference on Learning Representations,\n",
      "ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021.\n",
      "12[31] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. General-\n",
      "ization through memorization: Nearest neighbor language models. In International Conference\n",
      "on Learning Representations , 2020.\n",
      "[32] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts,\n",
      "and Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for\n",
      "knowledge-intensive NLP. CoRR , abs/2212.14024, 2022.\n",
      "[33] Omar Khattab and Matei Zaharia. Colbert: Efﬁcient and effective passage search via contextu-\n",
      "alized late interaction over BERT. In Jimmy X. Huang, Yi Chang, Xueqi Cheng, Jaap Kamps,\n",
      "Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu, editors, Proceedings of the 43rd International\n",
      "ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020,\n",
      "Virtual Event, China, July 25-30, 2020 , pages 39–48. ACM, 2020.\n",
      "[34] Philipp Koehn. Statistical signiﬁcance tests for machine translation evaluation. In Proceedings\n",
      "of the 2004 Conference on Empirical Methods in Natural Language Processing , EMNLP 2004,\n",
      "A meeting of SIGDAT, a Special Interest Group of the ACL, held in conjunction with ACL 2004,\n",
      "25-26 July 2004, Barcelona, Spain , pages 388–395. ACL, 2004.\n",
      "[35] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani,\n",
      "Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory\n",
      "networks for natural language processing. In Maria Florina Balcan and Kilian Q. Weinberger,\n",
      "editors, Proceedings of The 33rd International Conference on Machine Learning , volume 48 of\n",
      "Proceedings of Machine Learning Research , pages 1378–1387, New York, New York, USA,\n",
      "20–22 Jun 2016. PMLR.\n",
      "[36] Ann Lee, Michael Auli, and Marc’Aurelio Ranzato. Discriminative reranking for neural\n",
      "machine translation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,\n",
      "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\n",
      "the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021,\n",
      "(Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pages 7250–7264. Association for\n",
      "Computational Linguistics, 2021.\n",
      "[37] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\n",
      "Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pre-\n",
      "training for natural language generation, translation, and comprehension. In Dan Jurafsky, Joyce\n",
      "Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting\n",
      "of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages\n",
      "7871–7880. Association for Computational Linguistics, 2020.\n",
      "[38] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\n",
      "Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and\n",
      "Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Hugo\n",
      "Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,\n",
      "editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural\n",
      "Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\n",
      "[39] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\n",
      "objective function for neural conversation models. In Kevin Knight, Ani Nenkova, and Owen\n",
      "Rambow, editors, NAACL HLT 2016, The 2016 Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics: Human Language Technologies, San Diego Cal-\n",
      "ifornia, USA, June 12-17, 2016 , pages 110–119. The Association for Computational Linguistics,\n",
      "2016.\n",
      "[40] Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. Dailydialog: A\n",
      "manually labelled multi-turn dialogue dataset. In Greg Kondrak and Taro Watanabe, editors,\n",
      "Proceedings of the Eighth International Joint Conference on Natural Language Processing,\n",
      "IJCNLP 2017, Taipei, Taiwan, November 27 - December 1, 2017 - Volume 1: Long Papers ,\n",
      "pages 986–995. Asian Federation of Natural Language Processing, 2017.\n",
      "[41] Zekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng, and Jie Zhou. Conversations are not ﬂat:\n",
      "Modeling the dynamic information ﬂow across dialogue utterances. In Proceedings of the 59th\n",
      "13Annual Meeting of the Association for Computational Linguistics and the 11th International\n",
      "Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 128–138,\n",
      "Online, August 2021. Association for Computational Linguistics.\n",
      "[42] Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and Xifeng Yan. Controllable\n",
      "dialogue simulation with in-context learning. In Findings of the Association for Computational\n",
      "Linguistics: EMNLP 2022 , pages 4330–4347, Abu Dhabi, United Arab Emirates, December\n",
      "2022. Association for Computational Linguistics.\n",
      "[43] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\n",
      "tion Branches Out , pages 74–81, Barcelona, Spain, July 2004. Association for Computational\n",
      "Linguistics.\n",
      "[44] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig,\n",
      "Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer,\n",
      "Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa\n",
      "Kozareva, Mona T. Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual\n",
      "generative language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,\n",
      "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,\n",
      "EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages 9019–9052.\n",
      "Association for Computational Linguistics, 2022.\n",
      "[45] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.\n",
      "What makes good in-context examples for gpt-3? In Eneko Agirre, Marianna Apidianaki, and\n",
      "Ivan Vulic, editors, Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge\n",
      "Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin,\n",
      "Ireland and Online, May 27, 2022 , pages 100–114. Association for Computational Linguistics,\n",
      "2022.\n",
      "[46] Lemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao, Mo Yu, and Conghui Zhu. Locally\n",
      "training the log-linear model for SMT. In Jun’ichi Tsujii, James Henderson, and Marius Pasca,\n",
      "editors, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language\n",
      "Processing and Computational Natural Language Learning, EMNLP-CoNLL 2012, July 12-14,\n",
      "2012, Jeju Island, Korea , pages 402–411. ACL, 2012.\n",
      "[47] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\n",
      "Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT\n",
      "pretraining approach. CoRR , abs/1907.11692, 2019.\n",
      "[48] Yixin Liu, Zi-Yi Dou, and Pengfei Liu. Refsum: Refactoring neural summarization. In Kristina\n",
      "Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard,\n",
      "Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021\n",
      "Conference of the North American Chapter of the Association for Computational Linguistics:\n",
      "Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 , pages 1437–1448.\n",
      "Association for Computational Linguistics, 2021.\n",
      "[49] Yixin Liu and Pengfei Liu. Simcls: A simple framework for contrastive learning of abstrac-\n",
      "tive summarization. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,\n",
      "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\n",
      "the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021,\n",
      "(Volume 2: Short Papers), Virtual Event, August 1-6, 2021 , pages 1065–1072. Association for\n",
      "Computational Linguistics, 2021.\n",
      "[50] Yixin Liu, Pengfei Liu, Dragomir R. Radev, and Graham Neubig. BRIO: bringing order to\n",
      "abstractive summarization. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, edi-\n",
      "tors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , pages 2890–2903.\n",
      "Association for Computational Linguistics, 2022.\n",
      "[51] Yuchen Liu, Long Zhou, Yining Wang, Yang Zhao, Jiajun Zhang, and Chengqing Zong. A\n",
      "comparable study on model averaging, ensembling and reranking in NMT. In Min Zhang,\n",
      "Vincent Ng, Dongyan Zhao, Sujian Li, and Hongying Zan, editors, Natural Language Processing\n",
      "14and Chinese Computing - 7th CCF International Conference, NLPCC 2018, Hohhot, China,\n",
      "August 26-30, 2018, Proceedings, Part II , volume 11109 of Lecture Notes in Computer Science ,\n",
      "pages 299–308. Springer, 2018.\n",
      "[52] Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg,\n",
      "Alan Blair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented classiﬁcation for\n",
      "long-tail visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision\n",
      "and Pattern Recognition , pages 6959–6969, 2022.\n",
      "[53] Yu Lu, Jiali Zeng, Jiajun Zhang, Shuangzhi Wu, and Mu Li. Learning conﬁdence for transformer-\n",
      "based neural machine translation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio,\n",
      "editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguis-\n",
      "tics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , pages 2353–2364.\n",
      "Association for Computational Linguistics, 2022.\n",
      "[54] Clara Meister, Ryan Cotterell, and Tim Vieira. If beam search is the answer, what was the\n",
      "question? In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of\n",
      "the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,\n",
      "Online, November 16-20, 2020 , pages 2173–2185. Association for Computational Linguistics,\n",
      "2020.\n",
      "[55] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the sum-\n",
      "mary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of\n",
      "the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1797–1807,\n",
      "Brussels, Belgium, October-November 2018. Association for Computational Linguistics.\n",
      "[56] Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alexan-\n",
      "der M. Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin,\n",
      "and Dragomir R. Radev. A smorgasbord of features for statistical machine translation. In Julia\n",
      "Hirschberg, Susan T. Dumais, Daniel Marcu, and Salim Roukos, editors, Human Language\n",
      "Technology Conference of the North American Chapter of the Association for Computational\n",
      "Linguistics, HLT-NAACL 2004, Boston, Massachusetts, USA, May 2-7, 2004 , pages 161–168.\n",
      "The Association for Computational Linguistics, 2004.\n",
      "[57] OpenAI. GPT-4 technical report. CoRR , abs/2303.08774, 2023.\n",
      "[58] Md. Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei\n",
      "Chang. Retrieval augmented code generation and summarization. In Marie-Francine Moens,\n",
      "Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for\n",
      "Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic,\n",
      "16-20 November, 2021 , pages 2719–2734. Association for Computational Linguistics, 2021.\n",
      "[59] Hao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan Dhingra, and Dipanjan Das. Text\n",
      "generation with exemplar-based adaptive decoding. In Jill Burstein, Christy Doran, and Thamar\n",
      "Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\n",
      "Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , pages 2555–2565.\n",
      "Association for Computational Linguistics, 2019.\n",
      "[60] Matt Post. A call for clarity in reporting BLEU scores. In Ondrej Bojar, Rajen Chatterjee,\n",
      "Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio\n",
      "Jimeno-Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana L. Neves,\n",
      "Matt Post, Lucia Specia, Marco Turchi, and Karin Verspoor, editors, Proceedings of the Third\n",
      "Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October\n",
      "31 - November 1, 2018 , pages 186–191. Association for Computational Linguistics, 2018.\n",
      "[61] Vikas Raunak, Siddharth Dalmia, Vivek Gupta, and Florian Metze. On long-tailed phenomena\n",
      "in neural machine translation. In Findings of the Association for Computational Linguistics:\n",
      "EMNLP 2020 , pages 3088–3095, Online, November 2020. Association for Computational\n",
      "Linguistics.\n",
      "15[62] Mathieu Ravaut, Shaﬁq R. Joty, and Nancy F. Chen. Summareranker: A multi-task mixture-of-\n",
      "experts re-ranking framework for abstractive summarization. In Smaranda Muresan, Preslav\n",
      "Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Associ-\n",
      "ation for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May\n",
      "22-27, 2022 , pages 4504–4524. Association for Computational Linguistics, 2022.\n",
      "[63] Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and\n",
      "beyond. Found. Trends Inf. Retr. , 3(4):333–389, 2009.\n",
      "[64] Julian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin Kirchhoff. Masked language model\n",
      "scoring. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceed-\n",
      "ings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,\n",
      "Online, July 5-10, 2020 , pages 2699–2712. Association for Computational Linguistics, 2020.\n",
      "[65] Eva Sharma, Chen Li, and Lu Wang. BIGPATENT: A large-scale dataset for abstractive and\n",
      "coherent summarization. In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors,\n",
      "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019,\n",
      "Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , pages 2204–2213. Association\n",
      "for Computational Linguistics, 2019.\n",
      "[66] Libin Shen, Anoop Sarkar, and Franz Josef Och. Discriminative reranking for machine transla-\n",
      "tion. In Julia Hirschberg, Susan T. Dumais, Daniel Marcu, and Salim Roukos, editors, Human\n",
      "Language Technology Conference of the North American Chapter of the Association for Com-\n",
      "putational Linguistics, HLT-NAACL 2004, Boston, Massachusetts, USA, May 2-7, 2004 , pages\n",
      "177–184. The Association for Computational Linguistics, 2004.\n",
      "[67] Weizhou Shen, Yeyun Gong, Yelong Shen, Song Wang, Xiaojun Quan, Nan Duan, and Weizhu\n",
      "Chen. Joint generator-ranker learning for natural language generation. CoRR , abs/2206.13974,\n",
      "2022.\n",
      "[68] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke\n",
      "Zettlemoyer, and Wen-tau Yih. REPLUG: retrieval-augmented black-box language models.\n",
      "CoRR , abs/2301.12652, 2023.\n",
      "[69] Suzanna Sia and Kevin Duh. In-context learning as maintaining coherency: A study of on-the-ﬂy\n",
      "machine translation using large language models. CoRR , abs/2305.03573, 2023.\n",
      "[70] Michel Simard and Pierre Isabelle. Phrase-based machine translation in a computer-assisted\n",
      "translation environment. In Proceedings of Machine Translation Summit XII: Papers, MTSummit\n",
      "2009, Ottawa, Canada, August 26-30, 2009 , 2009.\n",
      "[71] Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and Ming Zhang. Two are better than one: An\n",
      "ensemble of retrieval- and generation-based dialog systems. CoRR , abs/1610.07149, 2016.\n",
      "[72] Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tuﬁs,\n",
      "and Dániel Varga. The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages.\n",
      "In Nicoletta Calzolari, Khalid Choukri, Aldo Gangemi, Bente Maegaard, Joseph Mariani,\n",
      "Jan Odijk, and Daniel Tapias, editors, Proceedings of the Fifth International Conference on\n",
      "Language Resources and Evaluation, LREC 2006, Genoa, Italy, May 22-28, 2006 , pages\n",
      "2142–2147. European Language Resources Association (ELRA), 2006.\n",
      "[73] Yixuan Su, David Vandyke, Simon Baker, Yan Wang, and Nigel Collier. Keep the primary,\n",
      "rewrite the secondary: A two-stage approach for paraphrase generation. In Findings of the\n",
      "Association for Computational Linguistics: ACL-IJCNLP 2021 , pages 560–569, Online, August\n",
      "2021. Association for Computational Linguistics.\n",
      "[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\n",
      "Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von\n",
      "Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman\n",
      "Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference\n",
      "on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA ,\n",
      "pages 5998–6008, 2017.\n",
      "16[75] Oriol Vinyals and Quoc V . Le. A neural conversational model. CoRR , abs/1506.05869, 2015.\n",
      "[76] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu,\n",
      "and Michael Zeng. Training data is more valuable than you think: A simple and effective\n",
      "method by retrieving from training data. In Smaranda Muresan, Preslav Nakov, and Aline\n",
      "Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 ,\n",
      "pages 3170–3179. Association for Computational Linguistics, 2022.\n",
      "[77] Yuguang Wang, Shanbo Cheng, Liyang Jiang, Jiajun Yang, Wei Chen, Muze Li, Lin Shi,\n",
      "Yanfeng Wang, and Hongtao Yang. Sogou neural machine translation systems for WMT17. In\n",
      "Ondrej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry\n",
      "Haddow, Matthias Huck, Antonio Jimeno-Yepes, Philipp Koehn, and Julia Kreutzer, editors,\n",
      "Proceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen,\n",
      "Denmark, September 7-8, 2017 , pages 410–415. Association for Computational Linguistics,\n",
      "2017.\n",
      "[78] Jason Weston, Emily Dinan, and Alexander H. Miller. Retrieve and reﬁne: Improved sequence\n",
      "generation models for dialogue. In Aleksandr Chuklin, Jeff Dalton, Julia Kiseleva, Alexey\n",
      "Borisov, and Mikhail S. Burtsev, editors, Proceedings of the 2nd International Workshop on\n",
      "Search-Oriented Conversational AI, SCAI@EMNLP 2018, Brussels, Belgium, October 31, 2018 ,\n",
      "pages 87–92. Association for Computational Linguistics, 2018.\n",
      "[79] Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhoujun Li, and Ming Zhou. Response gener-\n",
      "ation by context-aware prototype editing. In The Thirty-Third AAAI Conference on Artiﬁcial\n",
      "Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial Intelligence\n",
      "Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intel-\n",
      "ligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pages 7281–7288.\n",
      "AAAI Press, 2019.\n",
      "[80] Mengzhou Xia, Guoping Huang, Lemao Liu, and Shuming Shi. Graph based translation memory\n",
      "for neural machine translation. In The Thirty-Third AAAI Conference on Artiﬁcial Intelligence,\n",
      "AAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial Intelligence Conference, IAAI\n",
      "2019, The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI\n",
      "2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pages 7297–7304. AAAI Press,\n",
      "2019.\n",
      "[81] Jitao Xu, Josep Maria Crego, and Jean Senellart. Boosting neural machine translation with\n",
      "similar translations. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors,\n",
      "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL\n",
      "2020, Online, July 5-10, 2020 , pages 1580–1590. Association for Computational Linguistics,\n",
      "2020.\n",
      "[82] Masaru Yamada. The effect of translation memory databases on productivity. Translation\n",
      "research projects , 3:63–73, 2011.\n",
      "[83] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang,\n",
      "Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language\n",
      "modeling. CoRR , abs/2211.12561, 2022.\n",
      "[84] Dani Yogatama, Cyprien de Masson d’Autume, and Lingpeng Kong. Adaptive semiparametric\n",
      "language models. Trans. Assoc. Comput. Linguistics , 9:362–373, 2021.\n",
      "[85] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. PEGASUS: pre-training with\n",
      "extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International\n",
      "Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of\n",
      "Proceedings of Machine Learning Research , pages 11328–11339. PMLR, 2020.\n",
      "[86] Jingyi Zhang, Masao Utiyama, Eiichiro Sumita, Graham Neubig, and Satoshi Nakamura.\n",
      "Guiding neural machine translation with retrieved translation pieces. In Marilyn A. Walker,\n",
      "Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American\n",
      "Chapter of the Association for Computational Linguistics: Human Language Technologies,\n",
      "NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers) ,\n",
      "pages 1325–1335. Association for Computational Linguistics, 2018.\n",
      "17[87] Tong Zhang, Wei Ye, Baosong Yang, Long Zhang, Xingzhang Ren, Dayiheng Liu, Jinan\n",
      "Sun, Shikun Zhang, Haibo Zhang, and Wen Zhao. Frequency-aware contrastive learning for\n",
      "neural machine translation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence ,\n",
      "volume 36, pages 11712–11720, 2022.\n",
      "[88] Xueliang Zhao, Lemao Liu, Tingchen Fu, Shuming Shi, Dongyan Zhao, and Rui Yan. Towards\n",
      "efﬁcient dialogue pre-training with transferable and interpretable latent structure. CoRR ,\n",
      "abs/2210.12461, 2022.\n",
      "[89] Yinhe Zheng, Zikai Chen, Rongsheng Zhang, Shilei Huang, Xiaoxi Mao, and Minlie Huang.\n",
      "Stylized dialogue response generation using stylized unpaired texts. In Thirty-Fifth AAAI\n",
      "Conference on Artiﬁcial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Appli-\n",
      "cations of Artiﬁcial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances\n",
      "in Artiﬁcial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021 , pages 14558–14567.\n",
      "AAAI Press, 2021.\n",
      "[90] Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation.\n",
      "CoRR , abs/2205.12674, 2022.\n",
      "18\n",
      "==================================================\n",
      "\n",
      "Document 9:\n",
      "Published as a conference paper at ICLR 2021\n",
      "RETRIEVAL -AUGMENTED GENERATION FOR CODE\n",
      "SUMMARIZATION VIA HYBRID GNN\n",
      "Shangqing Liu1\u0003, Yu Chen2y, Xiaofei Xie1y, Jingkai Siow1, Yang Liu1\n",
      "1Nanyang Technology University\n",
      "2Rensselaer Polytechnic Institute\n",
      "ABSTRACT\n",
      "Source code summarization aims to generate natural language summaries from\n",
      "structured code snippets for better understanding code functionalities. However, au-\n",
      "tomatic code summarization is challenging due to the complexity of the source code\n",
      "and the language gap between the source code and natural language summaries.\n",
      "Most previous approaches either rely on retrieval-based (which can take advantage\n",
      "of similar examples seen from the retrieval database, but have low generalization\n",
      "performance) or generation-based methods (which have better generalization per-\n",
      "formance, but cannot take advantage of similar examples). This paper proposes\n",
      "a novel retrieval-augmented mechanism to combine the beneﬁts of both worlds.\n",
      "Furthermore, to mitigate the limitation of Graph Neural Networks (GNNs) on\n",
      "capturing global graph structure information of source code, we propose a novel\n",
      "attention-based dynamic graph to complement the static graph representation of the\n",
      "source code, and design a hybrid message passing GNN for capturing both the local\n",
      "and global structural information. To evaluate the proposed approach, we release\n",
      "a new challenging benchmark, crawled from diversiﬁed large-scale open-source\n",
      "Cprojects (total 95k+ unique functions in the dataset). Our method achieves the\n",
      "state-of-the-art performance, improving existing methods by 1.42,2.44 and1.29\n",
      "in terms of BLEU-4, ROUGE-L and METEOR.\n",
      "1 I NTRODUCTION\n",
      "With software growing in size and complexity, developers tend to spend nearly 90% (Wan et al.,\n",
      "2018) effort on software maintenance ( e.g., version iteration and bug ﬁx) in the completed life\n",
      "cycle of software development. Source code summary, in the form of natural language, plays a\n",
      "critical role in the comprehension and maintenance process and greatly reduces the effort of reading\n",
      "and comprehending programs. However, manually writing code summaries is tedious and time-\n",
      "consuming, and with the acceleration of software iteration, it has become a heavy burden for software\n",
      "developers. Hence, source code summarization which automates concise descriptions of programs is\n",
      "meaningful.\n",
      "Automatic source code summarization is a crucial yet far from the settled problem. The key challenges\n",
      "include: 1) the source code and the natural language summary are heterogeneous, which means\n",
      "they may not share common lexical tokens, synonyms, or language structures and 2) the source\n",
      "code is complex with complicated logic and variable grammatical structure, making it hard to learn\n",
      "the semantics. Conventionally, information retrieval (IR) techniques have been widely used in\n",
      "code summarization (Eddy et al., 2013; Haiduc et al., 2010; Wong et al., 2015; 2013). Since code\n",
      "duplication (Kamiya et al., 2002; Li et al., 2006) is common in “big code” (Allamanis et al., 2018),\n",
      "early works summarize the new programs by retrieving the similar code snippet in the existing code\n",
      "database and use its summary directly. Essentially, the retrieval-based approaches transform the code\n",
      "summarization to the code similarity calculation task, which may achieve promising performance on\n",
      "similar programs, but are limited in generalization, i.e. they have poorer performance on programs\n",
      "that are very different from the code database.\n",
      "\u0003Contact:shangqin001@e.ntu.edu.sg\n",
      "yCorresponding authors\n",
      "1arXiv:2006.05405v5  [cs.LG]  13 May 2021Published as a conference paper at ICLR 2021\n",
      "To improve the generalization performance, recent works focus on generation-based approaches.\n",
      "Some works explore Seq2Seq architectures (Bahdanau et al., 2014; Luong et al., 2015) to generate\n",
      "summaries from the given source code. The Seq2Seq-based approaches (Iyer et al., 2016; Hu et al.,\n",
      "2018a; Alon et al., 2018) usually treat the source code or abstract syntax tree parsed from the source\n",
      "code as a sequence and follow a paradigm of encoder-decoder with the attention mechanism for\n",
      "generating a summary. However, these works only rely on sequential models, which are struggling\n",
      "to capture the rich semantics of source code e.g., control dependencies and data dependencies. In\n",
      "addition, generation-based approaches typically cannot take advantage of similar examples from the\n",
      "retrieval database, as retrieval-based approaches do.\n",
      "To better learn the semantics of the source code, Allamanis et al. (Allamanis et al., 2017) lighted\n",
      "up this ﬁeld by representing programs as graphs. Some follow-up works (Fernandes et al., 2018)\n",
      "attempted to encode more code structures ( e.g., control ﬂow, program dependencies) into code graphs\n",
      "with graph neural networks (GNNs), and achieved the promising performance than the sequence-\n",
      "based approaches. Existing works (Allamanis et al., 2017; Fernandes et al., 2018) usually convert\n",
      "code into graph-structured input during preprocessing, and directly consume it via modern neural\n",
      "networks ( e.g., GNNs) for computing node and graph embeddings. However, most GNN-based\n",
      "encoders only allow message passing among nodes within a k-hop neighborhood (where kis usually\n",
      "a small number such as 4) to avoid over-smoothing (Zhao & Akoglu, 2019; Chen et al., 2020a), thus\n",
      "capture only local neighborhood information and ignore global interactions among nodes. Even\n",
      "there are some works (Li et al., 2019) that try to address this challenging with deep GCNs (i.e., 56\n",
      "layers) (Kipf & Welling, 2016) by the residual connection (He et al., 2016), however, the computation\n",
      "cost cannot endure in the program especially for a large and complex program.\n",
      "To address these challenges, we propose a framework for automatic code summarization, namely\n",
      "Hybrid GNN (HGNN) . Speciﬁcally, from the source code, we ﬁrst construct a code property graph\n",
      "(CPG) based on the abstract syntax tree (AST) with different types of edges ( i.e., Flow To, Reach). In\n",
      "order to combine the beneﬁts of both retrieval-based and generation-based methods, we propose a\n",
      "retrieval-based augmentation mechanism to retrieve the source code that is most similar to the current\n",
      "program from the retrieval database (excluding the current program itself), and add the retrieved code\n",
      "as well as the corresponding summary as auxiliary information for training the model. In order to\n",
      "go beyond local graph neighborhood information, and capture global interactions in the program,\n",
      "we further propose an attention-based dynamic graph by learning global attention scores ( i.e., edge\n",
      "weights) in the augmented static CPG. Then, a hybrid message passing (HMP) is performed on both\n",
      "static and dynamic graphs. We also release a new code summarization benchmark by crawling data\n",
      "from popular and diversiﬁed projects containing 95k+ functions in Cprogramming language and\n",
      "make it public1. We highlight our main contributions as follows:\n",
      "•We propose a general-purpose framework for automatic code summarization, which combines the\n",
      "beneﬁts of both retrieval-based and generation-based methods via a retrieval-based augmentation\n",
      "mechanism.\n",
      "•We innovate a Hybrid GNN by fusing the static graph (based on code property graph) and dynamic\n",
      "graph (via structure-aware global attention mechanism) to mitigate the limitation of the GNN on\n",
      "capturing global graph information.\n",
      "• We release a new challenging Cbenchmark for the task of source code summarization.\n",
      "•We conduct an extensive experiment to evaluate our framework. The proposed approach achieves\n",
      "the state-of-the-art performance and improves existing approaches by 1.42,2.44 and1.29 in terms\n",
      "of BLEU-4, ROUGE-L and METEOR metrics.\n",
      "2 H YBRID GNN F RAMEWORK\n",
      "In this section, we introduce the proposed framework Hybrid GNN (HGNN) , as shown in Figure 1,\n",
      "which mainly includes four components: 1) Retrieval-augmented Static Graph Construction (c.f.,\n",
      "Section 2.2), which incorporates retrieved code-summary pairs to augment the original code for\n",
      "learning. 2) Attention-based Dynamic Graph Construction ( c.f.,Section 2.3), which allows message\n",
      "passing among any pair of nodes via a structure-aware global attention mechanism. 3) HGNN , (c.f.,\n",
      "1https://github.com/shangqing-liu/CCSD-benchmark-for-code-summarization\n",
      "2Published as a conference paper at ICLR 2021\n",
      "RetrievedSummarySourceCode\n",
      "RetrievalCPGAug\n",
      "Static Message Passing\n",
      "AX(     +     …     +      )       Dynamic Message PassingHybrid\n",
      "Decoder\n",
      "GeneratedSummaryHybrid GNNRetrieval-augmentedGraph\n",
      "SummaryEncodingGraphEmbeddingGRU Updates\n",
      "Node EmbeddingsAttention-basedGraphRetrievalDatabaseSourceCPGAggregation\n",
      "Figure 1: The overall architecture of the proposed HGNN framework.\n",
      "Section 2.4), which incorporates information from both static graphs and dynamic graphs with Hybrid\n",
      "Message Passing. 4) Decoder ( c.f.,Section 2.5), which utilizes an attention-based LSTM (Hochreiter\n",
      "& Schmidhuber, 1997) model to generate a summary.\n",
      "2.1 P ROBLEM FORMULATION\n",
      "In this work, we focus on generating natural language summaries for the given functions (Wan et al.,\n",
      "2018; Zhang et al., 2020). A simple example is illustrated in Listing 1, which is crawled from Linux\n",
      "Kernel. Our goal is to generate the best summary “set the time of day clock” based on the given source\n",
      "code. Formally, we deﬁne a dataset as D=f(c;s)jc2C;s2Sg, wherecis the source code of a\n",
      "function in the function set Candsrepresents its targeted summary in the summary set S. The task of\n",
      "code summarization is, given a source code c, to generate the best summary consisting of a sequence\n",
      "of tokens ^s= (t1;t2;:::;t T)that maximizes the conditional likelihood ^s= argmaxsP(sjc).\n",
      "Source Code:\n",
      "int pdc_tod_set(unsigned long sec, unsigned long usec){\n",
      "int retval; unsigned long flags;\n",
      "spin_lock_irqsave(&pdc_lock, flags);\n",
      "retval = mem_pdc_call(PDC_TOD, PDC_TOD_WRITE, sec, usec);\n",
      "spin_unlock_irqrestore(&pdc_lock, flags);\n",
      "return retval;\n",
      "}\n",
      "Ground-Truth: set the time of day clock\n",
      "Listing 1: An example in our dataset crawled from Linux Kernel.\n",
      "2.2 R ETRIEVAL -AUGMENTED STATIC GRAPH\n",
      "2.2.1 G RAPH INITIALIZATION\n",
      "The source code of a function can be represented as Code Property Graph (CPG) (Yamaguchi et al.,\n",
      "2014), which is built on the abstract syntax tree (AST) with different type of edges ( i.e., Flow To,\n",
      "Control, Deﬁne/Use, Reach). Formally, one raw function ccould be represented by a multi-edged\n",
      "graphg(V;E), whereVis the set of AST nodes, (v;u)2Edenotes the edge between the node vand\n",
      "the nodeu. A nodevconsists of two parts: the node sequence and the node type . An illustrative\n",
      "example is shown in Figure 2. For example, in the red node, a%2 == 0 is the node sequence and\n",
      "Condition is the node type. An edge (v;u)has a type, named edge type ,e.g., AST type and Flow\n",
      "To type. For more details about the CPG, please refer to Appendix A.\n",
      "Initialization Representation. Given a CPG, we utilize a BiLSTM to encode its nodes. We represent\n",
      "each token of the node sequence and each edge type using the learned embedding matrix Eseqtoken\n",
      "andEedgetype, respectively. Then nodes and edges of the CPG can be encoded as:\n",
      "h1; :::;hl= BiLSTM( Eseqtoken\n",
      "v;1 ; :::;Eseqtoken\n",
      "v;l )\n",
      "encode _node (v) = [h!\n",
      "l;h \n",
      "1]\n",
      "encode _edge(v; u) =Eedgetype\n",
      "v;u if(v; u)2 Eelse 0(1)\n",
      "3Published as a conference paper at ICLR 2021\n",
      "EntryCFGEntryNodeexample()FunctionDefint a = rand( )IdentiﬁerDeclStatementa % 2 == 0Conditionint b = a ++IdentiﬁerDeclStatementEXITCFGExitNodecall(b)ExpressionStatement ParameterListvoidReturnTypeintIdentiﬁerDeclTypeaIdentiﬁera = rand ( )AssignmentExprrand ( )CallExpressiona % 2 == 0EqualityExpressiona % 2MultiplicativeExpressionaIdentiﬁer2PrimaryExpression0PrimaryExpressionintIdentiﬁerDeclTypebIdentiﬁerb = a ++AssignmentExpra ++IncDecOp++IncDecaIdentiﬁeraSymbolcallCalleebArgumentbIdentiﬁerbSymbolbIdentiﬁerASTFlow  ToControlDeﬁne/UseReachvoid example ( ){   int a = rand( );   if ( a % 2 == 0 )   {        int b = a++;        call(b);   }}A simple code exampleparse\n",
      "Figure 2: An example of Code Property Graph (CPG).\n",
      "wherelis the number of tokens in the node sequence of v. For the sake of simplicity, in the following\n",
      "section, we use hvandev;uto represent the embedding of the node vand the edge (v;u), respectively,\n",
      "i.e.,encode _node (v)andencode _edge(v;u). Given the source code cof a function as well as the\n",
      "CPGg(V;E),Hc2Rm\u0002ddenotes the initial node matrix of the CPG, where mis the total number\n",
      "of nodes in the CPG and dis the dimension of the node embedding.\n",
      "2.2.2 R ETRIEVAL -BASED AUGMENTATION\n",
      "While retrieval-based methods can perform reasonably well on examples that are similar to those\n",
      "examples from a retrieval database, they typically have low generalization performance and might\n",
      "perform poorly on dissimilar examples. On the contrary, generation-based methods usually have better\n",
      "generalization performance, but cannot take advantage of similar examples from the retrieval database.\n",
      "In this work, we propose to combine the beneﬁts of the two worlds, and design a retrieval-augmented\n",
      "generation framework for the task of code summarization.\n",
      "In principle, the goal of code summarization is to learn a mapping from source code cto the natural\n",
      "language summary s=f(c). In other words, for any source code c0, a code summarization system\n",
      "can produce its summary s0=f(c0). Inspired by this observation, conceptually, we can derive\n",
      "the following formulation s=f(c)\u0000f(c0) +s0. This tells us that we can actually compute the\n",
      "semantic difference between candc0, and further obtain the desired summary sforcby considering\n",
      "both the above semantic difference and s0which is the summary for c0. Mathmatically, our goal\n",
      "becomes to learn a function which takes as input (c;c0;s0), and outputs the summary sforc, that\n",
      "is,s=g(c;c0;s0). This motivates us to design our Retrieval-based Augmentation mechanism, as\n",
      "detailed below.\n",
      "Step 1: Retrieving. For each sample (c;s)2D, we retrieve the most similar sample: (c0;s0) =\n",
      "argmax(c0;s0)2D0sim(c;c0), wherec6=c0,D0is a given retrieval database and sim(c;c0)is the text\n",
      "similarity. Following Zhang et al. (2020), we utilize Lucene for retrieval and calculate the similarity\n",
      "scorezbetween the source code cand the retrieved code c0via dynamic programming (Bellman,\n",
      "1966), namely, z= 1\u0000dis(c;c0)\n",
      "max(jcj;jc0j), wheredis(c;c0)is the text edit distance.\n",
      "Step 2: Retrieved Code-based Augmentation. Given the retrieved source code c0for the current\n",
      "samplec, we adopt a fusion strategy to inject retrieved semantics into the current sample. The fusion\n",
      "strategy is based on their initial graph representations ( HcandHc0) with an attention mechanism:\n",
      "•To capture the relevance between candc0, we design an attention function, which computes the\n",
      "attention score matrix Aaugbased on the embeddings of each pair of nodes in CPGs of candc0:\n",
      "Aaug/exp(ReLU( HcWC)ReLU( Hc0WQ)T) (2)\n",
      "where WC;WQ2Rd\u0002dis the weight matrix with d-dim embedding size and ReLU is the\n",
      "rectiﬁed linear unit.\n",
      "•We then multiply the attention matrix Aaugwith the retrieved representation Hc0to inject the\n",
      "retrieved features into Hc:\n",
      "H0\n",
      "c=zAaugHc0 (3)\n",
      "wherez2[0;1]is the similarity score and computed from Step 1, which is introduced to weaken\n",
      "the negative impact of c0on the original training data c,i.e., when the similarity of candc0is low.\n",
      "• Finally, we merge H0\n",
      "cand the original Hcto get the ﬁnal representation of c.\n",
      "comp =Hc+H0\n",
      "c (4)\n",
      "4Published as a conference paper at ICLR 2021\n",
      "where comp is the augmented node representation additionally encoding the retrieved semantics.\n",
      "Step 3: Retrieved Summary-based Augmentation. We further encode the retrieved summary s0\n",
      "with another BiLSTM model. We represent each token t0\n",
      "iofs0using the learned embedding matrix\n",
      "Eseqtoken. Thens0can be encoded as:\n",
      "ht0\n",
      "1;:::;ht0\n",
      "T= BiLSTM( Eseqtoken\n",
      "t0\n",
      "1;:::;Eseqtoken\n",
      "t0\n",
      "T) (5)\n",
      "where ht0\n",
      "iis the hidden state of the BiLSTM model for the token t0\n",
      "iins0andTis the length of\n",
      "s0. We multiply [ht0\n",
      "1;:::;ht0\n",
      "T]with the similarity score z, computed from Step 1, and concatenate\n",
      "it with the graph encoding results (i.e., the GNN encoder outputs) to obtain the input, namely,\n",
      "[GNN output ;zht0\n",
      "1;:::;zht0\n",
      "T], to the decoder.\n",
      "2.3 A TTENTION -BASED DYNAMIC GRAPH\n",
      "Due to that GNN-based encoders usually consider the k-hop neighborhood, the global relation among\n",
      "nodes in the static graph (see Section 2.2.1) may be ignored. In order to better capture the global\n",
      "semantics of source code, based on the static graph, we propose to dynamically construct a graph via\n",
      "structure-aware global attention mechanism, which allows message passing among any pair of nodes.\n",
      "The attention-based dynamic graph can better capture the global dependency among nodes, and thus\n",
      "supplement the static graph.\n",
      "Structure-aware Global Attention. The construction of the dynamic graph is motivated by the\n",
      "structure-aware self-attention mechanism proposed in Zhu et al. (2019). Given the static graph, we\n",
      "compute a corresponding dense adjacency matrix Adynbased on a structure-aware global attention\n",
      "mechanism, and obtain the constructed graph, namely, attention-based dynamic graph .\n",
      "Adyn\n",
      "v;u=ReLU( hT\n",
      "vWQ)(ReLU( hT\n",
      "uWK) + ReLU( eT\n",
      "v;uWR))T\n",
      "p\n",
      "d(6)\n",
      "wherehv;hu2comp are the augmented node embedding for any node pair (v;u)in the CPG. Note\n",
      "that the global attention considers each pair of nodes of the CPG, regardless of whether there is an\n",
      "edge between them. ev;u2Rdeis the edge embedding and WQ;WK2Rd\u0002d,WR2Rde\u0002dare\n",
      "parameter matrices, deanddare the dimensions of edge embedding and node embedding, respectively.\n",
      "The adjacency matrix Adynwill be further row normalized to obtain ~Adyn, which will be used to\n",
      "compute dynamic message passing (see Section 2.4).\n",
      "~Adyn= softmax( Adyn) (7)\n",
      "2.4 H YBRID GNN\n",
      "To better incorporate the information of the static graph and the dynamic graph, we propose the\n",
      "Hybrid Message Passing (HMP), which are performed on both retrieval-augmented static graph and\n",
      "attention-based dynamic graph.\n",
      "Static Message Passing. For every node vat each computation hop kin the static graph, we apply\n",
      "an aggregation function to calculate the aggregated vector hk\n",
      "vby considering a set of neighboring\n",
      "node embeddings computed from the previous hop.\n",
      "hk\n",
      "v= SUM(fhk\u00001\n",
      "uj8u2N (v)g) (8)\n",
      "whereN(v)is a set of the neighboring nodes which are directly connected with v. For each node v,\n",
      "h0\n",
      "vis the initial augmented node embedding of v,i.e.,hv2comp .\n",
      "Dynamic Message Passing. The node information and edge information are propagated on the\n",
      "attention-based dynamic graph with the adjacency matrices ~Adyn, deﬁned as\n",
      "h0k\n",
      "v=X\n",
      "u~Adyn\n",
      "v;u(WVh0k\u00001\n",
      "u+WFev;u)(9)\n",
      "wherevanduare any pair of nodes, WV2Rd\u0002d,WF2Rd\u0002deare learned matrices, and ev;u\n",
      "is the embedding of the edge connecting vandu. Similarly, h00\n",
      "vis the initial augmented node\n",
      "embedding of vincomp .\n",
      "5Published as a conference paper at ICLR 2021\n",
      "Hybrid Message Passing. Given the static/dynamic aggregated vectors hk\n",
      "v=h0k\n",
      "vfor static and\n",
      "dynamic graphs, we fuse both vectors and feed the resulting vector to a Gated Recurrent Unit (GRU)\n",
      "to update node representations.\n",
      "fk\n",
      "v= GRU( fk\u00001\n",
      "v;Fuse(hk\n",
      "v;h0k\n",
      "v)) (10)\n",
      "where f0\n",
      "vis the augmented node initialization in comp . The fusion function Fuse is designed as a\n",
      "gated sum of two inputs.\n",
      "Fuse(a;b) =z\f",
      "a+ (1\u0000z)\f",
      "b z =\u001b(Wz[a;b;a\f",
      "b;a\u0000b] +bz) (11)\n",
      "where Wzandbzare learnable weight matrix and vector, \f",
      "is the component-wise multiplication, \u001b\n",
      "is a sigmoid function and zis a gating vector. After nhops of GNN computation, we obtain the ﬁnal\n",
      "node representation fn\n",
      "vand then apply max-pooling over all nodes ffn\n",
      "vj8v2Vg to get the graph\n",
      "representation.\n",
      "2.5 D ECODER\n",
      "The decoder is similar with other state-of-the-art Seq2seq models (Bahdanau et al., 2014; Luong\n",
      "et al., 2015) where an attention-based LSTM decoder is used. The decoder takes the input of the\n",
      "concatenation of the node representation and the representation of the retrieved summary s0, namely,\n",
      "[fn\n",
      "v1;:::;fn\n",
      "vm;zht0\n",
      "1;:::;zht0\n",
      "T], wheremis the number of nodes in the input CPG graph. The initial\n",
      "hidden state of the decoder is the fusion (Eq. 11) of the graph representation and the weighted (i.e.,\n",
      "multiply similarity score z) ﬁnal state of the retrieved summary BiLSTM encoder.\n",
      "We train the model with the cross-entropy loss, deﬁned as L=P\n",
      "t\u0000logP(s\u0003\n",
      "tjc;s\u0003\n",
      "<t), wheres\u0003\n",
      "tis\n",
      "the word at the t-th position of the ground-truth output and cis the source code of the function. To\n",
      "alleviate the exposure bias, we utilize schedule teacher forcing (Bengio et al., 2015). During the\n",
      "inference, we use beam search to generate ﬁnal results.\n",
      "3 E XPERIMENTS\n",
      "3.1 S ETUP\n",
      "We evaluate our proposed framework against a number of state-of-the-art methods. Speciﬁcally,\n",
      "we classify the selected baseline methods into three groups: 1) Retrieval-based approaches: TF-\n",
      "IDF (Haiduc et al., 2010) and NNGen (Liu et al., 2018), 2) Sequence-based approaches: CODE-\n",
      "NN (Iyer et al., 2016; Barone & Sennrich, 2017), Transformer (Ahmad et al., 2020), Hybrid-\n",
      "DRL (Wan et al., 2018), Rencos (Zhang et al., 2020) and Dual model (Wei et al., 2019), 3) Graph-\n",
      "based approaches: SeqGNN (Fernandes et al., 2018). In addition, we implemented two another\n",
      "graph-based baselines: GCN2Seq and GAT2Seq, which respectively adopt the Graph Convolution\n",
      "(Kipf & Welling, 2016) and Graph Attention (Velickovic et al., 2018) as the encoder and a LSTM as\n",
      "the decoder for generating summaries. Note that Rencos (Zhang et al., 2020) combines the retrieval\n",
      "information into Seq2Seq model, we classify it into Sequence-based approaches. More detailed\n",
      "description about baselines and the conﬁguration of HGNN can be found in the Appendix B and C.\n",
      "Existing benchmarks (Barone & Sennrich, 2017; Hu et al., 2018b) are all based on high-level\n",
      "programming language i.e., Java, Python. Furthermore, they have been conﬁrmed to have extensive\n",
      "duplication, making the model overﬁt to the training data that overlapped with the testset (Fernandes\n",
      "et al., 2018; Allamanis, 2019). We are the ﬁrst to explore neural summarization on Cprogramming\n",
      "language, and make our CCode Summarization Dataset (CCSD) public to beneﬁt academia and\n",
      "industry. We crawled from popular Crepositories on GitHub and extracted function-summary pairs\n",
      "based on the documents of functions. After a deduplication process, we kept 95k+ unique function-\n",
      "summary pairs. To further test the model generalization ability, we construct in-domain functions\n",
      "and out-of-domain functions by dividing the projects into two sets, denoted as aandb. For each\n",
      "project ina, we randomly select some of the functions in this project as the training data and the\n",
      "unselected functions are the in-domain validation/test data. All functions in projects bare regarded as\n",
      "out-of-domain test data. Finally, we obtain 84,316 training functions, 4,432 in-domain validation\n",
      "functions, 4,203 in-domain test functions and 2,330 out-of-domain test functions. For the retrieval\n",
      "augmentation, we use the training set as the retrieval database, i.e.,D0=D(see Step 1 in Section\n",
      "2.2.2). For more details about data processing, please refer to Appendix D.\n",
      "6Published as a conference paper at ICLR 2021\n",
      "Table 1: Automatic evaluation results (in %) on the CCSD test set.\n",
      "In-domain Out-of-domain OverallMethodsBLEU-4 ROUGE-L METEOR BLEU-4 ROUGE-L METEOR BLEU-4 ROUGE-L METEOR\n",
      "TF-IDF 15.20 27.98 13.74 5.50 15.37 6.84 12.19 23.49 11.43\n",
      "NNGen 15.97 28.14 13.82 5.74 16.33 7.18 12.76 23.93 11.58\n",
      "CODE-NN 10.08 26.17 11.33 3.86 15.25 6.19 8.24 22.28 9.61\n",
      "Hybrid-DRL 9.29 30.00 12.47 6.30 24.19 10.30 8.42 28.64 11.73\n",
      "Transformer 12.91 28.04 13.83 5.75 18.62 9.89 10.69 24.65 12.02\n",
      "Dual Model 11.49 29.20 13.24 5.25 21.31 9.14 9.61 26.40 11.87\n",
      "Rencos 14.80 31.41 14.64 7.54 23.12 10.35 12.59 28.45 13.21\n",
      "GCN2Seq 9.79 26.59 11.65 4.06 18.96 7.76 7.91 23.67 10.23\n",
      "GAT2Seq 10.52 26.17 11.88 3.80 16.94 6.73 8.29 22.63 10.00\n",
      "SeqGNN 10.51 29.84 13.14 4.94 20.80 9.50 8.87 26.34 11.93\n",
      "HGNN w/o augment & static 11.75 29.59 13.86 5.57 22.14 9.41 9.98 26.94 12.05\n",
      "HGNN w/o augment & dynamic 11.85 29.51 13.54 5.45 21.89 9.59 9.93 26.80 12.21\n",
      "HGNN w/o augment 12.33 29.99 13.78 5.45 22.07 9.46 10.26 27.17 12.32\n",
      "HGNN w/o static 15.93 33.67 15.67 7.72 24.69 10.63 13.44 30.47 13.98\n",
      "HGNN w/o dynamic 15.77 33.84 15.67 7.64 24.72 10.73 13.31 30.59 14.01\n",
      "HGNN 16.72 34.29 16.25 7.85 24.74 11.05 14.01 30.89 14.50\n",
      "Similar to previous works (Zhang et al., 2020; Wan et al., 2018; Fernandes et al., 2018; Iyer et al.,\n",
      "2016), BLEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005) and ROUGE-L (Lin, 2004)\n",
      "are used as our automatic evaluation metrics. These metrics are popular for evaluating machine\n",
      "translation and text summarization tasks. Except for these automatic metrics, we also conduct a\n",
      "human evaluation study. We invite 5 PhD students and 10 master students as volunteers, who have\n",
      "rich C programming experiences. The volunteers are asked to rank summaries generated from\n",
      "the anonymized approaches from 1 to 5 ( i.e., 1: Poor, 2: Marginal, 3: Acceptable, 4: Good, 5:\n",
      "Excellent) based on the relevance of the generated summary to the source code and the degree of\n",
      "similarity between the generated summary and the actual summary. Speciﬁcally, we randomly choose\n",
      "50 functions for each model with the corresponding generated summaries and ground-truths. We\n",
      "calculate the average score and the higher the score, the better the quality.\n",
      "3.2 C OMPARISON WITH THE BASELINES\n",
      "Table 1 shows the evaluation results including two parts: the comparison with baselines and the\n",
      "ablation study. Consider the comparison with state-of-the-art baselines, in general, we ﬁnd that\n",
      "our proposed model outperforms existing methods by a signiﬁcant margin on both in-domain and\n",
      "out-of-domain datasets, and shows good generalization performance. Compared with others, on\n",
      "in-domain dataset, the retrieval-based approaches could achieve competitive performance on BLEU-4,\n",
      "however ROUGE-L and METEOR are fare less than ours. Moreover, they do not perform well on\n",
      "the out-of-domain dataset. Compared with the graph-based approaches ( i.e., GCN2Seq, GAT2Seq\n",
      "and SeqGNN), even without augmentation ( HGNN w/o augment ), our approach still outperforms\n",
      "them, which further demonstrates the effectiveness of Hybrid GNN for additionally capturing global\n",
      "graph information. Compared with Rencos that also considers the retrieved information in the\n",
      "Seq2Seq model, its performance is still lower than HGNN . On the overall dataset including both\n",
      "of in-domain and out-of-domain data, our model achieves 14.01 ,30.89 and14.50 , outperforming\n",
      "current state-of-the-art method Rencos by 1.42,2.44 and1.29 in terms of BLEU-4, ROUGE-L and\n",
      "METEOR metrics.\n",
      "3.3 A BLATION STUDY\n",
      "We also conduct an ablation study to evaluate the impact of different components of our framework,\n",
      "e.g., retrieval-based augmentation, static graph and dynamic graph in the last row of Table 1. Overall,\n",
      "we found that 1) retrieval-augmented mechanism could contribute to the overall model performance\n",
      "(HGNN vs.HGNN w/o augment ). Compared with HGNN , we see that the performance of HGNN w/o\n",
      "static andHGNN w/o dynamic decreases, which demonstrates the effectiveness of the Hybrid GNN\n",
      "and 2) the performance without static graph is worse than the performance without dynamic graph in\n",
      "ROUGE-L and METEOR, however, BLEU-4 is higher than the performance without dynamic graph.\n",
      "To further understand the impact of the static graph and dynamic graph, we evaluate the performance\n",
      "without augmentation and static graph/dynamic graph (see HGNN w/o augment& static andHGNN\n",
      "w/o augment& dynamic ). Compared with HGNN w/o augment , the results further conﬁrm the\n",
      "effectiveness of the Hybrid GNN ( i.e., static graph and dynamic graph).\n",
      "7Published as a conference paper at ICLR 2021\n",
      "Table 2: Human evaluation results on the CCSD test set.\n",
      "Metrics NNGen Transformer Rencos SeqGNN HGNN\n",
      "Relevance 3.23 3.17 3.48 3.09 3.69\n",
      "Similarity 3.18 3.02 3.32 3.06 3.51\n",
      "Table 3: Examples of generated summaries on the CCSD test set.\n",
      "Example Example 1 Example 2\n",
      "Source Codestatic void strInit(Str *p){\n",
      "p->z = 0;\n",
      "p->nAlloc = 0;\n",
      "p->nUsed = 0;\n",
      "}void ReleaseCedar(CEDAR *c){\n",
      "if (c == NULL)\n",
      "return;\n",
      "if (Release(c->ref) == 0)\n",
      "CleanupCedar(c);\n",
      "}\n",
      "Ground-Truth initialize a str object release reference of the cedar\n",
      "NNGen free the string release the virtual host\n",
      "Transformer reset a string release of the cancel object\n",
      "Rencos append a raw string to the json string release of the cancel object\n",
      "SeqGNN initialize the string release cedar communication mode\n",
      "HGNN initialize a string object release reference of cedar\n",
      "We also conduct experiments to investigate the impact of code-based augmentation and summary-\n",
      "based augmentation. Overall, we found that the summary-based augmentation could contribute more\n",
      "than the code-based augmentation. For example, after adding the code-based augmentation, the\n",
      "performance can be 10.22, 27.54 and 12.49 in terms of BLUE-4, ROUGE-L and METEOR on the\n",
      "overall dataset. With the summary-based augmentation, the results can reach to 13.76, 30.59 and\n",
      "14.11. Compared with the results without augmentation ( i.e., 10.26. 27.17, 12.32 with HGNN w/o\n",
      "augment ), we can see that code-based augmentation could have some improvement, but the effect\n",
      "is not signiﬁcant compared with summary-based augmentation. We conjecture that, due to that the\n",
      "code and summary are heterogeneous, the summary-based augmentation has a more direct impact on\n",
      "the code summarization task. When combining both code-based augmentation and summary-based\n",
      "augmentation, we can achieve the best results ( i.e., 14.01, 30.89, 14.50). We plan to explore more\n",
      "code-based augmentation ( e.g., semantic-equivalent code transformation) in our future work.\n",
      "3.4 H UMAN EVALUATION\n",
      "As shown in Table 2, we perform a human evaluation on the overall dataset to assess the quality\n",
      "of the generated summaries by our approach, NNGen, Transformer, Rencos and SeqGNN in terms\n",
      "of relevance and similarity. As depicted in Table 1, NNGen, Rencos and SeqGNN are the best\n",
      "retrieval-based, sequence-based, and graph-based approaches, respectively. We also compare with\n",
      "Transformer as it has been widely used in natural language processing. The results show that our\n",
      "method can generate better summaries which are more relevant with the source code and more similar\n",
      "with the ground-truth summaries.\n",
      "3.5 C ASE STUDY\n",
      "To perform qualitative analysis, we present two examples with generated summaries by different\n",
      "methods from the overall data set, shown in Table 3. We can see that, in the ﬁrst example, our\n",
      "approach can learn more code semantics, i.e.,pis a self-deﬁned struct variable. Thus, we could\n",
      "generate a token object for the variable p. However, other models can only produce string . Example 2\n",
      "is a more difﬁcult function with the functionality to “release reference of cedar”, as compared to other\n",
      "baselines, our approach effectively captures the functionality and generates a more precise summary.\n",
      "3.6 E XTENSION ON THE PYTHON DATASET\n",
      "We conducted additional experiments on a public dataset, i.e., the Python Code Summarization\n",
      "Dataset (PCSD), which was also used in Rencos (the most competitive baseline in our paper). We\n",
      "follow the setting of Rencos and split PCSD into the training set, validation set and testing set with\n",
      "fractions of 60%, 20% and 20%. We construct the static graph based on AST. The decoding step\n",
      "is set to 50, followed by Rencos, and the other settings are the same with CCSD. We compare our\n",
      "methods on PCSD against various competitive baselines, i.e., NNGen, CODE-NN, Rencos and\n",
      "8Published as a conference paper at ICLR 2021\n",
      "Table 4: Automatic evaluation results (in %) on the PCSD test set.\n",
      "Methods BLEU-4 ROUGE-L METEOR\n",
      "NNGen 21.60 31.61 15.96\n",
      "CODE-NN 16.39 28.99 13.68\n",
      "Transformer 17.06 31.16 14.37\n",
      "Rencos 24.02 36.21 18.07\n",
      "HGNN w/o static 24.06 38.28 18.66\n",
      "HGNN w/o dynamic 24.13 38.64 18.93\n",
      "HGNN 24.42 39.91 19.48\n",
      "Transformer, which are either retrieval-based, generation-based or hybrid methods. The results are\n",
      "shown in Table 4. The results indicate that, compared with the best results from NNGen, CODE-NN,\n",
      "Rencos and Transformer, our method can improve the performance by 0.40, 3.70 and 1.41 in terms of\n",
      "BLEU-4, ROUGE-L and METEOR. We also conduct the ablation study on PCSD to demonstrate\n",
      "the usefulness of the static graph ( i.e., HGNN w/o dynamic) and dynamic graph ( i.e., HGNN w/o\n",
      "static). The results also demonstrate that both static graph and dynamic graph can contribute to our\n",
      "framework. In summary, the results on both our released benchmark (CCSD) and existing benchmark\n",
      "(PCSD) demonstrate the effectiveness and the scalability of our method.\n",
      "4 R ELATED WORK\n",
      "Source Code Summarization Early works (Eddy et al., 2013; Haiduc et al., 2010; Wong et al., 2015;\n",
      "2013) for code summarization focused on using information retrieval to retrieve summaries. Later\n",
      "works attempted to employ attentional Seq2Seq model on the source code (Iyer et al., 2016; Siow\n",
      "et al., 2020) or some variants, i.e., AST (Hu et al., 2018a; Alon et al., 2018; Liu et al., 2020) for\n",
      "generation. However, these works are based on sequential models, ignoring rich code semantics.\n",
      "Some latest attempts (LeClair et al., 2020; Fernandes et al., 2018) embedded program semantics into\n",
      "GNNs. but they mainly rely on simple representations, which are limited to learn full semantics.\n",
      "Graph Neural Networks Over the past few years, GNNs (Li et al., 2015; Hamilton et al., 2017;\n",
      "Kipf & Welling, 2016; Chen et al., 2020b) have attracted increasing attention with many successful\n",
      "applications in computer vision (Norcliffe-Brown et al., 2018), natural language processing (Xu et al.,\n",
      "2018a; Chen et al., 2020d;c;e). Because by design GNNs can model graph-structured data, recently,\n",
      "some works have extended the widely used Seq2Seq architectures to Graph2Seq architectures for\n",
      "various tasks including machine translation (Beck et al., 2018), and graph (e.g., AMR, SQL)-to-text\n",
      "generation (Zhu et al., 2019; Xu et al., 2018b). Some works have also attempted to encode programs\n",
      "with graphs for diverse tasks e.g., V ARNAMING/V ARMISUSE (Allamanis et al., 2017), Source\n",
      "Code Vulnerability Detection (Zhou et al., 2019). As compared to these works, we innovate a hybrid\n",
      "message passing GNN performed on both static graph and dynamic graph for message fusion.\n",
      "5 C ONCLUSION AND FUTURE WORK\n",
      "In this paper, we proposed a general-purpose framework for automatic code summarization. A novel\n",
      "retrieval-augmented mechanism is proposed for combining the beneﬁts of both retrieval-based and\n",
      "generation-based approaches. Moreover, to capture global semantics among nodes, we develop a\n",
      "hybrid message passing GNN based on both static and dynamic graphs. The evaluation shows that\n",
      "our approach improves state-of-the-art techniques substantially. Our future work includes: 1) we plan\n",
      "to introduce more information such as API knowledge to learn the better semantics of programs, 2)\n",
      "we explore more code-based augmentation techniques to improve the performance and 3) we plan to\n",
      "adopt the existing techniques such as (Du et al., 2019; Xie et al., 2019a;b; Ma et al., 2018) to evaluate\n",
      "the robustness of the trained model.\n",
      "6 A CKNOWLEDGEMENT\n",
      "This research is supported by the National Research Foundation, Singapore under its AI Singapore\n",
      "Programme (AISG Award No: AISG2-RP-2020-019), the National Research Foundation under its\n",
      "National Cybersecurity R&D Program (Award No. NRF2018NCR-NCR005-0001), the Singapore\n",
      "National Research Foundation under NCR Award Number NRF2018NCR-NSOE003-0001 and NRF\n",
      "Investigatorship NRFI06-2020-0022.\n",
      "9Published as a conference paper at ICLR 2021\n",
      "REFERENCES\n",
      "Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. A transformer-based\n",
      "approach for source code summarization. arXiv preprint arXiv:2005.00653 , 2020.\n",
      "Miltiadis Allamanis. The adverse effects of code duplication in machine learning models of code. In\n",
      "Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms,\n",
      "and Reﬂections on Programming and Software , pp. 143–153, 2019.\n",
      "Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs\n",
      "with graphs. arXiv preprint arXiv:1711.00740 , 2017.\n",
      "Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of machine\n",
      "learning for big code and naturalness. ACM Computing Surveys (CSUR) , 51(4):1–37, 2018.\n",
      "Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. code2seq: Generating sequences from\n",
      "structured representations of code. arXiv preprint arXiv:1808.01400 , 2018.\n",
      "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "learning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.\n",
      "Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved\n",
      "correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic\n",
      "evaluation measures for machine translation and/or summarization , pp. 65–72, 2005.\n",
      "Antonio Valerio Miceli Barone and Rico Sennrich. A parallel corpus of python functions and\n",
      "documentation strings for automated code documentation and code generation. arXiv preprint\n",
      "arXiv:1707.02275 , 2017.\n",
      "Daniel Beck, Gholamreza Haffari, and Trevor Cohn. Graph-to-sequence learning using gated graph\n",
      "neural networks. arXiv preprint arXiv:1806.09835 , 2018.\n",
      "Richard Bellman. Dynamic programming. Science , 153(3731):34–37, 1966.\n",
      "Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence\n",
      "prediction with recurrent neural networks. In Advances in Neural Information Processing Systems ,\n",
      "pp. 1171–1179, 2015.\n",
      "Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-\n",
      "smoothing problem for graph neural networks from the topological view. In AAAI , pp. 3438–3445,\n",
      "2020a.\n",
      "Yu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural networks:\n",
      "Better and robust node embeddings. Advances in Neural Information Processing Systems , 33,\n",
      "2020b.\n",
      "Yu Chen, Lingfei Wu, and Mohammed J. Zaki. Graphﬂow: Exploiting conversation ﬂow with\n",
      "graph neural networks for conversational machine comprehension. In Proceedings of the Twenty-\n",
      "Ninth International Joint Conference on Artiﬁcial Intelligence , pp. 1230–1236. International Joint\n",
      "Conferences on Artiﬁcial Intelligence Organization, 2020c.\n",
      "Yu Chen, Lingfei Wu, and Mohammed J. Zaki. Reinforcement learning based graph-to-sequence\n",
      "model for natural question generation. In Proceedings of the 8th International Conference on\n",
      "Learning Representations , Apr. 26-30, 2020d.\n",
      "Yu Chen, Lingfei Wu, and Mohammed J Zaki. Toward subgraph guided knowledge graph question\n",
      "generation with graph neural networks. arXiv preprint arXiv:2004.06015 , 2020e.\n",
      "Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. Deepstellar: Model-based\n",
      "quantitative analysis of stateful deep learning systems. In Proceedings of the 2019 27th ACM Joint\n",
      "Meeting on European Software Engineering Conference and Symposium on the Foundations of\n",
      "Software Engineering , pp. 477–487, 2019.\n",
      "10Published as a conference paper at ICLR 2021\n",
      "Brian P Eddy, Jeffrey A Robinson, Nicholas A Kraft, and Jeffrey C Carver. Evaluating source code\n",
      "summarization techniques: Replication and expansion. In 2013 21st International Conference on\n",
      "Program Comprehension (ICPC) , pp. 13–22. IEEE, 2013.\n",
      "Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. Structured neural summarization.\n",
      "arXiv preprint arXiv:1811.01824 , 2018.\n",
      "Sonia Haiduc, Jairo Aponte, Laura Moreno, and Andrian Marcus. On the use of automated text\n",
      "summarization techniques for summarizing source code. In 2010 17th Working Conference on\n",
      "Reverse Engineering , pp. 35–44. IEEE, 2010.\n",
      "Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In\n",
      "Advances in neural information processing systems , pp. 1024–1034, 2017.\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n",
      "recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\n",
      "pp. 770–778, 2016.\n",
      "Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):\n",
      "1735–1780, 1997.\n",
      "Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. Deep code comment generation. In Proceedings of\n",
      "the 26th Conference on Program Comprehension , pp. 200–210, 2018a.\n",
      "Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. Summarizing source code with transferred\n",
      "api knowledge. 2018b.\n",
      "Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Summarizing source code\n",
      "using a neural attention model. In Proceedings of the 54th Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers) , pp. 2073–2083, 2016.\n",
      "Siyuan Jiang, Ameer Armaly, and Collin McMillan. Automatically generating commit messages\n",
      "from diffs using neural machine translation. In 2017 32nd IEEE/ACM International Conference on\n",
      "Automated Software Engineering (ASE) , pp. 135–146. IEEE, 2017.\n",
      "Toshihiro Kamiya, Shinji Kusumoto, and Katsuro Inoue. Ccﬁnder: a multilinguistic token-based code\n",
      "clone detection system for large scale source code. IEEE Transactions on Software Engineering ,\n",
      "28(7):654–670, 2002.\n",
      "Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.\n",
      "arXiv preprint arXiv:1609.02907 , 2016.\n",
      "Alexander LeClair, Sakib Haque, Linfgei Wu, and Collin McMillan. Improved code summarization\n",
      "via a graph neural network. arXiv preprint arXiv:2004.02843 , 2020.\n",
      "Guohao Li, Matthias Müller, Ali K. Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep\n",
      "as cnns? In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul,\n",
      "Korea (South), October 27 - November 2, 2019 , pp. 9266–9275. IEEE, 2019. doi: 10.1109/ICCV .\n",
      "2019.00936. URL https://doi.org/10.1109/ICCV.2019.00936 .\n",
      "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural\n",
      "networks. arXiv preprint arXiv:1511.05493 , 2015.\n",
      "Zhenmin Li, Shan Lu, Suvda Myagmar, and Yuanyuan Zhou. Cp-miner: Finding copy-paste and\n",
      "related bugs in large-scale software code. IEEE Transactions on software Engineering , 32(3):\n",
      "176–192, 2006.\n",
      "Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\n",
      "Branches Out , pp. 74–81. Association for Computational Linguistics, July 2004.\n",
      "Shangqing Liu, Cuiyun Gao, Sen Chen, Nie Lun Yiu, and Yang Liu. Atom: Commit message\n",
      "generation based on abstract syntax tree and hybrid ranking. IEEE Transactions on Software\n",
      "Engineering , 2020.\n",
      "11Published as a conference paper at ICLR 2021\n",
      "Zhongxin Liu, Xin Xia, Ahmed E Hassan, David Lo, Zhenchang Xing, and Xinyu Wang. Neural-\n",
      "machine-translation-based commit message generation: how far are we? In Proceedings of the\n",
      "33rd ACM/IEEE International Conference on Automated Software Engineering , pp. 373–384,\n",
      "2018.\n",
      "Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based\n",
      "neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n",
      "Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su,\n",
      "Li Li, Yang Liu, et al. Deepgauge: Multi-granularity testing criteria for deep learning systems. In\n",
      "Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering ,\n",
      "pp. 120–131, 2018.\n",
      "Will Norcliffe-Brown, Stathis Vafeias, and Sarah Parisot. Learning conditioned graph structures for\n",
      "interpretable visual question answering. In Advances in Neural Information Processing Systems ,\n",
      "pp. 8344–8353, 2018.\n",
      "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\n",
      "evaluation of machine translation. In Proceedings of the 40th annual meeting on association for\n",
      "computational linguistics , pp. 311–318. Association for Computational Linguistics, 2002.\n",
      "Jing Kai Siow, Cuiyun Gao, Lingling Fan, Sen Chen, and Yang Liu. Core: Automating review\n",
      "recommendation for code changes. In 2020 IEEE 27th International Conference on Software\n",
      "Analysis, Evolution and Reengineering (SANER) , pp. 284–295. IEEE, 2020.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\n",
      "Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\n",
      "processing systems , pp. 5998–6008, 2017.\n",
      "Petar Velickovic, Guillem Cucurull, A. Casanova, A. Romero, P. Liò, and Yoshua Bengio. Graph\n",
      "attention networks. ArXiv , abs/1710.10903, 2018.\n",
      "Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S Yu. Improving\n",
      "automatic source code summarization via deep reinforcement learning. In Proceedings of the 33rd\n",
      "ACM/IEEE International Conference on Automated Software Engineering , pp. 397–407, 2018.\n",
      "Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. Code generation as a dual task of code summariza-\n",
      "tion. In Advances in Neural Information Processing Systems , pp. 6563–6573, 2019.\n",
      "Edmund Wong, Jinqiu Yang, and Lin Tan. Autocomment: Mining question and answer sites for\n",
      "automatic comment generation. In 2013 28th IEEE/ACM International Conference on Automated\n",
      "Software Engineering (ASE) , pp. 562–567. IEEE, 2013.\n",
      "Edmund Wong, Taiyue Liu, and Lin Tan. Clocom: Mining existing source code for automatic\n",
      "comment generation. In 2015 IEEE 22nd International Conference on Software Analysis, Evolution,\n",
      "and Reengineering (SANER) , pp. 380–389. IEEE, 2015.\n",
      "Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jianjun Zhao, Bo Li,\n",
      "Jianxiong Yin, and Simon See. Deephunter: a coverage-guided fuzz testing framework for deep\n",
      "neural networks. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software\n",
      "Testing and Analysis , pp. 146–157, 2019a.\n",
      "Xiaofei Xie, Lei Ma, Haijun Wang, Yuekang Li, Yang Liu, and Xiaohong Li. Diffchaser: Detecting\n",
      "disagreements for deep neural networks. In IJCAI , pp. 5772–5778, 2019b.\n",
      "Kun Xu, Lingfei Wu, Zhiguo Wang, and Vadim Sheinin. Graph2seq: Graph to sequence learning\n",
      "with attention-based neural networks. arXiv preprint arXiv:1804.00823 , 2018a.\n",
      "Kun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Liwei Chen, and Vadim Sheinin. Sql-to-text generation\n",
      "with graph-to-sequence model. arXiv preprint arXiv:1809.05255 , 2018b.\n",
      "Fabian Yamaguchi, Nico Golde, Daniel Arp, and Konrad Rieck. Modeling and discovering vulnera-\n",
      "bilities with code property graphs. In 2014 IEEE Symposium on Security and Privacy , pp. 590–604.\n",
      "IEEE, 2014.\n",
      "12Published as a conference paper at ICLR 2021\n",
      "Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. Retrieval-based neural\n",
      "source code summarization. In Proceedings of the 42nd International Conference on Software\n",
      "Engineering. IEEE , 2020.\n",
      "Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. arXiv preprint\n",
      "arXiv:1909.12223 , 2019.\n",
      "Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. Devign: Effective vulnera-\n",
      "bility identiﬁcation by learning comprehensive program semantics via graph neural networks. In\n",
      "Advances in Neural Information Processing Systems , pp. 10197–10207, 2019.\n",
      "Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. Modeling graph\n",
      "structure in transformer for better amr-to-text generation. arXiv preprint arXiv:1909.00136 , 2019.\n",
      "13Published as a conference paper at ICLR 2021\n",
      "Appendices\n",
      "A D ETAILS ON CODE PROPERTY GRAPH\n",
      "Code Property Graph (CPG) (Yamaguchi et al., 2014), which is constructed on abstract syntax tree\n",
      "(AST), combines different edges (i.e., Flow to, Control) to represent the semantics of the program.\n",
      "We describe each representation combining with Figure 2 as follows:\n",
      "•Abstract Syntax Tree (AST). AST contains syntactic information for a program and omits irrele-\n",
      "vant details that have no effect on the semantics. Figure 2 shows the completed AST nodes on the\n",
      "left simple program and each node has a code sequence in the ﬁrst line and type attribute in the\n",
      "second line. The black arrows represent the child-parent relations among ASTs.\n",
      "•Control Flow Graph (CFG). Compared with AST highlighting the syntactic structure, CFG\n",
      "displays statement execution order, i.e., the possible order in which statements may be executed\n",
      "and the conditions that must be met for this to happen. Each statement in the program is treated as\n",
      "an independent node as well as a designated entry and exit node. Based on the keywords if,for,\n",
      "goto,break andcontinue , control ﬂow graphs can be easily built and “Flow to” with green dashed\n",
      "arrows in Figure 2 represents this ﬂow order.\n",
      "•Program Dependency Graph (PDG). PDG includes data dependencies andcontrol dependen-\n",
      "cies: 1) data dependencies are described as the deﬁnition of a variable in a statement reaches the\n",
      "usage of the same variable at another statement. In Figure 2, the variable “ b” is deﬁned in the\n",
      "statement “ int b = a++ ” and used in “ call (b) ”. Hence, there is a “Reach” edge with blue arrows\n",
      "point from “ int b = a++ ” to “ call (b) ”. Furthermore, Deﬁne/Use edge with orange double arrows\n",
      "denotes the deﬁnition and usage of the variable. 2) different from CFG displaying the execution\n",
      "process of the complete program, control dependencies deﬁne the execution of a statement may be\n",
      "dependent on the value of a predicate, which more focus on the statement itself. For instance, the\n",
      "statements “ int b = a++ ” and “ call(b) ” are only performed “if a is even”. Therefore, a red double\n",
      "arrow “Control” points from “ if (a % 2) == 0 ” to “ int b = a++ ” and “ call(b) ”.\n",
      "B D ETAILS ON BASELINE METHODS\n",
      "We compare our approach with existing baselines. They can be divided into three groups: Retrieval-\n",
      "based approaches, Sequence-based approaches and Graph-based approaches. For papers that provide\n",
      "the source code, we directly reproduce their methods on CCSD dataset. Otherwise, we reimplement\n",
      "their approaches with reference to the papers.\n",
      "B.1 R ETRIEVAL -BASED APPROACHES\n",
      "TF-IDF (Haiduc et al., 2010) is the abbreviation of Term Frequency-Inverse Document Frequency,\n",
      "which is adopted in the early code summarization (Haiduc et al., 2010). It transforms programs\n",
      "into weight vectors by calculating term frequency and inverse document frequency. We retrieve the\n",
      "summary of the most similar programs by calculating the cosine similarity on the weighted vectors.\n",
      "NNGen (Liu et al., 2018) is a retrieved-based approach to produce commit messages for code changes.\n",
      "We reproduce such an algorithm on code summarization. Speciﬁcally, we retrieve the most similar\n",
      "top-k code snippets on a bag-of-words model and prioritizes the summary in terms of BLEU-4 scores\n",
      "in top-k code snippets.\n",
      "B.2 S EQUENCE -BASED APPROACHES\n",
      "CODE-NN (Iyer et al., 2016; Barone & Sennrich, 2017) adopts an attention-based Seq2Seq model\n",
      "to generate summaries on the source code.\n",
      "Transformer (Ahmad et al., 2020) adopts the transformer architecture (Vaswani et al., 2017) with\n",
      "self-attention to capture long dependency in the code for source code summrization.\n",
      "14Published as a conference paper at ICLR 2021\n",
      "Hybrid-DRL (Wan et al., 2018) is a reinforcement learning-based approach, which incorporates AST\n",
      "and sequential code snippets into a deep reinforcement learning framework and employ evaluation\n",
      "metrics e.g., BLEU as the reward.\n",
      "Dual Model (Wei et al., 2019) propose a dual training framework by training code summarization\n",
      "and code generation tasks simultaneously to boost each task performance.\n",
      "Rencos (Zhang et al., 2020) is the retrieval-based Seq2Seq model for code summarization. it utilized\n",
      "a pretrained Seq2Seq model during the testing phase by computing a joint probability conditioned on\n",
      "both the original source code and retrieved the most similar source code for the summary generation.\n",
      "Compared with Rencos, we propose a novel retrieval-augmented mechanism for the similar source\n",
      "code and use it at the model training phase.\n",
      "B.3 G RAPH -BASED APPROACHES\n",
      "We also compared with some latest GNN-based works, employing graph neural network for source\n",
      "code summarization.\n",
      "GCN2Seq, GAT2Seq modify Graph Convolution Network (Kipf & Welling, 2016) and Graph\n",
      "Attention Network (Velickovic et al., 2018) to perform convolution operation and attention operation\n",
      "on the code property graph for learning and followed by a LSTM to generate summaries. We\n",
      "implement the related code from scratch.\n",
      "SeqGNN (Fernandes et al., 2018) combines GGNNs and standard sequence encoders for summa-\n",
      "rization. They take the code and relationships between elements of the code as input. Specially, a\n",
      "BiLSTM is employed on the code sequence to learn representations and each source code token is\n",
      "modelled as a node in the graph, and employed GGNN for graph-level learning. Since our node\n",
      "sequences are sub-sequence of source code rather than individual token, we adjust to slice the output\n",
      "of BiLSTM and sum each token representation in node sequences as node initial representation for\n",
      "summarization. Furthermore, we implement the related code from scratch.\n",
      "C M ODEL SETTINGS\n",
      "We embed the most frequent 40,000 words in the training set with 512-dims and set the hidden size\n",
      "of BiLSTM to 256 and the concatenated state size for both directions is 512. The dropout is set to 0.3\n",
      "after the word embedding layer and BiLSTM. We set GNN hops to 1 for the best performance. The\n",
      "optimizer is selected with Adam with an initial learning rate of 0.001. The batch size is set to 64 and\n",
      "early stop for 10. The beam search width is set to 5 as usual. All experiments are conducted on the\n",
      "DGX server with four Nvidia Graphics Tesla V100 and each epoch takes 6 minutes averagely. All\n",
      "hyperparameters are tuned with grid search on the validation set.\n",
      "D D ETAILS ON DATA PREPARATION\n",
      "It is non-trivial to obtain high-quality datasets for code summarization. We noticed that despite some\n",
      "previous works (Barone & Sennrich, 2017; Hu et al., 2018b) released their datasets, however, they\n",
      "are all based on high-level programming languages i.e. Java, Python. We are the ﬁrst to explore\n",
      "summarization on Cprogramming language. Speciﬁcally, we crawled from popular Crepositories\n",
      "(e.g., Linux and QEMU) on GitHub, and then extracted separate function-summary pairs from these\n",
      "projects. Speciﬁcally, we extracted functions and associated comments marked by special characters\n",
      "\"/**\" and \"*/\" over the function declaration. These comments can be considered as explanations of\n",
      "the functions. We ﬁltered out functions with line exceeding 1000 and any other comments inside\n",
      "the function, and the ﬁrst sentence was selected as the summary. A similar practice can be found\n",
      "in (Jiang et al., 2017). Totally, we collected 500k+ raw function-summary pairs. Furthermore,\n",
      "functions with token size greater than 150 were removed for computational efﬁciency and there\n",
      "were 130k+ functions left. Since duplication is very common in existing datasets (Fernandes et al.,\n",
      "2018), followed by Allamanis (2019), we performed a de-duplication process and removed functions\n",
      "with similarity over 80%. Speciﬁcally, we calculated the cosine similarity by encoding the raw\n",
      "functions into vectors with sklearn. Finally, we kept 95k+ unique functions. We name this dataset C\n",
      "Code Summarization Dataset (CCSD). To testify model generalization ability, we randomly selected\n",
      "15Published as a conference paper at ICLR 2021\n",
      "Table 5: More Examples of generated summaries on the CCSD test set.\n",
      "Example Example 1 Example 2\n",
      "Source Codestatic void counterMutexFree\n",
      "(sqlite3_mutex *p){\n",
      "assert(g.isInit);\n",
      "g.m.xMutexFree(p->pReal);\n",
      "if( p->eType==SQLITE_MUTEX_FAST\n",
      "|| p->eType==\n",
      "S QLITE_MUTEX_RECURSIVE)\n",
      "{\n",
      "free(p);\n",
      "}\n",
      "}static void __exit wimax_subsys_exit(void)\n",
      "{\n",
      "wimax_id_table_release();\n",
      "genl_unregister_family\n",
      "(&wimax_gnl_family);\n",
      "}\n",
      "Ground-Truth free a countable mutex shutdown the wimax stack\n",
      "NNGen enter a countable mutex unregisters pmcraid event family return value none\n",
      "Transformer leave a mutex de initialize wimax driver\n",
      "Rencos try to enter a mutex unregister the wimax device subsystem\n",
      "SeqGNN free a mutex allocated by sqlite3 mutex this function is called when the driver is not held\n",
      "HGNN release a mutex free the wimax stack\n",
      "Retrieved_codestatic int counterMutexTry\n",
      "(sqlite3_mutex *p){\n",
      "assert( g.isInit );\n",
      "assert( p->eType>=0 );\n",
      "assert( p->eType<MAX_MUTEXES );\n",
      "g.aCounter[p->eType]++;\n",
      "if( g.disableTry )\n",
      "return SQLITE_BUSY;\n",
      "return g.m.xMutexTry(p->pReal);\n",
      "}static int __init wimax_subsys_init(void){\n",
      "int result; d_fnstart(4, NULL, \"()\\n\");\n",
      "d_parse_params(D_LEVEL, D_LEVEL_SIZE,\n",
      "wimax_debug_params, \"wimax.debug\");\n",
      "result = genl_register_family\n",
      "(&wimax_gnl_family);\n",
      "if (unlikely(result < 0)) {\n",
      "pr_err(\"cannot register generic\n",
      "netlink family: %d\\n\", result);\n",
      "goto error_register_family;}\n",
      "d_fnend(4, NULL, \"() = 0\\n\");\n",
      "return 0;\n",
      "error_register_family:\n",
      "d_fnend(4, NULL, \"() = %d\\n\", result);\n",
      "return result;\n",
      "}\n",
      "Retrieved_summary try to enter a mutex shutdown the wimax stack\n",
      "Example Example 3 Example 4\n",
      "Source Codestatic void udc_dd_free(\n",
      "struct lpc32xx_udc *udc,\n",
      "struct lpc32xx_usbd_dd_gad *dd)\n",
      "{\n",
      "dma_pool_free(udc->dd_cache,\n",
      "dd, dd->this_dma);\n",
      "}void ReleaseSockEvent(SOCK_EVENT *event) {\n",
      "if (event == NULL)\n",
      "{\n",
      "return;\n",
      "}\n",
      "if (Release(event->ref) == 0)\n",
      "{\n",
      "CleanupSockEvent(event);\n",
      "}\n",
      "}\n",
      "Ground-Truth free a dma descriptor release of the socket event\n",
      "NNGen allocate a dma descriptor clean up of the socket event\n",
      "Transformer free the usb device set the event\n",
      "Rencos allocate a dma descriptor set of the sock event\n",
      "SeqGNN free dma buffers release of the socket\n",
      "HGNN free a dma descriptor release the sock event\n",
      "Retrieved_codestatic struct lpc32xx_usbd_dd_gad\n",
      "*udc_dd_alloc(struct\n",
      "lpc32xx_udc *udc) {\n",
      "dma_addr_t dma;\n",
      "struct lpc32xx_usbd_dd_gad *dd;\n",
      "dd = dma_pool_alloc(udc->dd_cache,\n",
      "GFP_ATOMIC | GFP_DMA, &dma);\n",
      "if (dd)\n",
      "dd->this_dma = dma;\n",
      "return dd;\n",
      "}void SetL2TPServerSockEvent(\n",
      "L2TP_SERVER *l2tp,SOCK_EVENT *e){\n",
      "if (l2tp == NULL) {\n",
      "return;}\n",
      "if (e != NULL){\n",
      "AddRef(e->ref);}\n",
      "if (l2tp->SockEvent != NULL){\n",
      "ReleaseSockEvent(l2tp->SockEvent);\n",
      "l2tp->SockEvent = NULL;}\n",
      "l2tp->SockEvent = e;}\n",
      "Retrieved_summary allocate a dma descriptor set a sock event to the l2tp server\n",
      "some projects as the out-of-domain test set with 2,330 examples and the remaining were randomly\n",
      "split into train/validation/test with 84,316/4,432/4,203 examples. The open-source code analysis\n",
      "platform Joern (Yamaguchi et al., 2014) was applied to construct the code property graph.\n",
      "E M ORE EXAMPLES\n",
      "We show more examples along with the retrieved code and summary by dynamic programming in\n",
      "Table 5 and we can ﬁnd that HGNN can generate more high-quality summries based on our approach.\n",
      "16\n",
      "==================================================\n",
      "\n",
      "Document 10:\n",
      "T-RAG: End-to-End Table Question Answering via Retrieval-Augmented\n",
      "Generation\n",
      "Feifei Pan1, Mustafa Canim2, Michael Glass2, Alﬁo Gliozzo2, James Hendler1\n",
      "panf2@rpi.edu ,mustafa@us.ibm.com ,\n",
      "mrglass@us.ibm.com ,gliozzo@us.ibm.com\n",
      "hendler@cs.rpi.edu\n",
      "1Rensselaer Polytechnic Institute\n",
      "2IBM TJ Watson Research Center\n",
      "Abstract\n",
      "Most existing end-to-end Table Question An-\n",
      "swering (Table QA) models consist of a two-\n",
      "stage framework with a retriever to select rel-\n",
      "evant table candidates from a corpus and a\n",
      "reader to locate the correct answers from table\n",
      "candidates. Even though the accuracy of the\n",
      "reader models is signiﬁcantly improved with\n",
      "the recent transformer-based approaches, the\n",
      "overall performance of such frameworks still\n",
      "suffers from the poor accuracy of using tradi-\n",
      "tional information retrieval techniques as re-\n",
      "trievers. To alleviate this problem, we intro-\n",
      "duce T-RAG, an end-to-end Table QA model,\n",
      "where a non-parametric dense vector index is\n",
      "ﬁne-tuned jointly with BART, a parametric\n",
      "sequence-to-sequence model to generate an-\n",
      "swer tokens. Given any natural language ques-\n",
      "tion, T-RAG utilizes a uniﬁed pipeline to auto-\n",
      "matically search through a table corpus to di-\n",
      "rectly locate the correct answer from table cell.\n",
      "We apply T-RAG on recent open-domain Table\n",
      "QA benchmarks and demonstrate that the ﬁne-\n",
      "tuned T-RAG model is able to achieve state-\n",
      "of-the-art performance in both the end-to-end\n",
      "Table QA and the table retrieval tasks.\n",
      "1 Introduction\n",
      "Tabular data is commonly seen in open-domain\n",
      "documents (Cafarella et al., 2009; Zhang and Ba-\n",
      "log, 2018a), such as the Web and Wikipedia, as\n",
      "well as in domain-speciﬁc papers, journals, manu-\n",
      "als, and reports. Answering questions over these\n",
      "tables requires table retrieval and understanding\n",
      "of the table structure and content. Table QA task\n",
      "is generally more challenging than executing SQL\n",
      "queries over relational database tables due to the\n",
      "lack of schema information. Most existing studies\n",
      "tackle Table QA as two separate sub-tasks: (1) Ta-\n",
      "ble retrieval (Cafarella et al., 2008, 2009; Zhang\n",
      "and Balog, 2018a; Shraga et al., 2020a,b), and (2)\n",
      "QA over tables (Yu et al., 2018; Herzig et al., 2020;\n",
      "Yin et al., 2020; Glass et al., 2020). Recently, theDTR (Herzig et al., 2021) and the CLTR (Pan et al.,\n",
      "2021) models have been proposed as end-to-end\n",
      "solutions for Table QA. Both models consist of a\n",
      "two-step pipeline of a retriever to generate a set of\n",
      "candidate tables and a reader to answer questions\n",
      "over these tables. The two components are trained\n",
      "individually, causing error propagation from re-\n",
      "trievers to readers, i.e. with incorrect table can-\n",
      "didates, it is impossible for the readers to locate\n",
      "the correct answer despite the design of the mod-\n",
      "els. While dense retrieval and Retrieval Augmented\n",
      "Generation (RAG) (Karpukhin et al., 2020; Lewis\n",
      "et al., 2020b) have achieved great success in open-\n",
      "domain QA over free text, none of the studies in\n",
      "the literature leverage a non-parametric memory\n",
      "model along with a parametric memory model for\n",
      "the open-domain Table QA task.\n",
      "In this paper, we describe a novel end-to-end\n",
      "Table QA model, T-RAG, replacing the two-step\n",
      "framework with a single training process. To\n",
      "train T-RAG, we utilize Dense Passage Retrieval\n",
      "(DPR) (Karpukhin et al., 2020) and RAG strate-\n",
      "gies. Speciﬁcally, we jointly train a DPR compo-\n",
      "nent (Glass et al., 2021) together with the BART-\n",
      "based (Lewis et al., 2020a) sequence-to-sequence\n",
      "(Seq2Seq) model. To the best of our knowledge,\n",
      "T-RAG is the ﬁrst Table QA model where the\n",
      "query encoder for a non-parametric dense vector\n",
      "index is ﬁne-tuned along with a parametric gen-\n",
      "eration model. We evaluate the performance of\n",
      "T-RAG on NQ-TABLES (Herzig et al., 2021) and\n",
      "E2E_WTQ (Pan et al., 2021), two recent end-to-\n",
      "end Table QA benchmarks. The experimental re-\n",
      "sults indicate that T-RAG outperforms the state-of-\n",
      "the-art models on the end-to-end Table QA task.\n",
      "The major contribution of this work is that, we\n",
      "propose the ﬁrst end-to-end Table QA pipeline,\n",
      "leveraging DPR along with the Seq2Seq compo-\n",
      "nent of RAG. T-RAG employs a simple but effec-\n",
      "tive one-step training that reduces error accumu-\n",
      "lations and simpliﬁes model ﬁne-tuning. In thearXiv:2203.16714v1  [cs.CL]  30 Mar 2022experiments, T-RAG achieves state-of-the-art per-\n",
      "formance on two tasks. We ﬁnd T-RAG improves\n",
      "the results for end-to-end Table QA on two recent\n",
      "benchmarks. The RAG component of the end-to-\n",
      "end model ﬁne-tuned over Table QA benchmarks\n",
      "also yields state-of-the-art results on the table re-\n",
      "trieval task.\n",
      "2 Related Work\n",
      "Table Retrieval Traditional table retrieval mod-\n",
      "els usually concatenate tables into documents\n",
      "while disregarding the underlying tabular structure\n",
      "(Pyreddy and Croft, 1997; Wang and Hu, 2002;\n",
      "Liu et al., 2007; Cafarella et al., 2008, 2009). New\n",
      "approaches are proposed to retrieve tables with a\n",
      "set of features of the table, query and table-query\n",
      "pair (Zhang and Balog, 2018b; Sun et al., 2019;\n",
      "Bhagavatula et al., 2013; Shraga et al., 2020a).\n",
      "Zhang and Balog (2018b) uses semantic similari-\n",
      "ties to build an ad-hoc table retrieval model with\n",
      "various features. A neural ranking model is in-\n",
      "troduced in Shraga et al. (2020b), where tables are\n",
      "deﬁned as multi-modal objects and the Gated Multi-\n",
      "modal Units are used to learn the representation of\n",
      "query-table pairs. Pan et al. (2021) later follows\n",
      "this work and improves the table retrieval with a\n",
      "2-step retriever. Kosti ´c et al. (2021) discusses the\n",
      "use of dense vector embeddings to enhance the per-\n",
      "formance of bi- and tri-encoder in retrieving both\n",
      "table and text.\n",
      "Table QA Most early Table QA solutions are\n",
      "fully supervised models, focusing on converting\n",
      "natural language questions into SQL format and\n",
      "using the SQL-format questions to query the given\n",
      "tables, as seen in Yu et al. (2018); Lin et al. (2019);\n",
      "Xu et al. (2018). Open-domain QA over text (Yu\n",
      "et al., 2020) usually utilizes multiple knowledge\n",
      "sources. For instance, Oguz et al. (2021) pro-\n",
      "poses a model can convert structured, unstruc-\n",
      "tured and semi-structured knowledge into text for\n",
      "open-domain QA. Therefore, more recent efforts\n",
      "have been put into investigating the use of exter-\n",
      "nal knowledge in enhancing the performance of\n",
      "Table QA. Jiménez-Ruiz et al. (2020) ﬁrst proposes\n",
      "the Semantic Web Challenge on Tabular Data to\n",
      "Knowledge Graph Matching (SemTab) to encour-\n",
      "age such solutions for both table understanding\n",
      "and Table QA. Recently, the transformer-based,\n",
      "weakly supervised solutions have been proposed\n",
      "for Table QA. These solutions fall into two cat-\n",
      "egories: (1) Logic form-based solution, such astheTABERT (Yin et al., 2020) model, which is\n",
      "trained to capture the representation of natural lan-\n",
      "guage sentences as well as tabular data; (2) Answer\n",
      "cell prediction solutions, such as TAPAS(Herzig\n",
      "et al., 2020) and the RCI (Glass et al., 2020) model.\n",
      "The current state-of-the-art RCI model exploits a\n",
      "transformer-based framework. Instead of retrieving\n",
      "the table cells directly for any given question-table\n",
      "pairs, the RCI model identiﬁes the most relevant\n",
      "columns and rows independently and locates the\n",
      "intersection table cells as the ﬁnal answers.\n",
      "End-to-End Table QA Sun et al. (2016) pub-\n",
      "lishes the ﬁrst end-to-end table cell search frame-\n",
      "work. This work leverages the semantic relations\n",
      "between cells and maps queries to table cells with\n",
      "relational chains. The DTR model (Herzig et al.,\n",
      "2021) addresses the end-to-end Table QA problem\n",
      "with a table retriever and a TAPAS-based reader\n",
      "model. Later, the CLTR model (Pan et al., 2021)\n",
      "introduces a similar two-step solution, using BM25\n",
      "as the retriever. The model re-ranks the BM25\n",
      "results and locates the table cells using the RCI\n",
      "scores. Recently, Chen et al. (2021) proposes a\n",
      "new task for QA over both free text and tables\n",
      "and provides a solution including a retriever with\n",
      "early fusion techniques and a cross-block reader.\n",
      "In addition, the open-domain OTT-QA benchmark\n",
      "is released to evaluate models for end-to-end QA\n",
      "over text and table.\n",
      "3 The End-to-End Table QA with T-RAG\n",
      "The overall architecture of T-RAG is illustrated in\n",
      "Figure 1. In this example, we encode the questions\n",
      "“who was the editor for Ikar?” using the query en-\n",
      "coder and pre-process the tables, e.g., T 1and T 2,\n",
      "from the table corpus for encoding. The encoded ta-\n",
      "bles are later indexed into the Approximate Nearest\n",
      "Neighbors (ANN) data structure for querying. The\n",
      "encoded question is appended to each table before\n",
      "inputting it to the BART-based generator for answer\n",
      "prediction. The DPR and the RAG components are\n",
      "trained jointly without explicitly considering the\n",
      "table-level ground truth.\n",
      "Setup We deﬁne the one-step, end-to-end Table\n",
      "QA task as follows. Given a massive corpus C\n",
      "of tables tiand any natural language question qi,\n",
      "we train a model to directly generate answer to qi\n",
      "from the table cell without any intermediate steps.\n",
      "Labeled datasets are available to us with ground\n",
      "truth samples in the format of fqi; ti; aigwhere aiANNIndexNatural LanguageQuestionWhowastheeditorforIkar?SeriesYears…EditorIvan Zourine1979…Magic-StripSteve Severin1981–2003…Glénat…………Ikar1995–1997…Glénat…………QueryEncoder\n",
      "TableEncoderPierre Makyo at a book fair in Paris, France, in March 2008.BornJuly 16, 1952(age69)NationalityFrench……TableCorpus\n",
      "…Generator!1!2AnswersGlénatCitroënMagic-Strip…DPRRAGSeries[sep]Ivan Zourine[sep]Title[sep]Year……Figure 1: An overview of T-RAG, a model trained end-to-end to directly locate answers from table corpus.\n",
      "stands for the answers.\n",
      "Table pre-processing is implemented before the\n",
      "training. We process the tables tiinto a structure-\n",
      "preserving format, where: (1) column headers are\n",
      "appended before cell values, separated by a special\n",
      "symbol “ j”; and (2) the separator “*” is appended\n",
      "to the end of each row; (3) for the tables with addi-\n",
      "tional information such as titles, we append them\n",
      "in front of the tables. The tables are segmented\n",
      "into the length of 512 tokens for training. For each\n",
      "question, we retrieve hard negatives from the cor-\n",
      "pusCand use them as additional negative samples\n",
      "to enhance the T-RAG training.\n",
      "Soft Hard Negatives We implement a BM25-\n",
      "based hard negative mining for T-RAG. For each\n",
      "question, we ﬁrst retrieve a pool of the most rel-\n",
      "evant tables from the corpus using BM25. From\n",
      "the table pool, we discard the ground truth table.\n",
      "The top-ranked, non-positive tables are used as the\n",
      "hard negative candidates. In the training process,\n",
      "instead of using the top 1 negative table, we exploit\n",
      "a soft hard negative mechanism, where we select\n",
      "the hard negative at random from the top knegative\n",
      "tables.\n",
      "RAG For the implementation of RAG, we jointly\n",
      "train a DPR-based retriever and a BART-based gen-\n",
      "erator. We index the tables in Cusing a keyword-\n",
      "based search engine, Anserini1, to harvest the hard\n",
      "negative training samples using BM25. Later, T-\n",
      "RAG exploits BERT BASE to encode questions\n",
      "along with the ground truth table and the hard neg-\n",
      "ative tables. To train RAG, T-RAG employs the\n",
      "answer-level ground truth and use a Seq2Seq gen-\n",
      "1https://github.com/castorini/anserinierator, the BART LARGE model, for answer predic-\n",
      "tions. The previously encoded tables are indexed\n",
      "with the open-source FAISS (Johnson et al., 2017)\n",
      "library into the ANN data structure for querying.\n",
      "The encoded questions are concatenated to each\n",
      "of the top retrieved tables and used as a prompt to\n",
      "generate the answer. More concretely, the gener-\n",
      "ator predicts probability distributions for possible\n",
      "answer candidates as the next token. The probabil-\n",
      "ity distributions are later marginalized to produce\n",
      "a single weighted sequence probability for each\n",
      "answer candidate. Finally, a standard beam search\n",
      "decoder (Sutskever et al., 2014) is used to identify\n",
      "the most relevant candidates as the ﬁnal answers to\n",
      "the questions at test time. Along with the answers,\n",
      "our model can also return the relevant table ticon-\n",
      "taining the correct answers from Cfor evaluation\n",
      "and annotation purposes.\n",
      "4 Experiments\n",
      "Data We validate T-RAG on two open-domain\n",
      "benchmarks, NQ-TABLES andE2E_WTQ . NQ-\n",
      "TABLES is the table subset of the Natural Ques-\n",
      "tions dataset (Kwiatkowski et al., 2019), with a\n",
      "table corpus extracted from the English Wikipedia\n",
      "articles and samples in the fq; T; a gformat, where\n",
      "q,T, and adenote question, ground truth table,\n",
      "and answer, respectively. E2E_WTQ contains the\n",
      "look-up subset of WikiTableQuestions (Pasupat\n",
      "and Liang, 2015). While a substantial amount of\n",
      "tables in NQ-TABLES are transposed infobox ta-\n",
      "bles, the E2E_WTQ only contains well-formatted\n",
      "but more complex tables. The data statistics are\n",
      "shown in Table 1.Data Train Dev Test Corpus\n",
      "NQ-TABLES 9,594 1,068 966 169,898\n",
      "E2E_WTQ 851 124 241 2,108\n",
      "Table 1: Data Statistics\n",
      "Model EM F1 Oracle EM Oracle F1\n",
      "DTR+hn 37.69 47.70 48.20 61.50\n",
      "T-RAG 43.06 50.92 50.62 63.18\n",
      "(a) End-to-end Table QA results on the test set of NQ-TABLES.\n",
      "Model MRR Hit@1\n",
      "CLTR 0.5503 0.4675\n",
      "T-RAG 0.5923 0.5065\n",
      "(b) End-to-end Table QA results on the test set of E2E_WTQ.\n",
      "Table 2: Experimental results on End-to-end Table QA.\n",
      "Experimental Settings In the experiments, we\n",
      "ﬁrst encode the questions and tables using\n",
      "BERT BASE , and later jointly train the DPR-based\n",
      "retriever and the Seq2Seq generator of RAG. For\n",
      "the experiments, we set: (1) training batch size =\n",
      "128; (2) number of epochs = 2; (3) learning rate =\n",
      "3e-5; and (4) gradient accumulation steps = 64.\n",
      "Evaluation metrics: Following the evaluation\n",
      "script in SQUAD (Rajpurkar et al., 2016), we eval-\n",
      "uate end-to-end Table QA using exact match (EM)\n",
      "and token F1 metrics for NQ-TABLES. The ac-\n",
      "curacy for the top 1 returned answer and mean\n",
      "reciprocal rank (MRR) are used to measure the per-\n",
      "formance on E2E_WTQ. We also evaluate T-RAG\n",
      "on the table retrieval task for a fair comparison\n",
      "with existing work. We utilize the original met-\n",
      "rics in Herzig et al. (2021) and Pan et al. (2021),\n",
      "with recall (R) for NQ-TABLES, and precision (P),\n",
      "normalized discounted gain (N), and mean average\n",
      "precision (MAP) for E2E_WTQ.\n",
      "Experimental Results We compare the end-to-\n",
      "end Table QA performance of T-RAG against the\n",
      "state-of-the-art DTR and CLTR models in Table 2.\n",
      "We ﬁnd T-RAG yields better results than the previ-\n",
      "ous best models for both datasets with all evalua-\n",
      "tion metrics.\n",
      "To further validate T-RAG against the existing\n",
      "models, we also evaluate the model performance on\n",
      "table retrieval. The table retrieval results for NQ-\n",
      "TABLES and E2E_WTQ are shown in Table 3a\n",
      "and 3b, respectively. The results indicate that T-\n",
      "RAG outperforms the simple baselines models such\n",
      "as BM25, as well as the strong state-of-the-art mod-\n",
      "els in the experiments.\n",
      "Qualitative Analysis We further evaluate the\n",
      "table retrieval results on NQ-TABLES. We no-Model R@1 R@10 R@50\n",
      "BM25 16.77 40.06 58.39\n",
      "DTR+hn 42.42 81.13 92.56\n",
      "T-RAG 46.07 85.40 95.03\n",
      "(a) Table retrieval results on the test set of NQ-TABLES.\n",
      "Model P@5 P@10 N@5 N@10 MAP\n",
      "BM25 0.5938 0.6587 0.5228 0.5356 0.4704\n",
      "CLTR 0.7437 0.8735 0.6915 0.7119 0.5971\n",
      "T-RAG 0.7806 0.8943 0.7250 0.7467 0.6404\n",
      "(b) Table retrieval results on the test set of E2E_WTQ.\n",
      "Table 3: Experimental results on Table Retrieval.\n",
      "tice that the DPR-based baseline of our approach\n",
      "achieves 43.89 for R@1 and 81.57 for R@10; both\n",
      "outperform the state-of-the-art DTR results. In\n",
      "addition, the retrieval performance is further im-\n",
      "proved with the more effective end-to-end RAG\n",
      "training. To validate the effectiveness of our soft\n",
      "hard negative technique, we test the method on the\n",
      "E2E_WTQ dataset. Instead of using the top 1 nega-\n",
      "tive table from the BM25 results, we set k= 3and\n",
      "achieve a 27.17% absolute gain for Hit@1 accuracy\n",
      "in the end-to-end Table QA task.\n",
      "Besides, we perform thorough error analysis on\n",
      "on E2E_WTQ and ﬁnd that over 21% of the errors\n",
      "come from questions that involve numerical values.\n",
      "The ﬁnding indicates that understanding different\n",
      "types of numbers remains a challenge in Table QA.\n",
      "5 Conclusion and Future Work\n",
      "In this paper, we present a novel Table QA model\n",
      "that achieves state-of-the-art performance on recent\n",
      "benchmarks. Instead of training a retriever and\n",
      "a reader model independently, T-RAG uniﬁes the\n",
      "procedure into a single pipeline of only one training\n",
      "step, which reduces the error accumulations from\n",
      "two separate models. In the experiments, T-RAG\n",
      "outperforms the current best models for end-to-\n",
      "end Table QA. We additionally demonstrate the\n",
      "advantages of T-RAG with the table retrieval task,\n",
      "and T-RAG beats the existing numbers on both\n",
      "benchmarks.\n",
      "In the future, we plan to validate T-RAG on\n",
      "domain-speciﬁc datasets, such as AIT-QA and TAT-\n",
      "QA (Katsis et al., 2021; Zhu et al., 2021) and\n",
      "extend the model to solve multi-modal QA prob-\n",
      "lems, with the corpus containing both tables and\n",
      "passages, as presented in the OTT-QA and Hybrid-\n",
      "QA benchmarks (Chen et al., 2020a,b). To further\n",
      "improve the model performance, we also plan to\n",
      "investigate algorithms to better understand numeric\n",
      "values.References\n",
      "Chandra Bhagavatula, Thanapon Noraset, and Doug\n",
      "Downey. 2013. Methods for exploring and min-\n",
      "ing tables on wikipedia. Proceedings of the ACM\n",
      "SIGKDD Workshop on Interactive Data Exploration\n",
      "and Analytics .\n",
      "Michael J Cafarella, Alon Halevy, and Nodira Khous-\n",
      "sainova. 2009. Data integration for the rela-\n",
      "tional web. Proceedings of the VLDB Endowment ,\n",
      "2(1):1090–1101.\n",
      "Michael J Cafarella, Alon Halevy, Daisy Zhe Wang, Eu-\n",
      "gene Wu, and Yang Zhang. 2008. Webtables: ex-\n",
      "ploring the power of tables on the web. Proceedings\n",
      "of the VLDB Endowment , 1(1):538–549.\n",
      "Wenhu Chen, Ming-Wei Chang, Eva Schlinger,\n",
      "William Yang Wang, and William W Cohen. 2020a.\n",
      "Open question answering over tables and text. In\n",
      "International Conference on Learning Representa-\n",
      "tions .\n",
      "Wenhu Chen, Ming-Wei Chang, Eva Schlinger,\n",
      "William Yang Wang, and William W. Cohen. 2021.\n",
      "Open question answering over tables and text. In\n",
      "International Conference on Learning Representa-\n",
      "tions .\n",
      "Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan\n",
      "Xiong, Hong Wang, and William Yang Wang. 2020b.\n",
      "Hybridqa: A dataset of multi-hop question answer-\n",
      "ing over tabular and textual data. In Proceedings of\n",
      "the 2020 Conference on Empirical Methods in Nat-\n",
      "ural Language Processing: Findings , pages 1026–\n",
      "1036.\n",
      "Michael Glass, Mustafa Canim, Alﬁo Gliozzo, Saneem\n",
      "Chemmengath, Rishav Chakravarti, Avi Sil, Feifei\n",
      "Pan, Samarth Bharadwaj, and Nicolas Rodolfo\n",
      "Fauceglia. 2020. Capturing row and column seman-\n",
      "tics in transformer based question answering over ta-\n",
      "bles. Proceedings of the Annual Conference of the\n",
      "North American Chapter of the Association for Com-\n",
      "putational Linguistics (NAACL-HLT2020) .\n",
      "Michael Glass, Gaetano Rossiello, Md Faisal Mahbub\n",
      "Chowdhury, and Alﬁo Gliozzo. 2021. Robust re-\n",
      "trieval augmented generation for zero-shot slot ﬁll-\n",
      "ing. In Proceedings of the 2021 Conference on Em-\n",
      "pirical Methods in Natural Language Processing ,\n",
      "pages 1939–1949, Online and Punta Cana, Domini-\n",
      "can Republic. Association for Computational Lin-\n",
      "guistics.\n",
      "Jonathan Herzig, Thomas Müller, Syrine Krichene, and\n",
      "Julian Martin Eisenschlos. 2021. Open domain ques-\n",
      "tion answering over tables via dense retrieval. arXiv\n",
      "preprint arXiv:2103.12011 .\n",
      "Jonathan Herzig, Pawel Krzysztof Nowak, Thomas\n",
      "Müller, Francesco Piccinno, and Julian Eisensch-\n",
      "los. 2020. TaPas: Weakly supervised table pars-\n",
      "ing via pre-training. In Proceedings of the 58th An-\n",
      "nual Meeting of the Association for ComputationalLinguistics , pages 4320–4333, Seattle, Washington,\n",
      "United States. Association for Computational Lin-\n",
      "guistics.\n",
      "Ernesto Jiménez-Ruiz, Oktie Hassanzadeh, Vasilis\n",
      "Efthymiou, Jiaoyan Chen, and Kavitha Srinivas.\n",
      "2020. Semtab 2019: Resources to benchmark tab-\n",
      "ular data to knowledge graph matching systems. In\n",
      "ESWC , pages 514–530.\n",
      "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.\n",
      "Billion-scale similarity search with gpus. arXiv\n",
      "preprint arXiv:1702.08734 .\n",
      "Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\n",
      "Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\n",
      "Wen-tau Yih. 2020. Dense passage retrieval for\n",
      "open-domain question answering. In Proceedings of\n",
      "the 2020 Conference on Empirical Methods in Nat-\n",
      "ural Language Processing (EMNLP) , pages 6769–\n",
      "6781, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Yannis Katsis, Saneem Chemmengath, Vishwajeet\n",
      "Kumar, Samarth Bharadwaj, Mustafa Canim,\n",
      "Michael Glass, Alﬁo Gliozzo, Feifei Pan, Jay-\n",
      "deep Sen, Karthik Sankaranarayanan, and Soumen\n",
      "Chakrabarti. 2021. Ait-qa: Question answering\n",
      "dataset over complex tables in the airline industry.\n",
      "Bogdan Kosti ´c, Julian Risch, and Timo Möller. 2021.\n",
      "Multi-modal retrieval of tables and texts using tri-\n",
      "encoder models.\n",
      "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\n",
      "ﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\n",
      "Danielle Epstein, Illia Polosukhin, Matthew Kelcey,\n",
      "Jacob Devlin, Kenton Lee, Kristina N. Toutanova,\n",
      "Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\n",
      "Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\n",
      "ral questions: a benchmark for question answering\n",
      "research. Transactions of the Association of Compu-\n",
      "tational Linguistics .\n",
      "Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\n",
      "jan Ghazvininejad, Abdelrahman Mohamed, Omer\n",
      "Levy, Veselin Stoyanov, and Luke Zettlemoyer.\n",
      "2020a. BART: Denoising sequence-to-sequence pre-\n",
      "training for natural language generation, translation,\n",
      "and comprehension. In Proceedings of the 58th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics , pages 7871–7880, Online. Association\n",
      "for Computational Linguistics.\n",
      "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\n",
      "Petroni, Vladimir Karpukhin, Naman Goyal, Hein-\n",
      "rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\n",
      "täschel, Sebastian Riedel, and Douwe Kiela. 2020b.\n",
      "Retrieval-augmented generation for knowledge-\n",
      "intensive nlp tasks. In Advances in Neural Infor-\n",
      "mation Processing Systems , volume 33, pages 9459–\n",
      "9474. Curran Associates, Inc.\n",
      "Kevin Lin, Ben Bogin, Mark Neumann, Jonathan Be-\n",
      "rant, and Matt Gardner. 2019. Grammar-based neu-\n",
      "ral text-to-sql generation.Ying Liu, Kun Bai, Prasenjit Mitra, and C Lee Giles.\n",
      "2007. Tableseer: automatic table metadata extrac-\n",
      "tion and searching in digital libraries. In Proceed-\n",
      "ings of the 7th ACM/IEEE-CS joint conference on\n",
      "Digital libraries , pages 91–100.\n",
      "Barlas Oguz, Xilun Chen, Vladimir Karpukhin,\n",
      "Stan Peshterliev, Dmytro Okhonko, Michael\n",
      "Schlichtkrull, Sonal Gupta, Yashar Mehdad, and\n",
      "Scott Yih. 2021. Unik-qa: Uniﬁed representations\n",
      "of structured and unstructured knowledge for\n",
      "open-domain question answering.\n",
      "Feifei Pan, Mustafa Canim, Michael Glass, Alﬁo\n",
      "Gliozzo, and Peter Fox. 2021. CLTR: An end-to-\n",
      "end, transformer-based system for cell-level table re-\n",
      "trieval and table question answering. In Proceed-\n",
      "ings of the 59th Annual Meeting of the Association\n",
      "for Computational Linguistics and the 11th Interna-\n",
      "tional Joint Conference on Natural Language Pro-\n",
      "cessing: System Demonstrations , pages 202–209,\n",
      "Online. Association for Computational Linguistics.\n",
      "Panupong Pasupat and Percy Liang. 2015. Composi-\n",
      "tional semantic parsing on semi-structured tables.\n",
      "Pallavi Pyreddy and W Bruce Croft. 1997. Tintin: A\n",
      "system for retrieval in text tables. In Proceedings of\n",
      "the second ACM international conference on Digital\n",
      "libraries , pages 193–200.\n",
      "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\n",
      "Percy Liang. 2016. SQuAD: 100,000+ questions for\n",
      "machine comprehension of text. In Proceedings of\n",
      "the 2016 Conference on Empirical Methods in Natu-\n",
      "ral Language Processing , pages 2383–2392, Austin,\n",
      "Texas. Association for Computational Linguistics.\n",
      "Roee Shraga, Haggai Roitman, Guy Feigenblat, and\n",
      "Mustafa Canim. 2020a. Ad hoc table retrieval using\n",
      "intrinsic and extrinsic similarities. In Proceedings of\n",
      "The Web Conference 2020 , pages 2479–2485.\n",
      "Roee Shraga, Haggai Roitman, Guy Feigenblat, and\n",
      "Mustafa Cannim. 2020b. Web table retrieval using\n",
      "multimodal deep learning. In Proceedings of the\n",
      "43rd International ACM SIGIR Conference on Re-\n",
      "search and Development in Information Retrieval ,\n",
      "SIGIR ’20, page 1399–1408, New York, NY , USA.\n",
      "Association for Computing Machinery.\n",
      "Huan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su,\n",
      "and Xifeng Yan. 2016. Table cell search for question\n",
      "answering. In Proceedings of the 25th International\n",
      "Conference on World Wide Web , pages 771–782.\n",
      "Yibo Sun, Zhao Yan, Duyu Tang, Nan Duan, and Bing\n",
      "Qin. 2019. Content-based table retrieval for web\n",
      "queries. Neurocomputing , 349:183–189.\n",
      "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\n",
      "Sequence to sequence learning with neural networks.\n",
      "InAdvances in neural information processing sys-\n",
      "tems, pages 3104–3112.Yalin Wang and Jianying Hu. 2002. A machine learn-\n",
      "ing based approach for table detection on the web.\n",
      "InProceedings of the 11th International Conference\n",
      "on World Wide Web , WWW ’02, page 242–250, New\n",
      "York, NY , USA. Association for Computing Machin-\n",
      "ery.\n",
      "Xiaojun Xu, Chang Liu, and Dawn Song. 2018. SQL-\n",
      "Net: Generating structured queries from natural lan-\n",
      "guage without reinforcement learning.\n",
      "Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\n",
      "bastian Riedel. 2020. TaBERT: Pretraining for joint\n",
      "understanding of textual and tabular data. In Pro-\n",
      "ceedings of the 58th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics , pages 8413–\n",
      "8426, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\n",
      "Dongxu Wang, Zifan Li, James Ma, Irene Li,\n",
      "Qingning Yao, Shanelle Roman, Zilin Zhang,\n",
      "and Dragomir Radev. 2018. Spider: A large-\n",
      "scale human-labeled dataset for complex and cross-\n",
      "domain semantic parsing and text-to-SQL task.\n",
      "Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,\n",
      "Qingyun Wang, Heng Ji, and Meng Jiang. 2020.\n",
      "A survey of knowledge-enhanced text generation.\n",
      "arXiv preprint arXiv:2010.04389 .\n",
      "Shuo Zhang and K. Balog. 2018a. Ad hoc table re-\n",
      "trieval using semantic similarity. Proceedings of the\n",
      "2018 World Wide Web Conference .\n",
      "Shuo Zhang and Krisztian Balog. 2018b. Ad hoc ta-\n",
      "ble retrieval using semantic similarity. In Proceed-\n",
      "ings of the 2018 World Wide Web Conference , pages\n",
      "1553–1562.\n",
      "Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao\n",
      "Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and\n",
      "Tat-Seng Chua. 2021. TAT-QA: A question answer-\n",
      "ing benchmark on a hybrid of tabular and textual\n",
      "content in ﬁnance. In Proceedings of the 59th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics and the 11th International Joint Confer-\n",
      "ence on Natural Language Processing (Volume 1:\n",
      "Long Papers) , pages 3277–3287, Online. Associa-\n",
      "tion for Computational Linguistics.\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            text += reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "def convert_folder_to_list(folder_path):\n",
    "    document_list = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            text = pdf_to_text(pdf_path)\n",
    "            document_list.append(text)\n",
    "    return document_list\n",
    "\n",
    "pdf_folder_path = \"./data\"\n",
    "document_list = convert_folder_to_list(pdf_folder_path)\n",
    "\n",
    "# Print the list of document contents\n",
    "for i, document in enumerate(document_list, start=1):\n",
    "    print(f\"Document {i}:\\n{document}\\n{'='*50}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e156039e-619a-4d32-9c4a-7bd7a04e8ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x2e62c21ec20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94a3dcdd-ae23-4f20-b144-9fd89db2545d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Resource -efficient FIR Filter Design Based on an \\nRAG Improve d Algorithm  \\n \\nMengwei Hu  \\nNational Demonstration Center for \\nExperimental Physics and Education  \\nSchool of Physics and Technology, \\nWuhan University  \\nWuhan, China  \\nhumengv@whu.edu.cn  Xianyang Jiang*  \\nNational Demonstration Center for \\nExperimental Physics and Education  \\nSchool of Physics and Technology, \\nWuhan University  \\nWuhan, China  \\njiang@whu. edu.cn  Zhengxiong Li  \\nNational Demonstration Center for \\nExperimental Physics and Education  \\nHongyi Honor College of Wuhan \\nUniversity  \\nWuhan, China  \\nli_zhengxiong@whu.edu.cn  \\n \\nAbstract —In modern digital filter chip design, efficient \\nresource u tilization is a hot topic. Due to linear phase \\ncharacteristics of FIR filters, a pulsed fully parallel structure \\ncan be applied to attack the problem . In order to further reduce \\nhardware resource consumption especially caused by \\nmultiplication function, an improved RAG algorithm is \\nproposed. Filters with different orders and for different \\nalgorithms are compared, and the experimental results show \\nthat the improved RAG algorithm is excellent in terms of logic \\nresource utilization, resource allocation, running  speed, and \\npower consumption under  different application scenarios. The \\nproposed algorithm invokes a better circuit structure for FIR \\nfilter , it gives full play to resource allocation strategy and \\nreduces logic resource consumption. The proposed circuit i s \\nfaster and more stable, and suitable for a variety of complex \\napplication scenarios.  \\nKeywords —FIR filter, pulsed fully parallel structure, \\nimproved RAG algorithm, resource allocation strategy.  \\nI. INTRODUCTION  \\nFIR filters have a wide range of applications in  \\ncommunication, audio processing, image processing, and \\nother fields. With blowout type increase ment  of portable \\ndevices and Internet of Things, there is an increasing demand \\nfor low power consumption and small size in this field. FIR \\nfilter design also ev olves in this tide to meet requirements of \\nembedded systems. Efficient FIR filter design methods have \\nbeen continuously explored to reduce computational cost and \\nproduct cost, which may involve new optimization algorithms, \\napproximation techniques, and res ource allocation strategies.  \\nFIR filters have an important linear phase property, which \\nallows us to exploit the symmetry of coefficients to build both \\nserial and parallel structures. Compared to IIR filters, FIR \\nfilters have many significant advantages su ch as bounded \\ninput and output stability, phase linearity, and low coefficient \\nsensitivity, which makes it more suitable under many \\napplications [1]. However, FIR filters involve a large number \\nof arithmetic operations , which limits their processing speed \\n[2]. In order to overcome this limitation, a fully parallel \\nstructure can be exploited which allows a single filtering \\noperation to perform multiple multiplications simultaneously \\nto improve performance. Compared to fully parallel FIR \\nfilters, adoption of improved Reduced Adder Graph (RAG) \\nalgorithm [3] can significantly reduce hardware consumption \\nby exploiting redundancy between coefficients [4]. Our work focuses on how to efficiently implement FIR filters with fixed \\ncoefficients. In the proposed design, fully parallel structure \\nand RAG algorithm characteristics have been utilized to \\neffectively reduce hardware cost. Meanwhile, a better \\nresource allocation strategy is taken to further improve FIR \\nfilter implementation.  \\nII. PULSED F ULLY PARALLEL FIR  FILTERS  \\nFIR filters differ from IIR filters in that their impulse \\nresponse can be expressed in terms of a finite number of \\nsampled values and can be described by a difference equation \\n(1), where N is the number of filters tap coefficients and x(n) \\nis the input time s eries [5].  \\n   𝑦(𝑛)=∑ℎ(𝑘)𝑥(𝑛−𝑘)𝑁−1\\n𝑘=0       (\\uf031) \\nIn order to implement an efficient specific circuit for FIR, \\nthe \"pulsation\" structure, originally proposed by H.T. Kung, \\nrepresents a parallel pipelined approach for high -speed signal \\nprocessing and data processing. This architecture is known for \\na number  of advantages such as modularity, regularity, local \\nlinking, and high degree of pipelining. In pulsation \\narchitecture, Process Element (PE) constitutes a \\nmultiprocessor system and these PEs work together in a \\nsynchronized manner so that this architecture offers \\nsignificant performance in processing large -scale data. The \\nhardware structure of a pulsation FIR filter is shown in Fig. 1, \\nand N PEs are required to accomplish one such operation.  \\n \\nFig. 1  Hardware structure of pulsation FIR filter  \\nAs for an FIR fil ter with symmetric coefficients, its linear \\nphase property can be further utilized to reduce the number of \\nPEs by pre -addition. For example, a hardware structure of \\neven -symmetric filter is shown in Fig. 2, and it is clear that the \\nnumber of PEs is reduced  to N/2 for the same N tap \\ncoefficients.   \\nZ -1 Z -1 Z -1Z -1\\nout\\n……\\nPE1din\\nPE2 PE3 PEn0h(0) h(1) h(2) h(n-1)Fig. 2  Structure of pulsation FIR filter with even symmetry of coefficients  \\nIII. FILTER IMPLEMENTATION OF RAG  IMPROVEMENT \\nALGORITHM  \\nA. RAG Improved Algorithm  \\nPulsation filter designs are usually based on PEs that have \\nidentical coefficients. Since such coefficients are constant and \\nshift operations can be easily implemented in hardware, \\nmultiplication can be replaced using shift, add, and subtract \\noperations to reduc e resource consumption of multiplier. The \\nconcept of u sing an addition tree in a multiplier was first \\nintroduced by Bull, who advocated the implementation of \\nmultiplication by constructing a structural graph consisting of \\nsimple addition and shift operations, A.G. Dempster and M.D. \\nMacleod proposed RAG algori thm [ 6], the core idea of this \\nalgorithm is to use an equivalent structure to convert all \\ncoefficients to bases, and at the same time, introduce \\nsubtraction structure to make all intermediate values positive, \\nwhich greatly simplifies the structure of addit ion tree. The \\nredundancy relationship between coefficients is also utilized \\nto reduce logical depth in order to cut down the total resource \\nconsumption [ 7,8]. \\nIn addition, the RAG algorithm introduces a concept \\ncalled adder depth \"cost\". Despite obvious ad vantages of \\nRAG algorithm, its disadvantage lies in the need to calculate \\ncost value for each coefficient. For smaller coefficients, cost \\nvalue can be obtained directly by looking up the table, but \\nwhen coefficients are larger, cost value calculation becom es \\nmore difficult [1 1]. \\nIn order to address shortcomings of the RAG algorithm in \\ncircuit design, an improved algorithm is proposed, which \\ncombines the advantages of pulsation structure fully parallel \\nFIR filter and the RAG algorithm structure FIR filter.  \\nLet \"coeff\" be all filter coefficients to be realized, \"coeff -\\nr\" is the set of smaller coefficients, \"coeff -s\" is the set of larger \\ncoefficients. \"cost -n\" (n=1,2,3,4) is the set of coefficients with \\ndifferent adder depths, \"cost -o\" is the set of coefficient s with \\nother adder depths, and the improved algorithm [ 9-12] are as \\nfollows:  \\n⚫ Take the absolute values of all coefficients and store the \\nresults in \"coeff\" set;  \\n⚫ Remove duplicate coefficients and coefficients with \\nvalue 2n, and the number of remaining coefficients is \\ndenoted as N;  \\n⚫ The smaller coefficients are deposited into set \"coeff -r\", and the number of coefficients deposited is N/2 or (N -\\n1)/2; \\n⚫ Deposit the remaining larger coefficients into set \\n\"coeff -s\"; \\n⚫ Divide the even numbers in the \" coeff -r\" set by 2n to \\nobtain the base;  \\n⚫ Look up the table to get the depth of adder \\ncorresponding to each base number, store these \\ncoefficients in \"cost -n\" set, and store the coefficients \\nthat cannot be categorized by the table in \"cost -o\" set;  \\n⚫ Realize coef ficients in \"cost -1\" set;  \\n⚫ Check the sum/difference of coefficients in all realized \\ncost sets, realize the coefficients in higher cost sets by \\nthe sum/difference of coefficients and the realized \\ncoefficients, and finally realize the coefficients in \\n\"cost -o\" set; \\n⚫ Realize the coefficients in \"coeff -s\" set according to \\nthe hardware structure of pulsation FIR filter with \\nsymmetric coefficients.  \\nB. Implementation Example  \\nTaking a 64th order filter as an example, Fs=250KHz \\nand Fc=20KHz, the filter coefficients after  quantization and \\nrounding [1 3] are shown in Table 1, and due to the symmetry \\nof the coefficients of the FIR filters, only coefficients with \\n0≤n≤31 need to be di scussed here.  \\nTABLE I.  FILTER COEFFICIENTS , H(N) = H(63-N), 32≤N≤63 \\nh(0)=219  h(1)=137  h(2)=162  h(3)=174  \\nh(4)=168  h(5)=137  h(6)=79  h(7)= -9 \\nh(8)= -127 h(9)= -269 h(10)= -428 h(11)= -592 \\nh(12)= -747 h(13)= -875 h(14)= -957 h(15)= -972 \\nh(16)= -903 h(17)= -733 h(18)= -450 h(19)= -49 \\nh(20)=470  h(21)=1100  h(22)=1825  h(23)=2622  \\nh(24)=3462  h(25)=4311  h(26)=5134  h(27)=5891  \\nh(28)=6548  h(29)=7072  h(30)=7437  h(31)=7624  \\nFirstly, all the coefficients in the above table are taken as \\nabsolute values, and duplicates  and numbers divisible by 2n \\nare removed, and then the coefficients are divided into the two \\nparts, one part of the coefficients is smaller, which is easy to \\noptimize using the RAG improvement algorithm, and the \\ncoefficients are stored into the \"coeff -r\" set = [9, 49, 79, 127, \\n137, 1 62, 168, 174, 219, 269, 428, 450, 470, 592, 733], and \\none part is larger, and the corresponding coefficients can be \\nstored according to the pulsation structure and the symmetry \\nof the coefficients. 162, 168, 174, 219, 269, 428, 450, 470, \\n592, 733], part s of the coefficients are larger, according to the \\npulsation structure and the symmetry of the coefficients, the \\ncorresponding input signals can be pre -added or subtracted, \\nand then multiplied by the filter coefficients, which are \\ndeposited in the \"coeff -s\" collection. The coefficients are \\nstored in the set of \"coeff -s\" = [747, 875, 957, 972, 903, 1100, \\n1825, 2622, 3462, 4311, 5134, 5891, 6548, 7072, 7437, 7624].  \\nCheck the adder depth table to categorize the coefficients \\nand store the coefficients in the corre sponding sets:  \\n\"cost -1\" set = [9, 127];   \\nout……\\nPE1din\\nPE2 PE3 PEn0h(0) h(1) h(2) h(n-1)Z -1Z -1\\nZ -1 Z -1Z -1 Z -1Z -1 ……\"cost -2\" set = [49, 79, 137, 162, 168];  \\n\"cost -3\" set = [174, 219];  \\n\"cost -o\" set = [269, 428, 450, 470, 592, 733].  \\nFirst realize the coefficients of cost -1: \\n {x9 = xin<<3 + xin,   \\nx127 = xin<<7 – xin, \\nContinue to reali ze the cost -2 factor : \\n \\n{    x49 = x9<<4 + x9 + xin<<2,        \\nx79 = x127 – x49 + xin,                           \\nx137 = x127 + x9 + xin,                         \\nx162 = x137 + x9 + xin<<4,             \\nx168 = x162 + xin<<2 + xin<<1, \\nThe cost -3 factor is then reali zed: \\n {x174 = x127 + x49 – xin<<1,\\nx219 = x168 + x49 – xin<<1, \\nThe flexible use of multiplexing makes it possible to use \\nonly two layers of adder depth for the coefficients of cost -3 as \\nwell, and then gradually implement the other coefficients at \\nthe end. The final result of the RAG improvement algorithm \\ndesign is as follows:  \\n {x9 = xin<<3 + xin,   \\nx127 = xin<<7 – xin, \\n \\n{    x49 = x9<<4 + x9 + xin<<2,        \\nx79 = x127 – x49 + xin,                           \\nx137 = x127 + x9 + xin,                         \\nx162 = x137 + x9 + xin<<4,             \\nx168 = x162 + xin<<2 + xin<<1, \\n {x174 = x127 + x49 – xin<<1,\\nx219 = x168 + x49 – xin<<1, \\n {x450 = x428 + x9<<1 + xin<<2,\\nx470 = x450 + x9<<1 + xin<<1,\\nx592 = x450 + x269 – x127,                 \\nx733 = x592 + x137 + xin<<2,       \\nComparing to the pre -optimization design, which uses a \\ntotal of  28 adders, performs 7 shift operations and keeps the \\nadder depth at 2 and below, saves more than half of the total \\nnumber of adders and a large number of shift operations, while \\nalso reducing the adder depth, compared to the unimproved \\nalgorithm.  \\nIV. COMPARI SON OF HARDWARE SYNTHESIS RESULTS  \\nThe consumption of FPGA hardware resources can be \\nmeasured by FPGA LUT resources, FF register resources, and \\nDSP resources, and the hardware performance can be \\nmeasured by power consumption and device junction \\ntemperature [9,14]. In our design , we adopt a  Virtex -7 series \\nxc7vx485tffg1157 -1 FPGA, and implement and compare \\npulsed fully parallel structure, traditional RAG algorithmic \\nstructure, and RAG improved algorithmic structure according \\nto these measuements . The realizat ion results are shown in \\nTable 2 and Figure 3.  \\nTABLE II.  COMPARISON OF 64TH ORDER FILTER HARDWARE  Resource \\nperformance \\nindicators  different algorithmic structures  \\nPulsed Fully \\nParallel  RAG \\nalgorithm  RAG Improved \\nAlgorithm  \\nLUT  574 4956 934 \\nFF 1286 528 904 \\nDSP 4 0 2 \\nPower( W) 32.8 234.7 38.6 \\nTem(℃) 70.8 125.0 79 \\n \\n \\nFig. 3  Comprehensive comparison of 64th order filter hardware  \\nFrom the results, we can see that the pulsation structure \\nfully parallel filter uses the most DSP and FF and the least \\nLUT resources, while the conventional RAG algorithmic \\nstructure filter does not use DSP and consumes the least FF \\nresources, but it is cl ear that the RAG algorithmic structure \\nuses too much LUT resources compared  to the other two \\nstructures to the extent that the power and junction \\ntemperatures are too high to be used for practical applications. \\nThe RAG improved algorithmic structure filter reduces the \\nDSP usage by half and the FF resources by about 29.7% \\ncompared  to the pulsed fully parallel structure, while the LUT \\nresources are reduced by about 81.2% compared  to the RAG \\nalgorithmic structure while achieving low power consumption \\nand junction temperature.  \\nTable 3 and 4 show the hardware synthesis comparison of \\nthe 8th and 32nd order filters, respectively, to validate the \\nhardware synthesis results for different order filters.  \\nTABLE III.  COMPARISON OF 8TH ORDER FILTER HARDWARE  \\nResource \\nperformance \\nindicators  different algorithmic structures  \\nPulsed Fully \\nParallel  RAG \\nalgorithm  RAG Improved \\nAlgorithm  \\nLUT  141 212 185 \\nFF 203 120 222 \\nDSP 4 0 2 \\nPower( W) 41.762  33.673  36.75  \\nTem(℃) 83.4 72.1 76.4 \\nIt can be seen that the traditional RAG algorithm structure \\nhas the best integrated performance when the filter order is 8, \\nthe RAG improved algorithm structure has excellent \\nperformance, and the puls ed fully parallel structure has the \\nworst integrated performance, and there is little difference in \\nthe integrated performance of the three except for the DSP \\nresource consumption.    \\nTABLE IV.  COMPARISON OF 32ND ORDER FILTER HARDWARE  \\nResource \\nperformance \\nindicators  different algorithmic structures  \\nPulsed Fully \\nParallel  RAG \\nalgorithm  RAG Improved \\nAlgorithm  \\nLUT  358 695 555 \\nFF 679 287 538 \\nDSP 4 0 2 \\nPower( W) 21.34  24.52  19.75  \\nTem(℃) 54.8 59.3 52.6 \\n While the puls ed fully parallel structure consumes the \\nmost FF and DSP resources and the RAG algorithm structure \\nconsumes the most LUT resources when the filter order is 32, \\nthe LUT, FF, and DSP resource consumption of the RAG \\nimproved algorithm structure filter are all in the middle \\nbetween the puls ed fully parallel structure and the traditional \\nRAG algorithm filter, which are more balanced in logic \\ndistribution, and the power consumption and junction \\ntemperature are the lowest  among the three [1 5]. \\n Comprehensive comparison concluded that the puls ed \\nfully parallel structure is more suitable for higher order filters, \\nthe traditional RAG algorithm structure is  more  suitable for \\nlow order filters, and regardless of the order  numbe r, the RAG \\nimproved algorithm structure filters have excellent \\nperformance, effectively take advantage of the resource \\nallocation strategy,  and achieve low power consumption, \\nlow junction temperature , and optimal resource consumption \\nfor 32-order and 64 -order filters . Specifically, the 64th order \\nfilter reduces DSP usage and balances logic resource \\nconsumption, and stabilizes power consumption and junction \\ntemperature  as well. The proposed design  is very suitable for \\napplication s in the case of many coefficients and high \\ncomplexity.  \\nV. CONCLUSION  \\nFIR filters exists in more and more application scenarios, \\nand practical requirements are becoming more and more \\nindividualized, so novel design solutions must b e \\ncontinuously explored to meet these requirements under \\nvarious scenarios. The algorithmic structure described \\nprovides a new design scheme that is suitable for most \\napplication scenarios and is well suited when the number of \\nfilter orders is large or the  coefficients are large. FIR filter \\ndesign for improved RAG algorithm gives full play to \\nresource allocation strategy by using shift and add operations \\ninstead of a direct multiplier structure. Combining with \\npulsation fully parallel structure, both operat ion speed and \\nresource utilization efficiency are enhanced . Comparative \\nsimulation experiments demonstrate that the  improved RAG \\nalgorithm and filter structure save a large amount of logic \\nresources, meet low -power requirements, and decrease  \\ncontradiction between speed and resource -consumption as \\nwell.  \\nACKNOWLEDGMENT  \\nThis work was supported in part by National Science \\nFoundation of China under Grant 61072135, 81971702, the \\nFundamental Research Funds for the Central Universities \\nunder Grant 2042017gf0075, 20 42019gf0072, and Natural \\nScience Foundation of Hubei Province under Grant \\n2017CFB721.  REFERENCES  \\n[1] Abhijit Chandra, Sudipta Chattopadhyay,  Design of hardware efficient \\nFIR filter: A review of the state -of-the-art approaches,  Engineering \\nScience and Technolog y, an International Journal,  Volume 19, Issue 1,  \\n2016,  Pages 212 -226, ISSN 2215 -0986.  \\n[2] A. Pathan, A. H. Chandio and R. Aziz, \"An Optimization in \\nConventional Shift &Add Multiplier for Area -Efficient \\nImplementation on FPGA,\" 2022 International Conference on \\nEmerging Technologies in Electronics, Computing and \\nCommunication (ICETECC), Jamshoro, Sindh, Pakistan, 2022, pp. 1 -\\n6, doi: 10.1109/ICETECC56662.2022.10069099.  \\n[3] A. G. Dempster and M. D. Macleod, \"Use of minimum -adder \\nmultiplier blocks in FIR digital filters ,\" in IEEE Transactions on \\nCircuits and Systems II: Analog and Digital Signal Processing, vol. 42, \\nno. 9, pp. 569 -577, Sept. 1995, doi: 10.1109/82.466647.  \\n[4] U. Meyer -Baese, J. Chen, C. H. Chang and A. G. Dempster, \"A \\nComparison of Pipelined RAG -n and DA FPGA -based Multiplierless \\nFilters,\" APCCAS 2006 - 2006 IEEE Asia Pacific Conference on \\nCircuits and Systems, Singapore, 2006, pp. 1555 -1558, doi: \\n10.1109/APCCAS.2006.342540.  \\n[5] U. Meyer -Baese, \"Digital Signal Processing with Field Programmable \\nGate Arrays,\" T allahassee, USA: Springer Berlin, Heidelberg, Sept. \\n2007, doi: 10.1007/978 -3-540-72613 -5, ISSN 1860 -4862.  \\n[6] O. Gustafsson, \"A Difference Based Adder Graph Heuristic for \\nMultiple Constant Multiplication Problems,\" 2007 IEEE International \\nSymposium on Circuits  and Systems, New Orleans, LA, USA, 2007, \\npp. 1097 -1100, doi: 10.1109/ISCAS.2007.378201.   \\n[7] M. Potkonjak, M. B. Srivastava and A. P. Chandrakasan, \"Multiple \\nconstant multiplications: efficient and versatile framework and \\nalgorithms for exploring common subex pression elimination,\" in IEEE \\nTransactions on Computer -Aided Design of Integrated Circuits and \\nSystems, vol. 15, no. 2, pp. 151 -165, Feb. 1996, doi: \\n10.1109/43.486662.  \\n[8] A. G. Dempster and M. D. Macleod, \"Comments on \"Minimum \\nnumber of adders for implementi ng a multiplier and its application to \\nthe design of multiplierless digital filters\",\" in IEEE Transactions on \\nCircuits and Systems II: Analog and Digital Signal Processing, vol. 45, \\nno. 2, pp. 242 -243, Feb. 1998, doi: 10.1109/82.661661.  \\n[9] S. Mirzaei, A. Hos angadi and R. Kastner, \"FPGA Implementation of \\nHigh Speed FIR Filters Using Add and Shift Method,\" 2006 \\nInternational Conference on Computer Design, San Jose, CA, USA, \\n2006, pp. 308 -313, doi: 10.1109/ICCD.2006.4380833.  \\n[10] Jeong -Ho Han and I. -C. Park, \"Digita l filter synthesis considering \\nmultiple adder graphs for a coefficient,\" 2008 IEEE International \\nConference on Computer Design, Lake Tahoe, CA, USA, 2008, pp. \\n315-320, doi: 10.1109/ICCD.2008.4751879.  \\n[11] A. Abbaszadeh, A. Azerbaijan and K. D. Sadeghipour, \"A n ew \\nhardware efficient reconfigurable fir filter architecture suitable for \\nFPGA applications,\" 2011 17th International Conference on Digital \\nSignal Processing (DSP), Corfu, Greece, 2011, pp. 1 -4, doi: \\n10.1109/ICDSP.2011.6004958.  \\n[12] H V Kumaraswamy, AmruthKaran th P, Samarth Athreyas and Akash \\nBharadwaj B R, \"  Comparative Analysis of Different Area-efficient Fir \\nFilter Structures for Symmetric Convolutions ,\" International Journal of \\nElectrical and Electronic Engineering & Telecommunications, Vol. 3, \\nNo. 2, pp. 50 -55, April 2014.  \\n[13] Y. J. Yu and Y. C. Lim, \"Design of Linear Phase FIR Filters in \\nSubexpression Space Using Mixed Integer Linear Programming,\" in \\nIEEE Transactions on Circuits and Systems I: Regular Papers, vol. 54, \\nno. 10, pp. 2330 -2338, Oct. 2007, doi: 10. 1109/TCSI.2007.904599.  \\n[14] LI Ying,LU Weijun,YU Dunshan et al. A resource optimization \\nalgorithm for implementing FIR digital filters on FPGA[J]. Journal of \\nPeking University (Natural Science Edition),2009,45(02):222 -\\n226.doi:10.13209/j.0479 -8023.2009.034.  \\n[15] Nakka Sivaraju and S Suman, \"  Area Power and Delay Efficiency \\nEvaluation of Truncated and Modified Wallace Fir Filters ,\" \\nInternational Journal of Electrical and Electronic Engineering & \\nTelecommunications, Vol. 4, No. 4, pp. 43 -52, October 2015.  ',\n",
       " 'A Survey on Retrieval-Augmented Text Generation\\nHuayang Li~;\\x03Yixuan Su\\x7f;\\x03Deng Cai};\\x03Yan Wang|;\\x03Lemao Liu|;\\x03\\n~Nara Institute of Science and Technology\\x7fUniversity of Cambridge\\n}The Chinese University of Hong Kong|Tencent AI Lab\\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\\nthisisjcykcd@gmail.com, brandenwang@tencent.com\\nlemaoliu@gmail.com\\nAbstract\\nRecently, retrieval-augmented text generation\\nattracted increasing attention of the compu-\\ntational linguistics community. Compared\\nwith conventional generation models, retrieval-\\naugmented text generation has remarkable ad-\\nvantages and particularly has achieved state-of-\\nthe-art performance in many NLP tasks. This\\npaper aims to conduct a survey about retrieval-\\naugmented text generation. It ﬁrstly highlights\\nthe generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable ap-\\nproaches according to different tasks including\\ndialogue response generation, machine trans-\\nlation, and other generation tasks. Finally, it\\npoints out some promising directions on top of\\nrecent methods to facilitate future research.\\n1 Introduction\\nRetrieval-augmented text generation, as a new\\ntext generation paradigm that fuses emerging deep\\nlearning technology and traditional retrieval tech-\\nnology, has achieved state-of-the-art (SOTA) per-\\nformance in many NLP tasks and attracted the at-\\ntention of the computational linguistics community\\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\\n2021). Compared with generation-based counter-\\npart, this new paradigm has some remarkable ad-\\nvantages: 1) The knowledge is not necessary to be\\nimplicitly stored in model parameters, but is explic-\\nitly acquired in a plug-and-play manner, leading\\nto great scalibility; 2) Instead of generating from\\nscratch, the paradigm generating text from some re-\\ntrieved human-written reference, which potentially\\nalleviates the difﬁculty of text generation.\\nThis paper aims to review many representative\\napproaches for retrieval-augmented text generation\\ntasks including dialogue response generation (We-\\nston et al., 2018), machine translation (Gu et al.,\\n2018) and others (Hashimoto et al., 2018). We\\n\\x03All authors contributed equally.ﬁrstly present the generic paradigm of retrieval-\\naugmented generation as well as three key com-\\nponents under this paradigm, which are retrieval\\nsources, retrieval metrics and generation models.\\nThen, we introduce notable methods about\\nretrieval-augmented generation, which are orga-\\nnized with respect to different tasks. Speciﬁcally,\\non the dialogue response generation task, exem-\\nplar/template retrieval as an intermediate step has\\nbeen shown beneﬁcial to informative response gen-\\neration (Weston et al., 2018; Wu et al., 2019; Cai\\net al., 2019a,b). In addition, there has been growing\\ninterest in knowledge-grounded generation explor-\\ning different forms of knowledge such as knowl-\\nedge bases and external documents (Dinan et al.,\\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\\n2021). On the machine translation task, we summa-\\nrize the early work on how the retrieved sentences\\n(called translation memory) are used to improve\\nstatistical machine translation (SMT) (Koehn et al.,\\n2003) models (Simard and Isabelle, 2009; Koehn\\nand Senellart, 2010) and in particular, we inten-\\nsively highlight several popular methods to inte-\\ngrating translation memory to NMT models (Gu\\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\\nHe et al., 2021). We also review the applications\\nof retrieval-augmented generation in other genera-\\ntion tasks such as abstractive summarization (Peng\\net al., 2019), code generation (Hashimoto et al.,\\n2018), paraphrase (Kazemnejad et al., 2020; Su\\net al., 2021b), and knowledge-intensive generation\\n(Lewis et al., 2020b). Finally, we also point out\\nsome promising directions on retrieval-augmented\\ngeneration to push forward the future research.\\n2 Retrieval-Augmented Paradigm\\nIn this section, we ﬁrst give a general formulation\\nof retrieval-augmented text generation. Then, we\\ndiscuss three major components of the retrieval-\\naugmented generation paradigm, including the re-arXiv:2202.01110v2  [cs.CL]  13 Feb 2022Input\\nSources (Sec. 2.2):Training CorpusExternal DataUnsupervised DataMetrics(Sec. 2.3):Sparse-vector RetrievalDense-vector RetrievalTask-specific RetrievalRetrieval MemoryGeneration ModelSec. 4: Machine TranslationSec. 5: Other TasksData AugmentationAttention MechanismSkeleton & TemplatesInformation RetrievalTasks:Sec. 3: Dialogue GenerationModels (Sec 2.4):OutputFigure 1: The overview of this survey.\\ntrieval source, retrieval metric and integration meth-\\nods.\\n2.1 Formulation\\nMost text generation tasks can be formulated as a\\nmapping from input sequence xto output sequence\\ny:y=f(x). For instance, xandycould be the\\ndialogue history and the corresponding response\\nfor dialogue response generation, the text in the\\nsource language and the translation in the target\\nlanguage for machine translation, and so on.\\nRecently, some researchers suggest to endow\\nmodels the capability to access external memory\\nvia some information retrieval techniques, so that\\nthey can acquire more information in the generation\\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\\net al., 2019b). The retrieval-augmented generation\\ncan be further formulated as:\\ny=f(x;z) (1)\\nwhere z=fhxr;yrigis a set of relevant instances\\nretrieved from the original training set or external\\ndatasets. The main idea of this paradigm is that yr\\nmay beneﬁt the response generation, if xr(oryr)\\nis similar (or relevant) to the input x. It is worth\\nnoting that xr=;when unsupervised retrieval\\nsources are used. In general, the retrieval mem-\\nory can be retrieved from three kinds of sources:\\nthe training corpus, external datasets in the same\\nformat with the training corpus, and large-scale\\nunsupervised corpus (§2.2). Metrics that evaluate\\nthe relevance between text are varied as well, in\\n§2.3 we divided them into three categories: sparse-\\nvector retrieval, dense-vector retrieval, and training-\\nbased retrieval. Finally, how to integrate the re-\\ntrieval memory to the generation model is also sig-\\nniﬁcant, we also introduce some popular integra-\\ntion approaches in §2.4.2.2 Retrieval Sources\\nTraining Corpus Most previous studies search\\nthe external memory from its training corpus (Song\\net al., 2016; Gu et al., 2018; Weston et al., 2018).\\nIn the inference time, retrieved examples with high\\nrelevant scores could be regarded as extra refer-\\nences and reduce model’s uncertainty in generation.\\nThe main motivation of those works is to to store\\nknowledge not only in the model parameters but\\nalso in an explicit and accessible form, making the\\nmodel be able to re-access it during inference.\\nExternal Data Some researchers also propose to\\nretrieval relevant samples from external datasets\\n(Su et al., 2021c; Xiao et al., 2021). In these stud-\\nies, the retrieval pool is different with the training\\ncorpus, which can further provide additional infor-\\nmation that are not contained in the training corpus.\\nThis is especially beneﬁcial for applications such\\nas domain adaptation and knowledge update. For\\nexample, Khandelwal et al. (2020a); Zheng et al.\\n(2021a) employ the in-domain dataset as the exter-\\nnal memory to achieve fast domain adaptation for\\nmachine translation.\\nUnsupervised Data One limitation for previous\\ntwo sources is that the datasets have to be super-\\nvised datasets consisting of aligned input-output\\npairs. For machine translation, Cai et al. (2021) pro-\\npose a cross-lingual retriever to directly retrieve tar-\\nget sentence from unsupervised corpus (i.e., mono-\\nlingual corpus in the target language). The main\\nidea is aligning source-side sentences and the corre-\\nsponding target-side translations in a dense vector\\nspace, i.e., aligning xandyrwhen xris absent.\\nAs a result, the retriever directly connects the dots\\nbetween the source-side input and target-side trans-\\nlations, enabling monolingual data in the targetlanguage to be used alone as memories.\\n2.3 Retrieval Metrics\\nSparse-vector Retrieval Given an input se-\\nquence xand a retrieval corpus, retrieval model\\naims to retrieve a set of relevant examples z=\\nfhxr;yrigfrom the corpus. When a supervised\\ncorpus is used,fhxr;yrigis retrieved by measur-\\ning the similarity between xandxr. For simi-\\nlarity measurement, sparse-vector retrieval meth-\\nods such as TF-IDF and BM25 (Robertson and\\nZaragoza, 2009) are widely used. They match key-\\nwords efﬁciently with an inverted index.\\nDense-vector Retrieval However, these meth-\\nods prefer examples with similar surfaces, and may\\nfail to retrieve examples that are only semantically\\nrelevant. To alleviate above problem, some stud-\\nies (Cao and Xiong, 2018) attempt to retrieve in\\ndense-vector space instead of the lexical overlap.\\nRecent work (Lee et al., 2019) makes use of pre-\\ntrained language models, which encodes the text to\\nlow-dimensional dense vectors via BERT-based en-\\ncoders. The retrieval score are computed via inner\\nproducts between vectors.\\nTask-speciﬁc Retrieval Similarity-based re-\\ntrieval is based on a simple heuristic. That is, the\\nmore xrresembles with x, the more likely xr\\nandyrwill help the generation. However, the\\nmost similar one by universal textual similarity\\ndoes not necessarily serve the best for downstream\\nmodels. Ideally, the retrieval metric would be\\nlearned from the data in a task-dependent way: we\\nwish to consider a memory only if it can indeed\\nboost the quality of ﬁnal generation. To this end,\\nCai et al. (2021) propose to unify the memory\\nretriever and its downstream generation model\\ninto a learnable whole. Such memory retrieval is\\nend-to-end optimized for task-speciﬁc objectives.\\n2.4 Integration\\nData Augmentation There are several ways to\\nintegrate the retrieved external memory in gener-\\nation. One straightforward way is data augmen-\\ntation , which constructs some augmented inputs\\nby concatenating spans from fhxr;yrigwith the\\noriginal input x. By training on the augmented\\ninputs, a generation model implicitly leans how\\nto integrate the retrieved information. Despite the\\nsimplicity, this kind of methods works efﬁciently\\nin lots of tasks (Song et al., 2016; Weston et al.,\\n2018; Bulte and Tezcan, 2019).Attention Mechanisms Another integration\\nmethod is based on attention mechanisms\\n(Bahdanau et al., 2014). The main idea of this\\nfashion is adopting additional encoders (in various\\narchitectures) to encode retrieved target sentences,\\nand integrate them through attention (Cao and\\nXiong, 2018; Gu et al., 2018; Bapna and Firat,\\n2019). Since the attention mechanism is becoming\\n(Bahdanau et al., 2014; Vaswani et al., 2017) a\\nkey module in lots of NLP models, integrating\\nretrieved memory through attention becomes a\\nvery nature and efﬁcient way.\\nSkeleton Extraction In the previous two meth-\\nods, the downstream generation model learns how\\nto ﬁlter out irrelevant or even harmful informa-\\ntion from the retrieved examples implicitly. There\\nalso exist some works that try to explicitly extract\\nuseful information, i.e., skeleton extraction , from\\nthe retrieved memory (Cai et al., 2019a; Wu et al.,\\n2019; Cai et al., 2019b). For example, one skeleton\\nshould be a part of a whole utterance with irrelevant\\ncontent masked, and the generation model only in-\\ntegrate this skeleton in the generation process.\\n3 Dialogue Response Generation\\nBackground Dialogue systems can be grouped\\ninto two categories: chit-chat systems and task-\\noriented systems. While task-oriented dialogue\\nsystems are designed to accomplish speciﬁc user\\ntasks such as air tickets booking, chit-chat dialogue\\nsystems aim at giving a meaningful and ﬂuent re-\\nsponse for any dialogue history in the open domain.\\nDialogue response generation in chit-chat dialogue\\nsystem is challenging partly due to the diversity\\nof possible responses to a single dialogue history\\n(i.e., the one-to-many problem). The dialogue his-\\ntory alone cannot decide a meaningful and speciﬁc\\nresponse. Also, external knowledge that is not\\npresent in the dialogue history are often necessary\\nfor avoiding safe but boring responses. We focus\\non recent efforts tackling the challenges to develop\\nchit-chat dialogue systems.\\nMost modern chit-chat dialogue systems can\\nbe categorized into two classes, namely, retrieval-\\nbased models and generation-based models. The\\nretrieval-based models (Ji et al., 2014; Hu et al.,\\n2014) directly copy an existing response from cu-\\nrated dialogue corpora (i.e., the retrieval pool)\\nwhen receiving a response request. The retrieved\\nresponses are often informative and grammatical\\nas they are collected from real-world conversa-tions and possibly post-edited by a human. How-\\never, such systems perform poorly when a given\\ndialogue history is substantially different from\\nthose in the retrieval pool. On the other hand,\\nthe generation-based models (Shang et al., 2015;\\nVinyals and Le, 2015; Li et al., 2016a) generate\\na new utterance from scratch. Those generation-\\nbased models have better generalization capacity\\nwhen handling unseen dialogue contexts. Never-\\ntheless, the generated utterances are inclined to be\\ndull and non-informative (e.g., “I don’t know”, “I\\nthink so”, “Me too” etc.) (Li et al., 2016a).\\nShallow Integration As discussed, retrieval-\\nbased models may give informative but inappro-\\npriate responses while generation-based models\\noften do the opposite. It is desirable to combine the\\nbest of both worlds. Early work (Qiu et al., 2017)\\nattempts to re-rank the output from both models.\\nFor a deep integration, Song et al. (2016) and Yang\\net al. (2019) extend the standard SEQ2SEQencoder-\\ndecoder model (Bahdanau et al., 2014) with an ex-\\ntra encoder for encoding the retrieval result. The\\noutput of the extra encoder, along with the output\\nfrom the original encoder for dialogue history, is\\nused to feed the decoder. Weston et al. (2018) use\\na single encoder that takes the concatenation of\\nthe original dialogue history and the retrieved as\\ninput. Wu et al. (2019) note that the retrieved infor-\\nmation should be used in awareness of the context\\ndifference, and further proposed to construct an\\nedit vector by explicitly encoding the lexical differ-\\nences between the input dialogue history and the\\nretrieved dialogue history. Pandey et al. (2018) fur-\\nther propose to weight different training instances\\nby context similarity.\\nDeep Integration To prevent the inﬂow of er-\\nroneous information, Cai et al. (2019a) propose\\na general framework that ﬁrst extracts a skeleton\\nfrom the retrieved response and then generates the\\nresponse based on the extracted skeleton. This\\nframework is also adopted for stylistic response\\ngeneration (Su et al., 2021c). Gupta et al. (2021)\\nsuggest to use the semantic structure of an exem-\\nplar response, instead of the tokens of the exem-\\nplar response, to guide generation. Despite their\\ndifferences, a common issue is that the genera-\\ntion model easily learns to ignore the retrieved re-\\nsponse entirely and collapses to a vanilla seq2seq\\nmodel. This happens with improper training in-\\nstances. Due to the one-to-many nature, it hap-pens frequently that a retrieved response (extracted\\nskeleton) is suitable for responding to the query,\\nbut inconsistent with the current target response.\\nEarlier studies (Weston et al., 2018; Wu et al.,\\n2019; Cai et al., 2019a) alleviate the above prob-\\nlems by putting hard constraints on the data (e.g.,\\ndiscarding data with low similarity of the retrieved\\nresponse and the target response), which, however,\\ngreatly reduces the amount of usable data. Cai\\net al. (2019b) employ a random mechanism for\\ngenerating the skeletons used for training, which\\nextract skeletons from the corresponding responses\\nwith some deliberate disturbance. Paranjape et al.\\n(2021) propose to model the retriever after the pos-\\nterior distribution of retrieval given the input and\\nthe target output and train it jointly with the stan-\\ndard retriever and the generator by maximizing the\\nevidence lower bound (ELBo) in expectation over\\nretrieval.\\nKnowledge-Enhanced Generation The afore-\\nmentioned work demonstrates that retrieval-based\\ndialogue systems can be used for building bet-\\nter generation-based models. In general, this is\\ndone by conditioning the generation on some re-\\ntrieved responses. More traditionally, to infuse\\nthe response with external knowledge, the retrieval\\npool is not necessarily a dialogue corpus. In fact,\\nknowledge-grounded dialogue response generation\\nexploring different forms of knowledge such as\\nknowledge bases and external documents (Dinan\\net al., 2018; Zhou et al., 2018; Lian et al., 2019;\\nLi et al., 2019; Qin et al., 2019; Wu et al., 2021;\\nZhang et al., 2021; Komeili et al., 2021) has been\\nactively explored.\\nLimitations We note that there are three major\\nlimitations in existing work for dialogue response\\ngeneration. First, current methods only use one\\nretrieved response for generation. It can be more\\nbeneﬁcial to combine multiple retrieval responses.\\nHowever, this can be difﬁcult due to the one-to-\\nmany nature of dialogue response generation. Sec-\\nond, current methods use universal relevance score\\nfor retrieval. It can be more effective if we can\\nuse more customized retrieval metric especially\\nfor controlled dialogue response generation (e.g.,\\npersona, emotion, etc). Third, the retrieval pool\\nof existing methods is limited to dialogue corpora\\n(context-response pairs) or documents. It might\\nbe useful to enlarge the retrieval pool by including\\nmore corpora in other domains or in other modali-ties. As discussed, there leaves plenty of possible\\ndirections to explore in the future.\\n4 Machine Translation\\nRetrieval augmented translation originates from hu-\\nman translation scenarios (Somers, 2003). When\\ntranslating ^yfrom an input source sentence x, a hu-\\nman translator typically involves a search engine to\\nretrieve similar sentences fhxr;yrigfrom a bilin-\\ngual database. Such a technique called translation\\nmemory is helpful to improve the translation qual-\\nity and efﬁciency for human translators (Dillon\\nand Fraser, 2006). As the development of ma-\\nchine translation techniques, there is a surge of\\ninterests in improving machine translation models\\nwith translation memory. In the rest of this section,\\nwe will review translation memory for both statisti-\\ncal machine translation (SMT) and neural machine\\ntranslation (NMT).\\n4.1 Translation Memory in SMT\\nGenerally, SMT includes three key components in\\na pipeline manner such as phrase table extraction,\\nparameter tuning and decoding (Koehn et al., 2003;\\nChiang, 2007). As a result, many efforts have been\\nmade to make use of translation memory (TM) on\\ntop of each component.\\nConstrained Decoding with TM Constrained\\ndecoding is the most straightforward way to in-\\ntegrating TM into SMT (Smith and Clark, 2009;\\nKoehn and Senellart, 2010; Zhechev and Van Gen-\\nabith, 2010; Ma et al., 2011). Its basic idea is\\nto reuse the useful segments in yrwhile trans-\\nlate other segments by SMT. Speciﬁcally, the ap-\\nproach consists of three steps: 1) identify the un-\\nmatched segments in both xrandxthrough the\\nedit-distance algorithm; 2) identify the unmatched\\nsegments in yr, each of which is aligned to one\\nunmatched segment in xrby a word alignment\\nalgorithm; 3) decode each unmatched segment in\\nxby SMT and then use the result to replace its\\ncorresponding unmatched segment in yr. Li et al.\\n(2016b) further extend this approach from sentence\\nlevel to phrase level. The advantage in constrained\\ndecoding is that it does not require to change the\\ntranslation model (including phrase table and pa-\\nrameters) and can be applied in a plug-and-play\\nway. This approach is successful when xis highly\\nsimilar to xr; otherwise its performance is de-\\ngraded largely, because it explicitly isolates TMmatching and SMT decoding and reuses the results\\ninxror not in a deterministic way.\\nPhrase Table Aggregation with TM There are\\nalso notable efforts to augment the phrase table\\nfor SMT by extracting translation rules from the\\nretrieved bilingual sentences fhxr;yrig. Then\\nthey re-tune the parameters for the SMT model\\nwhich makes use of translation knowledge from\\nfhxr;yrigin a implicit way when translating x.\\nFor example, Biçici and Dymetman (2008); Simard\\nand Isabelle (2009) directly combine the extracted\\ntranslation rules into the phrase table in a shallow\\ncombination way. They introduce an additional fea-\\nture to indicate that whether translation rule is from\\nfhxr;yrigor not and then train all feature weights\\nwith MERT (Och, 2003). One characteristic of\\nthese work is that a translation rule extracted from\\nfhxr;yrigwhich can not exactly match any seg-\\nments in xis useless even if it may contain some\\nuseful words in its target side. To remedy this ob-\\nservation, Wang et al. (2013, 2014) resort to a deep\\ncombination way to using the extracted translation\\nrules. For each rule in the phrase table, it designs\\na generative model to reward the rules which are\\nsimilar to those extracted from fhxr;yrig. Then\\nthis generative model is used as a feature in the log-\\nlinear based SMT model whose weight is tuned\\ntogether with other features by MERT. In addition,\\nLi et al. (2014) employ a similar way to reward\\nthe rules but it relies on a discriminative model\\nwhich is easy to integrate potential features from\\nfhxr;yrig.\\nParameter Tuning with TM Unlike the above\\ntwo research lines, Liu et al. (2012, 2014) make use\\nof translation memory only in tuning parameters.\\nTo be speciﬁc, when translating an input sentence\\nx, they ﬁrstly retrieve many similar bilingual sen-\\ntencesfhxr;yrig, and then tune the parameters on\\ntop of the retrieved sentences as well as a given de-\\nvelopment dataset in a sentence-wise manner, i.e.,\\nit performs an independent tuning for each input\\nsentence. To improve the efﬁciency of each tuning\\nstep, it propose a local update on top of fhxr;yrig\\nfrom a baseline model.\\nDespite the successes of translation memory in\\nSMT, there are still some limitations for the above\\nthree kinds of methods. Firstly, all these methods\\nemploy fuzzy score for retrieval which is highly de-\\npendent on word matching and thus can not recall\\nsuch examples which are similar in word seman-tics but different in surface form. Secondly, these\\nmethods integrate the retrieved examples into a\\nmodule of SMT in the ways which can not make\\nfull use of the knowledge in retrieved examples.\\nFor example, the integration ways in the ﬁrst two\\nkinds (constrained decoding and phrase table ag-\\ngregation) are heuristic and not optimized towards\\ntranslation quality; the parameter tuning method\\nﬁne-tunes few parameters for log-linear based SMT\\nwhich are not enough to preserve sufﬁcient knowl-\\nedge from retrieved examples. Thirdly, since SMT\\nperforms in a pipeline manner, it is intractable to\\njointly optimize retrieval metrics as well as SMT\\nmodels. Consequently, all these methods adopt an\\noff-the-shelf metric for retrieval, leading to sub-\\noptimal performance.\\n4.2 Translation Memory in NMT\\nTranslation memory has been widely explored in\\nNeural Machine Translation (NMT). Depending\\non when retrieval is involved, we can categorize\\nprevious works into two classes: 1) an NMT model\\nleans how to cooperate with the retrieval model in\\nthe training phase; 2) an NMT model is only aware\\nof the retrieved data in the inference phase.\\nInference Phase The key point of literature in\\nthis line is to reward some target words based on\\nwords in yrin the inference process. Thus, a de-\\ncision can be made based on both the distribution\\nof generation model and the additional reward of\\nretrieval model. Some previous works propose to\\nreward target words based on the sentence-level\\nsimilarity between xandxr, and the word align-\\nment between xrandyr. Given the input sentence\\nx, Zhang et al. (2018) try to assign target words\\nin^ywith higher rewards, when they appear in yr\\nand the aligned source words are in both xrand\\nx. He et al. (2019) follow a similar framework\\nand consider the position information of those tar-\\nget words when rewarding. Those works reward\\nthe target words in an explicit way, however, the\\none-sentence-one-model approach (Li et al., 2016c;\\nTurchi et al., 2017) propose to reward target word\\nimplicitly. For each testing input x, their approach\\nwill ﬁrst ﬁnetune the translation model on retrieved\\nmemoryfhxr;yrigand then translate x.\\nOthers try to reward target words based on token-\\nlevel similarity score. Most works in this line are\\nbased on the dense retriever (Khandelwal et al.,\\n2020a), e.g., faiss. Khandelwal et al. (2020a) build\\na key-value datastore, where key h(xr;yr\\n<t)is thehidden state at each time step when translating yr\\nfromxr, and value is its golden-truth target word\\nyr\\nt. Therefore, in the inference time, they can use\\ntheh(x;^y<t)as query and reward target words\\nwith similar hidden representations in the datas-\\ntore. Although this method achieves signiﬁcant\\nperformance gain, one drawback of it is the high la-\\ntency. To address this issue, Meng et al. (2021) use\\nsome heuristics, e.g., pre-ﬁltering, to avoid search-\\ning on the entire datastore. The reward score of\\nprevious works is got from some non-parametric\\napproaches, however, Zheng et al. (2021a) propose\\na light-weight network to learn the reward score.\\nSince dense retrieval has the potential of cross-\\nlingual retrieval, Zheng et al. (2021b) use a similar\\napproach to achieve unsupervised domain adapta-\\ntion, where a main change is to create the datastore\\nbased on synthetic sources sentence and the real\\ntarget sentences.\\nTraining Phase Different from those model-\\nagnostic approaches, previous works in this line\\naim to train the generation model to learn how\\nto cooperate with the retrieval model. It is also\\nworth noting that most works in this line adopt\\nthe sentence-level retrieval, when integrating the\\nretrieval information in the training process. To\\nachieve its goal, Bulte and Tezcan (2019) and\\nHossain et al. (2020) propose a data augmenta-\\ntion method to integrate the retrieved information,\\nwhere xis concatenated with yrbefore feeding\\ninto the model . Following the data augmentation\\napproach, Xu et al. (2020) propose more matching\\nmethods to determine including which retrieved\\nexample in the source is better.\\nThere also exist some works that propose new\\narchitectures to integrate the retrieval information.\\nUnder the RNN-based framework, Cao and Xiong\\n(2018) and Gu et al. (2018) use the gating and at-\\ntention mechanism to incorporate the retrieved tar-\\nget sentences. When Transformer (Vaswani et al.,\\n2017) becomes the backbone of NMT, some works\\nalso use additional transformer encoders to en-\\ncode retrieved target sentences, and integrate them\\nthrough attention mechanism (Bapna and Firat,\\n2019; Cao et al., 2019). Xia et al. (2019) repre-\\nsent the retrieved target sentences in a different\\ndata structure, i.e., a graph structure, and integrate\\nit through attention mechanism. He et al. (2021)\\npropose a light-weight method to encode the re-\\ntrieved target sentences and leverage the alignment\\ninformation to ﬁlter out irrelevant information. Dif-ferent from previous works that rely on bilingual\\nmemories, Cai et al. (2021) propose a framework\\nthat can retrieve the most similar target sentence in\\na monolingual dataset, using a source sentence as\\nquery.\\nLimitations In the section of SMT, we have\\nshowed some limitations of the retrieval augmented\\napproaches. There also exist some limitations in\\nthe line of NMT. First, the information used for\\nderiving reward scores is limited. The similarity\\nbetween an input and retrieved examples is the\\nprimary feature to derive reward scores. How-\\never, some information, e.g., frequencies of words\\nand context, may also be beneﬁcial for integrating\\nthe translation memory. Second, it remains to be\\nan open question that when should we use the re-\\ntrieved information and when not. In the inference\\nphase, approaches tend to integrate the translation\\nmemory excessively, e.g., at each time step, which\\nnot only reduces the translation efﬁciency but may\\nalso dampen the ﬂuency of generated results.\\n5 Other Tasks\\nIn addition to dialogue system and machine trans-\\nlation, retrieval-augmented generation techniques\\nhave shown to be beneﬁcial in many other tasks. In\\nthe following, we highlight several key tasks that\\napply retrieval-augmented generation approaches.1\\nLanguage Modelling It has been shown that\\nproperly leveraging information from retrieval\\nmemory could improve the performance of large\\npre-trained language model. To build a more accu-\\nrate language model, Khandelwal et al. (2020b) pro-\\npose to incorporate a soft memory module into the\\nsystem. Speciﬁcally, an index is built by caching\\nthe hidden states of the training corpus. Then, the\\nlanguage model accesses the index via k-NN search\\nand displays a greatly improved performance. As\\nanother example, Guu et al. (2020) propose a new\\nparadigm that applies retrieval-augmented tech-\\nnique into the pre-training of generative language\\nmodel. During learning, they train a neural se-\\nlector that dynamically samples a relevant text to\\nguide the reconstruction of a corrupted input se-\\nquence. In this way, the pre-trained model deliv-\\ners better results by explicitly grounding on the\\nretrieval memory. Lewis et al. (2020a) combine\\nlanguage model pre-training with a paraphrasing\\n1Here, we focus on tasks other than question answering.\\nWe refer readers interested in QA to Chen and Yih (2020).approach. During learning, an input sequence to\\nthe model is ﬁrst corrupted. In the meantime, a set\\nof multi-lingual texts are retrieved based on which\\nthe model learns to reconstruct the original input\\nsequence. Recently, Borgeaud et al. (2021) pro-\\npose RETRO , a large pre-trained language model\\nenhanced with retrieved documents, and obtained\\ncomparable performances with GPT-3 using 25 \\x02\\nfewer parameters.\\nSummarization Text summarization is another\\nresearch area that beneﬁts from retrieval-\\naugmented text generation. Peng et al. (2019)\\npropose an adaptive decoding framework which\\nﬁrst retrieves an exemplar document given the\\nsource document. Then, the summarization of the\\nsource document is derived through an adaptive\\ngeneration process based on the retrieved template.\\nDifferent from Peng et al. (2019), Cao et al.\\n(2018) and Hossain et al. (2020) introduce an\\nintermediate re-ranking stage into the generation\\npipeline. Speciﬁcally, before generating the\\ndocument summary, the retrieval documents are\\nﬁrst re-ranked based on their similarity scores\\nwith respect to the source document. Then, the\\ndocument summarization is produced by re-writing\\nthe selected templates.\\nParaphrase Generation To address the lack of\\nquality as well as diversity in the generation of para-\\nphrases, Kazemnejad et al. (2020) propose a gen-\\neration framework which ﬁrst retrieves a sentence\\nthat is similar to input sentence. Then, based on\\nthe retrieved sentence, a neural editor produces the\\nresulting paraphrased sentence. Chen et al. (2019)\\ninvestigate a different aspect of paraphrasing, i.e.\\nhow to control the linguistic syntax displayed in\\nthe generated text. To achieve this goal, Chen et al.\\n(2019) propose to ﬁrst extract a sentential exem-\\nplar that serves as the syntax template. A neural\\nmodel then generates the paraphrase with desired\\nlinguistic syntax following the retrieved exemplar.\\nText Style Transfer To improve the quality of\\ngenerated text, Li et al. (2018) propose a retrieval-\\naugmented framework which ﬁrst retrieves texts\\nthat are similar to the input based on lexical-level\\nsimilarity. Then, the retrieved tokens that are irrel-\\nevant to the source are deleted, and the output is\\nderived from the edited template. Xiao et al. (2021)\\nalso adopte this framework by incorporating re-\\ntrieval information from two sources (i.e. sparse\\nand dense memories) and obtained an improvedmodel performance.\\nData-to-Text Generation Recently, retrieval-\\naugmented generation has been adapted to the task\\nof data-to-text generation. To bridge the gap be-\\ntween the structured data and natural language\\ntext, Su et al. (2021a) propose a novel retrieval-\\naugmented framework. Speciﬁcally, given the\\nsource data, a set of candidate texts are ﬁrst re-\\ntrieved from a large unlabelled corpus. Then, a\\nneural selector is applied to measure the similari-\\nties between the source data and candidate texts,\\nand extract a set of more ﬁne-grained prototypes\\nfrom the candidates. Lastly, a generation model\\ntakes the prototypes as input to produce the text\\nthat describes the given structured data.\\nWhile retrieval-augmented generation has been\\nwidely explored in the NLP community, we sug-\\ngest that future research could extend this approach\\nto tasks that involve data from multiple modali-\\nties. For instance, with recent advancements in\\nimage-text retrieval (Jia et al., 2021; Radford et al.,\\n2021), the structural gap between images and texts\\nis largely bridged. Some early studies (Zhang et al.,\\n2020) have shown that information retrieved from\\nimages could improve the performance of neural\\nmachine translation model. Naturally, such meth-\\nods could be extended to other multi-modal tasks,\\nsuch as image captioning (Karpathy and Li, 2015).\\nA similar idea could also be applied to tasks be-\\nyond images, such as speech-to-text transcription\\n(Gales and Young, 2007).\\n6 Future Directions\\nDespite the current success of retrieval augmented\\ntext generation, there is still a long way to go as\\ndiscussed in previous sections. We highlight some\\ndirections to facilitate the future research as fol-\\nlows:\\nRetrieval Sensitivity The performance of re-\\ntrieval augmented text generation is very sensitive\\nto the retrieval quality, i.e., the similarity between\\nthe query and the retrieved examples. Currently, re-\\ntrieval augmented text generation models perform\\nwell when the retrieved examples are very simi-\\nlar to the query. However, they are even worse\\nthan the generation models without retrieval when\\nthe retrieval examples are less similar. Therefore,\\nit would be important to exploit new methods to\\naddress such an issue on similarity.Retrieval Efﬁciency Generally, if one enlarges\\nthe retrieval memory to some extent, it would be\\npossible to retrieve an example which is very simi-\\nlar to the query.Unfortunately, the downside is that\\nthe overall inference for the retrieval augmented\\ngeneration models is less efﬁcient due the consid-\\nerable retrieval overhead. In this sense, it is urgent\\nto consider some methods to trade off the retrieval\\nmemory size and retrieval efﬁciency, for example,\\ndata compression for the retrieval memory.\\nLocal vs. Global Optimization Theoretically, it\\nseems promising to jointly learn retrieval metrics\\nand generation models. However, in practice, there\\nis an essential gap about the retrieval metric be-\\ntween the training and inference phrases. In the\\ntraining phase, the loss is locally back-propagated\\nto only a few retrieved examples while in the infer-\\nence phase the metric is globally conducted among\\nall examples in the memory. It would be interesting\\nto narrow such a gap when learning a better metric\\nfor generation tasks.\\nMulti-Modalities With recent advancement in\\nimage-text retrieval, directly associating images\\nwith relevant text becomes possible. This urges\\nresearchers to investigate the possibility of retrieval-\\nbased text generation in tasks that involve data from\\ndifferent modalities. One typical task is image\\ncaptioning. Beyond images, other tasks like speech-\\nto-text transcription could potentially beneﬁt from\\nretrieval-based generation methods as well.\\nDiverse & Controllable Retrieval Most of the\\nexisting approaches adopt a universal metric for\\nretrieval, such as lexical similarities of sentences.\\nFuture work should explore how to use customized\\nmetrics for retrieval. This can be beneﬁcial for\\nmore controlled text generation. For example, in-\\nstances with emotions and styles may be more de-\\nsirable in the personalized dialogue generation, par-\\nallel data that contains speciﬁc terminologies is\\nmore helpful in machine translation, and so on. On\\nthe other hand, using a universal metric for retrieval\\nmay lead to the lack of diversity of the retrieval re-\\nsults. Collecting a diverse set of retrieval results\\ncan improve the coverage of useful information.\\nThus, considering multiple different metrics for re-\\ntrieval may lead to generation with higher quality\\nin the future.7 Conclusion\\nIn this paper, we surveyed recent approaches for\\nretrieval-augmented text generation. We reviewed\\nand summarized the development of different com-\\nponents of retrieval-augmented text generation in-\\ncluding retrieval metrics, retrieval sources, and in-\\ntegration paradigms. We gave in-depth discussions\\nwhen retrieval-augmented text generation comes to\\ndifferent applications including dialogue response\\ngeneration, machine translation, and other genera-\\ntion tasks. We also pointed out some future direc-\\ntions for retrieval-augmented text generation.\\nReferences\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\\ngio. 2014. Neural machine translation by jointly\\nlearning to align and translate. arXiv preprint\\narXiv:1409.0473 .\\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\\nadaptation for neural machine translation. In Pro-\\nceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Vol-\\nume 1 (Long and Short Papers) , pages 1921–1931.\\nErgun Biçici and Marc Dymetman. 2008. Dynamic\\ntranslation memory: Using statistical machine trans-\\nlation to improve translation memory fuzzy matches.\\nInInternational Conference on Intelligent Text Pro-\\ncessing and Computational Linguistics , pages 454–\\n465. Springer.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\nGeorge van den Driessche, Jean-Baptiste Lespiau,\\nBogdan Damoc, Aidan Clark, Diego de Las Casas,\\nAurelia Guy, Jacob Menick, Roman Ring, Tom Hen-\\nnigan, Saffron Huang, Loren Maggiore, Chris Jones,\\nAlbin Cassirer, Andy Brock, Michela Paganini, Ge-\\noffrey Irving, Oriol Vinyals, Simon Osindero, Karen\\nSimonyan, Jack W. Rae, Erich Elsen, and Laurent\\nSifre. 2021. Improving language models by retriev-\\ning from trillions of tokens. CoRR , abs/2112.04426.\\nBram Bulte and Arda Tezcan. 2019. Neural fuzzy re-\\npair: Integrating fuzzy matches into neural machine\\ntranslation. In Proceedings of the 57th Annual Meet-\\ning of the Association for Computational Linguistics ,\\npages 1800–1809.\\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xi-\\naojiang Liu, Wai Lam, and Shuming Shi. 2019a.\\nSkeleton-to-response: Dialogue generation guided\\nby retrieval memory. In Proceedings of the 2019\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short\\nPapers) , pages 1219–1228.Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiao-\\njiang Liu, and Shuming Shi. 2019b. Retrieval-\\nguided dialogue response generation via a matching-\\nto-generation framework. In Proceedings of the\\n2019 Conference on Empirical Methods in Natu-\\nral Language Processing and the 9th International\\nJoint Conference on Natural Language Processing\\n(EMNLP-IJCNLP) , pages 1866–1875.\\nDeng Cai, Yan Wang, Huayang Li, Wai Lam, and\\nLemao Liu. 2021. Neural machine translation with\\nmonolingual translation memory. In Proceedings of\\nthe 59th Annual Meeting of the Association for Com-\\nputational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing\\n(Volume 1: Long Papers) , pages 7307–7318, Online.\\nAssociation for Computational Linguistics.\\nQian Cao, Shaohui Kuang, and Deyi Xiong. 2019.\\nLearning to reuse translations: Guiding neural ma-\\nchine translation with examples. arXiv preprint\\narXiv:1911.10732 .\\nQian Cao and Deyi Xiong. 2018. Encoding gated\\ntranslation memory into neural machine translation.\\nInProceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n3042–3047.\\nZiqiang Cao, Wenjie Li, Sujian Li, and Furu Wei.\\n2018. Retrieve, rerank and rewrite: Soft template\\nbased neural summarization. In Proceedings of the\\n56th Annual Meeting of the Association for Com-\\nputational Linguistics, ACL 2018, Melbourne, Aus-\\ntralia, July 15-20, 2018, Volume 1: Long Papers ,\\npages 152–161. Association for Computational Lin-\\nguistics.\\nDanqi Chen and Wen-tau Yih. 2020. Open-domain\\nquestion answering. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics: Tutorial Abstracts , pages 34–37, On-\\nline. Association for Computational Linguistics.\\nMingda Chen, Qingming Tang, Sam Wiseman, and\\nKevin Gimpel. 2019. Controllable paraphrase gen-\\neration with a syntactic exemplar. In Proceedings of\\nthe 57th Conference of the Association for Compu-\\ntational Linguistics, ACL 2019, Florence, Italy, July\\n28- August 2, 2019, Volume 1: Long Papers , pages\\n5972–5984. Association for Computational Linguis-\\ntics.\\nDavid Chiang. 2007. Hierarchical phrase-based trans-\\nlation. computational linguistics , 33(2):201–228.\\nSarah Dillon and Janet Fraser. 2006. Translators and\\ntm: An investigation of translators’ perceptions of\\ntranslation memory adoption. Machine Translation ,\\n20(2):67–79.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\\nFan, Michael Auli, and Jason Weston. 2018. Wizard\\nof wikipedia: Knowledge-powered conversational\\nagents. arXiv preprint arXiv:1811.01241 .Mark J. F. Gales and Steve J. Young. 2007. The applica-\\ntion of hidden markov models in speech recognition.\\nFound. Trends Signal Process. , 1(3):195–304.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-\\ntor OK Li. 2018. Search engine guided neural ma-\\nchine translation. In Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence , volume 32.\\nPrakhar Gupta, Jeffrey Bigham, Yulia Tsvetkov, and\\nAmy Pavel. 2021. Controlling dialogue generation\\nwith semantic exemplars. In Proceedings of the\\n2021 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies , pages 3018–3029, On-\\nline. Association for Computational Linguistics.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\\naugmented language model pre-training. CoRR ,\\nabs/2002.08909.\\nTatsunori B Hashimoto, Kelvin Guu, Yonatan Oren,\\nand Percy S Liang. 2018. A retrieve-and-edit frame-\\nwork for predicting structured outputs. In Advances\\nin Neural Information Processing Systems , pages\\n10052–10062.\\nQiuxiang He, Guoping Huang, Qu Cui, Li Li, and\\nLemao Liu. 2021. Fast and accurate neural machine\\ntranslation with translation memory. In Proceed-\\nings of the 59th Annual Meeting of the Association\\nfor Computational Linguistics and the 11th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing (Volume 1: Long Papers) , pages 3170–3180.\\nQiuxiang He, Guoping Huang, Lemao Liu, and Li Li.\\n2019. Word position aware translation memory for\\nneural machine translation. In CCF International\\nConference on Natural Language Processing and\\nChinese Computing , pages 367–379. Springer.\\nNabil Hossain, Marjan Ghazvininejad, and Luke Zettle-\\nmoyer. 2020. Simple and effective retrieve-edit-\\nrerank text generation. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computa-\\ntional Linguistics , pages 2532–2538.\\nBaotian Hu, Zhengdong Lu, Hang Li, and Qingcai\\nChen. 2014. Convolutional neural network architec-\\ntures for matching natural language sentences. In\\nNIPS , pages 2042–2050.\\nZongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An\\ninformation retrieval approach to short text conver-\\nsation. arXiv preprint arXiv:1408.6988 .\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\\nParekh, Hieu Pham, Quoc V . Le, Yun-Hsuan Sung,\\nZhen Li, and Tom Duerig. 2021. Scaling up visual\\nand vision-language representation learning with\\nnoisy text supervision. In Proceedings of the 38th In-\\nternational Conference on Machine Learning, ICML\\n2021, 18-24 July 2021, Virtual Event , volume 139 of\\nProceedings of Machine Learning Research , pages\\n4904–4916. PMLR.Andrej Karpathy and Fei-Fei Li. 2015. Deep visual-\\nsemantic alignments for generating image descrip-\\ntions. In IEEE Conference on Computer Vision and\\nPattern Recognition, CVPR 2015, Boston, MA, USA,\\nJune 7-12, 2015 , pages 3128–3137. IEEE Computer\\nSociety.\\nAmirhossein Kazemnejad, Mohammadreza Salehi, and\\nMahdieh Soleymani Baghshah. 2020. Paraphrase\\ngeneration by learning how to edit from samples. In\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics , pages 6010–\\n6021, Online. Association for Computational Lin-\\nguistics.\\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2020a. Near-\\nest neighbor machine translation. arXiv preprint\\narXiv:2010.00710 .\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2020b. Generaliza-\\ntion through memorization: Nearest neighbor lan-\\nguage models. In 8th International Conference on\\nLearning Representations, ICLR 2020, Addis Ababa,\\nEthiopia, April 26-30, 2020 . OpenReview.net.\\nPhilipp Koehn, Franz J. Och, and Daniel Marcu. 2003.\\nStatistical phrase-based translation. In Proceedings\\nof the 2003 Human Language Technology Confer-\\nence of the North American Chapter of the Associa-\\ntion for Computational Linguistics , pages 127–133.\\nPhilipp Koehn and Jean Senellart. 2010. Convergence\\nof translation memory and statistical machine trans-\\nlation. In Proceedings of AMTA Workshop on MT\\nResearch and the Translation Industry , pages 21–31.\\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\\n2021. Internet-augmented dialogue generation.\\narXiv preprint arXiv:2107.07566 .\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised\\nopen domain question answering. arXiv preprint\\narXiv:1906.00300 .\\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\\n2020a. Pre-training via paraphrasing. In Advances\\nin Neural Information Processing Systems 33: An-\\nnual Conference on Neural Information Processing\\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\\nvirtual .\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. 2020b. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. arXiv\\npreprint arXiv:2005.11401 .\\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\\nand Bill Dolan. 2016a. A diversity-promoting ob-\\njective function for neural conversation models. In\\nNAACL , pages 110–119.Juncen Li, Robin Jia, He He, and Percy Liang. 2018.\\nDelete, retrieve, generate: a simple approach to sen-\\ntiment and style transfer. In Proceedings of the 2018\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, NAACL-HLT 2018, New\\nOrleans, Louisiana, USA, June 1-6, 2018, Volume\\n1 (Long Papers) , pages 1865–1874. Association for\\nComputational Linguistics.\\nLiangyou Li, Andy Way, and Qun Liu. 2014. A\\ndiscriminative framework of integrating translation\\nmemory features into smt. In Proceedings of the\\n11th Conference of the Association for Machine\\nTranslation in the Americas , volume 1, pages 249–\\n260.\\nLiangyou Li, Andy Way, and Qun Liu. 2016b. Phrase-\\nlevel combination of smt and tm using constrained\\nword lattice. Association for Computational Lin-\\nguistics (ACL).\\nXiaoqing Li, Jiajun Zhang, and Chengqing Zong.\\n2016c. One sentence one model for neural machine\\ntranslation. arXiv preprint arXiv:1609.06490 .\\nZekang Li, Cheng Niu, Fandong Meng, Yang Feng,\\nQian Li, and Jie Zhou. 2019. Incremental trans-\\nformer with deliberation decoder for document\\ngrounded conversations. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computa-\\ntional Linguistics , pages 12–21.\\nRongzhong Lian, Min Xie, Fan Wang, Jinhua Peng,\\nand Hua Wu. 2019. Learning to select knowledge\\nfor response generation in dialog systems. arXiv\\npreprint arXiv:1902.04911 .\\nLemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao,\\nMo Yu, and Conghui Zhu. 2012. Locally training\\nthe log-linear model for smt. In Proceedings of the\\n2012 Joint Conference on Empirical Methods in Nat-\\nural Language Processing and Computational Natu-\\nral Language Learning , pages 402–411.\\nLemao Liu, Tiejun Zhao, Taro Watanabe, Hailong Cao,\\nand Conghui Zhu. 2014. Discriminative training for\\nlog-linear based smt: Global or local methods. ACM\\nTransactions on Asian Language Information Pro-\\ncessing (TALIP) , 13(4):1–25.\\nYanjun Ma, Yifan He, Andy Way, and Josef van Gen-\\nabith. 2011. Consistent translation using discrim-\\ninative learning-a translation memory-inspired ap-\\nproach. In Proceedings of the 49th Annual Meet-\\ning of the Association for Computational Linguistics:\\nHuman Language Technologies , pages 1239–1248.\\nYuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xi-\\naofei Sun, Tianwei Zhang, and Jiwei Li. 2021.\\nFast nearest neighbor machine translation. arXiv\\npreprint arXiv:2105.14528 .\\nFranz Josef Och. 2003. Minimum error rate training in\\nstatistical machine translation. In Proceedings of the41st Annual Meeting of the Association for Compu-\\ntational Linguistics , pages 160–167, Sapporo, Japan.\\nAssociation for Computational Linguistics.\\nGaurav Pandey, Danish Contractor, Vineet Kumar, and\\nSachindra Joshi. 2018. Exemplar encoder-decoder\\nfor neural conversation generation. In ACL, pages\\n1329–1338.\\nAshwin Paranjape, Omar Khattab, Christopher Potts,\\nMatei Zaharia, and Christopher D Manning. 2021.\\nHindsight: Posterior-guided training of retrievers for\\nimproved open-ended generation. arXiv preprint\\narXiv:2110.07752 .\\nHao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan\\nDhingra, and Das Dipanjan. 2019. Text generation\\nwith exemplar-based adaptive decoding. In Proceed-\\nings of the Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies .\\nLianhui Qin, Michel Galley, Chris Brockett, Xiaodong\\nLiu, Xiang Gao, William B Dolan, Yejin Choi, and\\nJianfeng Gao. 2019. Conversing by reading: Con-\\ntentful neural conversation with on-demand machine\\nreading. In Proceedings of the 57th Annual Meet-\\ning of the Association for Computational Linguistics ,\\npages 5427–5436.\\nMinghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan\\nChen, Weipeng Zhao, Haiqing Chen, Jun Huang,\\nand Wei Chu. 2017. Alime chat: A sequence to se-\\nquence and rerank based chatbot engine. In ACL,\\npages 498–503.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\\ning transferable visual models from natural lan-\\nguage supervision. In Proceedings of the 38th In-\\nternational Conference on Machine Learning, ICML\\n2021, 18-24 July 2021, Virtual Event , volume 139 of\\nProceedings of Machine Learning Research , pages\\n8748–8763. PMLR.\\nStephen Robertson and Hugo Zaragoza. 2009. The\\nprobabilistic relevance framework: BM25 and be-\\nyond . Now Publishers Inc.\\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-\\nral responding machine for short-text conversation.\\nInACL, pages 1577–1586.\\nMichel Simard and Pierre Isabelle. 2009. Phrase-based\\nmachine translation in a computer-assisted transla-\\ntion environment. Proceedings of the Twelfth Ma-\\nchine Translation Summit (MT Summit XII) , pages\\n120–127.\\nJames Smith and Stephen Clark. 2009. Ebmt for smt:\\na new ebmt-smt hybrid. In Proceedings of the 3rd\\nInternational Workshop on Example-Based Machine\\nTranslation , pages 3–10. Citeseer.Harold Somers. 2003. Translation memory systems.\\nBenjamins Translation Library , 35:31–48.\\nYiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and\\nMing Zhang. 2016. Two are better than one: An en-\\nsemble of retrieval-and generation-based dialog sys-\\ntems. arXiv preprint arXiv:1610.07149 .\\nYixuan Su, Zaiqiao Meng, Simon Baker, and Nigel Col-\\nlier. 2021a. Few-shot table-to-text generation with\\nprototype memory. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2021, Vir-\\ntual Event / Punta Cana, Dominican Republic, 16-\\n20 November, 2021 , pages 910–917. Association for\\nComputational Linguistics.\\nYixuan Su, David Vandyke, Simon Baker, Yan Wang,\\nand Nigel Collier. 2021b. Keep the primary, rewrite\\nthe secondary: A two-stage approach for paraphrase\\ngeneration. In Findings of the Association for Com-\\nputational Linguistics: ACL-IJCNLP 2021 , pages\\n560–569, Online. Association for Computational\\nLinguistics.\\nYixuan Su, Yan Wang, Deng Cai, Simon Baker, Anna\\nKorhonen, and Nigel Collier. 2021c. PROTOTYPE-\\nTO-STYLE: dialogue generation with style-aware\\nediting on retrieval memory. IEEE ACM Trans. Au-\\ndio Speech Lang. Process. , 29:2152–2161.\\nMarco Turchi, Matteo Negri, M Farajian, and Marcello\\nFederico. 2017. Continuous learning from human\\npost-edits for neural machine translation.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in neural information pro-\\ncessing systems , pages 5998–6008.\\nOriol Vinyals and Quoc Le. 2015. A neural conversa-\\ntional model. In ICML (Deep Learning Workshop) .\\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2013.\\nIntegrating translation memory into phrase-based\\nmachine translation during decoding. In Proceed-\\nings of the 51st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Pa-\\npers) , pages 11–21.\\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\\nDynamically integrating cross-domain translation\\nmemory into phrase-based machine translation dur-\\ning decoding. In Proceedings of COLING 2014,\\nthe 25th International Conference on Computational\\nLinguistics: Technical Papers , pages 398–408.\\nJason Weston, Emily Dinan, and Alexander Miller.\\n2018. Retrieve and reﬁne: Improved sequence gen-\\neration models for dialogue. In Proceedings of the\\n2018 EMNLP Workshop SCAI: The 2nd Interna-\\ntional Workshop on Search-Oriented Conversational\\nAI, pages 87–92.Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\\njun Li, and Ming Zhou. 2019. Response generation\\nby context-aware prototype editing. In Proceedings\\nof the AAAI Conference on Artiﬁcial Intelligence ,\\nvolume 33, pages 7281–7288.\\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\\nJianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\\net al. 2021. A controllable model of grounded re-\\nsponse generation. In Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence , volume 35, pages\\n14085–14093.\\nMengzhou Xia, Guoping Huang, Lemao Liu, and\\nShuming Shi. 2019. Graph based translation mem-\\nory for neural machine translation. In Proceedings\\nof the AAAI Conference on Artiﬁcial Intelligence ,\\nvolume 33, pages 7297–7304.\\nFei Xiao, Liang Pang, Yanyan Lan, Yan Wang, Huawei\\nShen, and Xueqi Cheng. 2021. Transductive learn-\\ning for unsupervised text style transfer. In Proceed-\\nings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing, EMNLP 2021, Vir-\\ntual Event / Punta Cana, Dominican Republic, 7-11\\nNovember, 2021 , pages 2510–2521. Association for\\nComputational Linguistics.\\nJitao Xu, Josep M Crego, and Jean Senellart. 2020.\\nBoosting neural machine translation with similar\\ntranslations. In Proceedings of the 58th Annual\\nMeeting of the Association for Computational Lin-\\nguistics , pages 1580–1590.\\nLiu Yang, Junjie Hu, Minghui Qiu, Chen Qu, Jian-\\nfeng Gao, W Bruce Croft, Xiaodong Liu, Yelong\\nShen, and Jingjing Liu. 2019. A hybrid retrieval-\\ngeneration neural conversation model. In Proceed-\\nings of the 28th ACM international conference on in-\\nformation and knowledge management , pages 1341–\\n1350.\\nJingyi Zhang, Masao Utiyama, Eiichiro Sumita, Gra-\\nham Neubig, and Satoshi Nakamura. 2018. Guiding\\nneural machine translation with retrieved translation\\npieces. In Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technolo-\\ngies, Volume 1 (Long Papers) , pages 1325–1335.\\nYizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris\\nBrockett, Michel Galley, Jianfeng Gao, and Bill\\nDolan. 2021. Joint retrieval and generation train-\\ning for grounded text generation. arXiv preprint\\narXiv:2105.06597 .\\nZhuosheng Zhang, Kehai Chen, Rui Wang, Masao\\nUtiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao.\\n2020. Neural machine translation with universal\\nvisual representation. In 8th International Confer-\\nence on Learning Representations, ICLR 2020, Ad-\\ndis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-\\nview.net.Ventsislav Zhechev and Josef Van Genabith. 2010.\\nSeeding statistical machine translation with trans-\\nlation memory output through tree-based structural\\nalignment. In Proceedings of the 4th Workshop\\non Syntax and Structure in Statistical Translation ,\\npages 43–51.\\nXin Zheng, Zhirui Zhang, Junliang Guo, Shujian\\nHuang, Boxing Chen, Weihua Luo, and Jiajun Chen.\\n2021a. Adaptive nearest neighbor machine transla-\\ntion. arXiv preprint arXiv:2105.13022 .\\nXin Zheng, Zhirui Zhang, Shujian Huang, Boxing\\nChen, Jun Xie, Weihua Luo, and Jiajun Chen. 2021b.\\nNon-parametric unsupervised domain adaptation for\\nneural machine translation. In Findings of the As-\\nsociation for Computational Linguistics: EMNLP\\n2021 , pages 4234–4241.\\nKangyan Zhou, Shrimai Prabhumoye, and Alan W\\nBlack. 2018. A dataset for document grounded con-\\nversations. arXiv preprint arXiv:1809.07358 .',\n",
       " 'Benchmarking Large Language Models in Retrieval-Augmented Generation\\nJiawei Chen1,3, Hongyu Lin1,*, Xianpei Han1,2,*, Le Sun1,2\\n1Chinese Information Processing Laboratory2State Key Laboratory of Computer Science\\nInstitute of Software, Chinese Academy of Sciences, Beijing, China\\n3University of Chinese Academy of Sciences, Beijing, China\\n{jiawei2020,hongyu,xianpei,sunle }@iscas.ac.cn\\nAbstract\\nRetrieval-Augmented Generation (RAG) is a promising ap-\\nproach for mitigating the hallucination of large language\\nmodels (LLMs). However, existing research lacks rigorous\\nevaluation of the impact of retrieval-augmented generation\\non different large language models, which make it challeng-\\ning to identify the potential bottlenecks in the capabilities\\nof RAG for different LLMs. In this paper, we systemati-\\ncally investigate the impact of Retrieval-Augmented Gener-\\nation on large language models. We analyze the performance\\nof different large language models in 4 fundamental abili-\\nties required for RAG, including noise robustness, negative\\nrejection, information integration, and counterfactual robust-\\nness. To this end, we establish Retrieval-Augmented Genera-\\ntion Benchmark (RGB), a new corpus for RAG evaluation in\\nboth English and Chinese. RGB divides the instances within\\nthe benchmark into 4 separate testbeds based on the afore-\\nmentioned fundamental abilities required to resolve the case.\\nThen we evaluate 6 representative LLMs on RGB to diag-\\nnose the challenges of current LLMs when applying RAG.\\nEvaluation reveals that while LLMs exhibit a certain degree\\nof noise robustness, they still struggle significantly in terms of\\nnegative rejection, information integration, and dealing with\\nfalse information. The aforementioned assessment outcomes\\nindicate that there is still a considerable journey ahead to ef-\\nfectively apply RAG to LLMs.\\nIntroduction\\nRecently, there have been impressive advancements in large\\nlanguage models (LLMs) like ChatGPT (OpenAI 2022),\\nLLaMA-2 (Touvron et al. 2023), and ChatGLM (THUDM\\n2023a). Although these models have shown remarkable gen-\\neral abilities (Bang et al. 2023; Guo et al. 2023), they still\\nsuffer severely from challenges including factual halluci-\\nnation (Cao et al. 2020; Raunak, Menezes, and Junczys-\\nDowmunt 2021; Ji et al. 2023), knowledge out-dating (He,\\nZhang, and Roth 2022), and the lack of domain-specific ex-\\npertise (Li et al. 2023c; Shen et al. 2023).\\nIncorporating external knowledge via information re-\\ntrieval, i.e., Retrieval-Augmented Generation (RAG), has\\nbeen regarded as a promising way to resolve the above chal-\\nlenges. (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.\\n2022; Izacard et al. 2022). With the help of external knowl-\\n*Corresponding authors.\\nNoise Robustness Negative Rejection\\nWho was awarded the 2022 Nobel prize in \\nliterature?\\nThe Nobel Prize in Literature for 2022 is \\nawarded to the French author Annie Ernaux, \\n“for the courage and clinical acuity …\\nThe Nobel Prize in Literature for 2021 is \\nawarded to the novelist Abdulrazak Gurnah, \\nborn in Zanzibar and active in …\\nAnnie ErnauxQuestion\\nExternal documents contain noises\\nRetrieval Augmented \\nGenerationWho was awarded the 2022 Nobel prize in \\nliterature?\\nThe Nobel Prize in Literature for 2021 is \\nawarded to the novelist Abdulrazak Gurnah, \\nborn in Zanzibar and active in …\\nThe 2020 Nobel Laureate in Literature, \\npoet Louise Glück, has written both poetry \\nand essays about poetry. Since her…\\nI can not answer the question because of the \\ninsufficient information in documentsQuestion\\nExternal documents are all noises\\nInformation Integration\\nWhen were the ChatGPT app for iOS and \\nChatGPT apilaunched?\\nOn May 18th, 2023, OpenAI introduced its \\nown ChatGPT app for iOS…\\nThat changed on March 1, when OpenAI \\nannounced the release of API access to \\nChatGPT and Whisper,…\\nMay 18 and March 1 .Question\\nExternal documents contain all answers\\nRetrieval Augmented \\nGenerationCounterfactual Robustness\\nWhich city hosted the Olympic games in \\n2004?\\nThe 2004 Olympic Games returned home to \\nNew York , birthplace of the … \\nAfter leading all voting rounds, New York\\neasily defeated Rome in the fifth and \\nfinal vote …\\nThere are factual errors in the provided \\ndocuments. The answer should be Athens . Question\\nCounterfactual external documents\\nRetrieval Augmented \\nGenerationRetrieval Augmented \\nGenerationFigure 1: Illustration of 4 kinds of abilities required for\\nretrieval-augmented generation of LLMs.\\nedge, LLMs can generate more accurate and reliable re-\\nsponses. The most common method is to use a search engine\\nas a retriever such as New Bing. Due to the vast amount of\\ninformation available on the Internet, using a search engine\\ncan provide more real-time information.\\nHowever, Retrieval-Augmented Generation brings not\\nonly positive effects to LLMs (Liu, Zhang, and Liang 2023;\\nMaynez et al. 2020). On one hand, there is a significant\\namount of noise information even fake news in the content\\navailable on the Internet, which poses challenges for search\\nengines in accurately retrieving desirable knowledge. On the\\nother hand, LLMs suffer from unreliable generation chal-\\nlenge. LLMs can be misled by incorrect information con-\\ntained in the context (Bian et al. 2023) and also suffer from\\nhallucination during the generation (Adlakha et al. 2023),\\nresulting in generating content that goes beyond external in-\\nformation. These challenges result in LLMs being unable toarXiv:2309.01431v1  [cs.CL]  4 Sep 2023consistently generate reliable and accurate responses. Un-\\nfortunately, currently there lacks of comprehensive under-\\nstanding on how these factors can influence RAG, and how\\ncould each model survives from these drawbacks and im-\\nprovement their performance via information retrieval. As a\\nresult, there is a pressing need for a comprehensive evalua-\\ntion of LLMs on their ability to effectively utilize retrieved\\ninformation, as well as their ability to withstand the various\\ndrawbacks present in information retrieval.\\nTo this end, this paper conducts a comprehensive evalua-\\ntion of RAG for current LLMs. Specifically, we create a new\\nRetrieval-Augmented Generation Benchmark, namely RGB,\\nin both English and Chinese. In order to ensure that the in-\\nternal knowledge of LLMs does not introduce bias into the\\nevaluation results, RGB chooses to aggregate the latest news\\ninformation and constructs queries based on the news infor-\\nmation. Then, based on these queries, we use Search API to\\nfetch relevant documents and select most relevant snippets\\nfrom the content as external retrieved documents. Finally,\\nbased on different compositions of query and document-set\\npairs, we expand the corpus and divided it into 4 testbeds to\\nevaluate the following basic abilities of LLMs according to\\nthe common challenges in RAG, as shown in Figure 1:\\n•Noise Robustness , which means a model can extract use-\\nful information from noisy documents. In this paper, we\\ndefine noisy documents as those that are relevant to the\\nquestion but do not contain any information of the an-\\nswer. For the instance in Figure 1, the noisy documents\\nrelated to the question “Who was awarded the 2022 No-\\nbel Prize in Literature” include reports about the 2021\\nNobel Prize in Literature. To this end, the testbed for\\nnoise robustness contains instances whose external doc-\\numents contain a certain number of noisy documents\\nbased on the desired noise ratio.\\n•Negative Rejection , which means that a model should\\nreject to answer the question when the required knowl-\\nedge is not present in any retrieved document. The\\ntestbed for negative rejection contains instances whose\\nexternal documents are only with noisy documents.\\nLLMs are expected to indicate “insufficient information”\\nor other rejection signals.\\n•Information Integration , which evaluates whether the\\nmodel can answer complex questions that require inte-\\ngrating information from multiple documents. For the\\ninstance in Figure 1, for the question “When were the\\nChatGPT app for iOS and ChatGPT api launched?”, the\\nLLMs are expected to provide information on the launch\\ndates for both the ChatGPT app on iOS and the ChatGPT\\nAPI. The testbed for information integration contains in-\\nstances that can only be answered using multiple external\\ndocuments.\\n•Counterfactual Robustness , which evaluates whether\\nthe model can identify risks of known factual errors in the\\nretrieved documents when the LLMs are given warnings\\nabout potential risks in the retrieved information through\\ninstruction. The testbed for counterfactual robustness in-\\ncludes instances that can be answered directly by the\\nLLMs, but the external documents contain factual errors.Based on RGB, we conduct evaluation on 6 state-of-\\nthe-art large language models including ChatGPT (Ope-\\nnAI 2022), ChatGLM-6B (THUDM 2023a), ChatGLM2-\\n6B (THUDM 2023b), Vicuna-7b (Chiang et al. 2023),\\nQwen-7B-Chat (QwenLM 2023), BELLE-7B (Yunjie Ji\\n2023). We found that even though RAG can improve the re-\\nsponse accuracy of LLMs, they still suffer from the above-\\nmentioned challenges significantly. Specifically, we found\\nthat even though LLMs demonstrate some level of noise ro-\\nbustness, they tend to confuse similar information and fre-\\nquently generate inaccurate answers when relevant informa-\\ntion exists. For example, when faced with a question about\\nthe 2022 Nobel Prize in Literature, if there are noisy docu-\\nments about the 2021 Nobel Prize in Literature in external\\ndocuments, LLMs may become confused and provide inac-\\ncurate answers. Besides, LLMs frequently fail to reject an-\\nswering and generate incorrect answers when none of the\\nexternal documents contain relevant information. Further-\\nmore, LLMs lack the ability to summarize from multiple\\ndocuments, and therefore if multiple documents are needed\\nto answer a question, LLMs often fail to provide accurate\\nanswer. Finally, we found that even when the LLMs contain\\nthe required knowledge and are given warnings about po-\\ntential risks in the retrieved information through instruction,\\nthey still tend to trust and prioritize the retrieved information\\nover their own existing knowledge. The experimental results\\nmentioned above highlight the need for further resolution of\\nimportant issues in the existing RAG method. Therefore, it\\nis crucial to exercise caution and carefully design its usage.\\nGenerally speaking, the contributions of this paper are1:\\n• We proposed to evaluate four capabilities for retrieval-\\naugmented generation of LLMs and created the\\nRetrieval-Augmented Generation Benchmark in both En-\\nglish and Chinese. To best of our knowledge, it is the first\\nbenchmark designed to assess these four capabilities for\\nretrieval-augmented generation of LLMs.\\n• We evaluated the existing LLMs using RGB and found\\nthe limitations of them in the four different abilities.\\n• We analyzed the responses of LLMs in RGB and identi-\\nfied their current shortcomings as well as suggested di-\\nrections for improvement.\\nRelated work\\nRetrieval-augmented models The knowledge stored in\\nlarge language models is commonly out-of-date (He, Zhang,\\nand Roth 2022) and they also sometimes generate hallu-\\ncination (Cao et al. 2020; Raunak, Menezes, and Junczys-\\nDowmunt 2021; Ji et al. 2023) i.e., they may generate ir-\\nrelevant or factually incorrect contents. By using external\\nknowledge as guidance, retrieval-augmented models can\\ngenerate more accurate and reliable responses (Guu et al.\\n2020; Lewis et al. 2020; Borgeaud et al. 2022; Izacard\\net al. 2022; Shi et al. 2023; Ren et al. 2023). Retrieval-\\naugmented models have achieved remarkable results in var-\\nious tasks such as open-domain QA (Izacard and Grave\\n1We will release the code and RGB of this paper in https:\\n//github.com/chen700564/RGB.News CollectionThe 2022 Nobel Prize for Physiology and Medicine was awarded on \\nMonday to Swedish scientist Svante Pä ä bo for sequencing the \\ngenome of the Neanderthal.Data adjustment \\nand filtering by \\nHuman{\\n“Question”: “Who was awarded the 2022 \\nNobel Prize for Physiology and Medicine ?”,\\n“Answer”: [\\'Svante Pä ä bo \\',\\'Svante Paabo ’]\\n}\\nData generation by \\nChatGPTRetrieve using \\nsearch engineRerank by dense \\nretrieval model\\nWe simulate the process of a user querying and obtaining \\ninformation. Suppose the user retrieves a current event news, \\nspeculate the event that the user is concerned about and the \\nquestion that he/she may want to know, and generate the key \\ninformation corresponding to the answer to the question. …\\n…\\nNews: The 2022 Nobel Prize for Physiology and Medicine was …Related event: 2022 Nobel Prize for Physiology and Medicine\\nQuestion: Who was awarded the 2022 Nobel Prize for Physiology \\nand Medicine?\\nKey information: Svante Pääbo and Svante Paabo\\ngpt-3.5-turbo apiQuery: Who was awarded the 2022 Nobel Prize for Physiology and \\nMedicine?”,{\"link\": \"https://www.nobelprize.org/prizes/medicine/\", \"title\": \\n\"The Nobel Prize in Physiology or Medicine 2022\", \"snippet\": \"The \\nNobel Assembly...\"}, ...\\nGoogle Search APIChun2ChunkWho was awarded the 2022 Nobel \\nPrize for Physiology and Medicine?”,Dense retrieval modelTop1 Chunk Top30 Chunk Top2 Chunk ……\\n……Figure 2: The process of data generation. Firstly, we use\\nmodels to extract (event, question, answer) from news ar-\\nticles. Next, we utilize search engines to retrieve relevant\\nweb pages. Finally, a dense retrieval model is employed to\\nre-rank the content of these web pages.\\n2021; Trivedi et al. 2023; Li et al. 2023a), dialogue (Cai\\net al. 2019a,b; Peng et al. 2023), domain-specific ques-\\ntion answering (Cui et al. 2023) and code generation (Zhou\\net al. 2023b). Recently, with the development of large mod-\\nels, a series of retrieval-enhanced tools and products have\\ngained widespread attention, such as ChatGPT retrieval plu-\\ngin, Langchain, New Bing, etc. However, in real-world sce-\\nnarios, the retrieved text inevitably contains noise. There-\\nfore, in this paper we conducted a systematic evaluation and\\nanalysis of retrieval-augmented generation in LLMs.\\nEvaluation of LLMs Evaluating LLMs has received sig-\\nnificant attention due to their remarkable general capabil-\\nity (Chang et al. 2023). It enables us to gain a deeper under-\\nstanding of the specific abilities and limitations of LLMs,\\nwhile also providing valuable guidance for future research.\\nIn the past, benchmarks such as GLUE (Wang et al. 2019b)\\nand SuperCLUE (Wang et al. 2019a) primarily focused on\\nevaluating NLP tasks, particularly in natural language un-\\nderstanding. However, these evaluations often fail to fully\\ncapture the capabilities of LLMs. MMLU (Hendrycks et al.\\n2021) was then proposed to measure the knowledge acquired\\nby language models when pre-training. Recently, with the\\ndevelopment of LLMs, a series of general evaluation bench-\\nmarks have emerged, such as AGIEval (Zhong et al. 2023),\\nC-Eval (Huang et al. 2023), AlpacaEval (Li et al. 2023b),OpenLLM Leaderboard (Edward Beeching 2023), etc. In\\naddition to general abilities, there are also specific bench-\\nmarks that focus on evaluating the capabilities of models.\\nFor example, CValues (Xu et al. 2023a) focuses on the safety\\nand responsibility of LLMs, M3Exam (Zhang et al. 2023)\\nfocuses on human exam and ToolBench (Qin et al. 2023)\\nevaluates how well LLMs use external tools. Recently, Ad-\\nlakha et al. (2023) evaluate the RAG of LLMs in exist QA\\ndataset. Different from their work, we focus on 4 required\\nabilities of RAG and create Retrieval-Augmented Genera-\\ntion Benchmark to evaluate the LLMs.\\nRetrieval-Augmented Generation Benchmark\\nIn this section, we first introduce the specific retrieval-\\naugmented generation abilities we aim to evaluate in LLMs.\\nNext, we outline the process of constructing the RAG bench-\\nmark for evaluation. Lastly, we present the evaluation met-\\nrics.\\nRequired abilities of RAG\\nExternal knowledge is the key to resolving the problems\\nof LLMs such as hallucination and outdated knowledge,\\nwhich can make LLMs generate more accurate and reliable\\nresponses through retrieval-augmented generation (RAG).\\nHowever, LLMs cannot always response as expected with\\nRAG. For one thing, there are numerous irrelevant docu-\\nments and false information on the Internet. Incorporating\\nthese external documents into LLMs could have a detrimen-\\ntal effect. For anthoer, LLMs suffer from the unreliable gen-\\neration challenge. The generation of LLMs is often unpre-\\ndictable, and we cannot guarantee that they will utilize the\\nuseful information entailed in the external documents. Ad-\\nditionally, LLMs can easily be misled by incorrect infor-\\nmation in the document. To this end, we build Retrieval-\\nAugmented Generation Benchmark (RGB) to evaluate the\\nretrieval-augmented generation of LLMs, and we concern\\nabout 4 specific abilities:\\nNoise Robustness is the robustness of LLMs in noisy\\ndocuments. As retrievers are not perfect, the external knowl-\\nedge they retrieve often contains a significant amount of\\nnoise, i.e., documents which are relevant to the question but\\ndo not contain any information about the answer. To effec-\\ntively answer user questions, LLMs must be able to extract\\nthe necessary information from documents despite there are\\nnoisy documents.\\nNegative Rejection is a measure of whether LLMs can\\ndecline to answer a question when none of the contexts pro-\\nvide useful information. In real-world situations, the search\\nengine often fails to retrieve documents containing the an-\\nswers. In these cases, it is important for the model to have\\nthe capability to reject recognition and avoid generating mis-\\nleading content.\\nInformation Integration is a capacity to integrate an-\\nswers from multiple documents. In many cases, the an-\\nswer to a question may be contained in multiple documents.\\nFor example, for the question ”Who are the champions of\\nthe U.S. Open 2022 men’s and women’s singles?” , the two\\nchampions may be mentioned in different documents. In or-der to provide better answers to complex questions, it is nec-\\nessary for LLMs to have the ability to integrate information.\\nCounterfactual Robustness refers to a capacity to han-\\ndle errors in external knowledge. In the real world, there is\\nan abundance of false information on the internet. Please\\nnote that we only evaluate the situation that LLMs are given\\nwarnings about potential risks in the retrieved information\\nthrough instruction.\\nIn real-world scenarios, it is not possible to obtain per-\\nfect documents with all the necessary external knowledge.\\nTherefore, evaluating these four abilities of the model be-\\ncomes essential in order to measure the RAG of LLMs.\\nData construction\\nInspired by previous benchmarks for LLMs, RGB utilizes\\na question-answering format for evaluation. We evaluate the\\nLLMs by judging the retrieval-augmented responses of them\\nto the questions. To simulate real-world scenarios, we con-\\nstruct question and answer data using actual news articles.\\nDue to the abundance of knowledge contained within the\\nLLMs there is a potential for bias when measuring the first\\nthree abilities. To mitigate this, the instances of RGB are\\nconstructed by latest news articles. Additionally, we retrieve\\nexternal documents from Internet through search engines.\\nFinally, we expand the corpus and divided it into 4 testbeds\\nto evaluate the above basic abilities of LLMs. The overall\\nprocedure of our data construction is illustrated in Figure 2.\\nQA instances generation. We first collect latest news ar-\\nticles and use prompts to have ChatGPT generate events,\\nquestions, and answers for each articles. For example, as\\nshown in the Figure 2, for a report about “The 2022 Nobel\\nPrize”, ChatGPT will generate corresponding event, ques-\\ntion and provide key information for answering it. By gen-\\nerating events, the model is able to preliminarily filter out\\nnews articles that do not contain any events. After genera-\\ntion, we manually check the answer and filter out data that\\nis difficult to retrieve through search engines.\\nRetrieve using search engine. For each query, we use\\nGoogle’s API to fetch 10 relevant web pages and extract\\ncorresponding snippets of text from them. Simultaneously,\\nwe read these web pages and convert their textual content\\ninto text chunks with a maximum length of 300 tokens. Us-\\ning an existing dense retrieval model2, we select the top-30\\ntext chunks that match the query most effectively. These re-\\ntrieved text chunks, along with the snippets provided by the\\nsearch API, will serve as our external documents. These doc-\\numents will be divided into positive documents and negative\\ndocuments based on whether they contain the answer.\\nTestbeds construction for each ability. We expand the\\ncorpus and divided it into 4 testbeds to evaluate the above\\nbasic abilities of LLMs. To evaluate the noise robustness,\\nwe sample varying numbers of negative documents ac-\\ncording to the desired ratio of noises. For negative rejec-\\ntion, all the external documents are sampled from negative\\ndocuments. For the information integration ability, we fur-\\n2https://huggingface.co/sentence-transformers/all-mpnet-base-\\nv2 for English; https://huggingface.co/moka-ai/m3e-base for\\nChinese.\\nSystem instruction\\nYou are an accurate and reliable AI assistant that can \\nanswer questions with the help of external documents. \\nPlease note that external documents may contain noisy \\nor factually incorrect information. If the information in \\nthe document contains the correct answer, you will give \\nan accurate answer. If the information in the document \\ndoes not contain the answer, you will generate ’I can not \\nanswer the question because of the insufficient \\ninformation in documents.‘ If there are inconsistencies \\nwith the facts in some of the documents, please generate \\nthe response \\'There are factual errors in the provided \\ndocuments.\\' and provide the correct answer.\\nUser input Instruction\\nDocument: \\\\n{DOCS} \\\\n\\\\nQuestion: \\\\n{QUERY}System instruction\\n你是一个准确和可靠的人工智能助手，\\n能够借助外部文档回答问题，请注意\\n外部文档可能存在噪声事实性错误。\\n如果文档中的信息包含了正确答案，\\n你将进行准确的回答。如果文档中的\\n信息不包含答案，你将生成“文档信\\n息不足，因此我无法基于提供的文档\\n回答该问题。”如果部分文档中存在\\n与事实不一致的错误，请先生成“提\\n供文档的文档存在事实性错误。”，\\n并生成正确答案。\\nUser input Instruction\\n文档：\\\\n{DOCS} \\\\n\\\\n问题：\\\\n{QUERY}English ChineseFigure 3: The instructions used in our experiments, which\\ninclude a system instruction followed by a user input instruc-\\ntion. The “ {DOCS }” and “ {QUERY }” will be replaced by\\nthe external documents and the question.\\nther construct data based on the above generated questions.\\nThis involves expanding or rewriting these questions so that\\ntheir answers encompass multiple aspects. For example, the\\nquestion “Who won the MVP of Super Bowl 2023?” can\\nbe rewrite as “Who won the MVPs of Super Bowl 2022\\nand 2023?”. Consequently, answering such questions re-\\nquires utilizing information from various documents. Dif-\\nferent from the first three abilities, the data of counterfactual\\nrobustness is constructed solely based on the internal knowl-\\nedge of the model. Based on the aforementioned generated\\nquestions mentioned above, we adopt ChatGPT to automat-\\nically generate its known knowledge. Specifically, we use\\nprompts to allow the model to generate both questions and\\nanswers that are already known. For example, based on the\\nquestion “Who was awarded the 2022 Nobel Prize for Phys-\\niology and Medicine?”, the model will generate the known\\nquestion “Who was awarded the 2021 Nobel Prize in Lit-\\nerature?” and answer “ Abdulrazak Gurnah ”. We then man-\\nually verified the generated answers, and retrieve relevant\\ndocuments as described above. In order to make documents\\ncontain factual errors, we manually modify the answers and\\nreplace the corresponding parts in the document.\\nFinally, we collect totally 600 base questions in RGB,\\nand 200 additional questions for the information integration\\nability and 200 additional questions for counterfactual ro-\\nbustness ability. Half of the instances are in English, and the\\nother half are in Chinese.\\nEvaluation metrics\\nThe core of this benchmark is to evaluate whether LLMs can\\nutilize the provided external documents to acquire knowl-\\nedge and generate reasonable answers. We evaluate the re-\\nsponses of LLMs in order to measure above-mentioned four\\nabilities of them.\\nAccuracy is used to measure noise robustness and infor-\\nmation integration We employ an exact matching approach\\nwhere if the generated text contains an exact match to the\\nanswer, it is considered as a correct answer.\\nRejection rate is used to measure negative rejection.\\nWhen only noisy documents are provided, LLMs should\\noutput the specific content – ”I can not answer the question\\nbecause of the insufficient information in documents.” (WeEnglish Chinese\\nNoise Ratio 0 0.2 0.4 0.6 0.8 0 0.2 0.4 0.6 0.8\\nChatGPT (OpenAI 2022) 96.33 94.67 94.00 90.00 76.00 95.67 94.67 91.00 87.67 70.67\\nChatGLM-6B (THUDM 2023a) 93.67 90.67 89.33 84.67 70.67 94.33 90.67 89.00 82.33 69.00\\nChatGLM2-6B (THUDM 2023b) 91.33 89.67 83.00 77.33 57.33 86.67 82.33 76.67 72.33 54.00\\nVicuna-7B-v1.3 (Chiang et al. 2023) 87.67 83.33 86.00 82.33 60.33 85.67 82.67 77.00 69.33 49.67\\nQwen-7B-Chat (QwenLM 2023) 94.33 91.67 91.00 87.67 73.67 94.00 92.33 88.00 84.33 68.67\\nBELLE-7B-2M (Yunjie Ji 2023) 83.33 81.00 79.00 71.33 64.67 92.00 88.67 85.33 78.33 67.68\\nTable 1: The experimental result of noise robustness measured by accuracy (%) under different noise ratios. We can see that the\\nincreasing noise rate poses a challenge for RAG in LLMs.\\nLong-distance information. Evidence uncertainty. Concept confusion.\\nQuestion Who did Iga Swiatek defeat to win the Qatar Open 2022? What is the name of Apple’s headset? What was Tesla’s revenue in Q1 2022?\\nAnswer Anett Kontaveit Vision Pro 18.76 billion\\nDocumentsPositive document\\nIn February, Swiatek entered into the Qatar Open ...\\nIn the final, she won ... Anett Kontaveit ...\\nNegative document\\nThis time, she defeated Ons Jabeur 6-2, 7-6(5) to win\\nthe 2022 US Open, ...Positive document\\nApple (AAPL.O) on Monday unveiled a costly\\naugmented-reality headset called the Vision Pro ...\\nNegative document\\n... is what Gurman believes will be called\\nApple Reality Pro. ...Positive document\\nTesla, Inc. (TSLA) reported Q1 FY 2022 earnings results\\n... detailed revenues of $ 18.76 billion ...\\nNegative document\\n...first-quarter earnings for 2022 ...\\n...Automotive revenue reached $16.86 billion...\\nResponsesIga Swiatek defeated Ons Jabeur in the second round\\nof the Qatar Open 2022 to win the tournament.According to the document, the name of Apple’s\\nheadset is Apple Reality Pro.According to the financial results provided in the article,\\nTesla’s revenue in Q1 2022 was $16.86 billion.\\nTable 2: Error cases of noise robustness, and only one positive document and one negative document are shown. The responses\\nare generated by ChatGLM2-6B. The blue text indicates the matching parts between the document and the question or answer,\\nwhile the red text highlights the non-matching parts.\\nuse instructions to inform the model.). If the model gener-\\nates this content, it indicates a successful rejection.\\nError detection rate measures whether the model can\\ndetect the factual errors in the documents for counterfactual\\nrobustness. When the provided documents contain factual\\nerrors, the model should output the specific content – ”There\\nare factual errors in the provided documents.” (We use in-\\nstructions to inform the model.). If the model generates this\\ncontent, it indicates that the model has detected erroneous\\ninformation in the document.\\nError correction rate measures whether the model can\\nprovide the correct answer after identifying errors for coun-\\nterfactual robustness. The model is asked to generate the cor-\\nrect answer after identifying the factual errors. If the model\\ngenerates the correct answer, it indicates that the model is\\ncapable of correcting errors in the document.\\nThe formula for the metrics is\\nACC =#tt\\n#nums(1)\\nwhere #tt is the number of correct responses and #nums is\\nthe number of instances to be evaluated.\\nConsidering that the model may not fully adhere to in-\\nstructions, for rejection rate and error detection rate, we\\nalso use ChatGPT to conduct additional evaluation of the\\nanswers. Specifically, we assess the model’s responses by\\nusing instructions and demonstrations to determine if they\\ncan reflect information that is not present in the document or\\nidentify any factual errors.Experiments\\nIn this section, we evaluate the performance of various\\nLLMs, analyze and discuss the results, summarizing the\\nmain challenges that existing LLMs encounter when using\\nexternal knowledge.\\nSettings\\nTask formats. Due to contextual limitations, we provide 5\\nexternal documents for each question. In our experiments\\non noise robustness, we evaluate scenarios with noise ra-\\ntios ranging from 0 to 0.8. To comprehensively evaluate the\\noverall capabilities, we have adopted a unified instruction\\nfor each language, as shown in Figure 3. The experiments\\nwere conducted using an NVIDIA GeForce RTX 3090.\\nModels We conduct evaluation on 6 state-of-the-art large\\nlanguage models which can generate both English and\\nChinese including ChatGPT3, ChatGPT (OpenAI 2022),\\nChatGLM-6B (THUDM 2023a), ChatGLM2-6B (THUDM\\n2023b), Vicuna-7b-v1.3 (Chiang et al. 2023), Qwen-7B-\\nChat (QwenLM 2023), BELLE-7B-2M (Yunjie Ji 2023).\\nResults on Noise Robustness\\nWe evaluated the accuracy based on the different noise ratios\\nin external documents, and the results are shown in Table 1.\\nWe can see that:\\n(1) RAG can effect improve the responses of LLMs.\\nLLMs have shown strong performance even in the presence\\nof noise, indicating that RAG is a promising way for LLMs\\nto generate accurate and reliable responses.\\n3We use gpt-3.5-turbo api in the experiments.(2) The increasing noise rate poses a challenge for\\nRAG in LLMs. Specifically, when the noise ratio exceeds\\n80%, the accuracy of the model decreases significantly. For\\nexample, the performance of ChatGPT has decreased from\\n96.33% to 76.00%, while the performance of ChatGLM2-\\n6B has decreased from 91.33% to 57.33%.\\nError Analysis. To better comprehend the negative im-\\npact of noise on model generation, we examined the incor-\\nrect answers and found that these errors typically originate\\nfrom three reasons, as shown in Table 2.\\n(1) Long-distance information. When dealing with an\\nexternal document, the model often faces difficulty in iden-\\ntifying the correct answer when the information related to\\nthe question is distant from the information related to the an-\\nswer. This scenario is quite common as longer texts are fre-\\nquently encountered on the internet. In such cases, it is typ-\\nical for the question’s information to be initially presented\\nat the start of the document and subsequently referred to us-\\ning pronouns. In Table 2, the question information (”Qatar\\nOpen 2022”) is only mentioned once at the beginning and is\\nfar from where the answer text ”Anett Kontaveit” appears.\\nThis situation may cause LLMs to depend on information\\nfrom other documents and create false impressions, i.e., hal-\\nlucination.\\n(2) Evidence uncertainty. Before highly anticipated\\nevents, like the release of new Apple products or the an-\\nnouncement of the Oscars, there is often a significant\\namount of speculative information circulating on the inter-\\nnet. Although the relevant documents explicitly state that\\nit is uncertain or speculative content, they can still impact\\non the retrieval-augmented generation of LLMs. In Table 2,\\nwhen the noise ratio increases, the content of erroneous\\ndocuments is all about some people’s predictions about the\\nname of the headset (“Apple Reality Pro”). Even if there is a\\ncorrect answer (“Vision Pro”) in the relevant documents, the\\nmodel can still be misled by uncertain evidences.\\n(3) Concept confusion. The concepts in external docu-\\nments may be similar to, but different from, the concepts in\\nthe question. This can cause confusion for LLMs and make\\nLLMs generate incorrect answers. In Table 2, the model an-\\nswer focuses on the concept “automotive revenue” in the\\ndocument rather than “revenue” in the question.\\nBased on the analysis above, we have identified certain\\nlimitations in LLMs regarding retrieval-augmented genera-\\ntion. To effectively handle the vast amount of noise present\\non the internet, further detailed enhancements are required\\nfor the model such as long documents modeling and precise\\nconcept comprehension.\\nResults on Negative Rejection testbed\\nWe evaluated the rejection rate when only noise documents\\nwere provided. The results are shown in Table 3. In addi-\\ntion to evaluating the rejection rate through exact matching\\n(Rej in Table 3), we also utilize ChatGPT to determine if\\nthe responses from the LLMs contain any rejection informa-\\ntion (Rej∗in Table 3). We can see that: Negative Rejection\\nposes a challenge for RAG in LLMs. The highest rejection\\nrates for LLMs in English and Chinese were only 45% andLanguages English Chinese\\nRej Rej∗Rej Rej∗\\nChatGPT 24.67 45.00 5.33 43.33\\nChatGLM-6B 9.00 25.00 6.33 17.00\\nChatGLM2-6B 10.33 41.33 6.33 36.33\\nVicuna-7B-v1.3 17.00 33.33 3.37 24.67\\nQwen-7B-Chat 31.00 35.67 8.67 25.33\\nBELLE-7B-2M 5.67 32.33 5.33 13.67\\nTable 3: The result of negative rejection. Rej means the re-\\njection rate (%) and Rej∗means the rejection rate evaluated\\nby ChatGPT. We can see that negative rejection poses a chal-\\nlenge for RAG in LLMs.\\nQuestion Answer Response\\nwho will direct\\nIrredeemable film?Jeymes\\nSamuelThe information provided in the document\\nis that Fox has attached ”The Big Short”\\nwriter-director Adam McKay to its movie\\nadaptation of ”Irredeemable” from Boom!\\nComics. Therefore, the answer is Adam McKay.\\nWhich country won the\\nmost medals at the\\n2022 Winter Olympics?NorwayBased on the information provided in the\\ndocument, the country that won the most medals\\nat the 2022 Winter Olympics is Germany.\\nGermany has won a total of 31 medals, including 14\\ngold medals, 14 bronze medals, and 31 silver medals.\\nTable 4: Error cases of negative rejection generated by\\nChatGLM2-6B. The red text highlights the error answers.\\n43.33%, respectively. This suggests that LLMs can be easily\\nmisled by noisy documents, leading to incorrect answers.\\nIn addition, through comparing Rej and Rej∗, we found\\nthat LLMs fail to strictly follow instructions, and they often\\ngenerate unpredictable responses, which make it hard to use\\nthem as state triggers (such as for recognizing rejection).\\nWe conduct case studies in Table 4. The first error is\\nbecause of Evidence uncertainty . Although the document\\nonly mentions contact with “Adam McKay” and does not\\nexplicitly state that he is the director of the movie, the\\nmodel still concludes that he holds this role. The first er-\\nror is because of Concept confusion . The information pro-\\nvided in the answer pertains to “the 2018 Winter Olympics”\\ninstead of “the 2022 Olympics” mentioned in the question.\\nRetrieval-augmented generation poses a greater challenge of\\nnegative rejection compared to answer directly as it presents\\nrelevant documents that could potentially mislead the LLMs\\nand result in incorrect responses. In future developments, it\\nwill be crucial for LLMs to enhance their ability to accu-\\nrately match questions with the appropriate documents.\\nResults on Information Integration testbed\\nWe evaluated the accuracy based on the different noise ratios\\nin external documents, and the results are shown in Table 5.\\nWhen comparing the model to Table 1, we observed that\\nit has a weak information integration ability, which in turn\\naffects its noise robustness. We can see that:\\n(1) Information integration poses a challenge for RAG\\nin LLMs. Even without noise, the highest accuracy of LLMs\\ncan only reach 60% and 67% for English and Chinese,\\nrespectively. After adding noise, the highest accuracy de-English Chinese\\nNoise Ratio 0 0.2 0.4 0 0.2 0.4\\nChatGPT 55 51 34 63 58 47\\nChatGLM-6B 45 36 35 60 53 52\\nChatGLM2-6B 34 32 21 44 43 32\\nVicuna-7B-v1.3 60 53 43 43 36 25\\nQwen-7B-Chat 55 50 37 67 56 55\\nBELLE-7B-2M 40 34 24 49 41 38\\nTable 5: The experimental result of information integration\\nmeasured by accuracy (%) under different noise ratios. We\\ncan see that information integration poses a challenge for\\nRAG in LLMs.\\nQuestion Answer Response Errors\\nWhat groupings are France and\\nGermany in World Cup 2022?Group D\\nGroup EFrance and Germany are in\\nGroup D of the 2022 FIFA\\nWorld Cup.Merging\\nError\\nWho were the MVP of Super\\nBowl 2022 and 2023?Cooper Kupp\\nPatrick\\nMahomesAccording to the provided\\ndocument, the MVP of Super\\nBowl LVI was Cooper Kupp\\nof the Los Angeles Rams.Ignoring\\nError\\nWhat films won the 2022 and\\n2023 Academy Awards for\\nBest Picture?CODA\\nEverything\\nEverywhere\\nAll at OnceThe film CODA won the\\naward for Best Picture at the\\n95th Academy Awards\\nceremony held on 2023.Misalignment\\nError\\nTable 6: Error cases of information integration, the re-\\nsponses are generated by ChatGLM2-6B. The blue and red\\ntexts represent the answers to two sub-questions.\\ncreases to 43% and 55%. These results suggest that LLMs\\nstruggle with integrating information effectively and are not\\nwell-suited for directly answering complex questions.\\n(2) Complex questions are more challenging for RAG\\nwith noisy documents. Performance decline becomes sig-\\nnificant when the noise ratio is 0.4, but for simple problems,\\na significant decline occurs only at a noise ratio of 0.8. This\\nindicates that complex problems are more vulnerable to in-\\nterference from noise. We speculate that this is because solv-\\ning complex problems requires integrating information from\\nmultiple documents, and this information can be considered\\nas noise to each other, making it harder for the model to ex-\\ntract relevant information from the documents.\\nError Analysis. We conducted an error analysis on\\nChatGLM2-6B (noise ratio is 0). Apart from the similar er-\\nrors founded in the noise robustness experiment (38% of the\\ntotal), there are also three types of unique errors. We have\\npresented these cases in Table 6.\\n(1) Merging Error (28% of the total). The model some-\\ntimes merges the answers of the two sub-questions, resulting\\nin an error. It mistakenly uses the answer from one question\\nto address both two questions. At this point, the model will\\ndisregard any documents related to one sub-question. For\\nexample, in Table 6, it incorrectly states that Group D is the\\nWorld Cup group for both France and Germany, while in fact\\nGermany is actually assigned to Group E.\\n(2) Ignoring Error (28% of the total). Sometimes, the\\nmodel may ignore one of the sub-questions and only answer\\nthe other. This error occurs when the model lacks a complete\\nunderstanding of the problem and fails to recognize that it\\nconsists of multiple sub-problems. As a result, the modelAcc Acc doc ED ED∗CR\\nChatGPT-zh 91 17 1 3 33.33\\nQwen-7B-Chat-zh 77 12 5 4 25.00\\nChatGPT-en 89 9 8 7 57.14\\nTable 7: The result of counterfactual robustness. ACC is the\\naccuracy (%) of LLMs without external documents. ACC doc\\nis the accuracy (%) of LLMs with counterfactual documents.\\nED and ED∗are error detection rates evaluated by exact\\nmatching and ChatGPT, respectively. CR is the error cor-\\nrection rate.\\nonly considers relevant documents for one sub-problem in\\norder to generate an answer, disregarding the question posed\\nby another sub-problem. For example, in Table 6, the model\\nonly provides the answer for the MVP of Super Bowl 2022\\nand does not consider 2023.\\n(3) Misalignment Error (6% of the total). Sometimes,\\nthe model incorrectly identifies the documents for one sub-\\nquestion as the documents for another sub-question, leading\\nto misaligned answers. For example, in Table 6, the third an-\\nswer has two errors: an ignoring error and a misalignment er-\\nror. Firstly, the model only mentioned the Best Picture of the\\n2023 (95th) Academy Awards, completely disregarding the\\n2022 awards. Additionally, it incorrectly stated that “CODA”\\nis the Best Picture of 2023 when it was actually awarded as\\nthe Best Picture in 2022.\\nThe errors mentioned above are primarily caused by the\\nlimited understanding of complex questions, which hinders\\nthe ability to effectively utilize information from different\\nsub-problems. The key lies in improving the model’s rea-\\nsoning capability. One possible solution is to use a chain-of-\\nthought approach to break down complex problems (Zhou\\net al. 2023a; Xu et al. 2023b; Drozdov et al. 2023). How-\\never, these methods slow down the inference speed and can-\\nnot provide timely responses.\\nResults on Counterfactual Robustness testbed\\nIn order to ensure that LLMs possess relevant knowledge,\\nwe assess their performance by directly asking them ques-\\ntions. We only consider LLMs that have an accuracy rate of\\nover 70%. The results are shown in Table 7. We present the\\nfollowing metrics: accuracy without any documents, accu-\\nracy with counterfactual documents, error detection rates,\\nand error correction rates. We can see that It is hard for\\nLLMs to identify and correct factual errors in the docu-\\nments. This suggests that the model can be easily misled by\\ndocuments containing incorrect facts.\\nIt is important to note that retrieval-augmented generation\\nis not designed to automatically address factual errors within\\na given context, as this contradicts the underlying assump-\\ntion that the model lacks knowledge and relies on retrieved\\ndocuments for additional information. However, this issue is\\ncrucial in practical applications due to the abundance of fake\\nnews on the internet. Existing LLMs do not have a safeguard\\nto handle inaccurate responses caused by misinformation. In\\nfact, they heavily depend on the information they retrieve.\\nEven when LLMs contain the internal knowledge about thequestions, they often trust false information that is retrieved.\\nThis presents significant a challenge for the future develop-\\nment of RAG in LLMs.\\nConclusion\\nIn this paper, we evaluated four abilities of retrieval-\\naugmented generation in LLMs: noise robustness, nega-\\ntive rejection, information integration, and counterfactual\\nrobustness. To conduct the evaluation, we built Retrieval-\\nAugmented Generation Benchmark (RGB). The instances of\\nRGB are generated from latest news articles and the external\\ndocuments obtained from search engines. The experimental\\nresults suggest that current LLMs have limitations in the 4\\nabilities. This indicates that there is still a significant amount\\nof work needed to effectively apply RAG to LLMs. To en-\\nsure accurate and reliable responses from LLMs, it is crucial\\nto exercise caution and carefully design for RAG.\\nReferences\\nAdlakha, V .; BehnamGhader, P.; Lu, X. H.; Meade, N.; and\\nReddy, S. 2023. Evaluating Correctness and Faithfulness\\nof Instruction-Following Models for Question Answering.\\narXiv:2307.16877.\\nBang, Y .; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie,\\nB.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; Do, Q. V .; Xu,\\nY .; and Fung, P. 2023. A Multitask, Multilingual, Multi-\\nmodal Evaluation of ChatGPT on Reasoning, Hallucination,\\nand Interactivity. arXiv:2302.04023.\\nBian, N.; Liu, P.; Han, X.; Lin, H.; Lu, Y .; He, B.; and\\nSun, L. 2023. A Drop of Ink Makes a Million Think: The\\nSpread of False Information in Large Language Models.\\narXiv:2305.04812.\\nBorgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Ruther-\\nford, E.; Millican, K.; van den Driessche, G.; Lespiau, J.-B.;\\nDamoc, B.; Clark, A.; de Las Casas, D.; Guy, A.; Menick, J.;\\nRing, R.; Hennigan, T.; Huang, S.; Maggiore, L.; Jones, C.;\\nCassirer, A.; Brock, A.; Paganini, M.; Irving, G.; Vinyals,\\nO.; Osindero, S.; Simonyan, K.; Rae, J. W.; Elsen, E.; and\\nSifre, L. 2022. Improving language models by retrieving\\nfrom trillions of tokens. arXiv:2112.04426.\\nCai, D.; Wang, Y .; Bi, W.; Tu, Z.; Liu, X.; Lam, W.; and\\nShi, S. 2019a. Skeleton-to-Response: Dialogue Genera-\\ntion Guided by Retrieval Memory. In Proceedings of the\\n2019 Conference of the North American Chapter of the As-\\nsociation for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) , 1219–\\n1228. Minneapolis, Minnesota: Association for Computa-\\ntional Linguistics.\\nCai, D.; Wang, Y .; Bi, W.; Tu, Z.; Liu, X.; and Shi, S.\\n2019b. Retrieval-guided Dialogue Response Generation via\\na Matching-to-Generation Framework. In Proceedings of\\nthe 2019 Conference on Empirical Methods in Natural Lan-\\nguage Processing and the 9th International Joint Confer-\\nence on Natural Language Processing (EMNLP-IJCNLP) ,\\n1866–1875. Hong Kong, China: Association for Computa-\\ntional Linguistics.Cao, M.; Dong, Y .; Wu, J.; and Cheung, J. C. K. 2020. Fac-\\ntual Error Correction for Abstractive Summarization Mod-\\nels. In Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP) , 6251–\\n6258. Online: Association for Computational Linguistics.\\nChang, Y .; Wang, X.; Wang, J.; Wu, Y .; Yang, L.; Zhu,\\nK.; Chen, H.; Yi, X.; Wang, C.; Wang, Y .; Ye, W.;\\nZhang, Y .; Chang, Y .; Yu, P. S.; Yang, Q.; and Xie, X.\\n2023. A Survey on Evaluation of Large Language Models.\\narXiv:2307.03109.\\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,\\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\\nImpressing GPT-4 with 90%* ChatGPT Quality.\\nCui, J.; Li, Z.; Yan, Y .; Chen, B.; and Yuan, L. 2023. Chat-\\nLaw: Open-Source Legal Large Language Model with Inte-\\ngrated External Knowledge Bases. arXiv:2306.16092.\\nDrozdov, A.; Sch ¨arli, N.; Aky ¨urek, E.; Scales, N.; Song,\\nX.; Chen, X.; Bousquet, O.; and Zhou, D. 2023. Compo-\\nsitional Semantic Parsing with Large Language Models. In\\nThe Eleventh International Conference on Learning Repre-\\nsentations .\\nEdward Beeching, N. H. S. H. N. L. N. R. O. S. L. T.\\nT. W., Cl ´ementine Fourrier. 2023. Open LLM Leader-\\nboard. https://huggingface.co/spaces/HuggingFaceH4/\\nopen llmleaderboard.\\nGuo, B.; Zhang, X.; Wang, Z.; Jiang, M.; Nie, J.; Ding, Y .;\\nYue, J.; and Wu, Y . 2023. How Close is ChatGPT to Hu-\\nman Experts? Comparison Corpus, Evaluation, and Detec-\\ntion. arXiv:2301.07597.\\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.-W.\\n2020. REALM: Retrieval-Augmented Language Model Pre-\\nTraining. In Proceedings of the 37th International Confer-\\nence on Machine Learning , ICML’20. JMLR.org.\\nHe, H.; Zhang, H.; and Roth, D. 2022. Rethinking\\nwith Retrieval: Faithful Large Language Model Inference.\\narXiv:2301.00303.\\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Mul-\\ntitask Language Understanding. In International Conference\\non Learning Representations .\\nHuang, Y .; Bai, Y .; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.;\\nLiu, J.; Lv, C.; Zhang, Y .; Lei, J.; Fu, Y .; Sun, M.; and He,\\nJ. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese\\nEvaluation Suite for Foundation Models. arXiv preprint\\narXiv:2305.08322 .\\nIzacard, G.; and Grave, E. 2021. Leveraging Passage Re-\\ntrieval with Generative Models for Open Domain Ques-\\ntion Answering. In Proceedings of the 16th Conference of\\nthe European Chapter of the Association for Computational\\nLinguistics: Main Volume , 874–880. Online: Association for\\nComputational Linguistics.\\nIzacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni,\\nF.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and\\nGrave, E. 2022. Atlas: Few-shot Learning with Retrieval\\nAugmented Language Models. arXiv:2208.03299.Ji, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii, E.;\\nBang, Y . J.; Madotto, A.; and Fung, P. 2023. Survey of Hal-\\nlucination in Natural Language Generation. ACM Comput.\\nSurv. , 55(12).\\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V .;\\nGoyal, N.; K ¨uttler, H.; Lewis, M.; Yih, W.-t.; Rockt ¨aschel,\\nT.; Riedel, S.; and Kiela, D. 2020. Retrieval-Augmented\\nGeneration for Knowledge-Intensive NLP Tasks. In Pro-\\nceedings of the 34th International Conference on Neural\\nInformation Processing Systems , NIPS’20. Red Hook, NY ,\\nUSA: Curran Associates Inc. ISBN 9781713829546.\\nLi, D.; Rawat, A. S.; Zaheer, M.; Wang, X.; Lukasik, M.;\\nVeit, A.; Yu, F.; and Kumar, S. 2023a. Large Language\\nModels with Controllable Working Memory. In Findings of\\nthe Association for Computational Linguistics: ACL 2023 ,\\n1774–1793. Toronto, Canada: Association for Computa-\\ntional Linguistics.\\nLi, X.; Zhang, T.; Dubois, Y .; Taori, R.; Gulrajani, I.;\\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023b. Al-\\npacaEval: An Automatic Evaluator of Instruction-following\\nModels. https://github.com/tatsu-lab/alpaca eval.\\nLi, X.; Zhu, X.; Ma, Z.; Liu, X.; and Shah, S. 2023c. Are\\nChatGPT and GPT-4 General-Purpose Solvers for Financial\\nText Analytics? An Examination on Several Typical Tasks.\\narXiv:2305.05862.\\nLiu, N. F.; Zhang, T.; and Liang, P. 2023. Evaluating Verifi-\\nability in Generative Search Engines. arXiv:2304.09848.\\nMaynez, J.; Narayan, S.; Bohnet, B.; and McDonald, R.\\n2020. On Faithfulness and Factuality in Abstractive Sum-\\nmarization. In Proceedings of the 58th Annual Meeting of\\nthe Association for Computational Linguistics , 1906–1919.\\nOnline: Association for Computational Linguistics.\\nOpenAI. 2022. Chatgpt: Optimizing language models for\\ndialogue. https://openai.com/blog/chatgpt.\\nPeng, B.; Galley, M.; He, P.; Cheng, H.; Xie, Y .; Hu, Y .;\\nHuang, Q.; Liden, L.; Yu, Z.; Chen, W.; and Gao, J. 2023.\\nCheck Your Facts and Try Again: Improving Large Lan-\\nguage Models with External Knowledge and Automated\\nFeedback. arXiv:2302.12813.\\nQin, Y .; Liang, S.; Ye, Y .; Zhu, K.; Yan, L.; Lu, Y .; Lin, Y .;\\nCong, X.; Tang, X.; Qian, B.; Zhao, S.; Tian, R.; Xie, R.;\\nZhou, J.; Gerstein, M.; Li, D.; Liu, Z.; and Sun, M. 2023.\\nToolLLM: Facilitating Large Language Models to Master\\n16000+ Real-world APIs. arXiv:2307.16789.\\nQwenLM. 2023. Qwen-7B. https://github.com/QwenLM/\\nQwen-7B.\\nRaunak, V .; Menezes, A.; and Junczys-Dowmunt, M. 2021.\\nThe Curious Case of Hallucinations in Neural Machine\\nTranslation. In Proceedings of the 2021 Conference of the\\nNorth American Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies , 1172–\\n1183. Online: Association for Computational Linguistics.\\nRen, R.; Wang, Y .; Qu, Y .; Zhao, W. X.; Liu, J.; Tian, H.;\\nWu, H.; Wen, J.-R.; and Wang, H. 2023. Investigating the\\nFactual Knowledge Boundary of Large Language Models\\nwith Retrieval Augmentation. arXiv:2307.11019.Shen, X.; Chen, Z.; Backes, M.; and Zhang, Y . 2023. In\\nChatGPT We Trust? Measuring and Characterizing the Re-\\nliability of ChatGPT. arXiv:2304.08979.\\nShi, W.; Min, S.; Yasunaga, M.; Seo, M.; James, R.;\\nLewis, M.; Zettlemoyer, L.; and tau Yih, W. 2023. RE-\\nPLUG: Retrieval-Augmented Black-Box Language Models.\\narXiv:2301.12652.\\nTHUDM. 2023a. ChatGLM-6B. https://github.com/\\nTHUDM/ChatGLM-6B.\\nTHUDM. 2023b. ChatGLM2-6B. https://github.com/\\nTHUDM/ChatGLM2-6B.\\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\\nS.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucu-\\nrull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.;\\nGao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hosseini,\\nS.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa, M.;\\nKloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.;\\nLavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\\nX.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poul-\\nton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.;\\nSilva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang,\\nB.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.;\\nZarov, I.; Zhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Ro-\\ndriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023.\\nLlama 2: Open Foundation and Fine-Tuned Chat Models.\\narXiv:2307.09288.\\nTrivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal,\\nA. 2023. Interleaving Retrieval with Chain-of-Thought Rea-\\nsoning for Knowledge-Intensive Multi-Step Questions. In\\nProceedings of the 61st Annual Meeting of the Associa-\\ntion for Computational Linguistics (Volume 1: Long Papers) ,\\n10014–10037. Toronto, Canada: Association for Computa-\\ntional Linguistics.\\nWang, A.; Pruksachatkun, Y .; Nangia, N.; Singh, A.;\\nMichael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019a. Su-\\nperGLUE: A Stickier Benchmark for General-Purpose Lan-\\nguage Understanding Systems . Red Hook, NY , USA: Curran\\nAssociates Inc.\\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\\nBowman, S. R. 2019b. GLUE: A Multi-Task Benchmark\\nand Analysis Platform for Natural Language Understanding.\\nInInternational Conference on Learning Representations .\\nXu, G.; Liu, J.; Yan, M.; Xu, H.; Si, J.; Zhou, Z.; Yi, P.;\\nGao, X.; Sang, J.; Zhang, R.; Zhang, J.; Peng, C.; Huang, F.;\\nand Zhou, J. 2023a. CValues: Measuring the Values of Chi-\\nnese Large Language Models from Safety to Responsibility.\\narXiv:2307.09705.\\nXu, S.; Pang, L.; Shen, H.; Cheng, X.; and Chua, T.-\\nS. 2023b. Search-in-the-Chain: Towards Accurate, Credi-\\nble and Traceable Large Language Models for Knowledge-\\nintensive Tasks. arXiv:2304.14732.\\nYunjie Ji, Y . G. Y . P. Q. N. B. M. X. L., Yong Deng. 2023.\\nBELLE: Bloom-Enhanced Large Language model Engine.\\nhttps://github.com/LianjiaTech/BELLE.Zhang, W.; Aljunied, S. M.; Gao, C.; Chia, Y . K.; and Bing,\\nL. 2023. M3Exam: A Multilingual, Multimodal, Multilevel\\nBenchmark for Examining Large Language Models.\\nZhong, W.; Cui, R.; Guo, Y .; Liang, Y .; Lu, S.; Wang,\\nY .; Saied, A.; Chen, W.; and Duan, N. 2023. AGIEval:\\nA Human-Centric Benchmark for Evaluating Foundation\\nModels. arXiv:2304.06364.\\nZhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q. V .; and\\nChi, E. H. 2023a. Least-to-Most Prompting Enables Com-\\nplex Reasoning in Large Language Models. In The Eleventh\\nInternational Conference on Learning Representations .\\nZhou, S.; Alon, U.; Xu, F. F.; Jiang, Z.; and Neubig, G.\\n2023b. DocPrompting: Generating Code by Retrieving the\\nDocs. In The Eleventh International Conference on Learn-\\ning Representations .',\n",
       " 'Generation-Augmented Retrieval for Open-Domain Question Answering\\nYuning Mao1\\x03, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\\nJianfeng Gao3, Jiawei Han1, Weizhu Chen2\\n1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\\n1fyuningm2, hanjg@illinois.edu\\n2;3fpenhe, xiaodl, yeshe, jfgao,wzchen g@microsoft.com\\nAbstract\\nWe propose Generation-Augmented Retrieval\\n(GAR) for answering open-domain questions,\\nwhich augments a query through text genera-\\ntion of heuristically discovered relevant con-\\ntexts without external resources as supervi-\\nsion. We demonstrate that the generated con-\\ntexts substantially enrich the semantics of the\\nqueries and G ARwith sparse representations\\n(BM25) achieves comparable or better per-\\nformance than state-of-the-art dense retrieval\\nmethods such as DPR (Karpukhin et al., 2020).\\nWe show that generating diverse contexts for a\\nquery is beneﬁcial as fusing their results con-\\nsistently yields better retrieval accuracy. More-\\nover, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it\\nis infeasible to examine every piece of information\\nin the entire document collection ( e.g., millions\\nof Wikipedia passages) and the retrieval accuracy\\nbounds the performance of the (extractive) reader.\\n\\x03Work was done during internship at Microsoft Azure AI.\\n1Our code and retrieval results are available at https:\\n//github.com/morningmoni/GAR .Early OpenQA systems (Chen et al., 2017)\\nuse classic retrieval methods such as TF-IDF and\\nBM25 with sparse representations. Sparse methods\\nare lightweight and efﬁcient, but unable to per-\\nform semantic matching and fail to retrieve rele-\\nvant passages without lexical overlap. More re-\\ncently, methods based on dense representations\\n(Guu et al., 2020; Karpukhin et al., 2020) learn to\\nembed queries and passages into a latent vector\\nspace, in which text similarity beyond lexical over-\\nlap can be measured. Dense retrieval methods can\\nretrieve semantically relevant but lexically differ-\\nent passages and often achieve better performance\\nthan sparse methods. However, the dense mod-\\nels are more computationally expensive and suffer\\nfrom information loss as they condense the entire\\ntext sequence into a ﬁxed-size vector that does not\\nguarantee exact matching (Luan et al., 2020).\\nThere have been some recent studies on query re-\\nformulation with text generation for other retrieval\\ntasks, which, for example, rewrite the queries to\\ncontext-independent (Yu et al., 2020; Lin et al.,\\n2020; Vakulenko et al., 2020) or well-formed (Liu\\net al., 2019) ones. However, these methods re-\\nquire either task-speciﬁc data ( e.g., conversational\\ncontexts, ill-formed queries) or external resources\\nsuch as paraphrase data (Zaiem and Sadat, 2019;\\nWang et al., 2020) that cannot or do not trans-\\nfer well to OpenQA. Also, some rely on time-\\nconsuming training process like reinforcement\\nlearning (RL) (Nogueira and Cho, 2017; Liu et al.,\\n2019; Wang et al., 2020) that is not efﬁcient enough\\nfor OpenQA (more discussions in Sec. 2).\\nIn this paper, we propose Generation-\\nAugmented Retrieval ( GAR), which augments\\na query through text generation of a pre-trained\\nlanguage model (PLM). Different from prior\\nstudies that reformulate queries, GARdoes not\\nrequire external resources or downstream feedback\\nvia RL as supervision, because it does not rewrite\\nthe query but expands it with heuristically discov-arXiv:2009.08553v4  [cs.CL]  6 Aug 2021ered relevant contexts, which are fetched from\\nPLMs and provide richer background information\\n(Table 2). For example, by prompting a PLM\\nto generate the title of a relevant passage given\\na query and appending the generated title to the\\nquery, it becomes easier to retrieve that relevant\\npassage. Intuitively, the generated contexts\\nexplicitly express the search intent not presented\\nin the original query. As a result, GARwith\\nsparse representations achieves comparable or\\neven better performance than state-of-the-art\\napproaches (Karpukhin et al., 2020; Guu et al.,\\n2020) with dense representations of the original\\nqueries, while being more lightweight and efﬁcient\\nin terms of both training and inference (including\\nthe cost of the generation model) (Sec. 6.4).\\nSpeciﬁcally, we expand the query (question) by\\nadding relevant contexts as follows. We conduct\\nseq2seq learning with the question as the input\\nand various freely accessible in-domain contexts as\\nthe output such as the answer, the sentence where\\nthe answer belongs to , and the title of a passage\\nthat contains the answer . We then append the gen-\\nerated contexts to the question as the generation-\\naugmented query for retrieval. We demonstrate\\nthat using multiple contexts from diverse gener-\\nation targets is beneﬁcial as fusing the retrieval\\nresults of different generation-augmented queries\\nconsistently yields better retrieval accuracy.\\nWe conduct extensive experiments on the Nat-\\nural Questions (NQ) (Kwiatkowski et al., 2019)\\nand TriviaQA (Trivia) (Joshi et al., 2017) datasets.\\nThe results reveal four major advantages of GAR:\\n(1)GAR, combined with BM25, achieves signif-\\nicant gains over the same BM25 model that uses\\nthe original queries or existing unsupervised query\\nexpansion (QE) methods. (2) GARwith sparse rep-\\nresentations (BM25) achieves comparable or even\\nbetter performance than the current state-of-the-art\\nretrieval methods, such as DPR (Karpukhin et al.,\\n2020), that use dense representations. (3) Since\\nGARuses sparse representations to measure lexical\\noverlap2, it is complementary to dense representa-\\ntions: by fusing the retrieval results of GARand\\nDPR (denoted as GAR+), we obtain consistently\\nbetter performance than either method used individ-\\nually. (4) GARoutperforms DPR in the end-to-end\\nQA performance (EM) when the same extractive\\nreader is used: EM=41.8 (43.8 for GAR+) on NQ\\n2Strictly speaking, GARwith sparse representations han-\\ndles semantics before retrieval by enriching the queries, while\\nmaintaining the advantage of exact matching.and 62.7 on Trivia, creating new state-of-the-art re-\\nsults for extractive OpenQA. GARalso outperforms\\nother retrieval methods under the generative setup\\nwhen the same generative reader is used: EM=38.1\\n(45.3 for G AR+) on NQ and 62.2 on Trivia.\\nContributions . (1) We propose Generation-\\nAugmented Retrieval ( GAR), which augments\\nqueries with heuristically discovered relevant con-\\ntexts through text generation without external su-\\npervision or time-consuming downstream feedback.\\n(2) We show that using generation-augmented\\nqueries achieves signiﬁcantly better retrieval and\\nQA results than using the original queries or ex-\\nisting unsupervised QE methods. (3) We show\\nthatGAR, combined with a simple BM25 model,\\nachieves new state-of-the-art performance on two\\nbenchmark datasets in extractive OpenQA and com-\\npetitive results in the generative setting.\\n2 Related Work\\nConventional Query Expansion .GARshares\\nsome merits with query expansion (QE) meth-\\nods based on pseudo relevance feedback (Rocchio,\\n1971; Abdul-Jaleel et al., 2004; Lv and Zhai, 2010)\\nin that they both expand the queries with relevant\\ncontexts (terms) without the use of external super-\\nvision. GARis superior as it expands the queries\\nwith knowledge stored in the PLMs rather than\\nthe retrieved passages and its expanded terms are\\nlearned through text generation.\\nRecent Query Reformulation . There are recent\\nor concurrent studies (Nogueira and Cho, 2017;\\nZaiem and Sadat, 2019; Yu et al., 2020; Vaku-\\nlenko et al., 2020; Lin et al., 2020) that reformu-\\nlate queries with generation models for other re-\\ntrieval tasks. However, these studies are not eas-\\nily applicable or efﬁcient enough for OpenQA be-\\ncause: (1) They require external resources such as\\nparaphrase data (Zaiem and Sadat, 2019), search\\nsessions (Yu et al., 2020), or conversational con-\\ntexts (Lin et al., 2020; Vakulenko et al., 2020)\\nto form the reformulated queries, which are not\\navailable or showed inferior domain-transfer per-\\nformance in OpenQA (Zaiem and Sadat, 2019);\\n(2) They involve time-consuming training process\\nsuch as RL. For example, Nogueira and Cho (2017)\\nreported a training time of 8 to 10 days as it uses\\nretrieval performance in the reward function and\\nconducts retrieval at each iteration. In contrast,\\nGARuses freely accessible in-domain contexts like\\npassage titles as the generation targets and standardseq2seq learning, which, despite its simplicity, is\\nnot only more efﬁcient but effective for OpenQA.\\nRetrieval for OpenQA . Existing sparse retrieval\\nmethods for OpenQA (Chen et al., 2017) solely rely\\non the information of the questions. GARextends\\nto contexts relevant to the questions by extracting\\ninformation inside PLMs and helps sparse meth-\\nods achieve comparable or better performance than\\ndense methods (Guu et al., 2020; Karpukhin et al.,\\n2020), while enjoying the simplicity and efﬁciency\\nof sparse representations. GARcan also be used\\nwith dense representations to seek for even better\\nperformance, which we leave as future work.\\nGenerative QA . Generative QA generates answers\\nthrough seq2seq learning instead of extracting an-\\nswer spans. Recent studies on generative OpenQA\\n(Lewis et al., 2020a; Min et al., 2020; Izacard and\\nGrave, 2020) are orthogonal to GARin that they\\nfocus on improving the reading stage and directly\\nreuse DPR (Karpukhin et al., 2020) as the retriever.\\nUnlike generative QA, the goal of GARis not to\\ngenerate perfect answers to the questions but perti-\\nnent contexts that are helpful for retrieval. Another\\nline in generative QA learns to generate answers\\nwithout relevant passages as the evidence but solely\\nthe question itself using PLMs (Roberts et al., 2020;\\nBrown et al., 2020). GARfurther conﬁrms that one\\ncan extract factual knowledge from PLMs, which\\nis not limited to the answers as in prior studies but\\nalso other relevant contexts.\\n3 Generation-Augmented Retrieval\\n3.1 Task Formulation\\nOpenQA aims to answer factoid questions with-\\nout pre-speciﬁed domains. We assume that a large\\ncollection of documents C(i.e., Wikipedia) are\\ngiven as the resource to answer the questions and\\na retriever-reader architecture is used to tackle the\\ntask, where the retriever retrieves a small subset\\nof the documents D\\x1aCand the reader reads the\\ndocuments Dto extract (or generate) an answer.\\nOur goal is to improve the effectiveness and efﬁ-\\nciency of the retriever and consequently improve\\nthe performance of the reader.\\n3.2 Generation of Query Contexts\\nInGAR, queries are augmented with various heuris-\\ntically discovered relevant contexts in order to re-\\ntrieve more relevant passages in terms of both quan-\\ntity and quality. For the task of OpenQA where the\\nquery is a question, we take the following threefreely accessible contexts as the generation targets.\\nWe show in Sec. 6.2 that having multiple gener-\\nation targets is helpful in that fusing their results\\nconsistently brings better retrieval accuracy.\\nContext 1: The default target (answer) . The de-\\nfault target is the label in the task of interest, which\\nis the answer in OpenQA. The answer to the ques-\\ntion is apparently useful for the retrieval of relevant\\npassages that contain the answer itself. As shown\\nin previous work (Roberts et al., 2020; Brown et al.,\\n2020), PLMs are able to answer certain questions\\nsolely by taking the questions as input ( i.e., closed-\\nbook QA). Instead of using the generated answers\\ndirectly as in closed-book QA, GARtreats them\\nas contexts of the question for retrieval. The ad-\\nvantage is that even if the generated answers are\\npartially correct (or even incorrect), they may still\\nbeneﬁt retrieval as long as they are relevant to the\\npassages that contain the correct answers ( e.g., co-\\noccur with the correct answers).\\nContext 2: Sentence containing the default tar-\\nget. The sentence in a passage that contains the\\nanswer is used as another generation target. Sim-\\nilar to using answers as the generation target, the\\ngenerated sentences are still beneﬁcial for retriev-\\ning relevant passages even if they do not contain\\nthe answers, as their semantics is highly related to\\nthe questions/answers (examples in Sec. 6.1). One\\ncan take the relevant sentences in the ground-truth\\npassages (if any) or those in the positive passages\\nof a retriever as the reference, depending on the\\ntrade-off between reference quality and diversity.\\nContext 3: Title of passage containing the de-\\nfault target . One can also use the titles of rele-\\nvant passages as the generation target if available.\\nSpeciﬁcally, we retrieve Wikipedia passages using\\nBM25 with the question as the query, and take the\\npage titles of positive passages that contain the an-\\nswers as the generation target. We observe that\\nthe page titles of positive passages are often entity\\nnames of interest, and sometimes (but not always)\\nthe answers to the questions. Intuitively, if GAR\\nlearns which Wikipedia pages the question is re-\\nlated to, the queries augmented by the generated\\ntitles would naturally have a better chance of re-\\ntrieving those relevant passages.\\nWhile it is likely that some of the generated\\nquery contexts involve unfaithful or nonfactual in-\\nformation due to hallucination in text generation\\n(Mao et al., 2020) and introduce noise during re-\\ntrieval, they are beneﬁcial rather than harmful over-all, as our experiments show that GARimprove\\nboth retrieval and QA performance over BM25 sig-\\nniﬁcantly. Also, since we generate 3 different (com-\\nplementary) query contexts and fuse their retrieval\\nresults, the distraction of hallucinated content is\\nfurther alleviated.\\n3.3 Retrieval with Generation-Augmented\\nQueries\\nAfter generating the contexts of a query, we append\\nthem to the query to form a generation-augmented\\nquery .3We observe that conducting retrieval with\\nthe generated contexts ( e.g., answers) alone as\\nqueries instead of concatenation is ineffective be-\\ncause (1) some of the generated answers are rather\\nirrelevant, and (2) a query consisting of the correct\\nanswer alone (without the question) may retrieve\\nfalse positive passages with unrelated contexts that\\nhappen to contain the answer. Such low-quality\\npassages may lead to potential issues in the follow-\\ning passage reading stage.\\nIf there are multiple query contexts, we conduct\\nretrieval using queries with different generated con-\\ntexts separately and then fuse their results. The per-\\nformance of one-time retrieval with all the contexts\\nappended is slightly but not signiﬁcantly worse.\\nFor simplicity, we fuse the retrieval results in a\\nstraightforward way: an equal number of passages\\nare taken from the top-retrieved passages of each\\nsource. One may also use weighted or more so-\\nphisticated fusion strategies such as reciprocal rank\\nfusion (Cormack et al., 2009), the results of which\\nare slightly better according to our experiments.4\\nNext, one can use any off-the-shelf retriever for\\npassage retrieval. Here, we use a simple BM25\\nmodel to demonstrate that GARwith sparse repre-\\nsentations can already achieve comparable or better\\nperformance than state-of-the-art dense methods\\nwhile being more lightweight and efﬁcient (includ-\\ning the cost of the generation model), closing the\\ngap between sparse and dense retrieval methods.\\n4 OpenQA with G AR\\nTo further verify the effectiveness of GAR, we\\nequip it with both extractive and generative read-\\ners for end-to-end QA evaluation. We follow the\\n3One may create a title ﬁeld during document indexing\\nand conduct multi-ﬁeld retrieval but here we append the titles\\nto the questions as other query contexts for generalizability.\\n4We use the fusion tools at https://github.com/\\njoaopalotti/trectools .reader design of the major baselines for a fair com-\\nparison, while virtually any existing QA reader can\\nbe used with G AR.\\n4.1 Extractive Reader\\nFor the extractive setup, we largely follow the de-\\nsign of the extractive reader in DPR (Karpukhin\\net al., 2020). Let D= [d1; d2; :::; d k]denote the list\\nof retrieved passages with passage relevance scores\\nD. LetSi= [s1; s2; :::; s N]denote the top Ntext\\nspans in passage diranked by span relevance scores\\nSi. Brieﬂy, the DPR reader uses BERT-base (De-\\nvlin et al., 2019) for representation learning, where\\nit estimates the passage relevance score Dkfor\\neach retrieved passage dkbased on the [CLS] to-\\nkens of all retrieved passages D, and assigns span\\nrelevance scores Sifor each candidate span based\\non the representations of its start and end tokens.\\nFinally, the span with the highest span relevance\\nscore from the passage with the highest passage rel-\\nevance score is chosen as the answer. We refer the\\nreaders to Karpukhin et al. (2020) for more details.\\nPassage-level Span Voting . Many extractive QA\\nmethods (Chen et al., 2017; Min et al., 2019b; Guu\\net al., 2020; Karpukhin et al., 2020) measure the\\nprobability of span extraction in different retrieved\\npassages independently, despite that their collec-\\ntive signals may provide more evidence in deter-\\nmining the correct answer. We propose a simple\\nyet effective passage-level span voting mechanism,\\nwhich aggregates the predictions of the spans in\\nthe same surface form from different retrieved pas-\\nsages. Intuitively, if a text span is considered as the\\nanswer multiple times in different passages, it is\\nmore likely to be the correct answer. Speciﬁcally,\\nGARcalculates a normalized score p(Si[j])for the\\nj-th span in passage diduring inference as follows:\\np(Si[j]) = softmax (D)[i]\\x02softmax (Si)[j].GAR\\nthen aggregates the scores of the spans with the\\nsame surface string among all the retrieved pas-\\nsages as the collective passage-level score.5\\n4.2 Generative Reader\\nFor the generative setup, we use a seq2seq frame-\\nwork where the input is the concatenation of the\\nquestion and top-retrieved passages and the target\\noutput is the desired answer. Such generative read-\\ners are adopted in recent methods such as SpanSe-\\n5We ﬁnd that the number of spans used for normalization\\nin each passage does not have signiﬁcant impact on the ﬁnal\\nperformance (we take N= 5) and using the raw or normalized\\nstrings for aggregation also perform similarly.qGen (Min et al., 2020) and Longformer (Belt-\\nagy et al., 2020). Speciﬁcally, we use BART-large\\n(Lewis et al., 2019) as the generative reader, which\\nconcatenates the question and top-retrieved pas-\\nsages up to its length limit (1,024 tokens, 7.8 pas-\\nsages on average). Generative GARis directly com-\\nparable with SpanSeqGen (Min et al., 2020) that\\nuses the retrieval results of DPR but not comparable\\nwith Fusion-in-Decoder (FID) (Izacard and Grave,\\n2020) since it encodes 100 passages rather than\\n1,024 tokens and involves more model parameters.\\n5 Experiment Setup\\n5.1 Datasets\\nWe conduct experiments on the open-domain ver-\\nsion of two popular QA benchmarks: Natural Ques-\\ntions (NQ) (Kwiatkowski et al., 2019) and Trivi-\\naQA (Trivia) (Joshi et al., 2017). The statistics of\\nthe datasets are listed in Table 1.\\nDataset Train / Val / Test Q-len A-len #-A\\nNQ 79,168 / 8,757 / 3,610 12.5 5.2 1.2\\nTrivia 78,785 / 8,837 / 11,313 20.2 5.5 13.7\\nTable 1: Dataset statistics that show the number of sam-\\nples per data split, the average question (answer) length,\\nand the number of answers for each question.\\n5.2 Evaluation Metrics\\nFollowing prior studies (Karpukhin et al., 2020),\\nwe use top-k retrieval accuracy to evaluate the per-\\nformance of the retriever and the Exact Match (EM)\\nscore to measure the performance of the reader.\\nTop-k retrieval accuracy is deﬁned as the pro-\\nportion of questions for which the top-k retrieved\\npassages contain at least one answer span, which\\nis an upper bound of how many questions are “an-\\nswerable” by an extractive reader.\\nExact Match (EM) is the proportion of the pre-\\ndicted answer spans being exactly the same as (one\\nof) the ground-truth answer(s), after string normal-\\nization such as article and punctuation removal.\\n5.3 Compared Methods\\nFor passage retrieval, we mainly compare with\\nBM25 and DPR, which represent the most used\\nstate-of-the-art methods of sparse and dense re-\\ntrieval for OpenQA, respectively. For query ex-\\npansion, we re-emphasize that GARis the ﬁrst QE\\napproach designed for OpenQA and most of the\\nrecent approaches are not applicable or efﬁcientenough for OpenQA since they have task-speciﬁc\\nobjectives, require external supervision that was\\nshown to transfer poorly to OpenQA, or take many\\ndays to train (Sec. 2). We thus compare with a clas-\\nsic unsupervised QE method RM3 (Abdul-Jaleel\\net al., 2004) that does not need external resources\\nfor a fair comparison. For passage reading, we\\ncompare with both extractive (Min et al., 2019a;\\nAsai et al., 2019; Lee et al., 2019; Min et al., 2019b;\\nGuu et al., 2020; Karpukhin et al., 2020) and gen-\\nerative (Brown et al., 2020; Roberts et al., 2020;\\nMin et al., 2020; Lewis et al., 2020a; Izacard and\\nGrave, 2020) methods when equipping GARwith\\nthe corresponding reader.\\n5.4 Implementation Details\\nRetriever . We use Anserini (Yang et al., 2017) for\\ntext retrieval of BM25 and GARwith its default\\nparameters. We conduct grid search for the QE\\nbaseline RM3 (Abdul-Jaleel et al., 2004).\\nGenerator . We use BART-large (Lewis et al.,\\n2019) to generate query contexts in GAR. When\\nthere are multiple desired targets (such as multi-\\nple answers or titles), we concatenate them with\\n[SEP] tokens as the reference and remove the [SEP]\\ntokens in the generation-augmented queries. For\\nTrivia, in particular, we use the value ﬁeld as the\\ngeneration target of answer and observe better per-\\nformance. We take the checkpoint with the best\\nROUGE-1 F1 score on the validation set, while\\nobserving that the retrieval accuracy of GARis rel-\\natively stable to the checkpoint selection since we\\ndo not directly use the generated contexts but treat\\nthem as augmentation of queries for retrieval.\\nReader . Extractive GARuses the reader of DPR\\nwith largely the same hyperparameters, which is\\ninitialized with BERT-base (Devlin et al., 2019)\\nand takes 100 (500) retrieved passages during train-\\ning (inference). Generative GARconcatenates the\\nquestion and top-10 retrieved passages, and takes\\nat most 1,024 tokens as input. Greedy decoding is\\nadopted for all generation models, which appears to\\nperform similarly to (more expensive) beam search.\\n6 Experiment Results\\nWe evaluate the effectiveness of GARin three\\nstages: generation of query contexts (Sec. 6.1),\\nretrieval of relevant passages (Sec. 6.2), and pas-\\nsage reading for OpenQA (Sec. 6.3). Ablation\\nstudies are mostly shown on the NQ dataset to un-\\nderstand the drawbacks of GARsince it achievesQuestion : when did bat out of hell get released?\\nAnswer :September 1977 fSeptember 1977g\\nSentence : Bat Out of Hell is the second studio album and the major - label debut by American rock singer Meat\\nLoaf ... released in September 1977 on Cleveland International / Epic Records.\\nfThe album was released in September 1977 on Cleveland International / Epic Records. g\\nTitle :Bat Out of Hell fBat Out of Hellg\\nQuestion : who sings does he love me with reba?\\nAnswer :Brooks & Dunn fLinda Davisg\\nSentence :Linda Kaye Davis ( born November 26, 1962 ) is an American country music singer.\\nf“ Does He Love You ” is a song written by Sandy Knox and Billy Stritch, and recorded as a duet by American\\ncountry music artists Reba McEntire and Linda Davis. g\\nTitle :Does He Love Me [SEP] Does He Love Me (Reba McEntire song) [SEP] I Do (Reba McEntire album)\\nfLinda Davis [SEP] Greatest Hits V olume Two (Reba McEntire album) [SEP] Does He Love You g\\nQuestion : what is the name of wonder womans mother?\\nAnswer :Mother Magda fQueen Hippolytag\\nSentence : In the Amazonian myths, she is the daughter of the Amazon queen Sifrat and the male dwarf Shuri,\\nand is the mother of Wonder Woman. fWonder Woman’s origin story relates that she was sculpted from clay\\nby her mother Queen Hippolyta and given life by Aphrodite. g\\nTitle :Wonder Woman [SEP] Diana Prince [SEP] Wonder Woman (2011 TV pilot)\\nfWonder Woman [SEP] Orana (comics) [SEP] Wonder Woman (TV series) g\\nTable 2: Examples of generated query contexts .Relevant andirrelevant contexts are shown in green and\\nred. Ground-truth references are shown in the fbracesg. The issue of generating wrong answers is alleviated by\\ngenerating other contexts highly related to the question/answer.\\nbetter performance on Trivia.\\n6.1 Query Context Generation\\nAutomatic Evaluation . To evaluate the quality\\nof the generated query contexts, we ﬁrst measure\\ntheir lexical overlap with the ground-truth query\\ncontexts. As suggested by the nontrivial ROUGE\\nscores in Table 3, GARdoes learn to generate\\nmeaningful query contexts that could help the re-\\ntrieval stage. We next measure the lexical overlap\\nbetween the query and the ground-truth passage.\\nThe ROUGE-1/2/L F1 scores between the original\\nquery and ground-truth passage are 6.00/2.36/5.01,\\nand those for the generation-augmented query are\\n7.05/2.84/5.62 (answer), 13.21/6.99/10.27 (sen-\\ntence), 7.13/2.85/5.76 (title) on NQ, respectively.\\nSuch results further demonstrate that the generated\\nquery contexts signiﬁcantly increase the word over-\\nlap between the queries and the positive passages,\\nand thus are likely to improve retrieval results.6\\nCase Studies . In Table 2, we show several ex-\\namples of the generated query contexts and their\\nground-truth references. In the ﬁrst example, the\\ncorrect album release date appears in both the gen-\\nerated answer and the generated sentence, and the\\ngenerated title is the same as the Wikipedia page\\n6We use F1 instead of recall to avoid the unfair favor of\\n(longer) generation-augmented query.Context ROUGE-1 ROUGE-2 ROUGE-L\\nAnswer 33.51 20.54 33.30\\nSentence 37.14 24.71 33.91\\nTitle 43.20 32.11 39.67\\nTable 3: ROUGE F1 scores of the generated query\\ncontexts on the validation set of the NQ dataset.\\ntitle of the album. In the last two examples, the\\ngenerated answers are wrong but fortunately, the\\ngenerated sentences contain the correct answer and\\n(or) other relevant information and the generated\\ntitles are highly related to the question as well,\\nwhich shows that different query contexts are com-\\nplementary to each other and the noise during query\\ncontext generation is thus reduced.\\n6.2 Generation-Augmented Retrieval\\nComparison w. the state-of-the-art . We next\\nevaluate the effectiveness of GARfor retrieval.\\nIn Table 4, we show the top-k retrieval accuracy\\nof BM25, BM25 with query expansion (+RM3)\\n(Abdul-Jaleel et al., 2004), DPR (Karpukhin et al.,\\n2020), G AR, and G AR+(GAR+DPR).\\nOn the NQ dataset, while BM25 clearly under-\\nperforms DPR regardless of the number of retrieved\\npassages, the gap between GARand DPR is signiﬁ-\\ncantly smaller and negligible when k\\x15100. When\\nk\\x15500,GARis slightly better than DPR despiteMethodNQ Trivia\\nTop-5 Top-20 Top-100 Top-500 Top-1000 Top-5 Top-20 Top-100 Top-500 Top-1000\\nBM25 (ours) 43.6 62.9 78.1 85.5 87.8 67.7 77.3 83.9 87.9 88.9\\nBM25 +RM3 44.6 64.2 79.6 86.8 88.9 67.0 77.1 83.8 87.7 88.9\\nDPR 68.3 80.1 86.1 90.3 91.2 72.7 80.2 84.8 - -\\nGAR 60.9 74.4 85.3 90.3 91.7 73.1 80.4 85.7 88.9 89.7\\nGAR+70.7 81.6 88.9 92.0 93.2 76.0 82.1 86.6 - -\\nTable 4: Top-k retrieval accuracy on the test sets . The baselines are evaluated by ourselves and better than\\nreported in Karpukhin et al. (2020). G ARhelps BM25 to achieve comparable or better performance than DPR.\\nBest and second best methods are bold and underlined , respectively.\\nthat it simply uses BM25 for retrieval. In con-\\ntrast, the classic QE method RM3, while showing\\nmarginal improvement over the vanilla BM25, does\\nnot achieve comparable performance with GARor\\nDPR. By fusing the results of GARand DPR in\\nthe same way as described in Sec. 3.3, we further\\nobtain consistently higher performance than both\\nmethods, with top-100 accuracy 88.9% and top-\\n1000 accuracy 93.2%.\\nOn the Trivia dataset, the results are even more\\nencouraging – GARachieves consistently better\\nretrieval accuracy than DPR when k\\x155. On\\nthe other hand, the difference between BM25 and\\nBM25 +RM3 is negligible, which suggests that\\nnaively considering top-ranked passages as relevant\\n(i.e., pseudo relevance feedback) for QE does not\\nalways work for OpenQA. Results on more cutoffs\\nofkcan be found in App. A.\\nEffectiveness of diverse query contexts . In\\nFig. 1, we show the performance of GARwhen\\ndifferent query contexts are used to augment the\\nqueries. Although the individual performance\\nwhen using each query context is somewhat similar,\\nfusing their retrieved passages consistently leads\\nto better performance, conﬁrming that different\\ngeneration-augmented queries are complementary\\nto each other (recall examples in Table 2).\\nPerformance breakdown by question type . In\\nTable 5, we show the top-100 accuracy of the com-\\npared retrieval methods per question type on the\\nNQ test set. Again, GARoutperforms BM25 on all\\ntypes of questions signiﬁcantly and GAR+achieves\\nthe best performance across the board, which fur-\\nther veriﬁes the effectiveness of G AR.\\n6.3 Passage Reading with G AR\\nComparison w. the state-of-the-art . We show\\nthe comparison of end-to-end QA performance of\\nextractive and generative methods in Table 6. Ex-\\ntractive GARachieves state-of-the-art performance\\n1 5 10 20 50 100 200 300 500 1000\\nk: # of retrieved passages30405060708090Top-k Accuracy (%)\\nAnswer+Sentence+Title\\nAnswer+Sentence\\nAnswer+Title\\nAnswer\\nTitle\\nSentenceFigure 1: Top-k retrieval accuracy on the test\\nset of NQ when fusing retrieval results of different\\ngeneration-augmented queries.\\nType Percentage BM25 DPR G AR GAR+\\nWho 37.5% 82.1 88.0 87.5 90.8\\nWhen 19.0% 73.1 86.9 83.8 88.6\\nWhat 15.0% 76.5 82.6 81.5 86.0\\nWhere 10.9% 77.4 89.1 87.0 90.8\\nOther 9.1% 79.3 78.1 81.8 84.2\\nHow 5.0% 78.2 83.8 83.2 85.5\\nWhich 3.3% 89.0 90.7 94.1 94.9\\nWhy 0.3% 90.0 90.0 90.0 90.0\\nTable 5: Top-100 retrieval accuracy breakdown of\\nquestion type on NQ . Best and second best methods\\nin each category are bold and underlined , respectively.\\namong extractive methods on both NQ and Trivia\\ndatasets, despite that it is more lightweight and\\ncomputationally efﬁcient. Generative GARoutper-\\nforms most of the generative methods on Trivia but\\ndoes not perform as well on NQ, which is some-\\nwhat expected and consistent with the performance\\nat the retrieval stage, as the generative reader only\\ntakes a few passages as input and GARdoes not\\noutperform dense retrieval methods on NQ when k\\nis very small. However, combining GARwith DPR\\nachieves signiﬁcantly better performance than bothMethod NQ TriviaExtractiveHard EM (Min et al., 2019a) 28.1 50.9 -\\nPath Retriever (Asai et al., 2019) 32.6 - -\\nORQA (Lee et al., 2019) 33.3 45.0 -\\nGraph Retriever (Min et al., 2019b) 34.5 56.0 -\\nREALM (Guu et al., 2020) 40.4 - -\\nDPR (Karpukhin et al., 2020) 41.5 57.9 -\\nBM25 (ours) 37.7 60.1 -\\nGAR 41.8 62.7 74.8\\nGAR+43.8 - -GenerativeGPT-3 (Brown et al., 2020) 29.9 - 71.2\\nT5 (Roberts et al., 2020) 36.6 60.5 -\\nSpanSeqGen (Min et al., 2020) 42.2 - -\\nRAG (Lewis et al., 2020a) 44.5 56.1 68.0\\nFID (Izacard and Grave, 2020) 51.4 67.6 80.1\\nBM25 (ours) 35.3 58.6 -\\nGAR 38.1 62.2 -\\nGAR+45.3 - -\\nTable 6: End-to-end comparison with the state-of-\\nthe-art methods in EM . For Trivia, the left column\\ndenotes the open-domain test set and the right is the\\nhidden Wikipedia test set on the public leaderboard.\\nmethods or baselines that use DPR as input such as\\nSpanSeqGen (Min et al., 2020) and RAG (Lewis\\net al., 2020a). Also, GARoutperforms BM25 sig-\\nniﬁcantly under both extractive and generative se-\\ntups, which again shows the effectiveness of the\\ngenerated query contexts, even if they are heuristi-\\ncally discovered without any external supervision.\\nThe best performing generative method FID\\n(Izacard and Grave, 2020) is not directly compara-\\nble as it takes more (100) passages as input. As an\\nindirect comparison, GARperforms better than FID\\nwhen FID encodes 10 passages (cf. Fig. 2 in Izac-\\nard and Grave (2020)). Moreover, since FID relies\\non the retrieval results of DPR as well, we believe\\nthat it is a low-hanging fruit to replace its input\\nwith GARorGAR+and further boost the perfor-\\nmance.7We also observe that, perhaps surprisingly,\\nextractive BM25 performs reasonably well, espe-\\ncially on the Trivia dataset, outperforming many\\nrecent state-of-the-art methods.8Generative BM25\\nalso performs competitively in our experiments.\\nModel Generalizability . Recent studies (Lewis\\net al., 2020b) show that there are signiﬁcant ques-\\ntion and answer overlaps between the training and\\ntest sets of popular OpenQA datasets. Speciﬁcally,\\n60% to 70% test-time answers also appear in the\\n7This claim is later veriﬁed by the best systems in the\\nNeurIPS 2020 EfﬁcientQA competition (Min et al., 2021).\\n8We ﬁnd that taking 500 passages during reader inference\\ninstead of 100 as in Karpukhin et al. (2020) improves the\\nperformance of BM25 but not DPR.training set and roughly 30% test-set questions\\nhave a near-duplicate paraphrase in the training\\nset. Such observations suggest that many questions\\nmight have been answered by simple question or\\nanswer memorization. To further examine model\\ngeneralizability, we study the per-category perfor-\\nmance of different methods using the annotations\\nin Lewis et al. (2020b).\\nMethod TotalQuestion\\nOverlapAnswer\\nOverlap\\nOnlyNo\\nOverlap\\nDPR 41.3 69.4 34.6 19.3\\nGAR+(E) 43.8 66.7 38.1 23.9\\nBART 26.5 67.6 10.2 0.8\\nRAG 44.5 70.7 34.9 24.8\\nGAR+(G) 45.3 67.9 38.1 27.0\\nTable 7: EM scores with question-answer overlap\\ncategory breakdown on NQ. (E) and (G) denote ex-\\ntractive and generative readers, respectively. Results of\\nbaseline methods are taken from Lewis et al. (2020b).\\nThe observations on Trivia are similar and omitted.\\nAs listed in Table 7, for the No Overlap category,\\nGAR+(E) outperforms DPR on the extractive setup\\nandGAR+(G) outperforms RAG on the generative\\nsetup, which indicates that better end-to-end model\\ngeneralizability can be achieved by adding GAR\\nfor retrieval. GAR+also achieves the best EM un-\\nder the Answer Overlap Only category. In addition,\\nwe observe that a closed-book BART model that\\nonly takes the question as input performs much\\nworse than additionally taking top-retrieved pas-\\nsages, i.e.,GAR+(G), especially on the questions\\nthat require generalizability. Notably, all methods\\nperform signiﬁcantly better on the Question Over-\\nlapcategory, which suggests that the high Total\\nEM is mostly contributed by question memoriza-\\ntion. That said, GAR+appears to be less dependent\\non question memorization given its lower EM for\\nthis category.9\\n6.4 Efﬁciency of G AR\\nGARis efﬁcient and scalable since it uses sparse\\nrepresentations for retrieval and does not in-\\nvolve time-consuming training process such as\\nRL (Nogueira and Cho, 2017; Liu et al., 2019).\\nThe only overhead of GARis on the generation of\\nquery contexts and the retrieval with generation-\\n9The same ablation study is also conducted on the retrieval\\nstage and similar results are observed. More detailed discus-\\nsions can be found in App. A.Training Indexing Retrieval\\nDPR 24h w. 8 GPUs 17.3h w. 8 GPUs 30 min w. 1 GPU\\nGAR 3\\x186h w. 1 GPU 0.5h w. 35 CPUs 5 min w. 35 CPUs\\nTable 8: Comparison of computational cost between\\nDPR and G ARat different stages. The training time\\nof G ARis for one generation target but different gener-\\nators can be trained in parallel.\\naugmented (thus longer) queries, whose computa-\\ntional complexity is signiﬁcantly lower than other\\nmethods with comparable retrieval accuracy.\\nWe use Nvidia V100 GPUs and Intel Xeon Plat-\\ninum 8168 CPUs in our experiments. As listed in\\nTable 8, the training time of GARis 3 to 6 hours\\non 1 GPU depending on the generation target. As\\na comparison, REALM (Guu et al., 2020) uses\\n64 TPUs to train for 200k steps during pre-training\\nalone and DPR (Karpukhin et al., 2020) takes about\\n24 hours to train with 8 GPUs. To build the indices\\nof Wikipedia passages, GARonly takes around 30\\nmin with 35 CPUs, while DPR takes 8.8 hours\\non 8 GPUs to generate dense representations and\\nanother 8.5 hours to build the FAISS index (John-\\nson et al., 2017). For retrieval, GARtakes about\\n1 min to generate one query context with 1 GPU,\\n1 min to retrieve 1,000 passages for the NQ test\\nset with answer/title-augmented queries and 2 min\\nwith sentence-augmented queries using 35 CPUs.\\nIn contrast, DPR takes about 30 min on 1 GPU.\\n7 Conclusion\\nIn this work, we propose Generation-Augmented\\nRetrieval and demonstrate that the relevant contexts\\ngenerated by PLMs without external supervision\\ncan signiﬁcantly enrich query semantics and im-\\nprove retrieval accuracy. Remarkably, GARwith\\nsparse representations performs similarly or better\\nthan state-of-the-art methods based on the dense\\nrepresentations of the original queries. GARcan\\nalso be easily combined with dense representa-\\ntions to produce even better results. Furthermore,\\nGARachieves state-of-the-art end-to-end perfor-\\nmance on extractive OpenQA and competitive per-\\nformance under the generative setup.\\n8 Future Extensions\\nPotential improvements . There is still much\\nspace to explore and improve for GARin future\\nwork. For query context generation, one can ex-\\nplore multi-task learning to further reduce computa-\\ntional cost and examine whether different contextscan mutually enhance each other when generated\\nby the same generator. One may also sample multi-\\nple contexts instead of greedy decoding to enrich a\\nquery. For retrieval, one can adopt more advanced\\nfusion techniques based on both the ranking and\\nscore of the passages. As the generator and re-\\ntriever are largely independent now, it is also inter-\\nesting to study how to jointly or iteratively optimize\\ngeneration and retrieval such that the generator is\\naware of the retriever and generates query contexts\\nmore beneﬁcial for the retrieval stage. Last but not\\nleast, it is very likely that better results can be ob-\\ntained by more extensive hyper-parameter tuning.\\nApplicability to other tasks . Beyond OpenQA,\\nGARalso has great potentials for other tasks that\\ninvolve text matching such as conversation utter-\\nance selection (Lowe et al., 2015; Dinan et al.,\\n2020) or information retrieval (Nguyen et al., 2016;\\nCraswell et al., 2020). The default generation tar-\\nget is always available for supervised tasks. For\\nexample, for conversation utterance selection one\\ncan use the reference utterance as the default target\\nand then match the concatenation of the conversa-\\ntion history and the generated utterance with the\\nprovided utterance candidates. For article search,\\nthe default target could be (part of) the ground-truth\\narticle itself. Other generation targets are more task-\\nspeciﬁc and can be designed as long as they can\\nbe fetched from the latent knowledge inside PLMs\\nand are helpful for further text retrieval (matching).\\nNote that by augmenting (expanding) the queries\\nwith heuristically discovered relevant contexts ex-\\ntracted from PLMs instead of reformulating them,\\nGARbypasses the need for external supervision to\\nform the original-reformulated query pairs.\\nAcknowledgments\\nWe thank Vladimir Karpukhin, Sewon Min, Gau-\\ntier Izacard, Wenda Qiu, Revanth Reddy, and Hao\\nCheng for helpful discussions. We thank the anony-\\nmous reviewers for valuable comments.\\nReferences\\nNasreen Abdul-Jaleel, James Allan, W Bruce Croft,\\nFernando Diaz, Leah Larkey, Xiaoyan Li, Mark D\\nSmucker, and Courtney Wade. 2004. Umass at trec\\n2004: Novelty and hard. Computer Science Depart-\\nment Faculty Publication Series , page 189.\\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\\nRichard Socher, and Caiming Xiong. 2019. Learn-\\ning to retrieve reasoning paths over wikipediagraph for question answering. arXiv preprint\\narXiv:1911.10470 .\\nIz Beltagy, Matthew E Peters, and Arman Cohan.\\n2020. Longformer: The long-document transformer.\\narXiv preprint arXiv:2004.05150 .\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. arXiv preprint arXiv:2005.14165 .\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 1870–\\n1879, Vancouver, Canada. Association for Computa-\\ntional Linguistics.\\nGordon V Cormack, Charles LA Clarke, and Stefan\\nBuettcher. 2009. Reciprocal rank fusion outper-\\nforms condorcet and individual rank learning meth-\\nods. In Proceedings of the 32nd international ACM\\nSIGIR conference on Research and development in\\ninformation retrieval , pages 758–759.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\\nCampos, and Ellen M V oorhees. 2020. Overview\\nof the trec 2019 deep learning track. arXiv preprint\\narXiv:2003.07820 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) ,\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\\nAlexander Miller, Kurt Shuster, Jack Urbanek,\\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\\nLowe, et al. 2020. The second conversational in-\\ntelligence challenge (convai2). In The NeurIPS’18\\nCompetition , pages 187–208. Springer.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training. arXiv\\npreprint arXiv:2002.08909 .\\nGautier Izacard and Edouard Grave. 2020. Lever-\\naging passage retrieval with generative models for\\nopen domain question answering. arXiv preprint\\narXiv:2007.01282 .\\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\\nBillion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 .Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale dis-\\ntantly supervised challenge dataset for reading com-\\nprehension. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 1601–1611, Van-\\ncouver, Canada. Association for Computational Lin-\\nguistics.\\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\\nWu, Sergey Edunov, Danqi Chen, and Wen-\\ntau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. arXiv preprint\\narXiv:2004.04906 .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\nNatural questions: A benchmark for question an-\\nswering research. Transactions of the Association\\nfor Computational Linguistics , 7:452–466.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 6086–6096, Florence,\\nItaly. Association for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\\nBart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and\\ncomprehension. arXiv preprint arXiv:1910.13461 .\\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\\nRockt ¨aschel, et al. 2020a. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. arXiv\\npreprint arXiv:2005.11401 .\\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\\n2020b. Question and answer test-train overlap in\\nopen-domain question answering datasets. arXiv\\npreprint arXiv:2008.02637 .\\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\\nNogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\\nJimmy Lin. 2020. Query reformulation using query\\nhistory for passage retrieval in conversational search.\\narXiv preprint arXiv:2005.02230 .\\nYe Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\\nPhilip S Yu. 2019. Generative question reﬁnement\\nwith deep reinforcement learning in retrieval-based\\nqa system. In Proceedings of the 28th ACM Inter-\\nnational Conference on Information and Knowledge\\nManagement , pages 1643–1652.Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle\\nPineau. 2015. The ubuntu dialogue corpus: A large\\ndataset for research in unstructured multi-turn dia-\\nlogue systems. arXiv preprint arXiv:1506.08909 .\\nYi Luan, Jacob Eisenstein, Kristina Toutanova, and\\nMichael Collins. 2020. Sparse, dense, and at-\\ntentional representations for text retrieval. arXiv\\npreprint arXiv:2005.00181 .\\nYuanhua Lv and ChengXiang Zhai. 2010. Positional\\nrelevance model for pseudo-relevance feedback. In\\nProceedings of the 33rd international ACM SIGIR\\nconference on Research and development in infor-\\nmation retrieval , pages 579–586.\\nYuning Mao, Xiang Ren, Heng Ji, and Jiawei Han.\\n2020. Constrained abstractive summarization: Pre-\\nserving factual consistency with constrained genera-\\ntion. arXiv preprint arXiv:2010.12723 .\\nSewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi\\nChen, Eunsol Choi, Michael Collins, Kelvin Guu,\\nHannaneh Hajishirzi, Kenton Lee, Jennimaria Palo-\\nmaki, et al. 2021. Neurips 2020 efﬁcientqa compe-\\ntition: Systems, analyses and lessons learned. arXiv\\npreprint arXiv:2101.00133 .\\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2019a. A discrete hard EM ap-\\nproach for weakly supervised question answering.\\nInProceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the\\n9th International Joint Conference on Natural Lan-\\nguage Processing (EMNLP-IJCNLP) , pages 2851–\\n2864, Hong Kong, China. Association for Computa-\\ntional Linguistics.\\nSewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\\nnaneh Hajishirzi. 2019b. Knowledge guided text re-\\ntrieval and reading for open domain question answer-\\ning. arXiv preprint arXiv:1911.03868 .\\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2020. Ambigqa: Answering\\nambiguous open-domain questions. arXiv preprint\\narXiv:2004.10645 .\\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\\n2016. Ms marco: A human-generated machine read-\\ning comprehension dataset.\\nRodrigo Nogueira and Kyunghyun Cho. 2017. Task-\\noriented query reformulation with reinforcement\\nlearning. In Proceedings of the 2017 Conference on\\nEmpirical Methods in Natural Language Processing ,\\npages 574–583, Copenhagen, Denmark. Association\\nfor Computational Linguistics.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the pa-\\nrameters of a language model? arXiv preprint\\narXiv:2002.08910 .Joseph Rocchio. 1971. Relevance feedback in in-\\nformation retrieval. The Smart retrieval system-\\nexperiments in automatic document processing ,\\npages 313–323.\\nSvitlana Vakulenko, Shayne Longpre, Zhucheng Tu,\\nand Raviteja Anantha. 2020. Question rewriting for\\nconversational question answering. arXiv preprint\\narXiv:2004.14652 .\\nXiao Wang, Craig Macdonald, and Iadh Ounis. 2020.\\nDeep reinforced query reformulation for informa-\\ntion retrieval. arXiv preprint arXiv:2007.07987 .\\nPeilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:\\nEnabling the use of lucene for information retrieval\\nresearch. In Proceedings of the 40th International\\nACM SIGIR Conference on Research and Develop-\\nment in Information Retrieval , pages 1253–1256.\\nShi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong,\\nPaul Bennett, Jianfeng Gao, and Zhiyuan Liu. 2020.\\nFew-shot generative conversational query rewriting.\\narXiv preprint arXiv:2006.05009 .\\nSalah Zaiem and Fatiha Sadat. 2019. Sequence to se-\\nquence learning for query expansion. In Proceed-\\nings of the AAAI Conference on Artiﬁcial Intelli-\\ngence, Student Abstract Track , volume 33, pages\\n10075–10076.A More Analysis of Retrieval\\nPerformance\\nWe show the detailed results of top-k retrieval accu-\\nracy of the compared methods in Figs. 2 and 3.\\nGARperforms comparably or better than DPR\\nwhen k\\x15100on NQ and k\\x155on Trivia.\\n1 5 10 20 50 100 200 300 500 1000\\nk: # of retrieved passages2030405060708090Top-k Accuracy (%)\\nGAR +DPR\\nDPR\\nGAR\\nBM25 +RM3\\nBM25\\nFigure 2: Top-k retrieval accuracy of sparse and\\ndense methods on the test set of NQ. GARimproves\\nBM25 and achieves comparable or better performance\\nthan DPR when k\\x15100.\\n1 5 10 20 50 100\\nk: # of retrieved passages5055606570758085Top-k Accuracy (%)\\nGAR +DPR\\nDPR\\nGAR\\nBM25 +RM3\\nBM25\\nFigure 3: Top-k retrieval accuracy on the Trivia test\\nset.GARachieves better results than DPR when k\\x155.\\nWe show in Table 9 the retrieval accuracy break-\\ndown using the question-answer overlap categories.\\nThe most signiﬁcant gap between BM25 and other\\nmethods is on the Question Overlap category,\\nwhich coincides with the fact that BM25 is un-\\nable to conduct question paraphrasing (semantic\\nmatching). GARhelps BM25 to bridge the gap by\\nproviding the query contexts and even outperform\\nDPR in this category. Moreover, GARconsistently\\nimproves over BM25 on other categories and GAR+\\noutperforms DPR as well.Method TotalQuestion\\nOverlapAnswer\\nOverlap\\nOnlyNo\\nOverlap\\nBM25 78.8 81.2 85.1 70.6\\nDPR 86.1 93.2 89.5 76.8\\nGAR 85.3 94.1 87.9 73.7\\nGAR+88.9 96.3 91.7 79.8\\nTable 9: Top-100 retrieval accuracy by question-\\nanswer overlap categories on the NQ test set.',\n",
       " 'In-Context Retrieval-Augmented Language Models\\nOri Ram∗Yoav Levine∗Itay Dalmedigos Dor Muhlgay\\nAmnon Shashua Kevin Leyton-Brown Yoav Shoham\\nAI21 Labs\\n{orir,yoavl,itayd,dorm,amnons,kevinlb,yoavs}@ai21.com\\nAbstract\\nRetrieval-Augmented Language Modeling\\n(RALM) methods, which condition a lan-\\nguage model (LM) on relevant documents\\nfrom a grounding corpus during generation,\\nwere shown to significantly improve lan-\\nguage modeling performance. In addition,\\nthey can mitigate the problem of factually\\ninaccurate text generation and provide natu-\\nral source attribution mechanism. Existing\\nRALM approaches focus on modifying the\\nLM architecture in order to facilitate the in-\\ncorporation of external information, signifi-\\ncantly complicating deployment. This paper\\nconsiders a simple alternative, which we dub\\nIn-Context RALM : leaving the LM architec-\\nture unchanged and prepending grounding\\ndocuments to the input, without any further\\ntraining of the LM . We show that In-Context\\nRALM that builds on off-the-shelf general\\npurpose retrievers provides surprisingly large\\nLM gains across model sizes and diverse cor-\\npora. We also demonstrate that the document\\nretrieval and ranking mechanism can be spe-\\ncialized to the RALM setting to further boost\\nperformance. We conclude that In-Context\\nRALM has considerable potential to increase\\nthe prevalence of LM grounding, particularly\\nin settings where a pretrained LM must be\\nused without modification or even via API\\naccess.1\\n1 Introduction\\nRecent advances in language modeling (LM) have\\ndramatically increased the usefulness of machine-\\ngenerated text across a wide range of use-cases\\nand domains (Brown et al., 2020). However, the\\nmainstream paradigm of generating text with LMs\\nbears inherent limitations in access to external\\nknowledge. First, LMs are not coupled with any\\n∗Equal contribution.\\n1Our code is available at https://github.com/\\nAI21Labs/in-context-ralm\\nPerplexity\\n10.015.020.025.030.0\\nGPT-2 345M (M) GPT-2 1.5B (XL)No Retrieval In-Context RALM (BM25)\\nIn-Context RALM (Predictive Reranking)Figure 1: Our framework, dubbed In-Context\\nRALM , provides large language modeling gains on\\nthe test set of WikiText-103, without modifying the\\nLM. Adapting the use of a BM25 retriever (Robert-\\nson and Zaragoza, 2009) to the LM task (§5) yields\\nsignificant gains, and choosing the grounding doc-\\numents via our new class of Predictive Rerankers\\n(§6) provides a further boost. See Table 1 for the\\nfull results on five diverse corpora.\\nsource attribution, and must be trained in order\\nto incorporate up-to-date information that was not\\nseen during training. More importantly, they tend\\nto produce factual inaccuracies and errors (Lin\\net al., 2022; Maynez et al., 2020; Huang et al.,\\n2020). This problem is present in any LM gen-\\neration scenario, and is exacerbated when gener-\\nation is made in uncommon domains or private\\ndata. A promising approach for addressing the\\nabove is Retrieval-Augmented Language Modeling\\n(RALM), grounding the LM during generation by\\nconditioning on relevant documents retrieved from\\nan external knowledge source. RALM systems in-\\nclude two high level components: (i) document se-\\nlection , selecting the set of documents upon which\\nto condition; and (ii) document reading , determin-\\ning how to incorporate the selected documents into\\nthe LM generation process.\\nLeading RALM systems introduced recentlyarXiv:2302.00083v3  [cs.CL]  1 Aug 2023Language \\nModel World Cup 2022 was the \\nlast with 32 teams, \\nbefore the increase to Retriever FIFA World Cup 2026 will \\nexpand to 48 teams. \\nWorld Cup 2022 was the \\nlast with 32 teams, before \\nthe increase to 48 in the 2026 \\ntournament. \\nFigure 2: An example of In-Context RALM : we simply prepend the retrieved document before the input prefix.\\ntend to be focused on altering the language model\\narchitecture (Khandelwal et al., 2020; Borgeaud\\net al., 2022; Zhong et al., 2022; Levine et al., 2022c;\\nLi et al., 2022). Notably, Borgeaud et al. (2022) in-\\ntroduced RETRO, featuring document reading via\\nnontrivial modifications that require further train-\\ning to the LM architecture, while using an off-the-\\nshelf frozen BERT retriever for document selec-\\ntion. Although the paper’s experimental findings\\nshowed impressive performance gains, the need for\\nchanges in architecture and dedicated retraining\\nhas hindered the wide adoption of such models.\\nIn this paper, we show that a very simple doc-\\nument reading mechanism can have a large im-\\npact, and that substantial gains can also be made\\nby adapting the document selection mechanism to\\nthe task of language modeling. Thus, we show that\\nmany of the benefits of RALM can be achieved\\nwhile working with off-the-shelf LMs, even via\\nAPI access. Specifically, we consider a simple but\\npowerful RALM framework, dubbed In-Context\\nRALM (presented in Section 3), which employs a\\nzero-effort document reading mechanism: we sim-\\nply prepend the selected documents to the LM’s\\ninput text (Figure 2).\\nSection 4 describes our experimental setup. To\\nshow the wide applicability of our framework, we\\nperformed LM experiments on a suite of five di-\\nverse corpora: WikiText-103 (Merity et al., 2016),\\nRealNews (Zellers et al., 2019), and three datasets\\nfrom The Pile (Gao et al., 2021): ArXiv, Stack\\nExchange and FreeLaw. We use open-source LMs\\nranging from 110M to 66B parameters (from the\\nGPT-2, GPT-Neo, OPT and LLaMA model fami-\\nlies).\\nIn Section 5 we evaluate the application of off-\\nthe-shelf retrievers to our framework. In this\\nminimal-effort setting, we found that In-Context\\nRALM led to LM performance gains equivalent to\\nincreasing the LM’s number of parameters by 2–\\n3×across all of the text corpora we examined. In\\nSection 6 we investigate methods for adapting doc-ument ranking to the LM task, a relatively under-\\nexplored RALM degree of freedom. Our adapta-\\ntion methods range from using a small LM to per-\\nform zero-shot ranking of the retrieved documents,\\nup to training a dedicated bidirectional reranker\\nby employing self-supervision from the LM signal .\\nThese methods lead to further gains in the LM task\\ncorresponding to an additional size increase of 2×\\nin the LM architecture. As a concrete example of\\nthe gains, a 345M parameter GPT-2 enhanced by\\nIn-Context RALM outperforms a 762M parame-\\nter GPT-2 when employing an off-the-shelf BM25\\nretriever (Robertson and Zaragoza, 2009), and out-\\nperforms a 1.5B parameter GPT-2 when employing\\nour trained LM-oriented reranker (see Figure 1).\\nFor large model sizes, our method is even more\\neffective: In-Context RALM with an off-the-shelf\\nretriever improved the performance of a 6.7B pa-\\nrameter OPT model to match that of a 66B param-\\neter parameter OPT model (see Figure 4).\\nIn Section 7 we demonstrate the applicability\\nof In-Context RALM to downstream open-domain\\nquestions answering (ODQA) tasks.\\nIn a concurrent work, Shi et al. (2023) also sug-\\ngest to augment off-the-shelf LMs with retrieved\\ntexts by prepending them to the input. Their re-\\nsults are based on training a dedicated retriever for\\nlanguage modeling. In contrast, we focus on the\\ngains achievable in using off-the-shelf retrievers\\nfor this task. We show strong gains of this simpler\\nsetting by investigating: (1) which off-the-shelf\\nretriever is best suited for language modeling, (2)\\nthe frequency of retrieval operations, and (3) the\\noptimal query length. In addition, we boost the off-\\nthe-shelf retrieval performance by introducing two\\nreranking methods that demonstrate further gains\\nin perplexity.\\nWe believe that In-Context RALM can play two\\nimportant roles in making RALM systems more\\npowerful and more prevalent. First, given its simple\\nreading mechanism, In-Context RALM can serve\\nas a clean probe for developing document retrievalmethods that are specialized for the LM task. These\\nin turn can be used to improve both In-Context\\nRALM and other more elaborate RALM methods\\nthat currently leverage general purpose retrievers.\\nSecond, due to its compatibility with off-the-shelf\\nLMs, In-Context RALM can help drive wider de-\\nployment of RALM systems.\\n2 Related Work\\nRALM approaches can be roughly divided into two\\nfamilies of models: (i) nearest-neighbor language\\nmodels (also called kNN-LM), and (ii) retrieve\\nand read models . Our work belongs to the second\\nfamily, but is distinct in that it involves no further\\ntraining of the LM.\\nNearest Neighbor Language Models ThekNN-\\nLM approach was first introduced in Khandel-\\nwal et al. (2020). The authors suggest a simple\\ninference-time model that interpolates between two\\nnext-token distributions: one induced by the LM\\nitself, and one induced by the kneighbors from the\\nretrieval corpus that are closest to the query token in\\nthe LM embedding space. Zhong et al. (2022) sug-\\ngest a framework for training these models. While\\nthey showed significant gains from kNN-LM, the\\napproach requires storing the representations for\\neach token in the corpus , an expensive requirement\\neven for a small corpus like Wikipedia. Although\\nnumerous approaches have been suggested for al-\\nleviating this issue (He et al., 2021; Alon et al.,\\n2022), scaling any of them to large corpora remains\\nan open challenge.\\nRetrieve and Read Models This family of\\nRALMs creates a clear division between document\\nselection anddocument reading components. All\\nprior work involves training the LM. We begin by\\ndescribing works that use this approach for tack-\\nling downstream tasks, and then mention works ori-\\nented towards RALM. Lewis et al. (2020) and Izac-\\nard and Grave (2021) fine tuned encoder–decoder\\narchitectures for downstream knowledge-intensive\\ntasks. Izacard et al. (2022b) explored different\\nways of pretraining such models, while Levine\\net al. (2022c) pretrained an autoregressive LM on\\nclusters of nearest neighbors in sentence embed-\\nding space. Levine et al. (2022a) showed competi-\\ntive open domain question-answering performance\\nby prompt-tuning a frozen LM as a reader. Guu\\net al. (2020) pretrained REALM, a retrieval aug-\\nmented bidirectional, masked LM, later fine-tunedfor open-domain question answering. The work\\nclosest to this paper—with a focus on the language\\nmodeling task—is RETRO (Borgeaud et al., 2022),\\nwhich modifies an autoregressive LM to attend to\\nrelevant documents via chunked cross-attention,\\nthus introducing new parameters to the model. Our\\nIn-Context RALM differs from prior work in this\\nfamily of models in two key aspects:\\n•We use off-the-shelf LMs for document read-\\ningwithout any further training of the LM .\\n•We focus on how to choose documents for\\nimproved LM performance .\\n3 Our Framework\\n3.1 In-Context RALM\\nLanguage models define probability distributions\\nover sequences of tokens. Given such a sequence\\nx1, ..., x n, the standard way to model its probabil-\\nity is via next-token prediction: p(x1, ..., x n) =Qn\\ni=1p(xi|x<i), where x<i:=x1, ..., x i−1is the\\nsequence of tokens preceding xi, also referred to\\nas its prefix . This autoregressive model is usu-\\nally implemented via a learned transformer net-\\nwork (Vaswani et al., 2017) parameterized by the\\nset of parameters θ:\\np(x1, ..., x n) =nY\\ni=1pθ(xi|x<i), (1)\\nwhere the conditional probabilities are modeled\\nby employing a causal self-attention mask (Rad-\\nford et al., 2018). Notably, leading LMs such\\nas GPT-2 (Radford et al., 2019), GPT-3 (Brown\\net al., 2020), OPT (Zhang et al., 2022) or Jurassic-\\n1 (Lieber et al., 2021) follow this simple parame-\\nterization.\\nRetrieval augmented language models (RALMs)\\nadd an operation that retrieves one or more docu-\\nments from an external corpus C, and condition the\\nabove LM predictions on these documents. Specifi-\\ncally, for predicting xi, the retrieval operation from\\nCdepends on its prefix: RC(x<i), so the most\\ngeneral RALM decomposition is: p(x1, ..., x n) =Qn\\ni=1p(xi|x<i,RC(x<i)). In order to condition\\nthe LM generation on the retrieved document, pre-\\nvious RALM approaches used specialized architec-\\ntures or algorithms (see §2). Inspired by the suc-\\ncess of In-Context Learning (Brown et al., 2020;\\nDong et al., 2023), In-Context RALM refers to the\\nfollowing specific, simple method of concatenatingthe retrieved documents2within the Transformer’s\\ninput prior to the prefix (see Figure 2), which does\\nnot involve altering the LM weights θ:\\np(x1, ..., x n) =\\nnY\\ni=1pθ(xi|[RC(x<i);x<i]),(2)\\nwhere [a;b]denotes the concatenation of strings a\\nandb.\\nSince common Transformer-based LM imple-\\nmentations support limited length input sequences,\\nwhen the concatenation of the document and the\\ninput sequence exceed this limit we remove to-\\nkens from the beginning of xuntil the overall input\\nlength equals that allowed by the model. Because\\nour retrieved documents are passages of limited\\nlength, we always have enough context left from x\\n(see §4.3).\\n3.2 RALM Design Choices\\nWe detail below two practical design choices often\\nmade in RALM systems. In §5, we investigate the\\neffect of these in the setting of In-Context RALM.\\nRetrieval Stride While in the above formulation\\na retrieval operation can occur at each generation\\nstep, we might want to perform retrieval only once\\nevery s >1tokens due to the cost of calling the\\nretriever, and the need to replace the documents in\\nthe LM prefix during generation. We refer to sas\\ntheretrieval stride . This gives rise to the follow-\\ning In-Context RALM formulation (which reduces\\nback to Eq. (2) for s= 1):\\np(x1, ..., x n) =\\nns−1Y\\nj=0sY\\ni=1pθ\\x00\\nxs·j+i|\\x02\\nRC(x≤s·j);x<(s·j+i)\\x03\\x01\\n,\\n(3)\\nwhere ns=n/sis the number of retrieval strides.\\nNotably, in this framework the runtime costs of\\neach retrieval operation is composed of (a) apply-\\ning the retriever itself, and (b) recomputing the\\nembeddings of the prefix. In §5.2 we show that us-\\ning smaller retrieval strides, i.e., retrieving as often\\nas possible, is superior to using larger ones (though\\nIn-Context RALM with larger strides already pro-\\nvides large gains over vanilla LM). Thus, choosing\\nthe retrieval stride is ultimately a tradeoff between\\nruntime and performance.\\n2We always use a single document , but it is conceptually\\nsimple to support multiple documents as well.Retrieval Query Length While the retrieval\\nquery above in principle depends on all prefix to-\\nkensx≤s·j, the information at the very end of the\\nprefix is typically the most relevant to the generated\\ntokens. If the retrieval query is too long then this in-\\nformation can be diluted. To avoid this, we restrict\\nthe retrieval query at stride jto the last ℓtokens\\nof the prefix, i.e., we use qs,ℓ\\nj:=xs·j−ℓ+1, ..., x s·j.\\nWe refer to ℓas the retrieval query length . Note that\\nprior RALM work couples the retrieval stride sand\\nthe retrieval query length ℓ(Borgeaud et al., 2022).\\nIn §5, we show that enforcing s=ℓdegrades LM\\nperformance. Integrating these hyper-parameters\\ninto the In-Context RALM formulation gives\\np(x1, ..., x n) =\\nns−1Y\\nj=0sY\\ni=1pθ\\x10\\nxs·j+i|h\\nRC(qs,ℓ\\nj);x<(s·j+i)i\\x11\\n.\\n(4)\\n4 Experimental Details\\nWe now describe our experimental setup, including\\nall models we use and their implementation details.\\n4.1 Datasets\\nWe evaluated the effectiveness of In-Context\\nRALM across five diverse language modeling\\ndatasets and two common open-domain question\\nanswering datasets.\\nLanguage Modeling The first LM dataset is\\nWikiText-103 (Merity et al., 2016), which has been\\nextensively used to evaluate RALMs (Khandelwal\\net al., 2020; He et al., 2021; Borgeaud et al., 2022;\\nAlon et al., 2022; Zhong et al., 2022). Second, we\\nchose three datasets spanning diverse subjects from\\nThe Pile (Gao et al., 2021): ArXiv ,Stack Exchange\\nandFreeLaw . Finally, we also investigated Real-\\nNews (Zellers et al., 2019), since The Pile lacks a\\ncorpus focused only on news (which is by nature a\\nknowledge-intensive domain).\\nOpen-Domain Question Answering In order\\nto evaluate In-Context RALM on downstream\\ntasks as well, we use the Natural Questions (NQ;\\nKwiatkowski et al. 2019) and TriviaQA (Joshi et al.,\\n2017) open-domain question answering datasets.\\n4.2 Models\\nLanguage Models We performed our experi-\\nments using the four models of GPT-2 (110M–\\n1.5B; Radford et al. 2019), three models of GPT-\\nNeo and GPT-J (1.3B–6B; Black et al. 2021; Wangand Komatsuzaki 2021), eight models of OPT\\n(125M–66B; Zhang et al. 2022) and three mod-\\nels of LLaMA (7B–33B; Touvron et al. 2023). All\\nmodels are open source and publicly available.3\\nWe elected to study these particular models for\\nthe following reasons. The first four (GPT-2) mod-\\nels were trained on WebText (Radford et al., 2019),\\nwith Wikipedia documents excluded from their\\ntraining datasets. We were thus able to evaluate our\\nmethod’s “zero-shot” performance when retrieving\\nfrom a novel corpus (for WikiText-103). The rest of\\nthe models brought two further benefits. First, they\\nallowed us to investigate how our methods scale\\nto models larger than GPT-2. Second, the fact that\\nWikipedia was part of their training data allowed us\\nto investigate the usefulness of In-Context RALM\\nfor corpora seen during training. The helpfulness\\nof such retrieval has been demonstrated for previ-\\nous RALM methods (Khandelwal et al., 2020) and\\nhas also been justified theoretically by Levine et al.\\n(2022c).\\nWe ran all models with a maximum sequence\\nlength of 1,024, even though GPT-Neo, OPT and\\nLLaMA models support a sequence length of\\n2,048.4\\nRetrievers We experimented with both sparse\\n(word-based) and dense (neural) retrievers. We\\nused BM25 (Robertson and Zaragoza, 2009) as our\\nsparse model. For dense models, we experimented\\nwith (i) a frozen BERT-base (Devlin et al., 2019)\\nfollowed by mean pooling, similar to Borgeaud\\net al. (2022); and (ii) the Contriever (Izacard et al.,\\n2022a) and Spider (Ram et al., 2022) models,\\nwhich are dense retrievers that were trained in un-\\nsupervised manners.\\nReranking When training rerankers (Sec-\\ntion 6.2), we initialized from RoBERTa-base (Liu\\net al., 2019).\\n4.3 Implementation Details\\nWe implemented our code base using the Trans-\\nformers library (Wolf et al., 2020). We based\\nour dense retrieval code on the DPR repository\\n(Karpukhin et al., 2020).\\n3All models are available for use use via https://\\nhuggingface.co/\\n4In preliminary experiments, we observed similar improve-\\nments from In-Context RALM when using a sequence length\\nof 2,048. We used a sequence length of 1,024 in order to\\nfacilitate a direct comparison between all models.\\nPerplexity\\n10203040\\nGPT-2 117M (S) GPT-2 1.5B (XL)No Retrieval BERT Contriever Spider BM25Figure 3: The performance of four off-the-shelf\\nretrievers used for In-Context RALM on the de-\\nvelopment set of WikiText-103. All RALMs are\\nrun with s= 4(i.e., retrieval is applied every four\\ntokens). For each RALM, we report the result of\\nthe best query length ℓ(see Figures 6, 9, 10).\\nRetrieval Corpora For WikiText-103 and\\nODQA datasets, we used the Wikipedia corpus\\nfrom Dec. 20, 2018, standardized by Karpukhin\\net al. (2020) using the preprocessing from Chen\\net al. (2017). To avoid contamination, we found\\nand removed all 120 articles of the development\\nand test set of WikiText-103 from the corpus. For\\nthe remaining datasets, we used their training\\ndata as the retrieval corpus. Similar to Karpukhin\\net al. (2020), our retrieval corpora consist of\\nnon-overlapping passages of 100 words (which\\ntranslate to less than 150 tokens for the vast\\nmajority of passages). Thus, we truncate our\\nretrieved passages at 256 tokens when input to the\\nmodels, but they are usually much smaller.\\nRetrieval For sparse retrieval, we used the Py-\\nserini library (Lin et al., 2021). For dense retrieval,\\nwe applied exact search using FAISS (Johnson\\net al., 2021).\\n5 The Effectiveness of In-Context RALM\\nwith Off-the-Shelf Retrievers\\nWe now empirically show that despite its simple\\ndocument reading mechanism, In-Context RALM\\nleads to substantial LM gains across our diverse\\nevaluation suite. We begin in this section by inves-\\ntigating the effectiveness of off-the-shelf retrievers\\nfor In-Context RALM; we go on in §6 to show\\nthat further LM gains can be made by tailoring\\ndocument ranking functions to the LM task.\\nThe experiments in this section provided us\\nwith a recommended configuration for applying In-Model Retrieval RerankingWikiText-103 RealNews ArXiv Stack Exch. FreeLaw\\nword ppl token ppl token ppl token ppl token ppl\\nGPT-2 S– – 37.5 21.3 12.0 12.8 13.0\\nBM25 §5 – 29.6 16.1 10.9 11.3 9.6\\nBM25 Zero-shot §6.1 28.6 15.5 10.1 10.6 8.8\\nBM25 Predictive §6.2 26.8 – – – –\\nGPT-2 M– – 26.3 15.7 9.3 8.8 9.6\\nBM25 §5 – 21.5 12.4 8.6 8.1 7.4\\nBM25 Zero-shot §6.1 20.8 12.0 8.0 7.7 6.9\\nBM25 Predictive §6.2 19.7 – – – –\\nGPT-2 L– – 22.0 13.6 8.4 8.5 8.7\\nBM25 §5 – 18.1 10.9 7.8 7.8 6.8\\nBM25 Zero-shot §6.1 17.6 10.6 7.3 7.4 6.4\\nBM25 Predictive §6.2 16.6 – – – –\\nGPT-2 XL– – 20.0 12.4 7.8 8.0 8.0\\nBM25 §5 – 16.6 10.1 7.2 7.4 6.4\\nBM25 Zero-shot §6.1 16.1 9.8 6.8 7.1 6.0\\nBM25 Predictive §6.2 15.4 – – – –\\nTable 1: Perplexity on the test set of WikiText-103, RealNews and three datasets from the Pile. For\\neach LM, we report: (a) its performance without retrieval, (b) its performance when fed the top-scored\\npassage by BM25 (§5), and (c) its performance when applied on the top-scored passage of each of our two\\nsuggested rerankers (§6). All models share the same vocabulary, thus token-level perplexity ( token ppl )\\nnumbers are comparable. For WikiText we follow prior work and report word-level perplexity ( word ppl ).\\nModel RetrievalWikiText-103\\nword ppl\\nLLaMA-7B- 9.9\\nBM25, §5 8.8\\nLLaMA-13B- 8.5\\nBM25, §5 7.6\\nLLaMA-33B- 6.3\\nBM25, §5 6.1\\nTable 2: The performance of models from the\\nLLaMA family, measured by word-level perplexity\\non the test set of WikiText-103.\\nContext RALM: applying a sparse BM25 retriever\\nthat receives ℓ= 32 query tokens and is applied\\nas frequently as possible. Practically, we retrieve\\nevery s= 4 tokens ( ℓandsare defined in §3).\\nTable 1 shows for the GPT-2 models that across\\nall the examined corpora, employing In-Context\\nRALM with an off-the-shelf retriever improved\\nLM perplexity to a sufficient extent that it matched\\nthat of a 2–3×larger model. Figure 4 and Tables 2\\nand 5 show that this trend holds across model sizes\\nup to 66B parameters, for both WikiText-103 andRealNews.\\n5.1 BM25 Outperforms Off-the-Shelf Neural\\nRetrievers in Language Modeling\\nWe experimented with different off-the-shelf gen-\\neral purpose retrievers, and found that the sparse\\n(lexical) BM25 retriever (Robertson and Zaragoza,\\n2009) outperformed three popular dense (neu-\\nral) retrievers: the self-supervised retrievers Con-\\ntriever (Izacard et al., 2022a) and Spider (Ram et al.,\\n2022), as well as a retriever based on the average\\npooling of BERT embeddings that was used in\\nthe RETRO system (Borgeaud et al., 2022). We\\nconducted a minimal hyper-parameter search on\\nthe query length ℓfor each of the retrievers, and\\nfound that ℓ= 32 was optimal for BM25 (Fig-\\nure 6), and ℓ= 64 worked best for dense retrievers\\n(Figures 9, 10).\\nFigure 3 compares the performance gains of In-\\nContext RALM with these four general-purpose re-\\ntrievers. The BM25 retriever clearly outperformed\\nall dense retrievers. This outcome is consistent\\nwith prior work showing that BM25 outperforms\\nneural retrievers across a wide array of tasks, when\\napplied in zero-shot settings (Thakur et al., 2021).\\nThis result renders In-Context RALM even moreWikiText-103Perplexity\\n10.020.030.040.0\\nOPT-125M OPT-350M OPT-1.3B OPT-2.7B OPT-6.7B OPT-13B OPT-30B OPT-66BNo Retrieval In-Context RALM (BM25)\\nRealNewsPerplexity\\n3.08.013.018.0\\nOPT-125M OPT-350M OPT-1.3B OPT-2.7B OPT-6.7B OPT-13B OPT-30B OPT-66BNo Retrieval In-Context RALM (BM25)Figure 4: Results of OPT models (Zhang et al., 2022) on the test set of WikiText-103 (word-level\\nperplexity) and the development set of RealNews (token-level perplexity). In-Context RALM models use\\na BM25 retriever with s= 4(i.e., the retriever is called every four tokens) and ℓ= 32 (i.e., the retriever\\nquery is comprised of the last 32 tokens of the prefix). In-Context RALM with an off-the-shelf retriever\\nimproved the performance of a 6.7B parameter OPT model to match that of a 66B parameter OPT model.\\nappealing since applying a BM25 retriever is sig-\\nnificantly cheaper than the neural alternatives.\\n5.2 Frequent Retrieval Improves Language\\nModeling\\nWe investigated the effect of varying the retrieval\\nstride s(i.e., the number of tokens between consec-\\nutive retrieval operations). Figure 5 shows that LM\\nperformance improved as the retrieval operation\\nbecame more frequent. This supports the intuition\\nthat retrieved documents become more relevant the\\ncloser the retrieval query becomes to the gener-\\nated tokens. Of course, each retrieval operation\\nimposes a runtime cost. To balance performance\\nand runtime, we used s= 4 in our experiments.\\nFor comparison, RETRO employed a retrieval fre-\\nquency of s= 64 (Borgeaud et al., 2022), which\\nleads to large degradation in perplexity. Intuitively,\\nretrieving with high frequency (low retrieval stride)\\nallows to ground the LM in higher resolution.5.3 A Contextualization vs. Recency Tradeoff\\nin Query Length\\nWe also investigated the effect of varying ℓ, the\\nlength of the retrieval query for BM25. Figure 6\\nreveals an interesting tradeoff and a sweet spot\\naround a query length of 32tokens. Similar ex-\\nperiments for dense retrievers are given in App. A.\\nWe conjecture that when the retriever query is too\\nshort, it does not include enough of the input con-\\ntext, decreasing the retrieved document’s relevance.\\nConversely, excessively growing the retriever query\\ndeemphasizes the tokens at the very end of the pre-\\nfix, diluting the query’s relevance to the LM task.\\n6 Improving In-Context RALM with\\nLM-Oriented Reranking\\nSince In-Context RALM uses a fixed document\\nreading component by definition, it is natural to\\nask whether performance can be improved by spe-\\ncializing its document retrieval mechanism to the\\nLM task. Indeed, there is considerable scope for\\nimprovement: the previous section considered con-\\nditioning the model only on the first document re-Retrieval Stride ( 𝑠)Perplexity\\n10.020.030.040.0\\n1 2 4 8 16 32 64GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)Figure 5: An analysis of perplexity as a function\\nofs, the retrieval stride ,i.e., the number of tokens\\nbetween consecutive retrieval operations, on the\\ndevelopment set of WikiText-103. Throughout the\\npaper, we use s= 4 to balance perplexity and\\nruntime.\\nRetrieval Query Length (ℓ)Perplexity\\n10.015.020.025.030.035.0\\n16 32 64GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)\\nFigure 6: An analysis of perplexity as a function\\nofthe number of tokens in the query ℓfor BM25\\non the development set of WikiText-103. In the\\nappendix, we show similar trade-offs for dense\\nretrievers within WikiText-103. Throughout the\\npaper, we use a query length of ℓ= 32 tokens.\\ntrieved by the BM25 retriever. This permits very\\nlimited semantic understanding of the query, since\\nBM25 is based only on the bag of words signal.\\nMoreover, it offers no way to accord different de-\\ngrees of importance to different retrieval query to-\\nkens, such as recognizing that later query tokens\\nare more relevant to the generated text.\\nIn this section, we focus on choosing which doc-\\nument to present to the model, by reranking the\\ntop-kdocuments returned by the BM25 retriever.5\\nWe use Figure 7 as motivation: it shows the large\\npotential for improvement among the top- 16docu-\\nments returned by the BM25 retriever. We act upon\\n5In both §6.1 and §6.2 we use k= 16 .\\nPerplexity\\n10.020.030.040.0\\nNo Retrieval BM25 (Top-1) Oracle: BM25 (Top-16)GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)Figure 7: Potential for gains from reranking: per-\\nplexity improvement (on the development set of\\nWikiText-103) from an oracle that takes the best\\nof the top-16 documents retrieved by BM25 rather\\nthan the first.\\nthis motivation by using two rerankers. Specifi-\\ncally, in §6.1 we show performance gains across\\nour evaluation suite obtained by using an LM to\\nperform zero-shot reranking of the top- kBM25\\nretrieved documents (results in third row for each\\nof the models in Table 1). Then, in §6.2 we show\\nthat training a specialized bidirectional reranker\\nof the top- kBM25 retrieved documents in a self-\\nsupervised manner via the LM signal can provide\\nfurther LM gains (results in forth row for each of\\nthe models in Table 1).\\n6.1 LMs as Zero-Shot Rerankers\\nFirst, we used off-the-shelf language models as\\ndocument rerankers for the In-Context RALM set-\\nting. Formally, for a query qconsisting of the\\nlastℓtokens in the prefix of the LM input x, let\\n{d1, ..., d k}be the top- kdocuments returned by\\nBM25. For retrieval iteration j, let the text for\\ngeneration be y:=xs·j+1, ..., x s·j+s. Ideally, we\\nwould like to find the document di∗that maximizes\\nthe probability of the text for generation, i.e.,\\ni∗= arg max\\ni∈[k]pθ(y|[di;x≤s·j]). (5)\\nHowever, at test time we do not have access to\\nthe tokens of y. Instead, we used the last pre-\\nfixtokens (which areavailable at test time), de-\\nnoted by y′, for reranking. Formally, let s′be\\na hyper-parameter that determines the number of\\nthe prefix tokens by which to rerank. We define\\ny′:=xs·j−s′+1, ..., x s·j(i.e., the stride of length s′\\nthat precedes y) and choose the document dˆisuchModelReranking\\nModelWikiText-103 RealNews\\nword ppl token ppl\\nGPT-2 345M (M)GPT-2 110M (S) 20.8 12.1\\nGPT-2 345M (M) 20.8 12.0\\nGPT-2 762M (L)GPT-2 110M (S) 17.7 10.7\\nGPT-2 762M (L) 17.6 10.6\\nGPT-2 1.5B (XL)GPT-2 110M (S) 16.2 9.9\\nGPT-2 1.5B (XL) 16.1 9.8\\nTable 3: Perplexity for zero-shot reranking (§6.1) where the reranking models is smaller than the LM, or\\nthe LM itself. Reranking is performed on the top 16 documents retrieved by BM25. Using a GPT-2 110M\\n(S) instead of a larger language model as a reranker leads to only a minor degradation.\\nthat\\nˆi= arg max\\ni∈[k]pϕ(y′|\\x02\\ndi;x≤(s·j−s′)\\x03\\n).(6)\\nThe main motivation is that since BM25 is a lexical\\nretriever, we want to incorporate a semantic signal\\ninduced by the LM. Also, this reranking shares con-\\nceptual similarities with the reranking framework\\nof Sachan et al. (2022) for open-domain question\\nanswering, where y′(i.e., the last prefix tokens) can\\nbe thought of as their “question”.\\nNote that our zero-shot reranking does not re-\\nquire that the LM used for reranking is the same\\nmodel as the LM used for generation ( i.e., the LM\\nin Eq. (6), parameterized by ϕ, does not need to be\\nthe LM in Eq. (2), parameterized by θ). This ob-\\nservation unlocks the possibility of reranking with\\nsmaller (and thus faster) models, which is impor-\\ntant for two main reasons: (i) Reranking kdocu-\\nments requires kforward passes; and (ii) it allows\\nour methods to be used in cases where the actual\\nLM’s log probabilities are not available (for exam-\\nple, when the LM is accessed through an API).6\\nResults A minimal hyper-parameter search on\\nthe development set of WikiText-103 revealed that\\nthe optimal query length is s′= 16 ,7so we proceed\\nwith this value going forward. Table 1 shows the\\nresults of letting the LM perform zero-shot rerank-\\ning on the top-16 documents retrieved by BM25\\n(third row for each of the models). It is evident\\nthat reranking yielded consistently better results\\nthan simply taking the first result returned by the\\nretriever.\\n6Note we do not require that the two models share the\\nsame vocabulary.\\n7We experimented with s′∈ {4,8,16,32}.Table 3 shows that a small LM (GPT-2 117M)\\ncan be used to rerank the documents for all larger\\nGPT-2 models, with roughly the same performance\\nas having each LM perform reranking for itself,\\nsupporting the applicability of this method for LMs\\nthat are only accessible via an API.\\n6.2 Training LM-dedicated Rerankers\\nNext, we trained a reranker to choose one of the\\ntop-kdocuments retrieved by BM25. We refer to\\nthis approach as Predictive Reranking , since the\\nreranker learns to choose which document will help\\nin “predicting” the upcoming text. For this process,\\nwe assume availability of training data from the\\ntarget corpus. Our reranker is a classifier that gets\\na prefix x≤s·jand a document di(fori∈[k]), and\\nproduces a scalar f(x≤s·j, di)that should resemble\\nthe relevance of diforthe continuation ofx≤s·j.\\nWe then normalize these relevance scores:\\nprank(di|x≤s·j) =exp(f(x≤s·j, di))Pk\\ni′=1exp(f(x≤s·j, di′)),(7)\\nand choose the document dˆisuch that\\nˆi= arg max\\ni∈[k]prank(di|x≤s·j). (8)\\nCollecting Training Examples To train our pre-\\ndictive reranker, we collected training examples\\nas follows. Let x≤s·jbe a prefix we sample from\\nthe training data, and y:=xs·j+1, ..., x s·j+sbe the\\ntext for generation upcoming in its next stride. We\\nrun BM25 on the query qs,ℓ\\njderived from x≤s·j\\n(see §3.2) and get kdocuments {d1, ..., d k}. For\\neach document di, we then run the LM to compute\\npθ(y|[di;x≤s·j])similar to Eq. (4).Number of documentsEM\\n0.020.040.060.080.0\\n0 1 2 3 4TriviaQA: LLaMa-7B TriviaQA: LLaMa-13B\\nNQ: LLaMa-7B NQ: LLaMa-13BFigure 8: Zero-shot performance of In-Context\\nRALM on the development set of Natural Ques-\\ntions and TriviaQA, when varying the number of\\ndocuments (retrieved by DPR) shown in-context.\\nTraining Our reranker was a fine-tuned\\nRoBERTa-base (Liu et al., 2019) that trained for\\n10,000 steps with a peak learning rate of 10−5and\\na batch size of 32. Overall, we created 300,000\\nexamples from the training set of WikiText-103 as\\nexplained above. The loss function we use to train\\nthe reranker follows previous work (Guu et al.,\\n2020; Lewis et al., 2020):\\n−logkX\\ni=1prank(di|x≤s·j)·pθ(y|[di;x≤s·j]).(9)\\nNote that unlike those works, we train only the\\nreranker ( prank), keeping the LM weights θfrozen.\\nResults Table 1 shows the result of our predictive\\nreranker, trained on WikiText-103. Specifically, we\\ntrained it with data produced by GPT-2 110M (S),\\nand tested its effectiveness for all GPT-2 models.\\nWe observed significant gains obtained from Predic-\\ntive Reranking. For example, the perplexity of GPT-\\n2 110M (S) improved from 29.6 to 26.8, and that of\\nGPT-2 1.5B (XL) improved from 16.6 to 15.4. This\\ntrend held for the other two models as well. Overall,\\nthese results demonstrate that training a reranker\\nwith domain-specific data was more effective than\\nzero-shot reranking (Section 6.1). Note that these\\nresults—while impressive—still leave room for fur-\\nther improvements, compared to the top-16 BM25\\noracle results (see Figure 7). Moreover, the oracle\\nresults themselves can be improved by retrieving\\nk > 16documents via a BM25 retriever, or by\\ntraining stronger retrievers dedicated to the RALM\\ntask. We leave this direction for future work.Model Retrieval NQ TriviaQA\\nLLaMA-7B- 10.3 47.5\\nDPR 28.0 56.0\\nLLaMA-13B- 12.0 54.8\\nDPR 31.0 60.1\\nLLaMA-33B- 13.7 58.3\\nDPR 32.3 62.7\\nTable 4: Zero-shot results of In-Context RALM on\\nthe test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the\\nquestion along with its corresponding retrieved doc-\\numents, and returns the answer) is simply a frozen\\nlarge LM: notpretrained, fine-tuned or prompted\\nto be retrieval-augmented. For the closed-book set-\\nting, we utilize the prompt of Touvron et al. (2023).\\nFor the open-book setting, we extend this prompt\\nto include retrieved documents (see App. C). We\\nuse DPR (Karpukhin et al., 2020) as our retriever.\\nVarying the Number of Documents To inves-\\ntigate the the effect of the number of documents\\nshown to the model, we performed a minimal anal-\\nysis on the development set of NQ and TriviaQA.\\nFigure 8 demonstrates that showing documents in-\\ncontext significantly improves the model’s perfor-\\nmance. In addition, most of the gain can be ob-\\ntained by using only two documents (or even a\\nsingle one in some cases).Results Table 4 gives the results of In-Context\\nRALM on the test set of Natural Questions and\\nTriviaQA. Motivated by our previous findings,\\nwe used two retrieved documents. It is evident\\nthat showing the model relevant documents sig-\\nnificantly boosted its performance. For example,\\nadding retrieved documents improved LLaMA-\\n13B in the zero-shot setting by more than 18 points\\non NQ (from 12.0% to 31.0%) and more than 5\\npoints on TriviaQA (from 54.8% to 60.1%).\\n8 Discussion\\nRetrieval from external sources has become a com-\\nmon practice in knowledge-intensive tasks (such\\nas factual question answering, fact checking, and\\nmore; Petroni et al. 2021). In parallel, recent break-\\nthroughs in LM generation capabilities has led to\\nLMs that can generate useful long texts. How-\\never, factual inaccuracies remain a common way in\\nwhich machine-generated text can fall short, and\\nlack of direct provenance makes it hard to trust\\nmachine generated text. This makes language mod-\\neling both a promising and an urgent new applica-\\ntion area for knowledge grounding, and motivates\\npromoting RALM approaches. Prior research has\\nalready investigated RALM, of course, but it is\\nnot yet widely deployed. One likely reason is that\\nexisting approaches rely upon fine-tuning the LM,\\nwhich is typically difficult and costly, and is even\\nimpossible for LMs accessible only via an API.\\nThis paper presented the framework of In-\\nContext RALM , enabling frozen, off-the-shelf LMs\\nto benefit from retrieval. We demonstrated that\\nsubstantial performance gains can be achieved by\\nusing general purpose retrievers, and showed that\\nadditional gains can be achieved by tailoring the\\ndocument selection to the LM setting. A recent\\nwork by Muhlgay et al. (2023) demonstrates that\\nIn-Context RALM is indeed able to improve the\\nfactuality of large LMs.\\nSeveral directions for further improvement re-\\nmain for future work. First, this paper considers\\nonly the case of prepending a single external docu-\\nment to the context; adding more documents could\\ndrive further gains (for example, using the frame-\\nwork of Ratner et al. 2022). Second, we retrieved\\ndocuments every fixed interval of stokens, but see\\npotential for large latency and cost gains by retriev-\\ning more sparsely, such as only when a specialized\\nmodel predicts that retrieval is needed.\\nWe release the code used in this work, for thecommunity to use and improve over. We hope it\\nwill drive further research of RALM, which will\\nenable its wider adoption.\\nAcknowledgements\\nWe would like to thank the reviewers and the Ac-\\ntion Editor for their valuable feedback.\\nReferences\\nUri Alon, Frank Xu, Junxian He, Sudipta Sengupta,\\nDan Roth, and Graham Neubig. 2022. Neuro-\\nsymbolic language modeling with automaton-\\naugmented retrieval. In ICML .\\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\\nand Stella Biderman. 2021. GPT-Neo: Large\\nScale Autoregressive Language Modeling with\\nMesh-Tensorflow.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Mil-\\nlican, George Bm Van Den Driessche, Jean-\\nBaptiste Lespiau, Bogdan Damoc, Aidan Clark,\\nDiego De Las Casas, Aurelia Guy, Jacob Menick,\\nRoman Ring, Tom Hennigan, Saffron Huang,\\nLoren Maggiore, Chris Jones, Albin Cassirer,\\nAndy Brock, Michela Paganini, Geoffrey Irv-\\ning, Oriol Vinyals, Simon Osindero, Karen Si-\\nmonyan, Jack Rae, Erich Elsen, and Laurent\\nSifre. 2022. Improving language models by re-\\ntrieving from trillions of tokens. In ICML .\\nTom B. Brown, Benjamin Mann, Nick Ryder,\\nMelanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish\\nSastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-V oss, Gretchen Krueger, Tom Henighan,\\nRewon Child, Aditya Ramesh, Daniel Ziegler,\\nJeffrey Wu, Clemens Winter, Christopher Hesse,\\nMark Chen, Eric Sigler, Mateusz Litwin, Scott\\nGray, Benjamin Chess, Jack Clark, Christopher\\nBerner, Sam McCandlish, Alec Radford, Ilya\\nSutskever, and Dario Amodei. 2020. Language\\nmodels are few-shot learners. In Advances in\\nNeural Information Processing Systems .\\nDanqi Chen, Adam Fisch, Jason Weston, and An-\\ntoine Bordes. 2017. Reading Wikipedia to an-\\nswer open-domain questions. In Proceedings\\nof the 55th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: LongPapers) , pages 1870–1879, Vancouver, Canada.\\nAssociation for Computational Linguistics.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training\\nof deep bidirectional transformers for language\\nunderstanding. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the\\nAssociation for Computational Linguistics: Hu-\\nman Language Technologies, Volume 1 (Long\\nand Short Papers) , pages 4171–4186, Minneapo-\\nlis, Minnesota. Association for Computational\\nLinguistics.\\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu,\\nLei Li, and Zhifang Sui. 2023. A survey on\\nin-context learning.\\nMartin Fajcik, Martin Docekal, Karel Ondrej, and\\nPavel Smrz. 2021. R2-D2: A modular baseline\\nfor open-domain question answering. In Find-\\nings of the Association for Computational Lin-\\nguistics: EMNLP 2021 , pages 854–870, Punta\\nCana, Dominican Republic. Association for\\nComputational Linguistics.\\nLeo Gao, Stella Biderman, Sid Black, Laurence\\nGolding, Travis Hoppe, Charles Foster, Jason\\nPhang, Horace He, Anish Thite, Noa Nabeshima,\\nShawn Presser, and Connor Leahy. 2021. The\\npile: An 800gb dataset of diverse text for lan-\\nguage modeling.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong\\nPasupat, and Ming-Wei Chang. 2020. REALM:\\nRetrieval-augmented language model pre-\\ntraining. In ICML .\\nJunxian He, Graham Neubig, and Taylor Berg-\\nKirkpatrick. 2021. Efficient nearest neighbor\\nlanguage models. In Proceedings of the 2021\\nConference on Empirical Methods in Natural\\nLanguage Processing , pages 5703–5714, Online\\nand Punta Cana, Dominican Republic. Associa-\\ntion for Computational Linguistics.\\nMinlie Huang, Xiaoyan Zhu, and Jianfeng Gao.\\n2020. Challenges in building intelligent open-\\ndomain dialog systems. ACM Trans. Inf. Syst. ,\\n38(3).\\nGautier Izacard, Mathilde Caron, Lucas Hosseini,\\nSebastian Riedel, Piotr Bojanowski, ArmandJoulin, and Edouard Grave. 2022a. Unsu-\\npervised dense information retrieval with con-\\ntrastive learning. Transactions on Machine\\nLearning Research .\\nGautier Izacard and Edouard Grave. 2021. Lever-\\naging passage retrieval with generative models\\nfor open domain question answering. In Pro-\\nceedings of the 16th Conference of the European\\nChapter of the Association for Computational\\nLinguistics: Main Volume , pages 874–880, On-\\nline. Association for Computational Linguistics.\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\\nDwivedi-Yu, Armand Joulin, Sebastian Riedel,\\nand Edouard Grave. 2022b. Atlas: Few-shot\\nlearning with retrieval augmented language mod-\\nels.\\nJeff Johnson, Matthijs Douze, and Hervé Jégou.\\n2021. Billion-scale similarity search with GPUs.\\nIEEE Transactions on Big Data , 7(3):535–547.\\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale\\ndistantly supervised challenge dataset for read-\\ning comprehension. In Proceedings of the 55th\\nAnnual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers) ,\\npages 1601–1611, Vancouver, Canada. Associa-\\ntion for Computational Linguistics.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min,\\nPatrick Lewis, Ledell Wu, Sergey Edunov, Danqi\\nChen, and Wen-tau Yih. 2020. Dense passage re-\\ntrieval for open-domain question answering. In\\nProceedings of the 2020 Conference on Empir-\\nical Methods in Natural Language Processing\\n(EMNLP) , pages 6769–6781, Online. Associa-\\ntion for Computational Linguistics.\\nUrvashi Khandelwal, Omer Levy, Dan Juraf-\\nsky, Luke Zettlemoyer, and Mike Lewis. 2020.\\nGeneralization through memorization: Nearest\\nneighbor language models. In International\\nConference on Learning Representations .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia\\nRedfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob\\nDevlin, Kenton Lee, Kristina Toutanova, Llion\\nJones, Matthew Kelcey, Ming-Wei Chang, An-\\ndrew M. Dai, Jakob Uszkoreit, Quoc Le, andSlav Petrov. 2019. Natural questions: A bench-\\nmark for question answering research. Trans-\\nactions of the Association for Computational\\nLinguistics , 7:452–466.\\nYoav Levine, Itay Dalmedigos, Ori Ram, Yoel\\nZeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin,\\nOpher Lieber, Barak Lenz, Shai Shalev-Shwartz,\\nAmnon Shashua, Kevin Leyton-Brown, and\\nYoav Shoham. 2022a. Standing on the shoul-\\nders of giant frozen language models.\\nYoav Levine, Ori Ram, Daniel Jannai, Barak Lenz,\\nShai Shalev-Shwartz, Amnon Shashua, Kevin\\nLeyton-Brown, and Yoav Shoham. 2022b. Huge\\nfrozen language models as readers for open-\\ndomain question answering. In ICML 2022\\nWorkshop on Knowledge Retrieval and Lan-\\nguage Models .\\nYoav Levine, Noam Wies, Daniel Jannai, Dan\\nNavon, Yedid Hoshen, and Amnon Shashua.\\n2022c. The inductive bias of in-context learn-\\ning: Rethinking pretraining example design. In\\nInternational Conference on Learning Represen-\\ntations .\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus,\\nFabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau\\nYih, Tim Rocktäschel, Sebastian Riedel, and\\nDouwe Kiela. 2020. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. In\\nAdvances in Neural Information Processing Sys-\\ntems, pages 9459–9474.\\nZonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022.\\nDecoupled context processing for context aug-\\nmented language modeling. In Advances in Neu-\\nral Information Processing Systems .\\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\\nShoham. 2021. Jurassic-1: Technical details and\\nevaluation.\\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin,\\nJheng-Hong Yang, Ronak Pradeep, and Rodrigo\\nNogueira. 2021. Pyserini: A python toolkit for\\nreproducible information retrieval research with\\nsparse and dense representations. In Proceed-\\nings of the 44th International ACM SIGIR Con-\\nference on Research and Development in Infor-\\nmation Retrieval , SIGIR ’21, page 2356–2362,\\nNew York, NY , USA. Association for Comput-\\ning Machinery.Stephanie Lin, Jacob Hilton, and Owain Evans.\\n2022. TruthfulQA: Measuring how models\\nmimic human falsehoods. In Proceedings of the\\n60th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers) ,\\npages 3214–3252, Dublin, Ireland. Association\\nfor Computational Linguistics.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\\nMandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019. RoBERTa: A robustly optimized bert\\npretraining approach.\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet,\\nand Ryan McDonald. 2020. On faithfulness and\\nfactuality in abstractive summarization. In Pro-\\nceedings of the 58th Annual Meeting of the As-\\nsociation for Computational Linguistics , pages\\n1906–1919, Online. Association for Computa-\\ntional Linguistics.\\nStephen Merity, Caiming Xiong, James Bradbury,\\nand Richard Socher. 2016. Pointer sentinel mix-\\nture models.\\nDor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\\nNir Ratner, Yonatan Belinkov, Omri Abend,\\nKevin Leyton-Brown, Amnon Shashua, and\\nYoav Shoham. 2023. Generating benchmarks\\nfor factuality evaluation of language models.\\nFabio Petroni, Aleksandra Piktus, Angela Fan,\\nPatrick Lewis, Majid Yazdani, Nicola De Cao,\\nJames Thorne, Yacine Jernite, Vladimir\\nKarpukhin, Jean Maillard, Vassilis Plachouras,\\nTim Rocktäschel, and Sebastian Riedel. 2021.\\nKILT: a benchmark for knowledge intensive lan-\\nguage tasks. In Proceedings of the 2021 Con-\\nference of the North American Chapter of the\\nAssociation for Computational Linguistics: Hu-\\nman Language Technologies , pages 2523–2544,\\nOnline. Association for Computational Linguis-\\ntics.\\nAlec Radford, Karthik Narasimhan, Tim Salimans,\\nand Ilya Sutskever. 2018. Improving language\\nunderstanding by generative pre-training.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Lan-\\nguage models are unsupervised multitask learn-\\ners.Ori Ram, Gal Shachaf, Omer Levy, Jonathan Be-\\nrant, and Amir Globerson. 2022. Learning to re-\\ntrieve passages without supervision. In Proceed-\\nings of the 2022 Conference of the North Amer-\\nican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technolo-\\ngies, pages 2687–2700, Seattle, United States.\\nAssociation for Computational Linguistics.\\nNir Ratner, Yoav Levine, Yonatan Belinkov,\\nOri Ram, Omri Abend, Ehud Karpas, Am-\\nnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2022. Parallel context windows im-\\nprove in-context learning of large language mod-\\nels.\\nStephen Robertson and Hugo Zaragoza. 2009. The\\nprobabilistic relevance framework: BM25 and\\nbeyond. Found. Trends Inf. Retr. , 3(4):333–389.\\nDevendra Sachan, Mike Lewis, Mandar Joshi, Ar-\\nmen Aghajanyan, Wen-tau Yih, Joelle Pineau,\\nand Luke Zettlemoyer. 2022. Improving passage\\nretrieval with zero-shot question generation. In\\nProceedings of the 2022 Conference on Empir-\\nical Methods in Natural Language Processing ,\\npages 3781–3797, Abu Dhabi, United Arab Emi-\\nrates. Association for Computational Linguis-\\ntics.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\\nmoyer, and Wen tau Yih. 2023. REPLUG:\\nRetrieval-augmented black-box language mod-\\nels.\\nNandan Thakur, Nils Reimers, Andreas Rücklé,\\nAbhishek Srivastava, and Iryna Gurevych. 2021.\\nBEIR: A heterogeneous benchmark for zero-shot\\nevaluation of information retrieval models. In\\nProceedings of the Neural Information Process-\\ning Systems Track on Datasets and Benchmarks ,\\nvolume 1.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard,\\nXavier Martinet, Marie-Anne Lachaux, Timo-\\nthée Lacroix, Baptiste Rozière, Naman Goyal,\\nEric Hambro, Faisal Azhar, Aurelien Rodriguez,\\nArmand Joulin, Edouard Grave, and Guillaume\\nLample. 2023. LLaMA: Open and efficient foun-\\ndation language models.\\nAshish Vaswani, Noam Shazeer, Niki Parmar,\\nJakob Uszkoreit, Llion Jones, Aidan Gomez,Ł ukasz Kaiser, and Illia Polosukhin. 2017. At-\\ntention is all you need. In Advances in Neural\\nInformation Processing Systems 30 , pages 5998–\\n6008.\\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\\n6B: A 6 Billion Parameter Autoregressive Lan-\\nguage Model.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi,\\nPierric Cistac, Tim Rault, Remi Louf, Morgan\\nFuntowicz, Joe Davison, Sam Shleifer, Patrick\\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu,\\nCanwen Xu, Teven Le Scao, Sylvain Gugger,\\nMariama Drame, Quentin Lhoest, and Alexan-\\nder Rush. 2020. Transformers: State-of-the-art\\nnatural language processing. In Proceedings of\\nthe 2020 Conference on Empirical Methods in\\nNatural Language Processing: System Demon-\\nstrations , pages 38–45, Online. Association for\\nComputational Linguistics.\\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\\nYonatan Bisk, Ali Farhadi, Franziska Roesner,\\nand Yejin Choi. 2019. Defending against neural\\nfake news. In Advances in Neural Information\\nProcessing Systems , volume 32. Curran Asso-\\nciates, Inc.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\\nArtetxe, Moya Chen, Shuohui Chen, Christopher\\nDewan, Mona Diab, Xian Li, Xi Victoria Lin,\\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt\\nShuster, Daniel Simig, Punit Singh Koura, An-\\njali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\\n2022. OPT: Open pre-trained transformer lan-\\nguage models.\\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022.\\nTraining language models with memory augmen-\\ntation. In Proceedings of the 2022 Conference\\non Empirical Methods in Natural Language Pro-\\ncessing , pages 5657–5673, Abu Dhabi, United\\nArab Emirates. Association for Computational\\nLinguistics.\\nA Query Length Ablations\\nFigure 9 and Figure 10 show ablations on the opti-\\nmal query length ℓfor off-the-shelf dense retrievers\\n(BERT and Contriever respectively). We omit the\\nresults of Spider as they are almost identical to\\nthose of Contriever. Consistently, using ℓ= 64(tokens) is optimal. This is in contrast to similar\\nexperiments we conducted for BM25 ( cf. Figure 6),\\nwhere ℓ= 32 is optimal.\\nB GPT-Neo Results\\nTable 5 gives the results of applying In-Context\\nRALM to the models from the GPT-Neo model\\nfamily on WikiText-103 and RealNews.\\nC Open-Domain Question Answering\\nExperiments: Further Details\\nClosed-Book Setting For the closed-book set-\\nting, we adopt the prompt of Touvron et al. (2023):\\nAnswer these questions:\\nQ: Who got the first nobel\\nprize in physics?\\nA:\\nOpen-Book Setting For the open-book setting,\\nwe extend the above prompt as follows:\\nNobel Prize\\nA group including 42\\nSwedish writers, artists,\\nand literary critics\\nprotested against this\\ndecision, having expected\\nLeo Tolstoy to be awarded.\\nSome, including Burton\\nFeldman, have criticised\\nthis prize because they...\\nNobel Prize in Physiology\\nor Medicine\\nIn the last half century\\nthere has been an\\nincreasing tendency\\nfor scientists to work\\nas teams, resulting in\\ncontroversial exclusions.\\nAlfred Nobel was born\\non 21 October 1833 in\\nStockholm, Sweden, into\\na family of engineers...\\nBased on these texts,\\nanswer these questions:\\nQ: Who got the first nobel\\nprize in physics?\\nA:Model RetrievalWiki-103 RealNews\\nword ppl token ppl\\nGPT-Neo 1.3B- 17.5 12.3\\nBM25, §5 14.6 9.9\\nGPT-Neo 2.7B- 15.1 11.0\\nBM25, §5 12.8 9.0\\nGPT-J 6B- 11.6 9.2\\nBM25, §5 10.0 7.7\\nTable 5: The performance of models from the GPT-\\nNeo family, measured by word-level perplexity on\\nthe test set of WikiText-103 and token-level per-\\nplexity on the development set of RealNews.\\nRetrieval Query Length (ℓ)Perplexity\\n15.025.035.045.0\\n16 32 64 128 256GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)\\nFigure 9: An analysis of perplexity as a function\\nofthe number of tokens in the query for an off-\\nthe-shelf BERT retriever on the development set of\\nWikiText-103.\\nRetrieval Query Length (ℓ)Perplexity\\n10.020.030.040.0\\n16 32 64 128 256GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)\\nFigure 10: An analysis of perplexity as a function\\nofthe number of tokens in the query for Contriever\\non the development set of WikiText-103.',\n",
       " 'MuRAG: Multimodal Retrieval-Augmented Generator\\nfor Open Question Answering over Images and Text\\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen\\nGoogle Research\\n{wenhuchen,hexiang,patverga,wcohen}@google.com\\nAbstract\\nWhile language Models store a massive\\namount of world knowledge implicitly in their\\nparameters, even very large models often fail\\nto encode information about rare entities and\\nevents, while incurring huge computational\\ncosts. Recently, retrieval-augmented models,\\nsuch as REALM, RAG, and RETRO, have\\nincorporated world knowledge into language\\ngeneration by leveraging an external non-\\nparametric index and have demonstrated im-\\npressive performance with constrained model\\nsizes. However, these methods are restricted\\nto retrieving only textual knowledge, neglect-\\ning the ubiquitous amount of knowledge in\\nother modalities like images – much of which\\ncontains information not covered by any text.\\nTo address this limitation, we propose the\\nﬁrst Multimodal Retrieval-Augmented Trans-\\nformer (MuRAG), which accesses an external\\nnon-parametric multimodal memory to aug-\\nment language generation. MuRAG is pre-\\ntrained with a mixture of large-scale image-\\ntext and text-only corpora using a joint con-\\ntrastive and generative loss. We perform ex-\\nperiments on two different datasets that re-\\nquire retrieving and reasoning over both im-\\nages and text to answer a given query: We-\\nbQA, and MultimodalQA. Our results show\\nthat MuRAG achieves state-of-the-art accu-\\nracy, outperforming existing models by 10-\\n20% absolute on both datasets and under both\\ndistractor and full-wiki settings.\\n1 Introduction\\nPre-trained language models like GPT-3 (Brown\\net al., 2020), PaLM (Chowdhery et al., 2022), etc\\nhave been shown to capture a massive amount\\nof world knowledge implicitly in their parame-\\nters. However, using such large models incurs an\\nextremely high computation cost. As an alterna-\\ntive to a singular monolithic transformer, retrieval-\\naugmented architectures like KNN-LM (Khandel-\\nwal et al., 2019), REALM (Guu et al., 2020),\\nFigure 1: Visual information-seeking queries : These\\nqueries are unanswerable with text-only retrieval and\\nrequire retrieving and reasoning over images.\\nRAG (Lewis et al., 2020), FiD (Izacard and Grave,\\n2021), and RETRO (Borgeaud et al., 2021) have\\nbeen proposed to decouple world knowledge from\\nthe model’s parameters. More speciﬁcally, these\\nmodels are trained to access an external mem-\\nory to enhance the model’s predictions. Such\\nretrieval-augmented architectures have multiple\\nbeneﬁcial properties including: decreased model\\nsize (Borgeaud et al., 2021), better attribution/-\\nexplanation for model predictions (Lewis et al.,\\n2020), and adaptability to new information with-\\nout retraining (Verga et al., 2021). However, pre-\\nvious retrieval-augmented models are limited to\\nmemories that contain only text or structured data\\nand hence cannot make use of the massive amount\\nof multimodal knowledge available on the web—\\nmuch of which contains information only available\\nin non-text modalities.\\nFigure 1, shows several information-seeking\\nqueries that require retrieving and reasoning over\\nvisual knowledge. Here, a user ﬁrst poses a ques-\\ntion such as “What can be found on the White\\nHouse balconies at Christmas” . The system then\\nretrieves relevant items from its memory, for exam-arXiv:2210.02928v2  [cs.CL]  20 Oct 2022ple, the ﬁrst image of Figure 1 with the caption\\n“White House during Christmas” , which it uses to\\nproduce the answer “wreaths and garlands” . Ex-\\nisting text retrieval-augmented models would strug-\\ngle with such queries because, in many cases, they\\nwould simply not have access to the answer as some\\nknowledge does not exist in text form. That, cou-\\npled with the abundance of multimodal knowledge\\nthat exists, leads to the conclusion that retrieval-\\naugmented models should ultimately be developed\\nto retrieve and reason over multiple modalities.\\nFigure 2: Model Overview : retrieval-and-predict pro-\\ncess of MuRAG on downstream datasets.\\nIn this paper, we are speciﬁcally interested in\\nendowing pre-trained language models with a non-\\nparametric multimodal memory containing images,\\ntext, or image-text pairs. To accomplish this, we\\nﬁrst combine pre-trained T5 (Raffel et al., 2020)\\nand ViT (Dosovitskiy et al., 2020) models to build\\na backbone encoder (Figure 3), which encodes\\nimage-text pairs, image-only, and text-only inputs\\ninto a multimodal representation. MuRAG uses the\\nbackbone encoder to embed items into an external\\nmemory as well as queries to retrieve multimodal\\nknowledge from that memory. These retrievals\\nthen augment a language model to generate more\\nvisually-grounded outputs.\\nWe pre-train MuRAG with a mixture of\\nimage-text and text-only datasets including\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption (Sharma et al., 2018), VQA (An-\\ntol et al., 2015) and Probably-Asked-Questions\\n(PAQ) (Lewis et al., 2021). More speciﬁcally, we\\nreformulate these datasets in a retrieve-and-predict\\nformat. Here, the model’s input is an image along\\nwith a text prompt. The model then retrieves from\\na memory containing captions and passages, which\\nit uses to generate a target token sequence. The\\nmodel is trained with both a contrastive and a gen-erative loss; this teaches the model to discriminate\\nrelevant from irrelevant memory entries, and guides\\nthe model to leverage the multimodal knowledge\\ninto generation.\\nUnlike the pre-training stage, during ﬁne-\\ntuning Figure 2 the model’s input is a question,\\nand the memory contains a collection of captioned\\nimages and text snippets. We ﬁne-tune MuRAG\\non the downstream datasets with a contrastive and\\ngenerative loss similar to pre-training. To avoid ex-\\ncessive computation cost, we develop a two-stage\\ntraining pipeline to ﬁrst train with small in-batch\\nmemory, and then with a statically encoded and\\nindexed large global memory.\\nOur experiments show that MuRAG achieves\\nstate-of-the-art performance on two different open-\\nmultimodal-QA datasets, both of which require\\nretrieving images and text from a large corpus to\\nanswer factoid questions: WebQA (Chang et al.,\\n2022) and MultimodalQA (Talmor et al., 2021). On\\nboth datasets, we outperform sophisticated base-\\nlines (Li et al., 2020; Radford et al., 2021; Zhang\\net al., 2021) by 10-20% accuracy under both dis-\\ntractor (from 40+ candidates) and full-wiki settings\\n(from 1M candidates). We also perform a compre-\\nhensive study to ablate different components of the\\npre-training to see their contributions. These em-\\npirical results demonstrate the effectiveness of our\\nproposed models to integrate multimodal knowl-\\nedge into pre-trained generation models and pave\\nthe way to uniﬁed retrieval-augmented frameworks.\\n2 Related Work\\nRetrieval Augmented Models Retrieval aug-\\nmented models are hybrid models containing\\nboth parameterized sequence models and a non-\\nparametric memory, infusing world knowledge into\\nexisting language models. Among them, KNN-\\nLM (Khandelwal et al., 2019) was ﬁrst proposed\\nto retrieve instances from a text training corpus to\\nhelp language modeling. Later, RETRO (Borgeaud\\net al., 2021) was proposed to scale up the text cor-\\npus to trillions of tokens, enabling the model to\\nachieve similar perplexity to GPT-3 (Brown et al.,\\n2020) with 25x fewer model parameters. Another\\nfamily of models, such as REALM (Guu et al.,\\n2020), RAG (Lewis et al., 2020), and FiD (Izacard\\nand Grave, 2021), integrate Wikipedia passages as\\na datastore to beneﬁt downstream knowledge in-\\ntensive tasks ( e.g.Question Answering). REALM\\nis an encoder-only model trained with masked lan-guage modeling, while RAG and FiD adopt an\\nencoder-decoder model with a generative language\\nmodeling objective. Compared to them, MuRAG\\nis the ﬁrst retrieval-augmented model that is ca-\\npable of using knowledge presented in multiple\\nmodalities ( i.e.visual and textual knowledge data),\\nwhereas all prior methods are restricted to using\\ntext-only knowledge.\\nMultimodal Transformers Multimodal trans-\\nformers have demonstrated strong performances\\nin learning cross-modal representation that are gen-\\nerally beneﬁcial on downstream vision and lan-\\nguage tasks, such as image-text retrieval (Karpa-\\nthy and Fei-Fei, 2015), image captioning (Chen\\net al., 2015), and VQA (Antol et al., 2015). These\\nmethods typically learn a joint transformer model\\non top of unimodal visual and textual backbones,\\nvia fusing deep features from each modality. The\\nearly version of multimodal transformers (Lu et al.,\\n2019; Chen et al., 2020; Li et al., 2020) usually\\nlearns a Transformer on pre-extracted unimodal\\nfeatures for contextualization, which makes it im-\\npossible to adjust those unimodal features to the\\ntarget tasks. Recently, SimVLM (Wang et al., 2022)\\nand COCA (Yu et al., 2022) proposed end-to-end\\ntraining for both deep multimodal transformers and\\nunimodal featurization networks and demonstrated\\nstrong performance in both multimodal and uni-\\nmodal downstream tasks. The multimodal memory\\nencoder of MuRAG is broadly similar to SimVLM\\nand CoCa, but has a different focus to encode and\\nretrieve multimodal knowledge ( i.e.images and\\ntexts) to augment language generation models.\\nMultimodal Question Answering The problem\\nof multimodal question answering has been ex-\\ntensively studied. VQA was the ﬁrst proposed to\\nanswer questions from visual-only inputs. Later,\\nOK-VQA (Marino et al., 2019) enlarged VQA’s\\nscope to annotate questions requiring both image\\nand implicit textual/common-sense knowledge to\\nanswer. More recently, MuMuQA (Reddy et al.,\\n2021), ManyModelQA (Hannan et al., 2020) and\\nMIMOQA (Singh et al., 2021) provide questions\\nwhich require reasoning over images and explicitly\\nprovided text snippets. However, these datasets\\nare restricted to dealing with given text and images\\nwithout requiring any retrieval from the web: they\\nare analogous to machine-reading approaches to\\nQA from text like SQuAD, rather than open-book\\nQA. To study the more realistic open multimodal\\nQA task, WebQA (Chang et al., 2022) and Multi-modalQA (Talmor et al., 2021) have been proposed\\nto evaluate answers to open queries which require\\nretrieving and reasoning over a large-scale web\\nmultimodal corpus. Our model uses these datasets\\nto study open-world multimodal question answer-\\ning, obtaining state-of-the-art results.\\n3 Model\\n3.1 Backbone Encoder\\nFigure 3: Backbone encoder: ViT encodes image\\npatches into a sequence of vectors eI, while word em-\\nbedding converts text tokens into another sequence of\\nvectors eT. These vectors are concatenated to form\\nf\\x12(e)and fed to a decoder for text generation.\\nMuRAG is built on top of a simpler model we\\ncall a “backbone” model, which is pre-trained to\\nencode image-text pairs such that they are suitable\\nfor both answer generation and retrieval. The back-\\nbone model’s encoder is used as a component of\\nthe MuRAG model. The backbone model is built\\nwith a pre-trained visual Transformer (Dosovitskiy\\net al., 2020) and a T5 text Transformer (Raffel et al.,\\n2020), and consists of a multimodal encoder f\\x12and\\ndecoder g\\x12. The encoder takes as input a sequence\\nof image-text pairs, where either the image or the\\ntext component can be empty to accommodate text-\\nonly and image-only cases.\\nAs depicted in Figure 3, the encoder can take a\\nsequence of images and text. For image input, we\\nﬁrst split each into 16x16 patches and feed them\\nto a ViT (Dosovitskiy et al., 2020) transformer to\\ngenerate a sequence of visual embedding denoted\\naseI2RLi\\x02D, where Liis the length of the im-\\nage tokens. For text input, we use word embedding\\nto produce another sequence of textual embedding\\neT2RLt\\x02D. Forkimages and ntext inputs, we\\nconcatenate all their embeddings in the input or-\\nder as e= [e1\\nI;e1\\nT;\\x01\\x01\\x01;ek\\nI;en\\nT]2R(kLt+nLi)\\x02D,\\nwhich is fed to another bi-directional transformer\\nf\\x12initialized from T5. We enable cross-attentionbetween the two modalities to produce a fused rep-\\nresentation, denoted as f\\x12(e)2R(kLt+nLi)\\x02D.\\nWe add a [CLS] token to obtain a pooled repre-\\nsentation f\\x12(e)[CLS]2RDfor dense retrieval.\\n3.2 MuRAG\\nWe build MuRAG (shown in Figure 4) on top of\\nthe backbone model. During the retriever stage,\\nMuRAG takes a query qof any modality as in-\\nput and retrieves from a memory Mof image-text\\npairs. Speciﬁcally, we apply the backbone encoder\\nf\\x12to encode a query q, and use maximum inner\\nproduct search (MIPS (Guo et al., 2020)) over all of\\nthe memory candidates m2M to ﬁnd the Top-K\\nnearest neighbors TopK(Mjq) = [m1;\\x01\\x01\\x01; mk].\\nFormally, we deﬁne TopK(Mjq)as follows:\\nTopK(Mjq) =TopK\\nm2Mf\\x12(q)[CLS]\\x01f\\x12(m)[CLS]\\nDuring the reader stage, the retrievals (the raw im-\\nage patches) are combined with the query qas\\nan augmented input [m1;\\x01\\x01\\x01; mk; q], which is fed\\nto the backbone encoder f\\x12to produce retrieval-\\naugmented encoding. The decoder model g\\x12uses\\nattention over this representation to generate tex-\\ntual outputs y=y1;\\x01\\x01\\x01; yntoken by token.\\np(yijyi\\x001) =g\\x12(yijf\\x12(TopK(Mjq);q);y1:i\\x001)\\nwhere yis decoded from a given vocabulary V.\\n3.3 Pre-training\\nThe pre-training implementation is depicted in the\\nupper portion of Figure 4, where the input query\\nis an image xIplus a text prompt xp. The exter-\\nnal memoryMcontains textual-only entries mT.\\nThe Top-K retrievals mT\\n1;\\x01\\x01\\x01; mT\\nkare leveraged to\\ngenerate the textual output. To avoid the excessive\\ncomputation cost of backpropagation over the mas-\\nsive external memory, we adopt an in-batch mem-\\noryMB, dynamically constructed from the input\\nexamples in a batch. The small in-batch memory\\nenables MuRAG to continuously update the mem-\\nory encoder efﬁciently similar to TOME (de Jong\\net al., 2022) and QAMAT (Chen et al., 2022).\\nDataset The pre-training corpus consists of\\nLAION (Schuhmann et al., 2021), Conceptual-\\nCaption-12M+3M (CC) (Sharma et al., 2018;\\nChangpinyo et al., 2021), VQA (Antol et al., 2015)\\nand PAQ (Lewis et al., 2021) Table 1. LAION is\\na publicly-released image-text dataset containingcrawled image-text pairs ﬁltered by CLIP (Rad-\\nford et al., 2021). We apply rules to ﬁlter LAION\\nfrom 400M to 200M by removing text with HTTP-\\nURLs or image width/height beyond 1000 pixels.\\nCC contains 15M (image, anonymized alt-text)\\npairs crawled from the web but ﬁltered more ex-\\ntensively to maintain high alignment quality. VQA\\ncontains annotated QA pairs aligned to MSCOCO\\nimages. We further add captions to each image\\nfrom MSCOCO-Captioning (Lin et al., 2014) to\\ncreate (Image, Caption, QA) triples. PAQ is a text-\\nonly dataset containing 65M machine-generated\\nQA pairs along with their source Wikipedia pas-\\nsage.\\nDataset #Size Format Source\\nCC 15M (Image, Caption) Crawled\\nLAION 200M (Image, Alt-Text) Crawled\\nPAQ 65M (Passage, QA) Generated\\nVQA 400K (Image, Caption, QA) Annotated\\nTable 1: Pre-training Dataset Statistics\\nFor LAION and CC, we use the input image as\\nxI, and ‘generate caption:’ as the text prompt xp.\\nFor VQA, we use the input image as xIand the\\nquestion as the prompt xp. For PAQ, we use an\\nempty array as the input image and the question\\nas the prompt. The in-batch memory MBis con-\\nstructed by stacking the captions associated with\\nthe input images in LAION/CC/VQA and the pas-\\nsages associated with the questions in PAQ. Each\\ntextual memory entry is denoted as mT. The de-\\ncoder is optimized to generate either a caption or\\nan answer, depending on the source dataset. Since\\nthe four dataset sizes are highly unbalanced, we\\nuse ﬁxed mixture sampling ratios to balance their\\npresence during pre-training.\\nWe train the model with a joint loss L=Lgen+\\nLconas follows:\\nLcon=\\x00logexp(f\\x12(xI; xp)\\x01f\\x12(mT))P\\nm2MBexp(f\\x12(xI; xp)\\x01f\\x12(mT))\\nLgen=\\x00logg\\x12(yjf\\x12(Mp;xI;xp))\\nMp=(\\nTopK(MBjxI; xp)If(xI; xp)2PAQ/VQA\\nØ If (xI; xp)2LAION/CC\\nwhere Mpis the retrieved augmentation: if the\\ninput query is from PAQ/VQA, we use the retrieved\\nmemory entries, otherwise, we use null. The reason\\nfor setting it to null for LAION/CC is to avoid a\\ntrivial solution when the generation target (caption)\\nalso exactly appears in the memory.\\nThe contrastive loss Lconis minimized to dis-\\ncriminate between the positive query-memory pairsFigure 4: Model Architecture: the model accesses an external memory to obtain multimodal knowledge contained\\nin images or text snippets, which is used to augment the generation. The upper part deﬁnes the pre-training\\nimplementation, while the lower part deﬁnes ﬁne-tuning implementation.\\nand all other query-memory pairs from the mem-\\nory. The pairwise matching score is computed as\\nthe dot product between query f\\x12(xI;xp)[CLS] and\\ncandidates f\\x12(mT)[CLS]. This objective enables\\nthe model to retrieve the most relevant knowledge\\nfrom the memory. The generative loss Lgenis min-\\nimized to generate target tokens yconditioned on\\nthe retrieval-augmented representation. This ob-\\njective enables the model to combine information\\nacross different modalities for text generation.\\n3.4 Fine-tuning\\nWe ﬁnetune MuRAG to align with the expected\\ninputs of the downstream datasets which require an-\\nswering text questions by retrieving image-caption\\npairs or text snippets from the external knowledge\\ndatastore. As depicted in the lower part of Figure 4,\\nthe input query for the downstream task is a text\\nquestion xq, and the memory Mcontaining (im-\\nage, text) pairs (mI; mT).1The Top-K retrievals\\nf(mI\\n1; mT\\n1);\\x01\\x01\\x01;(mI\\nk; mT\\nk)gare leveraged to gen-\\nerate the answer a. To minimize the computation\\ncost, we develop a two-stage pipeline to optimize\\nwith an in-batch memory and then resume with\\nﬁxed retrieval from global memory.\\nIn-Batch Training In this stage, we aim to mini-\\nmize the joint loss function L=Lcon+Lgenbased\\n1We set the image to a zero array if the memory entry is a\\ntext snippet.on the in-batch memory MBas follows:\\nLcon=\\x00logexp(f\\x12(xq)\\x01f\\x12(mI;mT))P\\nm2MBexp(f\\x12(xq)\\x01f\\x12(mI;mT))\\nLgen=\\x00logg\\x12(yjf\\x12(TopK(MBjxq);xq))\\nThe in-batch memory MBis constructed in the\\nfollowing way: the k-th example in the dataset is\\nrepresented as (xq;k; yk;fmI\\ni; mI\\nigk;f\\x16mI\\nj;\\x16mT\\njgk),\\nwhere mrepresents the positive (image, text)\\nsource, and \\x16mrepresents the hard negative\\n(image, text) source provided by the dataset2.\\nFor a batch with Bexamples, we assemble\\nall the associated positive and negative knowl-\\nedge source as our in-batch memory MB=\\nffmI\\ni; mI\\nig1;f\\x16mI\\nj;\\x16mT\\njg1;\\x01\\x01\\x01;f\\x16mI\\nj;\\x16mT\\njgBg.\\nFixed-Retrieval Training After in-batch train-\\ning, we encode all available cross-modal pairs, and\\nindex these encodings for fast MIPS retrieval. We\\nthen apply the trained retriever to search over the\\nfull multimodal corpus Mto obtain the global top-\\nK retrievals TopK(Mjxq)and continue to opti-\\nmizeLgen. During this training phase, the stored\\nencodings are not updated. During inference time,\\nwe use ﬁxed encodings to generate the answers.\\n2These hard negatives are mined through Bing Search API\\nand Wikipedia page, refer to (Chang et al., 2022) for details.4 Experiments\\n4.1 Implementation Details\\nThe backbone model uses T5-base (Raffel et al.,\\n2020) and a ViT-large model (Dosovitskiy et al.,\\n2020) as described in Table 2. We adopt the\\nsentence-piece model from T5 with a vocabulary\\nsize of 32128. The ViT model was pre-trained\\non the JFT dataset. We resize every image into\\n224x224 pixels and split them into a sequence of\\n16x16 patches. The output of ViT is a sequence\\nof 1024-dimension vectors, which are projected\\nto 768-dimension for consistency with T5 model.\\nMuRAG reuses the model as retriever and reader,\\nthus the full model size is 527M parameters.\\nModel #Enc #Dec Hidden Heads Params\\nViT-large 24 0 1024 16 307M\\nT5-base 12 12 768 12 220M\\nTable 2: The model size and conﬁgurations, with\\n#Enc/#Dec denoting encoder/decoder layers.\\nOur model is implemented in JAX (Bradbury\\net al., 2018), based on the T5X codebase (Roberts\\net al., 2022). During pre-training, we ﬁrst train the\\nmodel on LAION for 1M steps, and then continue\\ntraining on CC/PAQ/VQA with 1:1:1 sample ratio\\nfor another 200K steps. We optimize the model\\nwith Adafactor (Shazeer and Stern, 2018). For both\\nstages, we adopt a constant learning rate of 5e-4\\nand a batch size of 4096. The models are trained\\non 64 Cloud v4 TPUs (Jouppi et al., 2020).\\nWe then ﬁne-tune MuRAG on WebQA and Mul-\\ntimodalQA with a constant learning rate of 3e-4\\nfor 20K steps. The checkpoint with the highest\\nvalidation score is run on the test set. We use a\\nbatch size of 64 and set TopK=4 for both in-batch\\ntraining and ﬁxed-retrieval training. We noticed\\nthat increasing Top-K further does not yield further\\nimprovement. We use a beam size of 2 to search\\nfor the best hypothesis for both datasets (increasing\\nit further doesn’t yield better performance).\\n4.2 Datasets\\nFor evaluation, we choose two multimodal QA\\ndatasets: WebQA (Chang et al., 2022) and Mul-\\ntimodalQA (Talmor et al., 2021) and demonstrate\\ntheir statistics in Table 3.\\nWebQA This dataset contains multi-hop, multi-\\nmodal question-answer pairs where all questions\\nare knowledge-seeking queries. The queries re-\\nquire 1-2 images or 1-2 text snippets to answer.Dataset Train Dev Test\\nImage/Text Image/Text Image/Text\\nWebQA 18K/17K 2.5K/2.4K 3.4K/4K\\nMultimodalQA 2.1K/7.4K 230/721 -\\nTable 3: Overall Statistics of downstream dataset.\\nEach query in WebQA is associated with a set of\\nvisual/text distractors (hard negatives). The an-\\nswers in WebQA are normally complete sentences\\nto better assess the model’s generation capabil-\\nity. Two evaluation setups are used, namely dis-\\ntractor and full-wiki. Under the distractor setup,\\nthe model needs to retrieve from these hard neg-\\natives + positives to answer the question. Under\\nthe full-wiki setup, the model needs to search over\\n1.1M text and visual sources from Wikipedia to an-\\nswer the question. For evaluation, WebQA uses\\nBARTScore (Yuan et al., 2021) to measure the\\nﬂuency between the generation and the reference,\\nand keyword accuracy score to measure the cor-\\nrectness/truthfulness of the generation. These two\\nscores are multiplied to calculate the overall score.\\nMultimodalQA-Subset This dataset contains\\nhuman-annotated multimodal questions over differ-\\nent modalities including tables, text, and images.\\nWikipedia tables are used as anchors to connect dif-\\nferent modalities. The authors ﬁrst use the template\\nto generate questions and then ask crowd-workers\\nto ﬁlter and paraphrase the generated questions.\\nSince tables are outside the scope of our paper, we\\nfocus on the subset of queries requiring only text\\nand image information. Speciﬁcally, we choose the\\nquestions with types of ‘TextQ’ and ‘ImageQ’ to\\nconstruct the subset. The query requires 1 image\\nor 1 text snippet to answer. Each query in Multi-\\nmodalQA is also associated with visual and text dis-\\ntractors (hard negatives). Similarly, two evaluation\\nsetups are used as before. Under a full-wiki setup,\\nMultimodalQA uses a database containing 500K\\ntext and visual sources. The evaluation scores are\\nbased on Exact Match and F1.\\n4.3 Baselines\\nFor WebQA and MultimodalQA, we mainly\\ncompare different variants of pre-trained vision-\\nlanguage models.\\nVLP In WebQA, VLP-like models (Zhou et al.,\\n2020) like Oscar (Li et al., 2020) and VinvL (Zhang\\net al., 2021) are used as the standard baselines.\\nThese models were pre-trained on Conceptual3M (Sharma et al., 2018) with a masked language\\nobjective. During ﬁne-tuning, the VLP model takes\\na set of token inputs <[CLS], si, [SEP], Q, [SEP]>\\nﬁrst to select the most plausible source si, and then\\nfeedsiin the form of <[CLS], S,Q,A, [SEP]>\\nto autoregressively decode answer Awith masked\\nlanguage model prediction.\\nAutoRouting In MultimodalQA, this method\\nﬁrst applies a question type classiﬁer to detect the\\nmodality of the question (either a passage or an\\nimage), and then routes the question to its sub-\\nmodel. The method uses RoBERTa-large (Roberts\\net al., 2022) for text-questions and VilBERT (Lu\\net al., 2019) with features extracted from Faster-\\nRCNN (Ren et al., 2015) for image questions.\\nCLIP (K) CLIP (Radford et al., 2021) is used for\\nfull-wiki retrieval. Speciﬁcally, the baselines sys-\\ntems adopt CLIP to encode queries and all the im-\\nage/text candidates separately into vectors and then\\nrun approximated nearest neighbor searches to ﬁnd\\na set of K potential candidates. After the coarse-\\nlevel retrieval without cross-attention, it adopts a\\nreranker to further narrow down to the 1-2 candi-\\ndates to feed as input Sto the QA model.\\n4.4 Experimental Results\\nWe demonstrate WebQA’s results in Table 4. All\\nresults reported are the medium score from three\\nruns with different random seeds, and the variance\\nof the Overall score is within 0.2%. We can observe\\nthat MuRAG can signiﬁcantly outperform VLP\\nwith different backends including Oscar, ResNet,\\nand VinVL. In retrieval performance, our model\\noutperforms VLP by 15% in the full-wiki setting.\\nFor Fluency, our model outperforms VLP by 12%\\nunder the distractor setting and 14% under the full-\\nwiki setting. For Accuracy, our model manages\\nto achieve 16% under the distractor setting and\\neven 20% the under the full-wiki setting. These\\nimprovements reﬂect the high ﬂuency and accuracy\\nof MuRAG’s generation, and the improvement is\\nmore pronounced for full wiki.\\nWe show the MultimodalQA results in Table 5.\\nWe can see that MuRAG is also able to vastly\\noutperform the routing-based multimodality QA\\nmodel. For text questions, our model improves\\nover AutoRouting by 10+% EM under both set-\\ntings. For image questions, the gap becomes more\\nsigniﬁcant, with 20+% improvement under both\\nsettings. Similarly, we ﬁnd that our model is more\\ncapable of handling full-wiki corpus.Evaluation Distractor\\nMetrics Retr FL Accuracy Overall\\nQuestion-Only - 34.9 22.2 13.4\\nVLP (Oscar) 68.9 42.6 36.7 22.6\\nVLP + ResNeXt 69.0 43.0 37.0 23.0\\nVLP + VinVL 70.9 44.2 38.9 24.1\\nMuRAG 74.6 55.7 54.6 36.1\\nEvaluation Full-Wiki\\nCLIP (2) + VLP 11.9 34.2 24.1 14.6\\nCLIP (20) + VLP 24.0 36.1 27.2 16.1\\nMuRAG 39.7 50.7 47.8 31.5\\nTable 4: WebQA ofﬁcial test-set results indicated\\non leaderboard3as of May 2022. Retr denotes\\nthe retrieval-F1 score. FL refers to ﬂuency metric\\nBARTSCcore, and Accuracy refers to keyword match-\\ning F1 score, they are combined as Overall.\\nEvaluation Distractor\\nMetricsText Image All\\nEM F1 EM F1 EM\\nQuestion-Only 15.4 18.4 11.0 15.6 13.8\\nAutoRouting 49.5 56.9 37.8 37.8 46.6\\nMuRAG 60.8 67.5 58.2 58.2 60.2\\nEvaluation Full-Wiki\\nMetricsText Image All\\nEM F1 EM F1 EM\\nCLIP (10) +\\nAutoRouting35.6 40.2 32.5 32.5 34.7\\nMuRAG 49.7 56.1 56.5 56.5 51.4\\nTable 5: Multimodal dev-set results on the subset.\\n4.5 Ablation Study\\nHere we ablate the properties of MuRAG to better\\nunderstand our experimental results.\\nPre-training Corpus In order to study the contri-\\nbutions of different pre-training corpora, we investi-\\ngated several pre-training corpus combinations. We\\nreport their ﬁne-tuned results on WebQA test set\\nin Table 6. As can be seen, without any pre-training,\\nour model only achieves an overall score of 23.5,\\nwhich lags behind the baseline models. After pre-\\ntraining on different singular datasets, MuRAG is\\nable to achieve better performance than the base-\\nlines. Among the individual datasets, LAION is\\nshown to yield the highest score, and adding CC,\\nPAQ, and VQA to the pre-training corpus set one\\nby one produces steady improvements.\\nTwo-Stage Fine-tuning In order to study the ne-\\ncessity of the two-stage ﬁne-tuning, we perform an\\nablation study to see the impact of the two stages.\\nWe display our results in Table 7. (Only In-Batch)Pre-train Dataset FL Accuracy Overall\\nNone 42.5 36.1 23.5\\nCC 46.4 41.3 25.6\\nLAION 47.8 44.8 28.3\\nVQA 47.0 44.4 27.4\\nPAQ 46.8 42.8 27.0\\nLAION+CC 49.5 47.4 30.7\\nLAION+CC+PAQ 53.7 51.8 34.4\\nLAION+CC+PAQ+VQA 55.7 54.6 36.1\\nTable 6: Ablation Study for different pre-training cor-\\npus, score under distractor setting.\\nModel WebQA Multimodal\\nMuRAG (Only In-Batch) 29.4 49.6\\nMuRAG (Only Fixed-Retrieval) 25.8 40.7\\nMuRAG (Two Stage) 31.5 51.4\\nTable 7: Ablation Study for different ﬁne-tuning stages\\nto see their contributions. WebQA uses the overall\\nscore, and MultimodalQA refers to EM-all score.\\nEvaluation Model Correct Wrong\\nDistractorMuRAG (Text) 80% 20%\\nMuRAG (Image) 64% 36%\\nFull-WikiMuRAG (Text) 72% 28%\\nMuRAG (Image) 54% 46%\\nTable 8: The human evaluation results on WebQA\\ndataset separately for image/text queries.\\nrefers to the model trained only with in-batch mem-\\nory are directly used to generate outputs by access-\\ning the global memory. Without further tuning,\\nthe performance will drop by roughly 2% on both\\ndatasets. (Only Fixed-Retrieval) refers to using the\\npre-trained retriever directly to obtain Top-K and\\nthen optimize the generative loss. As can be seen,\\nthe performance drop is more severe in this case\\nfor both datasets. This is understandable due the\\nmisalignment between pre-training retrieval is (im-\\nage + text->text) while the ﬁne-tuning retrieval is\\n(text -> image+text). Thus, it is necessary to adapt\\nthe MuRAG’s pre-trained retriever to different use\\ncases depending on the downstream datasets.\\n4.6 Human Analysis\\nIn order to better understand the model’s perfor-\\nmance, we manually study 200 model outputs and\\nclassify them into three categories and show our\\nmanual analysis results in Table 8. As can be seen,\\nimage queries are much harder than text queries.\\nMuRAG only achieves 64% accuracy for the dis-\\ntractor setting and 54% accuracy for the full-wiki\\nsetting, falling signiﬁcantly behind text accuracy.\\nWe further categorize the image-query errors\\nFigure 5: Upper left: correct prediction, Upper Right:\\nerror due to miscounting, Lower: error due to misrecog-\\nnition (multiple image reasoning). Q refers to the ques-\\ntion, P refers to prediction and R refers to the reference.\\nmanually into the categories of Table 9. Counting\\nis the most difﬁcult question type, and constitutes\\n52% of the total errors, while object recognition\\nerrors rank second, constituting 29% of errors. In\\ncontrast, identifying color, shape, and gender is\\ncomparatively easier, with fairly low error rates.\\nWe demonstrate some correct and typical error\\ncases in Figure 5 including miscounting and mis-\\nrecognizing objects. We observe that these errors\\nare mostly due to several reasons: 1) the question\\nis related to infrequent objects, thus making recog-\\nnition errors, 2) the image scene is highly complex\\nwith a large number of objects, thus grounding to a\\nspeciﬁc region is difﬁcult, 3) the questions require\\noptical character recognition ability from images.\\nHence, the bottleneck of MuRAG is still in the\\nvisual understanding module.\\nCategory Count Object Color Shape Gender\\nRatio 52% 29.4% 5.8% 5.8% 5.8%\\nTable 9: Error categorization and their ratios on sam-\\npled WebQA-dev image queries.\\n5 Examples\\nWe list more examples in Figure 6 and Figure 7.\\nAs can be seen, in the ﬁrst example, the model isgrounded on the oracle image-text pair to make the\\ncorrect prediction. However, in the second exam-\\nple, though the model retrieves the wrong image-\\ntext pair, it is able to make the correct prediction of\\n‘the angel is holding a dead body’. We conjecture\\nthat the model utilizes textual clues to make the pre-\\ndiction rather than grounding on the image itself.\\nSuch shortcut learning is concerning and needs to\\nbe addressed through better learning algorithms.\\nFigure 6: Examples: we demonstrate model retrieval\\nvs. groundtruth and model answer vs. reference.\\n6 Conclusion\\nIn this paper, we build the ﬁrst visually-grounded\\nlanguage generator capable of retrieving multi-\\nmodal knowledge from a large-scale corpus. Our\\nexperiments show the promise of this approach, as\\nit outperforms existing baselines by a large margin.\\nAt the same time, the performance on knowledge-\\nseeking queries that require reasoning over images\\nis still signiﬁcantly lower than the performance on\\nqueries requiring only text. This indicates that there\\nis still ample room for further improvements and\\nwe hope our study can motivate more research on\\nbetter multimodal retrieval-augmented models.\\nLimitations\\nThe current approach has several limitations: 1)\\nsince we do not mine hard negatives during pre-\\ntraining, negatives come from other examples\\nwithin the same batch. This requires that we set the\\nbatch size sufﬁciently large enough to collect hard-\\nenough negatives. This results in the pre-training\\nFigure 7: Examples: we demonstrate model retrieval\\nvs. groundtruth, and model answer vs. reference.\\nrequiring a large number of computation resources\\nto reach competitive retrieval abilities. 2) our pre-\\ntraining corpus’s format (image -> text) is differ-\\nent from ﬁne-tuning (text -> image+text). This\\nmisalignment limits the model’s performance. Fu-\\nture work should consider how to design a better-\\naligned pre-training objective to achieve better per-\\nformance. 3) Current visual representation in the\\nreader stage is relatively expensive, i.e. 16x16=196\\ntokens per image, which poses great challenges for\\nthe transformer encoder to scale up to large Top-K\\nvalues due to the quadratic attention complexity.\\nEthical Statement\\nOur work uses the LAION dataset, a widely-used\\nand publicly available large-scale visual-language\\ncorpus crawled from the web. The authors have\\nconducted automatic ﬁltering to greatly reduce\\nharmful content. However, it is not possible to\\nfully remove all of the potential risks from the data\\ngiven its tremendous size. Being trained on this\\ndataset, we anticipate our model to contain some\\nbiases (racial, gender, etc.). During our manual\\ninspection, we saw some such biases, for example,\\n5% of errors are caused by misrecognition of gen-\\nder. However, there are other many other forms of\\nbiases that we cannot fully enumerate or observeexplicitly.\\nReferences\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\\nand Devi Parikh. 2015. Vqa: Visual question an-\\nswering. In Proceedings of the IEEE international\\nconference on computer vision , pages 2425–2433.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\nGeorge van den Driessche, Jean-Baptiste Lespiau,\\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\\ning language models by retrieving from trillions of\\ntokens. arXiv preprint arXiv:2112.04426 .\\nJames Bradbury, Roy Frostig, Peter Hawkins,\\nMatthew James Johnson, Chris Leary, Dougal\\nMaclaurin, George Necula, Adam Paszke, Jake\\nVanderPlas, Skye Wanderman-Milne, and Qiao\\nZhang. 2018. JAX: composable transformations of\\nPython+NumPy programs.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. Advances in neural information processing\\nsystems , 33:1877–1901.\\nYingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\\nhong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.\\nWebqa: Multihop and multimodal qa. The Confer-\\nence on Computer Vision and Pattern Recognition .\\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\\nscale image-text pre-training to recognize long-tail\\nvisual concepts. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recog-\\nnition , pages 3558–3568.\\nWenhu Chen, Pat Verga, Michiel de Jong, John Wi-\\neting, and William Cohen. 2022. Augmenting\\npre-trained language models with qa-memory for\\nopen-domain question answering. arXiv preprint\\narXiv:2204.04581 .\\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\\nishna Vedantam, Saurabh Gupta, Piotr Dollár, and\\nC Lawrence Zitnick. 2015. Microsoft coco captions:\\nData collection and evaluation server. arXiv preprint\\narXiv:1504.00325 .\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\\nJingjing Liu. 2020. Uniter: Universal image-text\\nrepresentation learning. In European conference on\\ncomputer vision , pages 104–120. Springer.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, et al. 2022. Palm: Scaling\\nlanguage modeling with pathways. arXiv preprint\\narXiv:2204.02311 .\\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\\nald, Fei Sha, and William Cohen. 2022. Mention\\nmemory: incorporating textual knowledge into trans-\\nformers through entity mention attention. ICLR .\\nAlexey Dosovitskiy, Lucas Beyer, Alexander\\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias\\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\\nAn image is worth 16x16 words: Transformers\\nfor image recognition at scale. In International\\nConference on Learning Representations .\\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\\nDavid Simcha, Felix Chern, and Sanjiv Kumar. 2020.\\nAccelerating large-scale inference with anisotropic\\nvector quantization. In International Conference on\\nMachine Learning , pages 3887–3896. PMLR.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\\nsupat, and Mingwei Chang. 2020. Retrieval aug-\\nmented language model pre-training. In Proceed-\\nings of the 37th International Conference on Ma-\\nchine Learning , volume 119 of Proceedings of Ma-\\nchine Learning Research , pages 3929–3938. PMLR.\\nDarryl Hannan, Akshay Jain, and Mohit Bansal. 2020.\\nManymodalqa: Modality disambiguation and qa\\nover diverse inputs. In Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence , volume 34, pages\\n7879–7886.\\nGautier Izacard and Édouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open\\ndomain question answering. In Proceedings of the\\n16th Conference of the European Chapter of the As-\\nsociation for Computational Linguistics: Main Vol-\\nume, pages 874–880.\\nNorman P Jouppi, Doe Hyun Yoon, George Kurian,\\nSheng Li, Nishant Patil, James Laudon, Cliff Young,\\nand David Patterson. 2020. A domain-speciﬁc\\nsupercomputer for training deep neural networks.\\nCommunications of the ACM , 63(7):67–78.\\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\\nsemantic alignments for generating image descrip-\\ntions. In Proceedings of the IEEE conference\\non computer vision and pattern recognition , pages\\n3128–3137.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\\nZettlemoyer, and Mike Lewis. 2019. Generalization\\nthrough memorization: Nearest neighbor language\\nmodels. In International Conference on Learning\\nRepresentations .\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, et al. 2020. Retrieval-augmented generation\\nfor knowledge-intensive nlp tasks. Advances in Neu-\\nral Information Processing Systems , 33:9459–9474.Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale\\nMinervini, Heinrich Küttler, Aleksandra Piktus, Pon-\\ntus Stenetorp, and Sebastian Riedel. 2021. Paq: 65\\nmillion probably-asked questions and what you can\\ndo with them. Transactions of the Association for\\nComputational Linguistics , 9:1098–1115.\\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\\nLi Dong, Furu Wei, et al. 2020. Oscar: Object-\\nsemantics aligned pre-training for vision-language\\ntasks. In European Conference on Computer Vision ,\\npages 121–137. Springer.\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. 2014. Microsoft coco:\\nCommon objects in context. In European confer-\\nence on computer vision , pages 740–755. Springer.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\\nolinguistic representations for vision-and-language\\ntasks. Advances in neural information processing\\nsystems , 32.\\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual\\nquestion answering benchmark requiring external\\nknowledge. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recogni-\\ntion, pages 3195–3204.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\\net al. 2021. Learning transferable visual models\\nfrom natural language supervision. In International\\nConference on Machine Learning , pages 8748–8763.\\nPMLR.\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\\nthe limits of transfer learning with a uniﬁed text-to-\\ntext transformer. Journal of Machine Learning Re-\\nsearch , 21(140):1–67.\\nRevanth Gangi Reddy, Xilin Rui, Manling Li, Xudong\\nLin, Haoyang Wen, Jaemin Cho, Lifu Huang, Mo-\\nhit Bansal, Avirup Sil, Shih-Fu Chang, et al. 2021.\\nMumuqa: Multimedia multi-hop news question an-\\nswering via cross-media knowledge extraction and\\ngrounding. arXiv preprint arXiv:2112.10728 .\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\\nSun. 2015. Faster r-cnn: Towards real-time object\\ndetection with region proposal networks. Advances\\nin neural information processing systems , 28.\\nAdam Roberts, Hyung Won Chung, Anselm Lev-\\nskaya, Gaurav Mishra, James Bradbury, Daniel An-\\ndor, Sharan Narang, Brian Lester, Colin Gaffney,\\nAfroz Mohiuddin, et al. 2022. Scaling up mod-\\nels and data with t5x and seqio. arXiv preprint\\narXiv:2203.17189 .Christoph Schuhmann, Richard Vencu, Romain Beau-\\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\\nsuzaki. 2021. Laion-400m: Open dataset of clip-\\nﬁltered 400 million image-text pairs. arXiv preprint\\narXiv:2111.02114 .\\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\\nRadu Soricut. 2018. Conceptual captions: A\\ncleaned, hypernymed, image alt-text dataset for au-\\ntomatic image captioning. In Proceedings of the\\n56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers) , pages\\n2556–2565.\\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\\nAdaptive learning rates with sublinear memory cost.\\nInInternational Conference on Machine Learning ,\\npages 4596–4604. PMLR.\\nHrituraj Singh, Anshul Nasery, Denil Mehta, Aish-\\nwarya Agarwal, Jatin Lamba, and Balaji Vasan Srini-\\nvasan. 2021. Mimoqa: Multimodal input multi-\\nmodal output question answering. In Proceedings\\nof the 2021 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies , pages 5317–5332.\\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\\nnaneh Hajishirzi, and Jonathan Berant. 2021. Multi-\\nmodalqa: complex question answering over text, ta-\\nbles and images. In ICLR .\\nPat Verga, Haitian Sun, Livio Baldini Soares, and\\nWilliam Weston Cohen. 2021. Adaptable and inter-\\npretable neural memory over symbolic knowledge.\\nInProceedings of NAACL-HLT , pages 3678–3691.\\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\\nlia Tsvetkov, and Yuan Cao. 2022. Simvlm: Simple\\nvisual language model pretraining with weak super-\\nvision. ICLR .\\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\\nCoca: Contrastive captioners are image-text founda-\\ntion models. arXiv preprint arXiv:2205.01917 .\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text gener-\\nation. Advances in Neural Information Processing\\nSystems , 34.\\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\\nfeng Gao. 2021. Vinvl: Revisiting visual representa-\\ntions in vision-language models. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and\\nPattern Recognition , pages 5579–5588.\\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\\nHu, Jason Corso, and Jianfeng Gao. 2020. Uni-\\nﬁed vision-language pre-training for image caption-\\ning and vqa. In Proceedings of the AAAI Conference\\non Artiﬁcial Intelligence , volume 34, pages 13041–\\n13049.A Pre-training\\nDuring Pre-trainnig, we found that directly train-\\ning with a mixture of all four datasets will lead to\\ninstability. We experimented with different vari-\\nants and found that a scheduled pre-training can\\nlead to a stable solution. We propose to ﬁrst pre-\\ntrain the model on the largest LAION dataset for\\n1M steps, and then continue training on the other\\nthree datasets with a ﬁxed sample ratio. We plot\\nthe ﬁrst stage of LAION training in Figure 8. We\\nmonitor the generation quality (LAION image ->\\ntext captioning), and the retrieval quality (image ->\\n4096 in-batch caption retrieval). As can be seen,\\nthe LAION pre-training converges after 1M steps,\\nwhere we ﬁrst warm up and then decrease the learn-\\ning rate using a scheduler.\\nFigure 8: LAION Pre-training, validation accuracy,\\ngeneration Cider score and retrieval recall score from\\nthe in-batch memory.\\nWe further the pre-training on a mixture of the\\nother three datasets. We plot their inference eval-\\nuation scores in Figure 9. We can see that the\\nmodel is able to achieve very strong performance\\non these datasets, i.e. higher than 1.2 CiDEron CC12M+3M validation set. The model also\\nachieves strong performance on text-only reading\\ncomprehension on PAQ (similar to NQ), i.e. higher\\nthan 55% EM score. On the VQA dataset, the\\nmodel is able to achieve higher than 72% VQA ac-\\ncuracy on the validation set. These results demon-\\nstrate the efﬁciency and multi-tasking capabilities\\nof the pre-trained model. The overall retrieval\\naccuracy from the multimodal memory consist-\\ning of captions, and passages are plotted in Fig-\\nure 10, where the model is able to achieve 85%\\nRECALL@1 from a 4K memory.\\nB Model Conﬁguration\\nWe demonstrate the ViT conﬁguration as follows:\\n\" v i t _ c o n f i g \" : {\\n\" model \" : \" ViT \" ,\\n\" p a t c h e s \" : {\\n\" s i z e \" : [ 1 6 , 16]\\n} ,\\n\" h i d d e n _ s i z e \" : 1024 ,\\n\" i m a g e _ s i z e \" : [ 2 2 4 , 2 2 4 ] ,\\n\" num_heads \" : 16 ,\\n\" num_layers \" : 24 ,\\n\" mlp_dim \" : 4096 ,\\n\" r e t u r n _ p o o l e d _ o u t p u t \" : f a l s e ,\\n\" d r o p o u t _ r a t e \" : 0 . 1\\n} ,\\nWe demonstrate the T5-EncDec conﬁguration as\\nfollows:\\n\" m o d e l _ c o n f i g \" : {\\n\" v o c a b _ s i z e \" : 32128 ,\\n\" h i d d e n _ s i z e \" : 768 ,\\n\" i n t e r m e d i a t e _ d i m \" : 2048 ,\\n\" n u m _ a t t e n t i o n _ h e a d s \" : 12 ,\\n\" memory_key_dim \" : 768 ,\\n\" e n c o d e r _ l a y e r s \" : 12 ,\\n\" d e c o d e r _ l a y e r s \" : 12 ,\\n\" d r o p o u t _ r a t e \" : 0 . 1 ,\\n\" m a x _ d i s t a n c e \" : 128 ,\\n\" num_buckets \" : 32 ,\\n\" s c a l e \" : 1 . 0 ,\\n\" r e t r i e v a l _ w e i g h t \" : 0 . 5 ,\\n}Figure 9: Mixture Pre-training, CiDEr, EM, and VQA\\naccuracy for CC, PAQ, and VQA datasets.\\nFigure 10: Mixture Pre-training retrieval accuracy over\\nCC, PAQ, and VQA datasets.',\n",
       " \"RETRIEVAL-GENERATION SYNERGY AUGMENTED LARGE LANGUAGE MODELS\\nZhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, Bing Qin\\nHarbin Institute of Technology, China\\nABSTRACT\\nLarge language models augmented with task-relevant docu-\\nments have demonstrated impressive performance on knowl-\\nedgeintensive tasks. However, regarding how to obtain effec-\\ntive documents, the existing methods are mainly divided into\\ntwo categories. One is to retrieve from an external knowledge\\nbase, and the other is to utilize large language models to gen-\\nerate documents. We propose an iterative retrieval-generation\\ncollaborative framework. It is not only able to leverage both\\nparametric and non-parametric knowledge, but also helps to\\nfind the correct reasoning path through retrieval-generation\\ninteractions, which is very important for tasks that require\\nmulti-step reasoning. We conduct experiments on four ques-\\ntion answering datasets, including single-hop QA and multi-\\nhop QA tasks. Empirical results show that our method signif-\\nicantly improves the reasoning ability of large language mod-\\nels and outperforms previous baselines.\\nIndex Terms —large language models, retrieval aug-\\nmented, question answering\\n1. INTRODUCTION\\nLarge Language models (LLMs) have demonstrated im-\\npressive performance on diverse language tasks through in-\\ncontext learning [1, 2, 3, 4, 5, 6]. However, they still struggle\\nwith knowledge-intensive tasks that require access to a large\\namount of knowledge, such as open-domain question answer-\\ning [7] and commonsense reasoning [8], since the implicit\\nknowledge preserved in the parameters may be partial and\\ninsufficient. As shown in the top of Figure 1, one promising\\ndirection is to incorporate non-parametric knowledge to help\\nalleviate this problem with large language models.\\nRecent research shows that retrieving relevant documents\\nfrom an external datastore [9, 10, 11] or directly generating\\ncontextual documents from LLMs [12, 13] both can improve\\nLLMs’ performance on knowledge-intensive tasks. The for-\\nmer, called retrieve-then-read, requires a retriever to retrieve\\nrelevant documents. The latter, known as generate-then-read,\\nleverages large language models to generate relevant docu-\\nments before answering questions. However, as shown in\\nFigure 1, the above two methods are isolated and lack co-\\nordination with each other. To fill this gap, in this paper, we\\nexplore an effective retrieval-generation collaboration frame-\\nWho is the lead singer of Depeche Mode?Depeche Mode currently consists of Dave Gahan (lead vocals, co-songwriting) and Martin Gore (keyboards, guitar, co-lead vocals, primary songwriting)…LLMsQuestionDocumentDave GahanAnswer\\nQuestionLLMsRetrieverLLMsRetrieverDocumentQuestion\\nDocumentQuestion\\nDocument(1) Retrieval(2) Generation(3) Retrieval-Generation SynergyFig. 1 : The top is the standard method utilizing LLMs for\\nquestion answering with relevant documents. The bottom\\nshows three methods to generate relevant documents.\\nwork to further improve the ability of large language models\\nto solve knowledge-intensive tasks.\\nIn this work, we present ITRG, an ITerative Retrieval-\\nGeneration synergy framework to generate relevant doc-\\numents that simultaneously exploits parametric and non-\\nparametric knowledge. In each iteration, ITRG consists of\\ntwo important steps: generation augmented retrieval (GAR)\\nand retrieval augmented generation (RAG). In the GAR step,\\nwe propose a simple and effective method to expand queries\\nby concatenating pseudo-documents generated from large\\nlanguage models and original questions. And expanded\\nqueries improve the accuracy of retrieving relevant docu-\\nments. In the RAG step, we use large language models to\\ncomprehensively understand retrieved documents to generate\\nnew documents for answering questions. We repeat these\\nsteps until we reach the maximum allowed number of itera-\\ntions. Through multiple retrieval generation collaborations,\\nour method aids in discovering the appropriate reasoning path\\nand providing correct answers to questions.\\nWe evaluate the efficacy of our method on 4 question\\nanswering datasets, including Natural Questions, TriviaQA,\\n2WikiMultiHopQA, and HotpotQA. Experimental results\\nshow that our method performs better than previous baselines\\non all datasets. In summary, our main contributions can be\\nsummarized as follows: (1) We propose ITRG, an iterative\\nretrieval-generation synergy framework using both paramet-\\nric and non-parametric knowledge. (2) We propose a simple\\nand effective generation-augmented retrieval strategy and\\ntwo retrieval-augmented generation strategies. (3) Empiri-\\ncal results show that ITRG outperforms previous retrieval-\\naugmented methods.arXiv:2310.05149v1  [cs.CL]  8 Oct 20232. ITERATIVE RETRIEVAL-GENERATION\\nSYNERGY\\nIn this section, we first introduce the overall framework, and\\nthen introduce the retrieval-generation collaboration frame-\\nwork in detail, including generation augmented retrieval and\\nretrieval augmented generation.\\n2.1. Overview\\nWe show the framework of ITRG in Figure 2. Given a user\\nquestion qand a document corpus D={di}|D|\\ni=1(i.e,diis a\\nWikipedia paragraph.), ITRG repeats generation augmented\\nretrieval (GAR) and retrieval augmented generation (RAG)\\nforTiterations. In the GAR process of iteration t, we con-\\ncatenate the output yt−1of the last iteration and question qto\\nform a new query, and then use a dense retriever to retrieve\\ntop-kparagraphs. In the first iteration, we only use the ques-\\ntion as the query. In the RAG process of iteration t, based on\\nthe question qand the retrieved top- kparagraphs, we exploit\\nlarge language models to generate new paragraphs to answer\\nquestions. Specifically, we propose two methods to generate\\nnew paragraphs, which will be introduced in detail in §2.3.\\n2.2. Generation Augmented Retrieval\\nKnowledge-intensive tasks (e.g., open-domain question an-\\nswering) often require access to additional documents. A\\ncommon approach is to directly employ the question as the\\nquery, and then equip a sparse or dense retriever to retrieve\\nrelevant documents. In practice, we find that in some cases\\nusing the question directly as the query fails to retrieve rel-\\nevant documents because there may exist semantic gaps be-\\ntween them. To alleviate this problem, we propose a simple\\nquery expansion method. At the first iteration ( t= 1), we use\\nthe original question qas the query. At iteration t(t >1), we\\nconcatenate the original question qand the document gener-\\natedyt−1in the last iteration as the new query qt= [q;yt−1].\\nThen, we utilize a pre-trained dense retriever to retrieve top- k\\ndocuments, which are denoted as Rt={d}.\\nGiven an input question q, the retriever aims to retrieve\\na small set of documents from a corpus D={di}|D|\\ni=1that\\nare relevant to q. Following prior work [14], we use a dense\\nretriever based on the dual encoder architecture, where an en-\\ncoder is used to encode both the input context qand the docu-\\nmentd. Specifically, the encoder maps each document d∈ D\\nto an embedding E(d)by taking the mean pooling of the last\\nhidden representation over the tokens in d. At query time,\\nthe same encoder is applied to the input context qto obtain a\\nquery embedding E(q). The similarity between the query em-\\nbedding and the document embedding is computed by their\\ncosine similarity: s(d, q) = cos( E(d),E(q)). The top- kdoc-\\numents that have the highest similarity scores are retrieved.\\nQuestion: What is the date of birth of Emilie HeghArntzen'smother?Retrieval:Generation:Retrieval:Generation:Retrieval:Generation:Iteration 1Iteration 2\\nIteration 3infoboxname: Emilie HeghArntzen; caption: HeghArntzenin 2018 ; birth_date: January 1, 1994 ; birth_place: Skien, Norway ; nationality: Norwegian ; Emilie HeghArntzenwas born on January 1, 1994 in Skien, Norway. Her mother is unknown.Camilla Marie Gjersemwas born together with a twin sister, Anne Line, on 6 January 1994 in Hønefoss, Norway. Their mother, PerlinaBangug, is a Filipina from Ilagan, Isabela, and their father, PetterGjersem, a Norwegian from Raufoss. Camilla Gjersemis a law student at the University of Oslo.Hanne Hegh(born 19 January 1960) is a Norwegian handball player. She played 220 matches for the Norwegian national handball team between 1978 and 1992. She is the mother of Emilie HeghArntzen.infoboxname: Hanne Hegh; caption: Hanne Hegh2008 ; nationality: Norwegian ; birth_date: April 27, 1960; birth_place: Oslo, Norway ; Hanne Heghwas born on April 27, 1960 in Oslo, Norway. She is the mother of Emilie HeghArntzen, who was born on January 1, 1994 in Skien, Norway.Fig. 2 : Iterative retrieval-generation synergy framework con-\\ntains two steps in each iteration: (1) generation augmented\\nretrieval (GAR): utilize the output of the previous iteration to\\nexpand the query to help retrieve more relevant documents;\\n(2) retrieval augmented generation (RAG): utilize retrieved\\ndocuments to generate new documents to answer questions.\\nWe only show three iterations in this figure for brevity. Solid\\narrows indicate RAG within an iteration, and dashed arrows\\nindicate GAR between iterations. Purple represents correct\\nand useful information, and red represents wrong or invalid\\ninformation.\\n2.3. Retrieval Augmented Generation\\nFollowing previous work [13], for a given question q, we\\ncould directly prompt large language models to generate re-\\nlated documents without retrieving them from an external cor-\\npus. However, we find that if only the parametric knowledge\\nlearned by the large model in the pre-training stage is used,\\nthe generated documents may be incomplete. Retrieval aug-\\nmented generation (RAG) aims to comprehensively under-\\nstand the retrieved non-parametric knowledge and the para-\\nmetric knowledge inside large language models to generate\\nmore accurate factual knowledge. Specifically, we propose\\ntwo strategies, which will be described in detail below.\\n2.3.1. Refine\\nAn intuitive idea is to refine the previously generated docu-\\nment yt−1based on the original question qand the retrieved\\ntop-kdocuments at the current iteration step Rtto obtain a\\nnew document yt. We call this method refine. Considering\\nthat the document retrieved in the last iteration Rt−1has been\\nused to generate the last document yt−1, we refine the previ-\\nous output yt−1with updated documents Rupdate .\\nRupdate =Rt−Rt−1, (1)\\nyt=M(prompt ( yt−1, q, R update )), (2)where Rupdate means that these documents are only retrieved\\nin the current iteration, not in the last iteration, Mdenotes a\\nwell pre-trained large language model. If Rupdate is an empty\\nset, we do not regenerate a new document and set yt=yt−1.\\n2.3.2. Refresh\\nIn order to avoid the negative effect of errors or hallucinations\\nin the previously generated document yt−1, we do not use\\nyt−1, which is used in refine. We refresh the memory and let\\nthe large language models directly generate the document yt\\nbased on the retrieved document Rtand the original question\\nq. This method is named refresh.\\nyt=M(prompt ( q, R t)) (3)\\nBoth refine and refresh are implemented through prompts.\\nWe give the prompt corresponding to refresh.\\nPrompt for refresh with all documents\\nIn the following task, you should write a document\\nthat contains the answer to the question.\\nPassage: {Rt}\\nQuestion: {q}\\nDocument: {yt}\\n3. EXPERIMENTAL SETUP\\n3.1. Datasets\\nWe evaluate the effectiveness of ITRG on four open domain\\nquestion answering datasets, including Natural Questions\\n(NQ) [15], TriviaQA [16], 2WikiMultiHopQA [17] and Hot-\\npotQA [18]. Following previous works [19, 20], we randomly\\nsub-sample 500 examples from each dataset due to the cost\\nof running experiments. We evaluate our method in 0-shot,\\n1-shot and 5-shot settings. The few-shot demonstrations are\\nrandomly sampled from the data that is not involved in the\\nevaluation process.\\n3.2. Baselines\\nGPT-3.5 [21] We use text-davinci-002 and text-davinci-003\\nas our baselines. Text-davinci-002 is an InstructGPT model\\nwhile Text-davinci-003 is trained with reinforcement learn-\\ning with reward models trained from comparisons by humans.\\nVanilla LM The vanilla LM baselines prompt an LLM to di-\\nrectly generate an answer following the few-shot in-context\\nlearning paradigm [1]. CoT We follow [22] to generate both\\nthe chain-of-thought (CoT) reasoning process and the final\\nanswer. We only evaluate this method on multi-hop reasoning\\ndatasets in 5-shot setting1.Retrieve-then-Read The retrieve-\\n1We also conduct evaluation in 1-shot setting, but the final answer could\\nnot be generated according to the corresponding instructionsthen-read baseline consists of a well-pre-trained dense re-\\ntriever and a large language model. The retriever retrieves\\nrelevant documents for the question, and then the LLM con-\\nditions on both the question and retrieved documents to gen-\\nerate the answer. Generate-then-Read Generate-then-read\\nbaseline first uses few-shot prompts to generate a question-\\nrelated document, and then concatenates it with the question\\nto regenerate the answer.\\n3.3. Details\\nLLaMA [6] is an open source well trained large language\\nmodel. Considering the performance and computational cost\\nof the model, we use LLaMA 33B as the backend LLM. We\\nuse greedy decoding for both document generation and an-\\nswer generation, and set up to generate 200 tokens and 15\\ntokens respectively. We retrieve the top-5 paragraphs for each\\nquery and set the maximum number of iterations Tto 5. We\\ndirectly use the pre-trained dense retriever [23] and used the\\nDecember 2018 Wikipedia dump as the retrieval corpus for all\\ndatasets. Generated answers are evaluated with the standard\\nexact match metric (EM score): a generated answer is con-\\nsidered correct if it matches any answer of the list of answers\\nafter normalization. For this normalization step, we lower-\\ncase generated answers and remove articles, punctuation and\\nduplicate whitespaces.\\n4. RESULTS\\n4.1. Main Results\\nTable 1 reports the results on the single-hop question answer-\\ning datasets. In the 1-shot and 5-shot settings, the perfor-\\nmance of LLaMA-33B based Vanilla LM is very close to\\nthat of text-davinci-003. This shows LLaMA-33B is a strong\\nlanguage model, and it is reasonable to choose LLaMA-33B\\nas our backend LLM. Retrieve-then-read and generate-then-\\nread all exceed vanilla LM, verifying that adding relevant\\nexternal knowledge can improve the reasoning ability of\\nlarge language models. In addition, we observe that our itera-\\ntive retrieval-generation collaborative method ITRG achieves\\nstate-of-the-art performance on both datasets. Specifically,\\nITRG (refresh) performs better on the NQ dataset, and ITRG\\n(refine) performs better on the TriviaQA dataset.\\nTable 2 presents the results on the multi-hop question an-\\nswering datasets. We observe that LLaMA-33B is still com-\\nparable to text-davinci-003 on the multi-hop question answer-\\ning datasets. In addition, CoT can answer questions more\\naccurately than vanilla LM by generating reasoning process.\\nCompared with different baseline models, ITRG significantly\\nimproves the exact match scores. Specifically, on the 2Wiki-\\nMultiHopQA dataset, the exact match score of ITRG (refresh)\\nin the zero-shot setting is 32.2, which exceeds the perfor-\\nmance of vanilla LM in the 5-shot setting with a score of 31.8.\\nIn the 5-shot setting, ITRG (refresh) achieves 38.6 EM scoreTable 1 : Exact match performance on single-hop question answering. All ITRG results are from the last iteration ( T= 5).\\nMethodNatural Questions TriviaQA\\n0-shot 1-shot 5-shot 0-shot 1-shot 5-shot\\nGPT 3.5Text-davinci-002 12.0 24.6 33.0 46.0 74.2 76.0\\nText-davinci-003 29.4 33.0 33.8 75.8 78.6 77.8\\nLLaMA 33BVanilla LM 27.0 29.4 32.4 74.8 70.8 75.8\\nRetrieve-then-Read 27.8 30.6 29.8 74.6 76.0 76.0\\nGenerate-then-Read 28.0 31.4 31.0 73.6 77.2 77.6\\nITRG (refine) 34.4 34.6 34.8 79.0 79.4 80.6\\nITRG (refresh) 37.6 38.4 38.0 77.0 78.6 79.4\\nTable 2 : Exact match performance on multi-hop question answering. All ITRG results are from the last iteration ( T= 5).\\nMethod2WikiMultiHopQA HotpotQA\\n0-shot 1-shot 5-shot 0-shot 1-shot 5-shot\\nGPT 3.5Text-davinci-002 16.4 27.6 30.8 12.2 20.2 22.2\\nText-davinci-003 27.2 27.0 29.8 25.0 25.8 26.6\\nLLaMA 33BVanilla LM 24.4 27.6 31.8 22.6 25.0 27.0\\nCOT - - 32.2 - - 28.6\\nRetrieve-then-Read 27.4 29.2 32.0 28.4 29.8 30.4\\nGenerate-then-Read 30.0 30.4 31.6 25.0 27.0 27.0\\nITRG (refine) 33.0 33.6 37.0 28.8 29.6 30.6\\nITRG (refresh) 32.2 36.2 38.6 31.0 32.6 33.4\\nTable 3 : Exact match performance of ITRG (refresh) at dif-\\nferent iterations in 5-shot setting.\\nIteration 1 2 3 4 5\\nNatural Questions 34.0 35.2 37.0 37.2 38.0\\nTriviaQA 79.8 79.2 79.8 79.8 79.4\\n2WikiMultiHopQA 34.8 37.4 37.2 38.6 38.6\\nHotpotQA 32.6 32.8 34.0 33.4 33.4\\nand improves by 6.8 points in absolute gains. Compared to\\nvanilla LM, ITRG (refresh) can improve the EM score by 9.4,\\n7.6, and 6.4 points respectively in 0-shot, 1-shot, and 5-shot\\nsettings on the Hotpotqa dataset.\\n4.2. Performance at Different Iterations\\nIn this section, we analyze the performance of our model and\\nthe quality of the generated documents during the iteration\\nprocess. Specifically, we present the results of ITRG (refresh)\\nat different iterations in 5-shot setting in Table 3. We measure\\nthe answer recall of generated documents at different itera-\\ntion steps and present results in Table 4. Table 3 shows that\\nthe performance of the model gradually improves with iter-\\nation. And Table 4 shows that the quality of the generated\\ndocuments also gradually improves with iteration. These re-\\nsults verify that our iterative retrieval-generation collaborativeTable 4 : Answer recall of generated documents at different\\niterations with ITRG (refresh).\\nIteration 1 2 3 4 5\\nNatural Questions 44.0 46.4 48.4 48.8 48.0\\nTriviaQA 18.8 19.0 20.2 19.2 19.2\\n2WikiMultiHopQA 34.2 36.6 35.0 40.0 37.0\\nHotpotQA 34.2 34.8 35.6 33.8 33.6\\nframework is effective and can further enhance the reasoning\\ncapabilities of large language models.\\n5. CONCLUSION\\nIn this paper, we present ITRG, which is an iterative retrieval-\\ngeneration synergy framework, containing two important\\nsteps: generation-augmented retrieval and retrieval-augmented\\ngeneration. They form a closed loop, and can improve\\neach other via multiple iterations. We propose a simple\\nand effective generation-augmented retrieval strategy and\\ntwo retrieval-augmented generation strategies. Empirical re-\\nsults show our approach significantly exceeds several strong\\nbaselines, including GPT 3.5, on four open domain ques-\\ntion answering datasets, which indicates that our method can\\nsignificantly improve the reasoning ability of large language\\nmodels.6. REFERENCES\\n[1] T. Brown et al. , “Language models are few-shot learners,” Ad-\\nvances in neural information processing systems , vol. 33, pp.\\n1877–1901, 2020.\\n[2] J. Hoffmann et al. , “Training compute-optimal large language\\nmodels,” 2022.\\n[3] A. Zeng et al. , “Glm-130b: An open bilingual pre-trained\\nmodel,” arXiv preprint arXiv:2210.02414 , 2022.\\n[4] A. Chowdhery et al. , “Palm: Scaling language modeling with\\npathways,” arXiv preprint arXiv:2204.02311 , 2022.\\n[5] OpenAI, “Gpt-4 technical report,” 2023.\\n[6] H. Touvron et al. , “Llama: Open and efficient foundation lan-\\nguage models,” 2023.\\n[7] K. Lee, M.-W. Chang, and K. Toutanova, “Latent retrieval\\nfor weakly supervised open domain question answering,” in\\nProceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics . Florence, Italy: Association\\nfor Computational Linguistics, Jul. 2019, pp. 6086–6096.\\n[Online]. Available: https://aclanthology.org/P19-1612\\n[8] R. Zellers, Y . Bisk, R. Schwartz, and Y . Choi, “SWAG:\\nA large-scale adversarial dataset for grounded common-\\nsense inference,” in Proceedings of the 2018 Conference\\non Empirical Methods in Natural Language Processing .\\nBrussels, Belgium: Association for Computational Lin-\\nguistics, Oct.-Nov. 2018, pp. 93–104. [Online]. Available:\\nhttps://www.aclweb.org/anthology/D18-1009\\n[9] O. Ram et al. , “In-context retrieval-augmented language mod-\\nels,” arXiv preprint arXiv:2302.00083 , 2023.\\n[10] O. Khattab et al. , “Demonstrate-search-predict: Composing\\nretrieval and language models for knowledge-intensive nlp,”\\n2023.\\n[11] W. Shi et al. , “Replug: Retrieval-augmented black-box lan-\\nguage models,” arXiv preprint arXiv:2301.12652 , 2023.\\n[12] W. Yu et al. , “Generate rather than retrieve: Large language\\nmodels are strong context generators,” 2023.\\n[13] Z. Sun, X. Wang, Y . Tay, Y . Yang, and D. Zhou, “Recitation-\\naugmented language models,” 2023.\\n[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the\\n28th International Conference on Computational Linguistics .Barcelona, Spain (Online): International Committee on Com-\\nputational Linguistics, Dec. 2020, pp. 6609–6625. [Online].\\nAvailable: https://aclanthology.org/2020.coling-main.580\\n[18] Z. Yang et al. , “HotpotQA: A dataset for diverse, explainable\\nmulti-hop question answering,” in Proceedings of the 2018\\nConference on Empirical Methods in Natural Language\\nProcessing . Brussels, Belgium: Association for Computa-\\ntional Linguistics, Oct.-Nov. 2018, pp. 2369–2380. [Online].\\nAvailable: https://aclanthology.org/D18-1259\\n[19] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabhar-\\nwal, “Interleaving retrieval with chain-of-thought reasoning\\nfor knowledge-intensive multi-step questions,” arXiv preprint\\narXiv:2212.10509 , 2022.\\n[20] Z. Jiang et al. , “Active retrieval augmented generation,” arXiv\\npreprint arXiv:2305.06983 , 2023.\\n[21] L. Ouyang et al. , “Training language models to follow instruc-\\ntions with human feedback,” Advances in Neural Information\\nProcessing Systems , vol. 35, pp. 27 730–27 744, 2022.\\n[22] J. Wei et al. , “Chain of thought prompting elicits reasoning\\nin large language models,” arXiv preprint arXiv:2201.11903 ,\\n2022.\\n[23] G. Izacard et al. , “Few-shot learning with retrieval augmented\\nlanguage models,” arXiv preprint arXiv:2208.03299 , 2022.\",\n",
       " 'Lift Yourself Up: Retrieval-augmented Text\\nGeneration with Self-Memory\\nXin Cheng1Di Luo2Xiuying Chen3Lemao Liu4Dongyan Zhao1Rui Yan2\\n1Peking University2Remin University of China\\n3KAUST4Tencent AI Lab\\nchengxin1998@stu.pku.edu.cn\\nAbstract\\nWith direct access to human-written reference as memory, retrieval-augmented\\ngeneration has achieved much progress in a wide range of text generation tasks.\\nSince better memory would typically prompt better generation (we deﬁne this as\\nprimal problem ). The traditional approach for memory retrieval involves selecting\\nmemory that exhibits the highest similarity to the input. However, this method\\nis constrained by the quality of the ﬁxed corpus from which memory is retrieved.\\nIn this paper, by exploring the duality of the primal problem: better generation\\nalso prompts better memory, we propose a novel framework, Selfmem , which\\naddresses this limitation by iteratively employing a retrieval-augmented generator\\nto create an unbounded memory pool and using a memory selector to choose one\\noutput as memory for the subsequent generation round. This enables the model\\nto leverage its own output, referred to as self-memory, for improved generation.\\nWe evaluate the effectiveness of Selfmem on three distinct text generation tasks:\\nneural machine translation, abstractive text summarization, and dialogue generation,\\nunder two generation paradigms: ﬁne-tuned small model and few-shot LLM. Our\\napproach achieves state-of-the-art results in four directions in JRC-Acquis ,XSum\\n(50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1), demonstrating the potential\\nof self-memory in enhancing retrieval-augmented generation models. Furthermore,\\nwe conduct thorough analyses of each component in the Selfmem framework to\\nidentify bottlenecks and provide insights for future research.\\n1 Introduction\\nIn recent years, retrieval-augmented text generation has garnered increasing interest across vari-\\nous ﬁelds such as neural machine translation [ 1,22,14], dialogue response generation [ 71,6,42],\\nlanguage modeling [ 21,31,90], and others. This novel generation paradigm initially equips the gener-\\nation model, either a ﬁne-tuned small model or a large language model (LLM) with in-context learning\\ncapability [ 5], with access to an external database (typically the training corpus) via information\\nretrieval techniques [63, 33] and then generates text based on the retrieved memory.\\nIn this paradigm, the guiding principle for memory retrieval is to ﬁnd the memory that exhibits the\\nhighest similarity to the current input [ 31,84,45]. This aligns with the human intuition that a more\\nsimilar demonstration sample typically offers more hints. As demonstrated in Figure 1, for a tunable\\nretrieval-augmented translation model, the memory similarity alone exhibits a strong correlation with\\nthe ﬁnal translation quality, regardless of other factors that may inﬂuence translation quality (e.g.,\\npolysemy, morphology, and coreference). We deﬁne this as the primal problem :better memory\\nprompts better generation . Consequently, numerous studies have focused on how to retrieve better\\nPreprint. Under review.arXiv:2305.02437v2  [cs.CL]  17 May 2023memory, ranging from sparse retrieval to dense retrieval [ 10,58], from a ﬁxed retriever to a learnable\\nretriever [ 38,8], and from sentence-level memory to more ﬁne-grained token-level memory [ 31,30].\\n0.0 0.2 0.4 0.6 0.8 1.0\\nMemory Similarity0.20.30.40.50.60.70.80.9Hypothesis BLEU\\nFigure 1: Relation between memory and hy-\\npothesis on JRC-Acquis En!De dataset.\\nThe hypothesis is generated by a retrieval-\\naugmented translator whose memory is re-\\ntrieved from the training set. The X-axis\\nrepresents the similarity between memory\\nand the reference.However, a fundamental limitation exists in all previous\\nworks: the memory is retrieved from a ﬁxed corpus and\\nis constrained by the corpus’s quality. Due to the ﬁnite\\nretrieval space, bounded memory signiﬁcantly limits\\nthe potential of memory-augmented generation models.\\nIn this paper, by exploring the primal problem ’s dual-\\nity,better generation also prompts better memory ,\\nwe propose a novel framework called Selfmem . This\\nframework iteratively employs a retrieval-augmented\\ngenerator to create an unbounded memory pool and\\nuses a memory selector to choose one output as mem-\\nory for the subsequent generation round. By combining\\ntheprimal anddual problem , a retrieval-augmented\\ngeneration model can elevate itself using its own out-\\nput, referred to as self-memory. The key insight behind\\nSelfmem is that the text more closely resembling the\\ndata distribution during inference is not the training\\ndata [76], but the model’s own output.\\nSelfmem consists of two complementary components:\\na retrieval-augmented generator and a memory selector. The generator operates under two distinct\\nparadigms: ﬁne-tuning a small model or few-shot prompting a LLM. For the former, we supervise the\\ngenerator with labeled data and retrieved memory, while for the latter, we employ a ﬁxed black-box\\nLLM exclusively for inference alongside retrieved in-context learning samples. We then use the\\ngenerator’s output to train a memory selector based on a speciﬁc performance metric. By simply\\nreplacing the retrieved memory with unbounded generated memory, we achieve higher-quality\\ngeneration output ( primal problem ), which subsequently serves as memory for the next round after\\nbeing reﬁned by the memory selector ( dual problem ).\\nTo evaluate the efﬁcacy of the Selfmem , we carry out comprehensive experiments in three distinct text\\ngeneration tasks: neural machine translation, abstractive text summarization, and dialogue generation.\\nWe witness substantial enhancements over robust baselines, attaining state-of-the-art outcomes in\\nJRC-Acquis (four directions), XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1). To gain\\ndeeper insights into the Selfmem , we meticulously investigate each crucial component and pinpoint\\nthe existing system bottleneck to guide future research endeavors.\\n2 Related Work\\n2.1 Retrieval-augmented Text Generation\\nSince the world is not a snapshot once the training corpus is collected, we can never expect an\\never-large model to capture everything in its parameters, even for LLMs like GPT-4 [ 57]. Therefore,\\nit is crucial to equip these models with an external memory bank to store additional knowledge or\\nuseful demonstration examples for solving various NLP tasks[38, 68, 83].\\nIn the translation domain, retrieval techniques have long been employed by the localization industry\\nto enhance human translators’ productivity and consistency even before the advent of machine\\ntranslation [ 82]. Early works on machine translation primarily focused on utilizing memory for\\nstatistical machine translation (SMT) systems [ 70,46]. For neural machine translation (NMT), [ 22]\\nwere the ﬁrst to use search engines to retrieve memory from the training set and incorporate it with\\nan external memory network. Subsequent research explored various aspects of retrieval-augmented\\nNMT, such as memory encoding methods [ 80,81,25], joint training of retrievers and generators\\nwith monolingual data [ 8], memory granularity [ 30], and memory diversity [ 14]. For few-shot LLM\\ngeneration, strategies for in-context example selection have been proposed to improve translation\\nquality [ 1]. Furthermore, in-context machine translation has been shown to be effective for on-the-ﬂy\\nadaptation [ 69]. For dialogue response generation tasks, employing exemplar/template retrieval as\\nan intermediate step has proven advantageous for generating informative responses [ 78,79,6,7].\\nIn-context learning example retrieval also aids in controllable dialogue [ 42]. Other applications\\n2include abstractive summarization [ 76,11,59], code generation [ 24], paraphrase generation [ 29,73],\\nlanguage modeling [ 31,90], counterfactual data generation [ 18], and question answering [ 32,28,27].\\n2.2 Neural Text Reranking\\nBy alleviating the discrepancy between training and inference (i.e., exposure bias) and directly\\noptimizing desired metrics, two-stage reranking methods have facilitated signiﬁcant progress in\\nvarious text generation tasks. In machine translation, pioneering works by [ 66] and [ 56] introduced\\nand popularized discriminative reranking for SMT. In the context of NMT, research has focused on\\ntwo primary reranking approaches: generative reranking [ 51,26,77] and discriminative reranking [ 36,\\n64,17]. For syntactic parsing, [ 15] were the ﬁrst to employ a two-stage reranking method to select\\noutputs from a base parser, while [ 12] introduced a maximum entropy reranker. In text summarization,\\nRefSum [ 48] proposed a second-stage summarization framework to address train-test distribution\\nmismatches. SimCLS [ 49] used pairwise Learning To Rank (LTR) to select candidates with the\\nhighest matching scores. SummaReranker [ 62] adopted a multi-task mixture-of-experts framework\\nto leverage different metrics capturing various aspects of generated candidates. BRIO [ 50] reused\\nthe base model for a second round of ﬁne-tuning with both cross-entropy loss and a candidate-level\\nranking loss. JGR [67] employed an alternate training paradigm to train the generator and reranker.\\nA key limitation of these reranking methods is that they represent a one-way process, wherein the\\nselected candidates become the system’s ﬁnal output. In contrast, our framework innovatively utilizes\\nthe chosen candidates as memory for the subsequent generation round of a retrieval-augmented\\ngenerator, which can produce better candidates with enhanced memory.\\n3 Methods\\nIn this section, we begin with a motivating experiment on generation as memory (§ 3.1). Then, we\\nintroduce Selfmem , a framework comprising a retrieval-augmented generator (§ 3.2) and a memory\\nselector (§ 3.3). The complete framework and algorithm are illustrated in Figure 2 and Algorithm 1.\\n3.1 Generation as Memory\\nThe primary motivation behind our framework stems from the observation that the memory, which\\nis more similar in distribution to the data during inference, is not the training data (38.89 BLEU,\\nas shown in Table 1). Instead, it is the model’s own output (58.58 BLEU) within the unbounded\\ngeneration space. One interesting exploration involves directly utilizing the generated output as\\nmemory in relation to the primal problem : better memory prompts better generation.\\nTable 1: Experiments on a\\nﬁxed retrieval-augmented trans-\\nlator with different memory.\\nMemory Hypothesis\\nRetrieval 38.89 58.58\\nBeam 58.58 58.43\\nReference 100 90.43\\nRandom 1.14 49.08We conduct experiments on the JRC-Acquis En!De dataset. The\\nﬁrst row in Table 1 represents conventional retrieval-augmented\\ntraining with retrieved memory and achieves a 58.58 BLEU score.\\nHowever, directly incorporating beam output of this trained model\\nas memory (Beam) back into the generation model does not yield\\nany improvements (row 2), despite its higher similarity to the\\nreference compared to the retrieved ones. We hypothesize two\\npotential reasons for this: (1) the retrieval-augmented generator\\nmay not generalize effectively in this context due to the memory\\ndistribution shift (from 38.89 to 58.58), and (2) the beam memory\\ndoes not offer any information gain in comparison to the retrieved\\nmemory, even when it exhibits more overlap with the references.\\nTo eliminate the ﬁrst hypothesis, we investigate the best and worst scenarios by using the reference\\nas memory (Reference) and randomly sampled sentences as memory (Random). Table 1 illustrates\\nthat a retrieval-augmented generator trained with retrieved memory has already learned to effectively\\nutilize memory information in both oracle and random scenarios, with ﬁxed parameters.\\nTo evaluate the second conjecture, we ﬁrst deﬁne the token sets of the reference, retrieved memory,\\nand beam memory as R;M, andB, respectively. The overlap token set, denoted by O, is deﬁned\\nas the tokens that overlap with the references in the beam memory but not in the retrieved memory,\\nwhich is represented as R\\\\B\\x00R\\\\M .Ois considered as the additional information provided\\n3Target Distribution\\xa0Frozen LLM / T rainable LM\\nNLL LossKL Loss\\nYN\\nY1\\n...\\nY\\n X\\n Y\\nX\\ncandidates\\nsource\\ntarget training\\nmemory\\n... ...\\n... ...\\n(a) Retrieval-augmented Generator (b) Memory SelectorRetrievalPredicted Distribution\\nM\\nPrimalDualFigure 2: Overall framework. There are two components in Selfmem , a retrieval-augmented genera-\\ntor (a) and a memory selector (b). For the primal problem, (a) takes source and memory as input to\\ngenerate candidates for (b). For the dual problem, (b) takes as input source and generated candidates\\nto select memory for (a).\\nby the beam memory. Inspired by the conﬁdence analysis of NMT model [ 53], we compute the set\\nconﬁdence score,  (\\x01), as follows:\\n (\\x01) =1\\nj\\x01jX\\nyi2\\x01p(yijx;y<i) (1)\\nwherep(yijx;y<i)is deﬁned by the generation model.  (\\x01)measures the conﬁdence with which the\\ngeneration model generates the tokens in the set. The value of  (R)is 0.58, while that of Ois 0.76,\\nindicating that the generator is relatively conﬁdent in generating tokens in O, and therefore does not\\nneed to resort to external memory [ 35]. Beam search ranks generated candidates based on p(yjx),\\nwhere the selected memory falls within the conﬁdence region of the generator and consequently\\nprovides no information gain. This observation motivates us to select memory according to metrics\\nother thanp(yjx)in the memory selector (§3.3).\\n3.2 Retrieval-augmented Generator\\nGiven a text pair (x;y), wherex=fx1;:::;xjxjgis the source, y=fy1;:::;yjyjgis the target. They\\ncould be (document, summary) in summarization, (context, response) in dialogue generation or\\n(source, target) in machine translation. The retrieval-augmented generation would ﬁrst use xto\\nretrieve memory mfrom datastore D. Then the generator G\\x18(x;m), parameterized by \\x18, would take\\nbothxandmas input to generate the target sentence y. In this paper, following standard practice,\\nwe choose the training set as D=f(xi;yi)gjDj\\ni=1. For LLM as G\\x18, we use the standard in-context\\nlearning format to give (x;y)as demonstration example. For tunable generator G\\x18, we only keep the\\ntarget side of top-1 retrieval results as memory and we consider two commonly used architectures:\\nJoint-Encoder [23, 76, 38] and Dual-Encoder [80, 8, 14].\\nJoint-Encoder This architecture is the standard encoder-decoder-based model [ 2,74]. The input is\\nthe concatenation of xandm. The encoder would ﬁrst map the input into the hidden states H:\\nH=Encoder (x[SEP]m) (2)\\nAnd the decoder would incorporate Hby attention mechanism and generate tokens in an auto-\\nregressive manner:\\nhi=Decoder (CrossAttn (H);y<i)PG\\x18(\\x01jx;y<i) =Softmax (hi) (3)\\nDual-Encoder Instead of treating xandmas a long sequence, this architecture has two encoders,\\none forxand the other for m. Their outputs are sequentially attended by the decoder with dual cross\\n4attention as in [14]:\\nHx=SourceEncoder (x)Hm=MemoryEncoder (x) (4)\\nhi=Decoder (CrossAttn (Hx;Hm);y<i) (5)\\nWe use Transformer [ 74] as the building block for both architectures and optimize G\\x18with NLL loss:\\nLnll=\\x00jyjX\\nt=1logPG\\x18(ytjx;m;y<t) (6)\\n3.3 Memory Selector\\nThe role of memory selector S\\x12(x;c), parameterized by \\x12, is to select one candidate cfrom the\\ncandidate pool Cgenerated by G\\x18based on a speciﬁc metric \\x01(\\x01;\\x01). The chosen candidate cis\\nthen utilized as memory mfor the subsequent generation round of G\\x18. As discussed in §3.1, using\\npG\\x18(yjx)as the metric \\x01(\\x01;\\x01)would result in falling into the conﬁdence region of G\\x18, leading to\\nno information gain. Moreover, a larger value of pG\\x18(yjx)does not necessarily guarantee improved\\ngeneration quality [ 54]. Consequently, we deﬁne \\x01(\\x01;\\x01)as model-free metrics that are widely\\nemployed for assessing generation quality, such as BLEU for Neural Machine Translation (NMT)\\nand ROUGE for Summarization. Our memory selector takes the concatenation of the source xand\\ncandidatecias input, and produces a multinomial distribution pS\\x12(\\x01jx)overC.\\nIn this paper, we focus on the role of the memory selector, S\\x12(x;c), which is parameterized by \\x12.\\nThe objective of this selector is to choose a single candidate cfrom the candidate pool C, generated\\nbyG\\x18, based on a speciﬁc metric, \\x01(\\x01;\\x01).\\npS\\x12(cijx) =exp(S\\x12(x[SEP]ci))\\nPjCj\\nj=1exp(S\\x12(x[SEP]cj))(7)\\nIn accordance with [ 36], the training goal for S\\x12is to minimize the discrepancy between the S\\x12’s\\npredictions and the scores determined by \\x01(\\x01;\\x01). This divergence is quantiﬁed using the Kullback-\\nLeibler (KL) divergence.\\nLkl=\\x00jCjX\\ni=1pM(ci)logpS\\x12(cijx)wherepM(ci) =exp(\\x01(ci;y)=\\x1c)\\nPjCj\\nj=1exp(\\x01(cj;y)=\\x1c)(8)\\n\\x1cis the temperature to control the smoothness of the distribution. At inference, the output of the S\\x12\\nisarg max\\nci2CpS\\x12(cijx).\\n3.4 Combine Generator and Selector\\nWe deﬁne two generation modes for G\\x18. The ﬁrst mode, referred to as the hypothesis mode , generates\\na single output for each input, which is utilized for system evaluation. The second mode, known as\\nthecandidate mode , produces N outputs for a given input, and is employed for training S\\x12as well as\\nmemory selection. By integrating two modes together, we present the complete framework of our\\nproposed model, Selfmem , as illustrated in Algorithm 1.\\n4 Experimental Setup\\n4.1 Dataset\\nWe assess the performance of Selfmem on three generation tasks, utilizing a total of seven datasets.\\nTranslation. We evaluate our framework on JRC-Acquis datasets [ 72], a collection of parallel\\nlegislative text of European Union Law. It is the benchmark dataset used in translation memory-\\naugmented NMT task [ 22,80,8,14]. We choose 4 translation directions, namely, Spanish $English\\n(Es$En), German$English (De$En).Summarization. We evaluate on 2 summarization datasets:\\n5Algorithm 1 Selfmem Framework\\nRequire: a dataset D, a retriever R, a memory selection metric \\x01(\\x01;\\x01), a retrieval-augmented\\ngeneratorG\\x18, and a memory selector S\\x12\\n1:retrieve memory MinDwithR\\n2:trainG\\x18withDandM(if not LLM)\\n3:useG\\x18to generate candidate pool CwithMin candidate mode\\n4:trainS\\x12onCwith\\x01(\\x01;\\x01)\\n5:while not converged in the validation set do\\n6:S\\x12selects memory from CasM\\n7:G\\x18generates candidate pool CwithMin candidate mode\\n8:end while\\n9:G\\x18generates the ﬁnal hypothesis with Min hypothesis mode\\n1)XSum [55], extreme summarization, a single-document summarization dataset with highly abstrac-\\ntive articles from British Broadcasting Corporation. 2) BigPatent [65], consisting of 1.3 million\\nrecords of U.S. patent documents along with human-written abstractive summaries. Dialogue. We\\nexperiment on DailyDialog [40], which contains multi-turn dialogs on daily life topics and is used\\nby [13, 3, 88]. The detailed statistics for these datasets can be found in the Appendix.\\n4.2 Implementation Details\\nWe utilize the BM25 algorithm [ 63] for retrieval purposes. For all tasks, the candidate generation\\nmethod consists of beam search with a beam width of 50. The number of iterations is determined\\nby the performance on the validation set. For translation , we follow the approach of [ 81,8,14],\\nemploying a randomly initialized Transformer basearchitecture as G\\x18for trainable small model and\\nXGLM [ 44] for LLM in-context learning. Evaluation metrics include BLEU, TER, and chrF++\\nobtained from SACRE BLEU [60]. The memory selector S\\x12utilizes an XLM-R base[16] as back-\\nbone, with BLEU serving as \\x01(\\x01;\\x01).For summarization , we initialize G\\x18with BART base[37]\\nforBigPatent and employ BRIO [ 50] for XSum . The evaluation metric comprises ROUGE (R-\\n1/2/L) [ 43].For dialogue generation , BART baseserves as the backbone for G\\x18. Our dialogue system\\nis evaluated using BLEU (B-1/2) and Distinct (D-1/2) scores [ 39]. For both dialogue and summariza-\\ntion tasks, we adhere to the methods of [ 49,20], adopting RoBERTa base[47] as the backbone for S\\x12.\\nThe linear combination of B-1/2 is chosen as \\x01(\\x01;\\x01)for Dialogue Generation, while R-1/2/L is used\\nfor Summarization, following [ 67]. For further implementation details, please refer to the Appendix.\\nOur code is open sourced1.\\n5 Experimental Results\\n5.1 Machine Translation\\nWe select four translation directions and experiment with two generation paradigms: trainable\\nsmall models and few-shot prompted LLMs. For trainable models, we explore two architectures\\n(joint and dual, as detailed in §3.2). The baselines comprise two types of translation systems: one\\nbeing the vanilla sequence-to-sequence model [ 2,74] without memory augmentation, and the other\\nconsisting of retrieval-augmented translation models focusing on memory encoding [ 22,80], memory\\nconstruction [ 86], memory retrieval [ 8], and memory diversity [ 14]. Based on the experimental\\nresults2shown in Table 2, Selfmem signiﬁcantly enhances the performance of G\\x18across four\\ntranslation datasets and two different architectures. This is noteworthy, given that the parameters of\\ntheG\\x18remain ﬁxed, with the only variable being the input memory. This ﬁnding is consistent with\\ntheprimal problem which posits that improved memory typically leads to better generation results.\\nThedual problem is revealed in Table 3. Self-memory, which essentially represents the model’s\\nown output, exhibits greater similarity with the ground truth and serves as a more effective memory\\nfor generating the ﬁnal output. This observation highlights a key distinction between Selfmem and\\n1https://github.com/hannibal046/selfmemory\\n2As higher BLEU scores in this range do not necessarily guarantee a superior translation system [ 9], we also\\nevaluate our system using TER and chrF++. The results can be found in the Appendix.\\n6Table 2: Results of translation task on JRC-Acquis measured by BLEU. Models denoted by the\\nsame symbol ( ?andy) have the same parameters and only differ in memory as input. The bolded\\nnumbers show the SOTA performance and the underlined numbers show the second-best result. \\x03\\ndenotes the system is signiﬁcantly better than baselines with p-value < 0.05 tested by [34].\\nSystemEs!En En!Es De!En En!De\\nDev Test Dev Test Dev Test Dev Test\\nNone Memory\\nRNNsearch [2] 55.02 59.34 50.54 50.48 50.20 49.74 44.94 43.98\\nTransformer [74] 64.08 64.63 62.02 61.80 60.18 60.16 54.65 55.43\\nRetrieval Memory\\nSEG-NMT [22] 60.28 59.34 57.62 57.27 55.63 55.33 49.26 48.80\\nNMT-pieces [86] 63.97 64.30 61.50 61.56 60.10 60.26 55.54 55.14\\nG-TFM [80] 66.37 66.21 62.50 62.76 61.85 61.72 57.43 56.88\\nMonoNMT [8] 67.73 67.42 64.18 63.86 64.48 64.62 58.77 58.42\\nCMM [14] 67.48 67.76 63.84 64.04 64.22 64.33 58.94 58.69\\nTransformer dual? 66.87 67.12 63.14 63.54 64.09 63.36 58.69 58.06\\nTransformer uniy 67.74 67.32 63.93 64.12 64.50 64.40 58.16 58.58\\nSelf-Memory\\nTransformer dual?68.63\\x0369.20\\x0364.12\\x0364.67\\x0365.06\\x0364.98\\x0359.26\\x0359.49\\x03\\nTransformer uniy 68.26\\x0368.80\\x0366.07\\x0365.94\\x0365.32\\x0365.65\\x0359.88\\x0360.11\\x03\\nTable 3: Comparison between retrieval memory and self-memory. The quality of memory and\\nhypothesis is measured by the n-gram overlap with reference (BLEU). All experiments are conducted\\nwith Transformer jointonJRC-Acquis .\\nRetrieval Self\\nmemory hypothesis memory hypothesis\\nEn-De! 38.89 58.58 57.92 60.11\\n 42.56 64.40 64.32 65.65\\nEn-Es! 40.67 64.12 63.57 65.94\\n 43.05 67.32 67.78 68.80\\nprevious reranking works [ 36,62]. Reranking aims to select candidates of higher quality than the\\nbeam output, whereas in Selfmem , the chosen candidates serve as memory for the retrieval-augmented\\ngenerator and do not necessarily need to surpass the quality of the beam hypotheses.\\nTable 4: Evaluation results of in-context learning with self-memory.\\nXGLM-1.7B XGLM-4.5B XGLM-7.5B\\nRandom kNN Self Random kNN Self Random kNN Self\\nEn-De! 11.51 37.87 40.94 17.51 37.60 38.25 18.48 47.82 48.32\\n 27.42 51.00 51.88 30.62 48.12 48.36 33.03 55.65 55.12\\nEn-Es! 23.87 46.20 48.56 31.83 48.37 49.17 29.97 53.86 54.32\\n 25.29 51.55 53.13 32.16 48.55 49.22 35.22 57.25 57.56\\nIn Table 4, we present the results of LLM with self-memory. We employ XGLM [ 44] as our backbone\\ngenerator, with three different sizes ranging from 1.7B to 7.5B. We utilize the recommended prompt\\nas described in [ 44]. We select three in-context learning examples and report the average scores\\nfrom three separate runs, taking into account the sensitivity of example selection in ICL [ 45]. From\\nthe table, we ﬁrst observe a general trend where few-shot translation performance improves as the\\nsize of the model increases. Furthermore, we ﬁnd that more similar translation demonstrations\\nsigniﬁcantly enhance performance across all model sizes (from random, kNN to Self). This suggests\\nthat demonstration examples in in-context learning not only act as triggers for model ability but also\\n7adhere to the primal problem , where better demonstration example leads to better generation. Also,\\nby comparing the results in Table 2 and Table 4, we can conclude that the cross-lingual LLM with\\ndesigned examples still falls short of the supervised baselines in this task.\\n5.2 Summarization\\nIn this paper, we compare the performance of our trainable model with those of REINA [ 76],\\nPEGASUS [ 85], and BART [ 37]. The results are presented in Table5. Initially, it can be observed\\nthat memory has varying impacts on different datasets. The enhancement brought by memory in the\\nBigPatent dataset is signiﬁcantly larger than that in the XSum dataset. This can be attributed to the\\ninherent characteristics of the BigPatent dataset, which consists of ofﬁcial patent documents that\\nexhibit considerable similarity. Consequently, this greatly improves the summarization quality in\\naccordance with the primal problem . Furthermore, we discovered that self-memory substantially\\nenhances the performance of both BRIO (+1.2 R1) and BART (+18.5 R1), achieving state-of-the-art\\nresults on both datasets. We selected these baselines for a fair comparison, as they share the same\\nbase generator. Due to space constraints, additional comparisons and the conﬁdence region of the\\nSOTA model can be found in the Appendix.\\nTable 5: Results of summarization task on XSum andBigPatent measured by ROUGE.\\nSystem Memory R-1 R-2 R-L\\nXSum\\nPEGASUS None 47.2 24.6 39.3\\nBRIO None 49.1 25.6 40.4\\nREINA (PG) Retrieval 48.2 26.0 40.2\\nREINA (B) Retrieval 43.2 21.0 35.5\\nREINA (L) Retrieval 46.5 24.1 38.6\\nBRIO dual? Retrieval 48.6 26.1 40.6\\nBRIO jointy Retrieval 49.5 26.5 41.2\\nBRIO dual? Self 49.2 26.2 40.8\\nBRIO jointy Self 50.3 26.7 41.6System Memory R-1 R-2 R-L\\nBigPatent\\nPEGASUS None 53.6 33.2 43.2\\nBART None 44.4 21.3 31.0\\nREINA (B) Retrieval 59.5 42.6 50.6\\nREINA (L) Retrieval 60.7 43.3 51.3\\nREINA (PG) Retrieval 44.6 21.5 33.3\\nBART dual? Retrieval 57.4 43.3 49.7\\nBART jointy Retrieval 59.6 43.4 51.0\\nBART dual? Self 61.2 44.6 52.3\\nBART jointy Self 62.9 48.1 59.6\\n5.3 Dialogue Generation\\nAs demonstrated in Table 6, the self-memory signiﬁcantly enhances the performance of the retrieval-\\naugmented generator for dialogue generation tasks. By optimizing memory using BLEU as \\x01(\\x01;\\x01),\\nthe self-memory improves the B-1,2 score over retrieved memory by 3.08 B-1 and 0.6 B-2 on\\nBART joint. Intriguingly, although Selfmem surpasses the baselines in terms of B-1/2, it falls behind in\\nD-1 and D-2, which can be attributed to the trade-off between BLEU score and Distinct score when\\nevaluating a dialogue system [ 89]. To address this issue, we opt for D-1,2 as \\x01(\\x01;\\x01)when optimizing\\nS\\x12, denoted as BART jointy(D). The results in Table 6 highlight the remarkable ﬂexibility of Selfmem\\nby directly optimizing memory to achieve the desired attributes for diverse and informative dialogue.\\n1 2 3 4\\nIteration60626466Hypothesis BLEU\\nTransformer-joint\\nrank1\\nrank2\\nrank3rank4\\nrank5\\nrank6\\n1 2 3 4\\nIteration60626466\\nTransformer-dual\\nrank1\\nrank2\\nrank3rank4\\nrank5\\nrank6\\n(a) Hypothesis\\n1 2 3 4 5\\nIteration455055606570Candidates BLEU (b) Candidates\\nFigure 3: (a) shows generation quality in the iteration process with different S\\x12in both trainable\\ngenerator architectures. (b) shows candidates quality in the iteration process with an oracle S\\x12.\\n8Table 6: Results of dialogue generation task on DailyDialog measured by B-1/2 and D-1/2.\\nBART joint(D) denotes the metric \\x01(\\x01;\\x01)forS\\x12is the average of D-1 and D-2.\\nSystem Memory B-1 B-2 D-1 D-2\\nNCM [75] None 33.60 26.80 3.00 12.80\\niV AE [19] None 30.90 24.90 2.90 25.00\\nPLATO-2 [4] None 34.80 25.12 3.54 25.11\\nDialoFlow [41] None 36.17 27.67 4.56 27.12\\nBART None 20.72 11.36 3.92 19.44\\nBART dual? Retrieval 29.50 21.89 4.74 26.01\\nBART jointy Retrieval 36.72 31.55 6.13 35.65\\nBART dual? Self 33.43 22.85 4.66 26.16\\nBART jointy Self 39.80 32.15 5.84 32.16\\nBART jointy(D) Self 36.92 32.09 9.12 37.05\\n6 Further Analysis\\nTo gain a deeper insight into Selfmem , we ﬁrst examine the impact of each key component, namely G\\x18\\nandS\\x12. Subsequently, we perform a detailed token-level analysis of the generated output concerning\\ntheir frequency in the training set. Experiments are conducted on the JRC-Acquis En!De dataset.\\nTuningS\\x12We explored various S\\x12by direct selection from the candidate pool based on gold\\nrankings. As shown in Figure 3a, both architectures with enhanced S\\x12signiﬁcantly outperform\\nthe current SOTA performance (60.11 BLEU). Moreover, we assessed the candidate pool quality\\nduring this iterative process using an oracle S\\x12, as displayed in Figure 3b. A clear pattern emerges\\nin this boxplot, revealing improvements in the oracle ,quartile ,average , and minimum scores of\\nthe candidate pool. These two experiments jointly clarify the Selfmem ’s underlying intuition: a\\nretrieval-augmented generator proﬁts from superior memory, which can be chosen from its own\\nunbounded output, and subsequently, the generator with improved memory produces a higher-quality\\ncandidate pool for the next selection round. Consequently, the model lift itself up.\\nThe Least 20% 40% 60% 80% The Most\\nFrequency Rank0.650.700.750.800.850.900.951.00F1Self\\nRetrieval\\nNone\\nFigure 4: 1-gram F1 score sorted by\\ntraining corpus frequency.TuningG\\x18As discussed in §3.1, we demonstrated that a\\ntrained retrieval-augmented generator, with ﬁxed parameters,\\npossesses the ability to distinguish between \"good\" and \"bad\"\\nmemory. This observation not only justiﬁes our decision to\\nmaintain a ﬁxed generator within our framework but also im-\\nplies that the G\\x18is not the current bottleneck of the Selfmem .\\nFrequency Analysis We conduct a comprehensive token-\\nlevel analysis by computing the 1-gram F1 scores for generated\\ntranslations and subsequently categorizing the tokens based\\non their frequency in the training set. The results are depicted\\nin Figure 4. A noticeable pattern emerges, suggesting that the\\nmore frequently a model encounters a token during training,\\nthe higher the accuracy of the generated output [ 87]. Moreover, our ﬁndings indicate that retrieval-\\naugmented models, particularly those incorporating self-memory augmentation, exhibit superior\\nperformance in handling long-tail inputs which are challenges for parametric models [61, 52].\\n7 Conclusion\\nFor the ﬁrst time, we investigate the fundamental limitation of bounded memory in the current\\nretrieval-augmented literature. We combine the primal anddual problems together and propose\\nSelfmem , a general framework for retrieval-augmented text generation by uplifting generation model\\nwith its own output. We conduct comprehensive experiments across various text generation tasks\\nand different generation paradigms, including trainable small model and few-shot prompted LLM.\\nWe surpass strong baselines and improve the state-of-the-art performance in serval datasets. We also\\n9meticulously investigate each crucial component and pinpoint the existing system bottleneck to guide\\nfuture research endeavors.\\nReferences\\n[1]Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad.\\nIn-context examples selection for machine translation. CoRR , abs/2212.02437, 2022.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. In Yoshua Bengio and Yann LeCun, editors, 3rd International\\nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\\nConference Track Proceedings , 2015.\\n[3]Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. PLATO: pre-trained dialogue\\ngeneration model with discrete latent variable. In Dan Jurafsky, Joyce Chai, Natalie Schluter,\\nand Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages 85–96. Association for\\nComputational Linguistics, 2020.\\n[4]Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin\\nLiu, and Xinchao Xu. PLATO-2: Towards building an open-domain chatbot via curriculum\\nlearning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 ,\\npages 2513–2525, Online, August 2021. Association for Computational Linguistics.\\n[5]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo\\nLarochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,\\neditors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural\\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\\n[6]Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, and Shuming Shi.\\nSkeleton-to-response: Dialogue generation guided by retrieval memory. In Jill Burstein, Christy\\nDoran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Technologies,\\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) ,\\npages 1219–1228. Association for Computational Linguistics, 2019.\\n[7]Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, and Shuming Shi. Retrieval-guided\\ndialogue response generation via a matching-to-generation framework. In Kentaro Inui, Jing\\nJiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 , pages\\n1866–1875. Association for Computational Linguistics, 2019.\\n[8]Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation\\nwith monolingual translation memory. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto\\nNavigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Conference on Natural Language Processing,\\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pages 7307–\\n7318. Association for Computational Linguistics, 2021.\\n[9]Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in\\nmachine translation research. In 11th Conference of the European Chapter of the Association\\nfor Computational Linguistics , pages 249–256, Trento, Italy, April 2006. Association for\\nComputational Linguistics.\\n10[10] Qian Cao and Deyi Xiong. Encoding gated translation memory into neural machine translation.\\nIn Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of\\nthe 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium,\\nOctober 31 - November 4, 2018 , pages 3042–3047. Association for Computational Linguistics,\\n2018.\\n[11] Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. Retrieve, rerank and rewrite: Soft template\\nbased neural summarization. In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers) , pages 152–161, Melbourne, Australia,\\nJuly 2018. Association for Computational Linguistics.\\n[12] Eugene Charniak and Mark Johnson. Coarse-to-ﬁne n-best parsing and maxent discriminative\\nreranking. In Kevin Knight, Hwee Tou Ng, and Kemal Oﬂazer, editors, ACL 2005, 43rd Annual\\nMeeting of the Association for Computational Linguistics, Proceedings of the Conference,\\n25-30 June 2005, University of Michigan, USA , pages 173–180. The Association for Computer\\nLinguistics, 2005.\\n[13] Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu,\\nBartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. Dialogved: A pre-trained\\nlatent variable encoder-decoder model for dialog response generation. In Smaranda Muresan,\\nPreslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of\\nthe Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,\\nIreland, May 22-27, 2022 , pages 4852–4864. Association for Computational Linguistics, 2022.\\n[14] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation\\nwith contrastive translation memories. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang,\\neditors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language\\nProcessing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages\\n3591–3601. Association for Computational Linguistics, 2022.\\n[15] Michael Collins and Terry Koo. Discriminative reranking for natural language parsing. Comput.\\nLinguistics , 31(1):25–70, 2005.\\n[16] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,\\nFrancisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsu-\\npervised cross-lingual representation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie\\nSchluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages 8440–8451.\\nAssociation for Computational Linguistics, 2020.\\n[17] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc’Aurelio Ranzato. Resid-\\nual energy-based models for text generation. In 8th International Conference on Learning\\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020.\\n[18] Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer. CORE: A\\nretrieve-then-edit framework for counterfactual data generation. In Yoav Goldberg, Zornitsa\\nKozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics:\\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages 2964–2984.\\nAssociation for Computational Linguistics, 2022.\\n[19] Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen. Implicit deep latent\\nvariable models for text generation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun\\nWan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\\nguage Processing and the 9th International Joint Conference on Natural Language Processing,\\nEMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 , pages 3944–3954. Association\\nfor Computational Linguistics, 2019.\\n[20] Jiazhan Feng, Chongyang Tao, Zhen Li, Chang Liu, Tao Shen, and Dongyan Zhao. Reciprocal\\nlearning of knowledge retriever and response ranker for knowledge-grounded conversations. In\\nNicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun\\nChoi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio,\\nNianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus,\\n11Francis Bond, and Seung-Hoon Na, editors, Proceedings of the 29th International Conference\\non Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17,\\n2022 , pages 389–399. International Committee on Computational Linguistics, 2022.\\n[21] Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language modeﬂs with\\na continuous cache. In 5th International Conference on Learning Representations, ICLR 2017,\\nToulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 2017.\\n[22] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O. K. Li. Search engine guided neural\\nmachine translation. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\\nthe Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative\\nApplications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\\nAdvances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\\n2018 , pages 5133–5140. AAAI Press, 2018.\\n[23] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval\\naugmented language model pre-training. In Proceedings of the 37th International Conference\\non Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings\\nof Machine Learning Research , pages 3929–3938. PMLR, 2020.\\n[24] Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, and Percy Liang. A retrieve-and-edit\\nframework for predicting structured outputs. In Samy Bengio, Hanna M. Wallach, Hugo\\nLarochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances\\nin Neural Information Processing Systems 31: Annual Conference on Neural Information\\nProcessing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages\\n10073–10083, 2018.\\n[25] Qiuxiang He, Guoping Huang, Qu Cui, Li Li, and Lemao Liu. Fast and accurate neural machine\\ntranslation with translation memory. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto\\nNavigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Conference on Natural Language Processing,\\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pages 3170–\\n3180. Association for Computational Linguistics, 2021.\\n[26] Kenji Imamura and Eiichiro Sumita. Ensemble and reranking: Using multiple models in\\nthe NICT-2 neural machine translation system at WAT2017. In Toshiaki Nakazawa and Isao\\nGoto, editors, Proceedings of the 4th Workshop on Asian Translation, WAT@IJCNLP 2017,\\nTaipei, Taiwan, November 27- December 1, 2017 , pages 127–134. Asian Federation of Natural\\nLanguage Processing, 2017.\\n[27] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models\\nfor open domain question answering. In Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty,\\neditors, Proceedings of the 16th Conference of the European Chapter of the Association for\\nComputational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021 , pages\\n874–880. Association for Computational Linguistics, 2021.\\n[28] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov,\\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.\\nIn Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020\\nConference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,\\nNovember 16-20, 2020 , pages 6769–6781. Association for Computational Linguistics, 2020.\\n[29] Amirhossein Kazemnejad, Mohammadreza Salehi, and Mahdieh Soleymani Baghshah. Para-\\nphrase generation by learning how to edit from samples. In Dan Jurafsky, Joyce Chai, Natalie\\nSchluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages 6010–6021.\\nAssociation for Computational Linguistics, 2020.\\n[30] Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Nearest\\nneighbor machine translation. In 9th International Conference on Learning Representations,\\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021.\\n12[31] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. General-\\nization through memorization: Nearest neighbor language models. In International Conference\\non Learning Representations , 2020.\\n[32] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts,\\nand Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for\\nknowledge-intensive NLP. CoRR , abs/2212.14024, 2022.\\n[33] Omar Khattab and Matei Zaharia. Colbert: Efﬁcient and effective passage search via contextu-\\nalized late interaction over BERT. In Jimmy X. Huang, Yi Chang, Xueqi Cheng, Jaap Kamps,\\nVanessa Murdock, Ji-Rong Wen, and Yiqun Liu, editors, Proceedings of the 43rd International\\nACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020,\\nVirtual Event, China, July 25-30, 2020 , pages 39–48. ACM, 2020.\\n[34] Philipp Koehn. Statistical signiﬁcance tests for machine translation evaluation. In Proceedings\\nof the 2004 Conference on Empirical Methods in Natural Language Processing , EMNLP 2004,\\nA meeting of SIGDAT, a Special Interest Group of the ACL, held in conjunction with ACL 2004,\\n25-26 July 2004, Barcelona, Spain , pages 388–395. ACL, 2004.\\n[35] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani,\\nVictor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory\\nnetworks for natural language processing. In Maria Florina Balcan and Kilian Q. Weinberger,\\neditors, Proceedings of The 33rd International Conference on Machine Learning , volume 48 of\\nProceedings of Machine Learning Research , pages 1378–1387, New York, New York, USA,\\n20–22 Jun 2016. PMLR.\\n[36] Ann Lee, Michael Auli, and Marc’Aurelio Ranzato. Discriminative reranking for neural\\nmachine translation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,\\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\\nthe 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021,\\n(Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pages 7250–7264. Association for\\nComputational Linguistics, 2021.\\n[37] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation, and comprehension. In Dan Jurafsky, Joyce\\nChai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting\\nof the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages\\n7871–7880. Association for Computational Linguistics, 2020.\\n[38] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and\\nDouwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Hugo\\nLarochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,\\neditors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural\\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\\n[39] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\\nobjective function for neural conversation models. In Kevin Knight, Ani Nenkova, and Owen\\nRambow, editors, NAACL HLT 2016, The 2016 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human Language Technologies, San Diego Cal-\\nifornia, USA, June 12-17, 2016 , pages 110–119. The Association for Computational Linguistics,\\n2016.\\n[40] Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. Dailydialog: A\\nmanually labelled multi-turn dialogue dataset. In Greg Kondrak and Taro Watanabe, editors,\\nProceedings of the Eighth International Joint Conference on Natural Language Processing,\\nIJCNLP 2017, Taipei, Taiwan, November 27 - December 1, 2017 - Volume 1: Long Papers ,\\npages 986–995. Asian Federation of Natural Language Processing, 2017.\\n[41] Zekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng, and Jie Zhou. Conversations are not ﬂat:\\nModeling the dynamic information ﬂow across dialogue utterances. In Proceedings of the 59th\\n13Annual Meeting of the Association for Computational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 128–138,\\nOnline, August 2021. Association for Computational Linguistics.\\n[42] Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and Xifeng Yan. Controllable\\ndialogue simulation with in-context learning. In Findings of the Association for Computational\\nLinguistics: EMNLP 2022 , pages 4330–4347, Abu Dhabi, United Arab Emirates, December\\n2022. Association for Computational Linguistics.\\n[43] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\\ntion Branches Out , pages 74–81, Barcelona, Spain, July 2004. Association for Computational\\nLinguistics.\\n[44] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig,\\nMyle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer,\\nPunit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa\\nKozareva, Mona T. Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual\\ngenerative language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,\\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,\\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages 9019–9052.\\nAssociation for Computational Linguistics, 2022.\\n[45] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.\\nWhat makes good in-context examples for gpt-3? In Eneko Agirre, Marianna Apidianaki, and\\nIvan Vulic, editors, Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge\\nExtraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin,\\nIreland and Online, May 27, 2022 , pages 100–114. Association for Computational Linguistics,\\n2022.\\n[46] Lemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao, Mo Yu, and Conghui Zhu. Locally\\ntraining the log-linear model for SMT. In Jun’ichi Tsujii, James Henderson, and Marius Pasca,\\neditors, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language\\nProcessing and Computational Natural Language Learning, EMNLP-CoNLL 2012, July 12-14,\\n2012, Jeju Island, Korea , pages 402–411. ACL, 2012.\\n[47] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT\\npretraining approach. CoRR , abs/1907.11692, 2019.\\n[48] Yixin Liu, Zi-Yi Dou, and Pengfei Liu. Refsum: Refactoring neural summarization. In Kristina\\nToutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard,\\nRyan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021\\nConference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 , pages 1437–1448.\\nAssociation for Computational Linguistics, 2021.\\n[49] Yixin Liu and Pengfei Liu. Simcls: A simple framework for contrastive learning of abstrac-\\ntive summarization. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,\\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and\\nthe 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021,\\n(Volume 2: Short Papers), Virtual Event, August 1-6, 2021 , pages 1065–1072. Association for\\nComputational Linguistics, 2021.\\n[50] Yixin Liu, Pengfei Liu, Dragomir R. Radev, and Graham Neubig. BRIO: bringing order to\\nabstractive summarization. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, edi-\\ntors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , pages 2890–2903.\\nAssociation for Computational Linguistics, 2022.\\n[51] Yuchen Liu, Long Zhou, Yining Wang, Yang Zhao, Jiajun Zhang, and Chengqing Zong. A\\ncomparable study on model averaging, ensembling and reranking in NMT. In Min Zhang,\\nVincent Ng, Dongyan Zhao, Sujian Li, and Hongying Zan, editors, Natural Language Processing\\n14and Chinese Computing - 7th CCF International Conference, NLPCC 2018, Hohhot, China,\\nAugust 26-30, 2018, Proceedings, Part II , volume 11109 of Lecture Notes in Computer Science ,\\npages 299–308. Springer, 2018.\\n[52] Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg,\\nAlan Blair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented classiﬁcation for\\nlong-tail visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition , pages 6959–6969, 2022.\\n[53] Yu Lu, Jiali Zeng, Jiajun Zhang, Shuangzhi Wu, and Mu Li. Learning conﬁdence for transformer-\\nbased neural machine translation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio,\\neditors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguis-\\ntics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , pages 2353–2364.\\nAssociation for Computational Linguistics, 2022.\\n[54] Clara Meister, Ryan Cotterell, and Tim Vieira. If beam search is the answer, what was the\\nquestion? In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of\\nthe 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,\\nOnline, November 16-20, 2020 , pages 2173–2185. Association for Computational Linguistics,\\n2020.\\n[55] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the sum-\\nmary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of\\nthe 2018 Conference on Empirical Methods in Natural Language Processing , pages 1797–1807,\\nBrussels, Belgium, October-November 2018. Association for Computational Linguistics.\\n[56] Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alexan-\\nder M. Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin,\\nand Dragomir R. Radev. A smorgasbord of features for statistical machine translation. In Julia\\nHirschberg, Susan T. Dumais, Daniel Marcu, and Salim Roukos, editors, Human Language\\nTechnology Conference of the North American Chapter of the Association for Computational\\nLinguistics, HLT-NAACL 2004, Boston, Massachusetts, USA, May 2-7, 2004 , pages 161–168.\\nThe Association for Computational Linguistics, 2004.\\n[57] OpenAI. GPT-4 technical report. CoRR , abs/2303.08774, 2023.\\n[58] Md. Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei\\nChang. Retrieval augmented code generation and summarization. In Marie-Francine Moens,\\nXuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for\\nComputational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic,\\n16-20 November, 2021 , pages 2719–2734. Association for Computational Linguistics, 2021.\\n[59] Hao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan Dhingra, and Dipanjan Das. Text\\ngeneration with exemplar-based adaptive decoding. In Jill Burstein, Christy Doran, and Thamar\\nSolorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , pages 2555–2565.\\nAssociation for Computational Linguistics, 2019.\\n[60] Matt Post. A call for clarity in reporting BLEU scores. In Ondrej Bojar, Rajen Chatterjee,\\nChristian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio\\nJimeno-Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana L. Neves,\\nMatt Post, Lucia Specia, Marco Turchi, and Karin Verspoor, editors, Proceedings of the Third\\nConference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October\\n31 - November 1, 2018 , pages 186–191. Association for Computational Linguistics, 2018.\\n[61] Vikas Raunak, Siddharth Dalmia, Vivek Gupta, and Florian Metze. On long-tailed phenomena\\nin neural machine translation. In Findings of the Association for Computational Linguistics:\\nEMNLP 2020 , pages 3088–3095, Online, November 2020. Association for Computational\\nLinguistics.\\n15[62] Mathieu Ravaut, Shaﬁq R. Joty, and Nancy F. Chen. Summareranker: A multi-task mixture-of-\\nexperts re-ranking framework for abstractive summarization. In Smaranda Muresan, Preslav\\nNakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Associ-\\nation for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May\\n22-27, 2022 , pages 4504–4524. Association for Computational Linguistics, 2022.\\n[63] Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and\\nbeyond. Found. Trends Inf. Retr. , 3(4):333–389, 2009.\\n[64] Julian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin Kirchhoff. Masked language model\\nscoring. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceed-\\nings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,\\nOnline, July 5-10, 2020 , pages 2699–2712. Association for Computational Linguistics, 2020.\\n[65] Eva Sharma, Chen Li, and Lu Wang. BIGPATENT: A large-scale dataset for abstractive and\\ncoherent summarization. In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors,\\nProceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019,\\nFlorence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , pages 2204–2213. Association\\nfor Computational Linguistics, 2019.\\n[66] Libin Shen, Anoop Sarkar, and Franz Josef Och. Discriminative reranking for machine transla-\\ntion. In Julia Hirschberg, Susan T. Dumais, Daniel Marcu, and Salim Roukos, editors, Human\\nLanguage Technology Conference of the North American Chapter of the Association for Com-\\nputational Linguistics, HLT-NAACL 2004, Boston, Massachusetts, USA, May 2-7, 2004 , pages\\n177–184. The Association for Computational Linguistics, 2004.\\n[67] Weizhou Shen, Yeyun Gong, Yelong Shen, Song Wang, Xiaojun Quan, Nan Duan, and Weizhu\\nChen. Joint generator-ranker learning for natural language generation. CoRR , abs/2206.13974,\\n2022.\\n[68] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke\\nZettlemoyer, and Wen-tau Yih. REPLUG: retrieval-augmented black-box language models.\\nCoRR , abs/2301.12652, 2023.\\n[69] Suzanna Sia and Kevin Duh. In-context learning as maintaining coherency: A study of on-the-ﬂy\\nmachine translation using large language models. CoRR , abs/2305.03573, 2023.\\n[70] Michel Simard and Pierre Isabelle. Phrase-based machine translation in a computer-assisted\\ntranslation environment. In Proceedings of Machine Translation Summit XII: Papers, MTSummit\\n2009, Ottawa, Canada, August 26-30, 2009 , 2009.\\n[71] Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and Ming Zhang. Two are better than one: An\\nensemble of retrieval- and generation-based dialog systems. CoRR , abs/1610.07149, 2016.\\n[72] Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tuﬁs,\\nand Dániel Varga. The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages.\\nIn Nicoletta Calzolari, Khalid Choukri, Aldo Gangemi, Bente Maegaard, Joseph Mariani,\\nJan Odijk, and Daniel Tapias, editors, Proceedings of the Fifth International Conference on\\nLanguage Resources and Evaluation, LREC 2006, Genoa, Italy, May 22-28, 2006 , pages\\n2142–2147. European Language Resources Association (ELRA), 2006.\\n[73] Yixuan Su, David Vandyke, Simon Baker, Yan Wang, and Nigel Collier. Keep the primary,\\nrewrite the secondary: A two-stage approach for paraphrase generation. In Findings of the\\nAssociation for Computational Linguistics: ACL-IJCNLP 2021 , pages 560–569, Online, August\\n2021. Association for Computational Linguistics.\\n[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von\\nLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman\\nGarnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference\\non Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA ,\\npages 5998–6008, 2017.\\n16[75] Oriol Vinyals and Quoc V . Le. A neural conversational model. CoRR , abs/1506.05869, 2015.\\n[76] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu,\\nand Michael Zeng. Training data is more valuable than you think: A simple and effective\\nmethod by retrieving from training data. In Smaranda Muresan, Preslav Nakov, and Aline\\nVillavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 ,\\npages 3170–3179. Association for Computational Linguistics, 2022.\\n[77] Yuguang Wang, Shanbo Cheng, Liyang Jiang, Jiajun Yang, Wei Chen, Muze Li, Lin Shi,\\nYanfeng Wang, and Hongtao Yang. Sogou neural machine translation systems for WMT17. In\\nOndrej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry\\nHaddow, Matthias Huck, Antonio Jimeno-Yepes, Philipp Koehn, and Julia Kreutzer, editors,\\nProceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen,\\nDenmark, September 7-8, 2017 , pages 410–415. Association for Computational Linguistics,\\n2017.\\n[78] Jason Weston, Emily Dinan, and Alexander H. Miller. Retrieve and reﬁne: Improved sequence\\ngeneration models for dialogue. In Aleksandr Chuklin, Jeff Dalton, Julia Kiseleva, Alexey\\nBorisov, and Mikhail S. Burtsev, editors, Proceedings of the 2nd International Workshop on\\nSearch-Oriented Conversational AI, SCAI@EMNLP 2018, Brussels, Belgium, October 31, 2018 ,\\npages 87–92. Association for Computational Linguistics, 2018.\\n[79] Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhoujun Li, and Ming Zhou. Response gener-\\nation by context-aware prototype editing. In The Thirty-Third AAAI Conference on Artiﬁcial\\nIntelligence, AAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial Intelligence\\nConference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intel-\\nligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pages 7281–7288.\\nAAAI Press, 2019.\\n[80] Mengzhou Xia, Guoping Huang, Lemao Liu, and Shuming Shi. Graph based translation memory\\nfor neural machine translation. In The Thirty-Third AAAI Conference on Artiﬁcial Intelligence,\\nAAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial Intelligence Conference, IAAI\\n2019, The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI\\n2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pages 7297–7304. AAAI Press,\\n2019.\\n[81] Jitao Xu, Josep Maria Crego, and Jean Senellart. Boosting neural machine translation with\\nsimilar translations. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors,\\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL\\n2020, Online, July 5-10, 2020 , pages 1580–1590. Association for Computational Linguistics,\\n2020.\\n[82] Masaru Yamada. The effect of translation memory databases on productivity. Translation\\nresearch projects , 3:63–73, 2011.\\n[83] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang,\\nMike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language\\nmodeling. CoRR , abs/2211.12561, 2022.\\n[84] Dani Yogatama, Cyprien de Masson d’Autume, and Lingpeng Kong. Adaptive semiparametric\\nlanguage models. Trans. Assoc. Comput. Linguistics , 9:362–373, 2021.\\n[85] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. PEGASUS: pre-training with\\nextracted gap-sentences for abstractive summarization. In Proceedings of the 37th International\\nConference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of\\nProceedings of Machine Learning Research , pages 11328–11339. PMLR, 2020.\\n[86] Jingyi Zhang, Masao Utiyama, Eiichiro Sumita, Graham Neubig, and Satoshi Nakamura.\\nGuiding neural machine translation with retrieved translation pieces. In Marilyn A. Walker,\\nHeng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language Technologies,\\nNAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers) ,\\npages 1325–1335. Association for Computational Linguistics, 2018.\\n17[87] Tong Zhang, Wei Ye, Baosong Yang, Long Zhang, Xingzhang Ren, Dayiheng Liu, Jinan\\nSun, Shikun Zhang, Haibo Zhang, and Wen Zhao. Frequency-aware contrastive learning for\\nneural machine translation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence ,\\nvolume 36, pages 11712–11720, 2022.\\n[88] Xueliang Zhao, Lemao Liu, Tingchen Fu, Shuming Shi, Dongyan Zhao, and Rui Yan. Towards\\nefﬁcient dialogue pre-training with transferable and interpretable latent structure. CoRR ,\\nabs/2210.12461, 2022.\\n[89] Yinhe Zheng, Zikai Chen, Rongsheng Zhang, Shilei Huang, Xiaoxi Mao, and Minlie Huang.\\nStylized dialogue response generation using stylized unpaired texts. In Thirty-Fifth AAAI\\nConference on Artiﬁcial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Appli-\\ncations of Artiﬁcial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances\\nin Artiﬁcial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021 , pages 14558–14567.\\nAAAI Press, 2021.\\n[90] Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation.\\nCoRR , abs/2205.12674, 2022.\\n18',\n",
       " 'Published as a conference paper at ICLR 2021\\nRETRIEVAL -AUGMENTED GENERATION FOR CODE\\nSUMMARIZATION VIA HYBRID GNN\\nShangqing Liu1\\x03, Yu Chen2y, Xiaofei Xie1y, Jingkai Siow1, Yang Liu1\\n1Nanyang Technology University\\n2Rensselaer Polytechnic Institute\\nABSTRACT\\nSource code summarization aims to generate natural language summaries from\\nstructured code snippets for better understanding code functionalities. However, au-\\ntomatic code summarization is challenging due to the complexity of the source code\\nand the language gap between the source code and natural language summaries.\\nMost previous approaches either rely on retrieval-based (which can take advantage\\nof similar examples seen from the retrieval database, but have low generalization\\nperformance) or generation-based methods (which have better generalization per-\\nformance, but cannot take advantage of similar examples). This paper proposes\\na novel retrieval-augmented mechanism to combine the beneﬁts of both worlds.\\nFurthermore, to mitigate the limitation of Graph Neural Networks (GNNs) on\\ncapturing global graph structure information of source code, we propose a novel\\nattention-based dynamic graph to complement the static graph representation of the\\nsource code, and design a hybrid message passing GNN for capturing both the local\\nand global structural information. To evaluate the proposed approach, we release\\na new challenging benchmark, crawled from diversiﬁed large-scale open-source\\nCprojects (total 95k+ unique functions in the dataset). Our method achieves the\\nstate-of-the-art performance, improving existing methods by 1.42,2.44 and1.29\\nin terms of BLEU-4, ROUGE-L and METEOR.\\n1 I NTRODUCTION\\nWith software growing in size and complexity, developers tend to spend nearly 90% (Wan et al.,\\n2018) effort on software maintenance ( e.g., version iteration and bug ﬁx) in the completed life\\ncycle of software development. Source code summary, in the form of natural language, plays a\\ncritical role in the comprehension and maintenance process and greatly reduces the effort of reading\\nand comprehending programs. However, manually writing code summaries is tedious and time-\\nconsuming, and with the acceleration of software iteration, it has become a heavy burden for software\\ndevelopers. Hence, source code summarization which automates concise descriptions of programs is\\nmeaningful.\\nAutomatic source code summarization is a crucial yet far from the settled problem. The key challenges\\ninclude: 1) the source code and the natural language summary are heterogeneous, which means\\nthey may not share common lexical tokens, synonyms, or language structures and 2) the source\\ncode is complex with complicated logic and variable grammatical structure, making it hard to learn\\nthe semantics. Conventionally, information retrieval (IR) techniques have been widely used in\\ncode summarization (Eddy et al., 2013; Haiduc et al., 2010; Wong et al., 2015; 2013). Since code\\nduplication (Kamiya et al., 2002; Li et al., 2006) is common in “big code” (Allamanis et al., 2018),\\nearly works summarize the new programs by retrieving the similar code snippet in the existing code\\ndatabase and use its summary directly. Essentially, the retrieval-based approaches transform the code\\nsummarization to the code similarity calculation task, which may achieve promising performance on\\nsimilar programs, but are limited in generalization, i.e. they have poorer performance on programs\\nthat are very different from the code database.\\n\\x03Contact:shangqin001@e.ntu.edu.sg\\nyCorresponding authors\\n1arXiv:2006.05405v5  [cs.LG]  13 May 2021Published as a conference paper at ICLR 2021\\nTo improve the generalization performance, recent works focus on generation-based approaches.\\nSome works explore Seq2Seq architectures (Bahdanau et al., 2014; Luong et al., 2015) to generate\\nsummaries from the given source code. The Seq2Seq-based approaches (Iyer et al., 2016; Hu et al.,\\n2018a; Alon et al., 2018) usually treat the source code or abstract syntax tree parsed from the source\\ncode as a sequence and follow a paradigm of encoder-decoder with the attention mechanism for\\ngenerating a summary. However, these works only rely on sequential models, which are struggling\\nto capture the rich semantics of source code e.g., control dependencies and data dependencies. In\\naddition, generation-based approaches typically cannot take advantage of similar examples from the\\nretrieval database, as retrieval-based approaches do.\\nTo better learn the semantics of the source code, Allamanis et al. (Allamanis et al., 2017) lighted\\nup this ﬁeld by representing programs as graphs. Some follow-up works (Fernandes et al., 2018)\\nattempted to encode more code structures ( e.g., control ﬂow, program dependencies) into code graphs\\nwith graph neural networks (GNNs), and achieved the promising performance than the sequence-\\nbased approaches. Existing works (Allamanis et al., 2017; Fernandes et al., 2018) usually convert\\ncode into graph-structured input during preprocessing, and directly consume it via modern neural\\nnetworks ( e.g., GNNs) for computing node and graph embeddings. However, most GNN-based\\nencoders only allow message passing among nodes within a k-hop neighborhood (where kis usually\\na small number such as 4) to avoid over-smoothing (Zhao & Akoglu, 2019; Chen et al., 2020a), thus\\ncapture only local neighborhood information and ignore global interactions among nodes. Even\\nthere are some works (Li et al., 2019) that try to address this challenging with deep GCNs (i.e., 56\\nlayers) (Kipf & Welling, 2016) by the residual connection (He et al., 2016), however, the computation\\ncost cannot endure in the program especially for a large and complex program.\\nTo address these challenges, we propose a framework for automatic code summarization, namely\\nHybrid GNN (HGNN) . Speciﬁcally, from the source code, we ﬁrst construct a code property graph\\n(CPG) based on the abstract syntax tree (AST) with different types of edges ( i.e., Flow To, Reach). In\\norder to combine the beneﬁts of both retrieval-based and generation-based methods, we propose a\\nretrieval-based augmentation mechanism to retrieve the source code that is most similar to the current\\nprogram from the retrieval database (excluding the current program itself), and add the retrieved code\\nas well as the corresponding summary as auxiliary information for training the model. In order to\\ngo beyond local graph neighborhood information, and capture global interactions in the program,\\nwe further propose an attention-based dynamic graph by learning global attention scores ( i.e., edge\\nweights) in the augmented static CPG. Then, a hybrid message passing (HMP) is performed on both\\nstatic and dynamic graphs. We also release a new code summarization benchmark by crawling data\\nfrom popular and diversiﬁed projects containing 95k+ functions in Cprogramming language and\\nmake it public1. We highlight our main contributions as follows:\\n•We propose a general-purpose framework for automatic code summarization, which combines the\\nbeneﬁts of both retrieval-based and generation-based methods via a retrieval-based augmentation\\nmechanism.\\n•We innovate a Hybrid GNN by fusing the static graph (based on code property graph) and dynamic\\ngraph (via structure-aware global attention mechanism) to mitigate the limitation of the GNN on\\ncapturing global graph information.\\n• We release a new challenging Cbenchmark for the task of source code summarization.\\n•We conduct an extensive experiment to evaluate our framework. The proposed approach achieves\\nthe state-of-the-art performance and improves existing approaches by 1.42,2.44 and1.29 in terms\\nof BLEU-4, ROUGE-L and METEOR metrics.\\n2 H YBRID GNN F RAMEWORK\\nIn this section, we introduce the proposed framework Hybrid GNN (HGNN) , as shown in Figure 1,\\nwhich mainly includes four components: 1) Retrieval-augmented Static Graph Construction (c.f.,\\nSection 2.2), which incorporates retrieved code-summary pairs to augment the original code for\\nlearning. 2) Attention-based Dynamic Graph Construction ( c.f.,Section 2.3), which allows message\\npassing among any pair of nodes via a structure-aware global attention mechanism. 3) HGNN , (c.f.,\\n1https://github.com/shangqing-liu/CCSD-benchmark-for-code-summarization\\n2Published as a conference paper at ICLR 2021\\nRetrievedSummarySourceCode\\nRetrievalCPGAug\\nStatic Message Passing\\nAX(     +     …     +      )       Dynamic Message PassingHybrid\\nDecoder\\nGeneratedSummaryHybrid GNNRetrieval-augmentedGraph\\nSummaryEncodingGraphEmbeddingGRU Updates\\nNode EmbeddingsAttention-basedGraphRetrievalDatabaseSourceCPGAggregation\\nFigure 1: The overall architecture of the proposed HGNN framework.\\nSection 2.4), which incorporates information from both static graphs and dynamic graphs with Hybrid\\nMessage Passing. 4) Decoder ( c.f.,Section 2.5), which utilizes an attention-based LSTM (Hochreiter\\n& Schmidhuber, 1997) model to generate a summary.\\n2.1 P ROBLEM FORMULATION\\nIn this work, we focus on generating natural language summaries for the given functions (Wan et al.,\\n2018; Zhang et al., 2020). A simple example is illustrated in Listing 1, which is crawled from Linux\\nKernel. Our goal is to generate the best summary “set the time of day clock” based on the given source\\ncode. Formally, we deﬁne a dataset as D=f(c;s)jc2C;s2Sg, wherecis the source code of a\\nfunction in the function set Candsrepresents its targeted summary in the summary set S. The task of\\ncode summarization is, given a source code c, to generate the best summary consisting of a sequence\\nof tokens ^s= (t1;t2;:::;t T)that maximizes the conditional likelihood ^s= argmaxsP(sjc).\\nSource Code:\\nint pdc_tod_set(unsigned long sec, unsigned long usec){\\nint retval; unsigned long flags;\\nspin_lock_irqsave(&pdc_lock, flags);\\nretval = mem_pdc_call(PDC_TOD, PDC_TOD_WRITE, sec, usec);\\nspin_unlock_irqrestore(&pdc_lock, flags);\\nreturn retval;\\n}\\nGround-Truth: set the time of day clock\\nListing 1: An example in our dataset crawled from Linux Kernel.\\n2.2 R ETRIEVAL -AUGMENTED STATIC GRAPH\\n2.2.1 G RAPH INITIALIZATION\\nThe source code of a function can be represented as Code Property Graph (CPG) (Yamaguchi et al.,\\n2014), which is built on the abstract syntax tree (AST) with different type of edges ( i.e., Flow To,\\nControl, Deﬁne/Use, Reach). Formally, one raw function ccould be represented by a multi-edged\\ngraphg(V;E), whereVis the set of AST nodes, (v;u)2Edenotes the edge between the node vand\\nthe nodeu. A nodevconsists of two parts: the node sequence and the node type . An illustrative\\nexample is shown in Figure 2. For example, in the red node, a%2 == 0 is the node sequence and\\nCondition is the node type. An edge (v;u)has a type, named edge type ,e.g., AST type and Flow\\nTo type. For more details about the CPG, please refer to Appendix A.\\nInitialization Representation. Given a CPG, we utilize a BiLSTM to encode its nodes. We represent\\neach token of the node sequence and each edge type using the learned embedding matrix Eseqtoken\\nandEedgetype, respectively. Then nodes and edges of the CPG can be encoded as:\\nh1; :::;hl= BiLSTM( Eseqtoken\\nv;1 ; :::;Eseqtoken\\nv;l )\\nencode _node (v) = [h!\\nl;h \\n1]\\nencode _edge(v; u) =Eedgetype\\nv;u if(v; u)2 Eelse 0(1)\\n3Published as a conference paper at ICLR 2021\\nEntryCFGEntryNodeexample()FunctionDefint a = rand( )IdentiﬁerDeclStatementa % 2 == 0Conditionint b = a ++IdentiﬁerDeclStatementEXITCFGExitNodecall(b)ExpressionStatement ParameterListvoidReturnTypeintIdentiﬁerDeclTypeaIdentiﬁera = rand ( )AssignmentExprrand ( )CallExpressiona % 2 == 0EqualityExpressiona % 2MultiplicativeExpressionaIdentiﬁer2PrimaryExpression0PrimaryExpressionintIdentiﬁerDeclTypebIdentiﬁerb = a ++AssignmentExpra ++IncDecOp++IncDecaIdentiﬁeraSymbolcallCalleebArgumentbIdentiﬁerbSymbolbIdentiﬁerASTFlow  ToControlDeﬁne/UseReachvoid example ( ){   int a = rand( );   if ( a % 2 == 0 )   {        int b = a++;        call(b);   }}A simple code exampleparse\\nFigure 2: An example of Code Property Graph (CPG).\\nwherelis the number of tokens in the node sequence of v. For the sake of simplicity, in the following\\nsection, we use hvandev;uto represent the embedding of the node vand the edge (v;u), respectively,\\ni.e.,encode _node (v)andencode _edge(v;u). Given the source code cof a function as well as the\\nCPGg(V;E),Hc2Rm\\x02ddenotes the initial node matrix of the CPG, where mis the total number\\nof nodes in the CPG and dis the dimension of the node embedding.\\n2.2.2 R ETRIEVAL -BASED AUGMENTATION\\nWhile retrieval-based methods can perform reasonably well on examples that are similar to those\\nexamples from a retrieval database, they typically have low generalization performance and might\\nperform poorly on dissimilar examples. On the contrary, generation-based methods usually have better\\ngeneralization performance, but cannot take advantage of similar examples from the retrieval database.\\nIn this work, we propose to combine the beneﬁts of the two worlds, and design a retrieval-augmented\\ngeneration framework for the task of code summarization.\\nIn principle, the goal of code summarization is to learn a mapping from source code cto the natural\\nlanguage summary s=f(c). In other words, for any source code c0, a code summarization system\\ncan produce its summary s0=f(c0). Inspired by this observation, conceptually, we can derive\\nthe following formulation s=f(c)\\x00f(c0) +s0. This tells us that we can actually compute the\\nsemantic difference between candc0, and further obtain the desired summary sforcby considering\\nboth the above semantic difference and s0which is the summary for c0. Mathmatically, our goal\\nbecomes to learn a function which takes as input (c;c0;s0), and outputs the summary sforc, that\\nis,s=g(c;c0;s0). This motivates us to design our Retrieval-based Augmentation mechanism, as\\ndetailed below.\\nStep 1: Retrieving. For each sample (c;s)2D, we retrieve the most similar sample: (c0;s0) =\\nargmax(c0;s0)2D0sim(c;c0), wherec6=c0,D0is a given retrieval database and sim(c;c0)is the text\\nsimilarity. Following Zhang et al. (2020), we utilize Lucene for retrieval and calculate the similarity\\nscorezbetween the source code cand the retrieved code c0via dynamic programming (Bellman,\\n1966), namely, z= 1\\x00dis(c;c0)\\nmax(jcj;jc0j), wheredis(c;c0)is the text edit distance.\\nStep 2: Retrieved Code-based Augmentation. Given the retrieved source code c0for the current\\nsamplec, we adopt a fusion strategy to inject retrieved semantics into the current sample. The fusion\\nstrategy is based on their initial graph representations ( HcandHc0) with an attention mechanism:\\n•To capture the relevance between candc0, we design an attention function, which computes the\\nattention score matrix Aaugbased on the embeddings of each pair of nodes in CPGs of candc0:\\nAaug/exp(ReLU( HcWC)ReLU( Hc0WQ)T) (2)\\nwhere WC;WQ2Rd\\x02dis the weight matrix with d-dim embedding size and ReLU is the\\nrectiﬁed linear unit.\\n•We then multiply the attention matrix Aaugwith the retrieved representation Hc0to inject the\\nretrieved features into Hc:\\nH0\\nc=zAaugHc0 (3)\\nwherez2[0;1]is the similarity score and computed from Step 1, which is introduced to weaken\\nthe negative impact of c0on the original training data c,i.e., when the similarity of candc0is low.\\n• Finally, we merge H0\\ncand the original Hcto get the ﬁnal representation of c.\\ncomp =Hc+H0\\nc (4)\\n4Published as a conference paper at ICLR 2021\\nwhere comp is the augmented node representation additionally encoding the retrieved semantics.\\nStep 3: Retrieved Summary-based Augmentation. We further encode the retrieved summary s0\\nwith another BiLSTM model. We represent each token t0\\niofs0using the learned embedding matrix\\nEseqtoken. Thens0can be encoded as:\\nht0\\n1;:::;ht0\\nT= BiLSTM( Eseqtoken\\nt0\\n1;:::;Eseqtoken\\nt0\\nT) (5)\\nwhere ht0\\niis the hidden state of the BiLSTM model for the token t0\\niins0andTis the length of\\ns0. We multiply [ht0\\n1;:::;ht0\\nT]with the similarity score z, computed from Step 1, and concatenate\\nit with the graph encoding results (i.e., the GNN encoder outputs) to obtain the input, namely,\\n[GNN output ;zht0\\n1;:::;zht0\\nT], to the decoder.\\n2.3 A TTENTION -BASED DYNAMIC GRAPH\\nDue to that GNN-based encoders usually consider the k-hop neighborhood, the global relation among\\nnodes in the static graph (see Section 2.2.1) may be ignored. In order to better capture the global\\nsemantics of source code, based on the static graph, we propose to dynamically construct a graph via\\nstructure-aware global attention mechanism, which allows message passing among any pair of nodes.\\nThe attention-based dynamic graph can better capture the global dependency among nodes, and thus\\nsupplement the static graph.\\nStructure-aware Global Attention. The construction of the dynamic graph is motivated by the\\nstructure-aware self-attention mechanism proposed in Zhu et al. (2019). Given the static graph, we\\ncompute a corresponding dense adjacency matrix Adynbased on a structure-aware global attention\\nmechanism, and obtain the constructed graph, namely, attention-based dynamic graph .\\nAdyn\\nv;u=ReLU( hT\\nvWQ)(ReLU( hT\\nuWK) + ReLU( eT\\nv;uWR))T\\np\\nd(6)\\nwherehv;hu2comp are the augmented node embedding for any node pair (v;u)in the CPG. Note\\nthat the global attention considers each pair of nodes of the CPG, regardless of whether there is an\\nedge between them. ev;u2Rdeis the edge embedding and WQ;WK2Rd\\x02d,WR2Rde\\x02dare\\nparameter matrices, deanddare the dimensions of edge embedding and node embedding, respectively.\\nThe adjacency matrix Adynwill be further row normalized to obtain ~Adyn, which will be used to\\ncompute dynamic message passing (see Section 2.4).\\n~Adyn= softmax( Adyn) (7)\\n2.4 H YBRID GNN\\nTo better incorporate the information of the static graph and the dynamic graph, we propose the\\nHybrid Message Passing (HMP), which are performed on both retrieval-augmented static graph and\\nattention-based dynamic graph.\\nStatic Message Passing. For every node vat each computation hop kin the static graph, we apply\\nan aggregation function to calculate the aggregated vector hk\\nvby considering a set of neighboring\\nnode embeddings computed from the previous hop.\\nhk\\nv= SUM(fhk\\x001\\nuj8u2N (v)g) (8)\\nwhereN(v)is a set of the neighboring nodes which are directly connected with v. For each node v,\\nh0\\nvis the initial augmented node embedding of v,i.e.,hv2comp .\\nDynamic Message Passing. The node information and edge information are propagated on the\\nattention-based dynamic graph with the adjacency matrices ~Adyn, deﬁned as\\nh0k\\nv=X\\nu~Adyn\\nv;u(WVh0k\\x001\\nu+WFev;u)(9)\\nwherevanduare any pair of nodes, WV2Rd\\x02d,WF2Rd\\x02deare learned matrices, and ev;u\\nis the embedding of the edge connecting vandu. Similarly, h00\\nvis the initial augmented node\\nembedding of vincomp .\\n5Published as a conference paper at ICLR 2021\\nHybrid Message Passing. Given the static/dynamic aggregated vectors hk\\nv=h0k\\nvfor static and\\ndynamic graphs, we fuse both vectors and feed the resulting vector to a Gated Recurrent Unit (GRU)\\nto update node representations.\\nfk\\nv= GRU( fk\\x001\\nv;Fuse(hk\\nv;h0k\\nv)) (10)\\nwhere f0\\nvis the augmented node initialization in comp . The fusion function Fuse is designed as a\\ngated sum of two inputs.\\nFuse(a;b) =z\\x0ca+ (1\\x00z)\\x0cb z =\\x1b(Wz[a;b;a\\x0cb;a\\x00b] +bz) (11)\\nwhere Wzandbzare learnable weight matrix and vector, \\x0cis the component-wise multiplication, \\x1b\\nis a sigmoid function and zis a gating vector. After nhops of GNN computation, we obtain the ﬁnal\\nnode representation fn\\nvand then apply max-pooling over all nodes ffn\\nvj8v2Vg to get the graph\\nrepresentation.\\n2.5 D ECODER\\nThe decoder is similar with other state-of-the-art Seq2seq models (Bahdanau et al., 2014; Luong\\net al., 2015) where an attention-based LSTM decoder is used. The decoder takes the input of the\\nconcatenation of the node representation and the representation of the retrieved summary s0, namely,\\n[fn\\nv1;:::;fn\\nvm;zht0\\n1;:::;zht0\\nT], wheremis the number of nodes in the input CPG graph. The initial\\nhidden state of the decoder is the fusion (Eq. 11) of the graph representation and the weighted (i.e.,\\nmultiply similarity score z) ﬁnal state of the retrieved summary BiLSTM encoder.\\nWe train the model with the cross-entropy loss, deﬁned as L=P\\nt\\x00logP(s\\x03\\ntjc;s\\x03\\n<t), wheres\\x03\\ntis\\nthe word at the t-th position of the ground-truth output and cis the source code of the function. To\\nalleviate the exposure bias, we utilize schedule teacher forcing (Bengio et al., 2015). During the\\ninference, we use beam search to generate ﬁnal results.\\n3 E XPERIMENTS\\n3.1 S ETUP\\nWe evaluate our proposed framework against a number of state-of-the-art methods. Speciﬁcally,\\nwe classify the selected baseline methods into three groups: 1) Retrieval-based approaches: TF-\\nIDF (Haiduc et al., 2010) and NNGen (Liu et al., 2018), 2) Sequence-based approaches: CODE-\\nNN (Iyer et al., 2016; Barone & Sennrich, 2017), Transformer (Ahmad et al., 2020), Hybrid-\\nDRL (Wan et al., 2018), Rencos (Zhang et al., 2020) and Dual model (Wei et al., 2019), 3) Graph-\\nbased approaches: SeqGNN (Fernandes et al., 2018). In addition, we implemented two another\\ngraph-based baselines: GCN2Seq and GAT2Seq, which respectively adopt the Graph Convolution\\n(Kipf & Welling, 2016) and Graph Attention (Velickovic et al., 2018) as the encoder and a LSTM as\\nthe decoder for generating summaries. Note that Rencos (Zhang et al., 2020) combines the retrieval\\ninformation into Seq2Seq model, we classify it into Sequence-based approaches. More detailed\\ndescription about baselines and the conﬁguration of HGNN can be found in the Appendix B and C.\\nExisting benchmarks (Barone & Sennrich, 2017; Hu et al., 2018b) are all based on high-level\\nprogramming language i.e., Java, Python. Furthermore, they have been conﬁrmed to have extensive\\nduplication, making the model overﬁt to the training data that overlapped with the testset (Fernandes\\net al., 2018; Allamanis, 2019). We are the ﬁrst to explore neural summarization on Cprogramming\\nlanguage, and make our CCode Summarization Dataset (CCSD) public to beneﬁt academia and\\nindustry. We crawled from popular Crepositories on GitHub and extracted function-summary pairs\\nbased on the documents of functions. After a deduplication process, we kept 95k+ unique function-\\nsummary pairs. To further test the model generalization ability, we construct in-domain functions\\nand out-of-domain functions by dividing the projects into two sets, denoted as aandb. For each\\nproject ina, we randomly select some of the functions in this project as the training data and the\\nunselected functions are the in-domain validation/test data. All functions in projects bare regarded as\\nout-of-domain test data. Finally, we obtain 84,316 training functions, 4,432 in-domain validation\\nfunctions, 4,203 in-domain test functions and 2,330 out-of-domain test functions. For the retrieval\\naugmentation, we use the training set as the retrieval database, i.e.,D0=D(see Step 1 in Section\\n2.2.2). For more details about data processing, please refer to Appendix D.\\n6Published as a conference paper at ICLR 2021\\nTable 1: Automatic evaluation results (in %) on the CCSD test set.\\nIn-domain Out-of-domain OverallMethodsBLEU-4 ROUGE-L METEOR BLEU-4 ROUGE-L METEOR BLEU-4 ROUGE-L METEOR\\nTF-IDF 15.20 27.98 13.74 5.50 15.37 6.84 12.19 23.49 11.43\\nNNGen 15.97 28.14 13.82 5.74 16.33 7.18 12.76 23.93 11.58\\nCODE-NN 10.08 26.17 11.33 3.86 15.25 6.19 8.24 22.28 9.61\\nHybrid-DRL 9.29 30.00 12.47 6.30 24.19 10.30 8.42 28.64 11.73\\nTransformer 12.91 28.04 13.83 5.75 18.62 9.89 10.69 24.65 12.02\\nDual Model 11.49 29.20 13.24 5.25 21.31 9.14 9.61 26.40 11.87\\nRencos 14.80 31.41 14.64 7.54 23.12 10.35 12.59 28.45 13.21\\nGCN2Seq 9.79 26.59 11.65 4.06 18.96 7.76 7.91 23.67 10.23\\nGAT2Seq 10.52 26.17 11.88 3.80 16.94 6.73 8.29 22.63 10.00\\nSeqGNN 10.51 29.84 13.14 4.94 20.80 9.50 8.87 26.34 11.93\\nHGNN w/o augment & static 11.75 29.59 13.86 5.57 22.14 9.41 9.98 26.94 12.05\\nHGNN w/o augment & dynamic 11.85 29.51 13.54 5.45 21.89 9.59 9.93 26.80 12.21\\nHGNN w/o augment 12.33 29.99 13.78 5.45 22.07 9.46 10.26 27.17 12.32\\nHGNN w/o static 15.93 33.67 15.67 7.72 24.69 10.63 13.44 30.47 13.98\\nHGNN w/o dynamic 15.77 33.84 15.67 7.64 24.72 10.73 13.31 30.59 14.01\\nHGNN 16.72 34.29 16.25 7.85 24.74 11.05 14.01 30.89 14.50\\nSimilar to previous works (Zhang et al., 2020; Wan et al., 2018; Fernandes et al., 2018; Iyer et al.,\\n2016), BLEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005) and ROUGE-L (Lin, 2004)\\nare used as our automatic evaluation metrics. These metrics are popular for evaluating machine\\ntranslation and text summarization tasks. Except for these automatic metrics, we also conduct a\\nhuman evaluation study. We invite 5 PhD students and 10 master students as volunteers, who have\\nrich C programming experiences. The volunteers are asked to rank summaries generated from\\nthe anonymized approaches from 1 to 5 ( i.e., 1: Poor, 2: Marginal, 3: Acceptable, 4: Good, 5:\\nExcellent) based on the relevance of the generated summary to the source code and the degree of\\nsimilarity between the generated summary and the actual summary. Speciﬁcally, we randomly choose\\n50 functions for each model with the corresponding generated summaries and ground-truths. We\\ncalculate the average score and the higher the score, the better the quality.\\n3.2 C OMPARISON WITH THE BASELINES\\nTable 1 shows the evaluation results including two parts: the comparison with baselines and the\\nablation study. Consider the comparison with state-of-the-art baselines, in general, we ﬁnd that\\nour proposed model outperforms existing methods by a signiﬁcant margin on both in-domain and\\nout-of-domain datasets, and shows good generalization performance. Compared with others, on\\nin-domain dataset, the retrieval-based approaches could achieve competitive performance on BLEU-4,\\nhowever ROUGE-L and METEOR are fare less than ours. Moreover, they do not perform well on\\nthe out-of-domain dataset. Compared with the graph-based approaches ( i.e., GCN2Seq, GAT2Seq\\nand SeqGNN), even without augmentation ( HGNN w/o augment ), our approach still outperforms\\nthem, which further demonstrates the effectiveness of Hybrid GNN for additionally capturing global\\ngraph information. Compared with Rencos that also considers the retrieved information in the\\nSeq2Seq model, its performance is still lower than HGNN . On the overall dataset including both\\nof in-domain and out-of-domain data, our model achieves 14.01 ,30.89 and14.50 , outperforming\\ncurrent state-of-the-art method Rencos by 1.42,2.44 and1.29 in terms of BLEU-4, ROUGE-L and\\nMETEOR metrics.\\n3.3 A BLATION STUDY\\nWe also conduct an ablation study to evaluate the impact of different components of our framework,\\ne.g., retrieval-based augmentation, static graph and dynamic graph in the last row of Table 1. Overall,\\nwe found that 1) retrieval-augmented mechanism could contribute to the overall model performance\\n(HGNN vs.HGNN w/o augment ). Compared with HGNN , we see that the performance of HGNN w/o\\nstatic andHGNN w/o dynamic decreases, which demonstrates the effectiveness of the Hybrid GNN\\nand 2) the performance without static graph is worse than the performance without dynamic graph in\\nROUGE-L and METEOR, however, BLEU-4 is higher than the performance without dynamic graph.\\nTo further understand the impact of the static graph and dynamic graph, we evaluate the performance\\nwithout augmentation and static graph/dynamic graph (see HGNN w/o augment& static andHGNN\\nw/o augment& dynamic ). Compared with HGNN w/o augment , the results further conﬁrm the\\neffectiveness of the Hybrid GNN ( i.e., static graph and dynamic graph).\\n7Published as a conference paper at ICLR 2021\\nTable 2: Human evaluation results on the CCSD test set.\\nMetrics NNGen Transformer Rencos SeqGNN HGNN\\nRelevance 3.23 3.17 3.48 3.09 3.69\\nSimilarity 3.18 3.02 3.32 3.06 3.51\\nTable 3: Examples of generated summaries on the CCSD test set.\\nExample Example 1 Example 2\\nSource Codestatic void strInit(Str *p){\\np->z = 0;\\np->nAlloc = 0;\\np->nUsed = 0;\\n}void ReleaseCedar(CEDAR *c){\\nif (c == NULL)\\nreturn;\\nif (Release(c->ref) == 0)\\nCleanupCedar(c);\\n}\\nGround-Truth initialize a str object release reference of the cedar\\nNNGen free the string release the virtual host\\nTransformer reset a string release of the cancel object\\nRencos append a raw string to the json string release of the cancel object\\nSeqGNN initialize the string release cedar communication mode\\nHGNN initialize a string object release reference of cedar\\nWe also conduct experiments to investigate the impact of code-based augmentation and summary-\\nbased augmentation. Overall, we found that the summary-based augmentation could contribute more\\nthan the code-based augmentation. For example, after adding the code-based augmentation, the\\nperformance can be 10.22, 27.54 and 12.49 in terms of BLUE-4, ROUGE-L and METEOR on the\\noverall dataset. With the summary-based augmentation, the results can reach to 13.76, 30.59 and\\n14.11. Compared with the results without augmentation ( i.e., 10.26. 27.17, 12.32 with HGNN w/o\\naugment ), we can see that code-based augmentation could have some improvement, but the effect\\nis not signiﬁcant compared with summary-based augmentation. We conjecture that, due to that the\\ncode and summary are heterogeneous, the summary-based augmentation has a more direct impact on\\nthe code summarization task. When combining both code-based augmentation and summary-based\\naugmentation, we can achieve the best results ( i.e., 14.01, 30.89, 14.50). We plan to explore more\\ncode-based augmentation ( e.g., semantic-equivalent code transformation) in our future work.\\n3.4 H UMAN EVALUATION\\nAs shown in Table 2, we perform a human evaluation on the overall dataset to assess the quality\\nof the generated summaries by our approach, NNGen, Transformer, Rencos and SeqGNN in terms\\nof relevance and similarity. As depicted in Table 1, NNGen, Rencos and SeqGNN are the best\\nretrieval-based, sequence-based, and graph-based approaches, respectively. We also compare with\\nTransformer as it has been widely used in natural language processing. The results show that our\\nmethod can generate better summaries which are more relevant with the source code and more similar\\nwith the ground-truth summaries.\\n3.5 C ASE STUDY\\nTo perform qualitative analysis, we present two examples with generated summaries by different\\nmethods from the overall data set, shown in Table 3. We can see that, in the ﬁrst example, our\\napproach can learn more code semantics, i.e.,pis a self-deﬁned struct variable. Thus, we could\\ngenerate a token object for the variable p. However, other models can only produce string . Example 2\\nis a more difﬁcult function with the functionality to “release reference of cedar”, as compared to other\\nbaselines, our approach effectively captures the functionality and generates a more precise summary.\\n3.6 E XTENSION ON THE PYTHON DATASET\\nWe conducted additional experiments on a public dataset, i.e., the Python Code Summarization\\nDataset (PCSD), which was also used in Rencos (the most competitive baseline in our paper). We\\nfollow the setting of Rencos and split PCSD into the training set, validation set and testing set with\\nfractions of 60%, 20% and 20%. We construct the static graph based on AST. The decoding step\\nis set to 50, followed by Rencos, and the other settings are the same with CCSD. We compare our\\nmethods on PCSD against various competitive baselines, i.e., NNGen, CODE-NN, Rencos and\\n8Published as a conference paper at ICLR 2021\\nTable 4: Automatic evaluation results (in %) on the PCSD test set.\\nMethods BLEU-4 ROUGE-L METEOR\\nNNGen 21.60 31.61 15.96\\nCODE-NN 16.39 28.99 13.68\\nTransformer 17.06 31.16 14.37\\nRencos 24.02 36.21 18.07\\nHGNN w/o static 24.06 38.28 18.66\\nHGNN w/o dynamic 24.13 38.64 18.93\\nHGNN 24.42 39.91 19.48\\nTransformer, which are either retrieval-based, generation-based or hybrid methods. The results are\\nshown in Table 4. The results indicate that, compared with the best results from NNGen, CODE-NN,\\nRencos and Transformer, our method can improve the performance by 0.40, 3.70 and 1.41 in terms of\\nBLEU-4, ROUGE-L and METEOR. We also conduct the ablation study on PCSD to demonstrate\\nthe usefulness of the static graph ( i.e., HGNN w/o dynamic) and dynamic graph ( i.e., HGNN w/o\\nstatic). The results also demonstrate that both static graph and dynamic graph can contribute to our\\nframework. In summary, the results on both our released benchmark (CCSD) and existing benchmark\\n(PCSD) demonstrate the effectiveness and the scalability of our method.\\n4 R ELATED WORK\\nSource Code Summarization Early works (Eddy et al., 2013; Haiduc et al., 2010; Wong et al., 2015;\\n2013) for code summarization focused on using information retrieval to retrieve summaries. Later\\nworks attempted to employ attentional Seq2Seq model on the source code (Iyer et al., 2016; Siow\\net al., 2020) or some variants, i.e., AST (Hu et al., 2018a; Alon et al., 2018; Liu et al., 2020) for\\ngeneration. However, these works are based on sequential models, ignoring rich code semantics.\\nSome latest attempts (LeClair et al., 2020; Fernandes et al., 2018) embedded program semantics into\\nGNNs. but they mainly rely on simple representations, which are limited to learn full semantics.\\nGraph Neural Networks Over the past few years, GNNs (Li et al., 2015; Hamilton et al., 2017;\\nKipf & Welling, 2016; Chen et al., 2020b) have attracted increasing attention with many successful\\napplications in computer vision (Norcliffe-Brown et al., 2018), natural language processing (Xu et al.,\\n2018a; Chen et al., 2020d;c;e). Because by design GNNs can model graph-structured data, recently,\\nsome works have extended the widely used Seq2Seq architectures to Graph2Seq architectures for\\nvarious tasks including machine translation (Beck et al., 2018), and graph (e.g., AMR, SQL)-to-text\\ngeneration (Zhu et al., 2019; Xu et al., 2018b). Some works have also attempted to encode programs\\nwith graphs for diverse tasks e.g., V ARNAMING/V ARMISUSE (Allamanis et al., 2017), Source\\nCode Vulnerability Detection (Zhou et al., 2019). As compared to these works, we innovate a hybrid\\nmessage passing GNN performed on both static graph and dynamic graph for message fusion.\\n5 C ONCLUSION AND FUTURE WORK\\nIn this paper, we proposed a general-purpose framework for automatic code summarization. A novel\\nretrieval-augmented mechanism is proposed for combining the beneﬁts of both retrieval-based and\\ngeneration-based approaches. Moreover, to capture global semantics among nodes, we develop a\\nhybrid message passing GNN based on both static and dynamic graphs. The evaluation shows that\\nour approach improves state-of-the-art techniques substantially. Our future work includes: 1) we plan\\nto introduce more information such as API knowledge to learn the better semantics of programs, 2)\\nwe explore more code-based augmentation techniques to improve the performance and 3) we plan to\\nadopt the existing techniques such as (Du et al., 2019; Xie et al., 2019a;b; Ma et al., 2018) to evaluate\\nthe robustness of the trained model.\\n6 A CKNOWLEDGEMENT\\nThis research is supported by the National Research Foundation, Singapore under its AI Singapore\\nProgramme (AISG Award No: AISG2-RP-2020-019), the National Research Foundation under its\\nNational Cybersecurity R&D Program (Award No. NRF2018NCR-NCR005-0001), the Singapore\\nNational Research Foundation under NCR Award Number NRF2018NCR-NSOE003-0001 and NRF\\nInvestigatorship NRFI06-2020-0022.\\n9Published as a conference paper at ICLR 2021\\nREFERENCES\\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. A transformer-based\\napproach for source code summarization. arXiv preprint arXiv:2005.00653 , 2020.\\nMiltiadis Allamanis. The adverse effects of code duplication in machine learning models of code. In\\nProceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms,\\nand Reﬂections on Programming and Software , pp. 143–153, 2019.\\nMiltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs\\nwith graphs. arXiv preprint arXiv:1711.00740 , 2017.\\nMiltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. A survey of machine\\nlearning for big code and naturalness. ACM Computing Surveys (CSUR) , 51(4):1–37, 2018.\\nUri Alon, Shaked Brody, Omer Levy, and Eran Yahav. code2seq: Generating sequences from\\nstructured representations of code. arXiv preprint arXiv:1808.01400 , 2018.\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.\\nSatanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved\\ncorrelation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic\\nevaluation measures for machine translation and/or summarization , pp. 65–72, 2005.\\nAntonio Valerio Miceli Barone and Rico Sennrich. A parallel corpus of python functions and\\ndocumentation strings for automated code documentation and code generation. arXiv preprint\\narXiv:1707.02275 , 2017.\\nDaniel Beck, Gholamreza Haffari, and Trevor Cohn. Graph-to-sequence learning using gated graph\\nneural networks. arXiv preprint arXiv:1806.09835 , 2018.\\nRichard Bellman. Dynamic programming. Science , 153(3731):34–37, 1966.\\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence\\nprediction with recurrent neural networks. In Advances in Neural Information Processing Systems ,\\npp. 1171–1179, 2015.\\nDeli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-\\nsmoothing problem for graph neural networks from the topological view. In AAAI , pp. 3438–3445,\\n2020a.\\nYu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural networks:\\nBetter and robust node embeddings. Advances in Neural Information Processing Systems , 33,\\n2020b.\\nYu Chen, Lingfei Wu, and Mohammed J. Zaki. Graphﬂow: Exploiting conversation ﬂow with\\ngraph neural networks for conversational machine comprehension. In Proceedings of the Twenty-\\nNinth International Joint Conference on Artiﬁcial Intelligence , pp. 1230–1236. International Joint\\nConferences on Artiﬁcial Intelligence Organization, 2020c.\\nYu Chen, Lingfei Wu, and Mohammed J. Zaki. Reinforcement learning based graph-to-sequence\\nmodel for natural question generation. In Proceedings of the 8th International Conference on\\nLearning Representations , Apr. 26-30, 2020d.\\nYu Chen, Lingfei Wu, and Mohammed J Zaki. Toward subgraph guided knowledge graph question\\ngeneration with graph neural networks. arXiv preprint arXiv:2004.06015 , 2020e.\\nXiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun Zhao. Deepstellar: Model-based\\nquantitative analysis of stateful deep learning systems. In Proceedings of the 2019 27th ACM Joint\\nMeeting on European Software Engineering Conference and Symposium on the Foundations of\\nSoftware Engineering , pp. 477–487, 2019.\\n10Published as a conference paper at ICLR 2021\\nBrian P Eddy, Jeffrey A Robinson, Nicholas A Kraft, and Jeffrey C Carver. Evaluating source code\\nsummarization techniques: Replication and expansion. In 2013 21st International Conference on\\nProgram Comprehension (ICPC) , pp. 13–22. IEEE, 2013.\\nPatrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. Structured neural summarization.\\narXiv preprint arXiv:1811.01824 , 2018.\\nSonia Haiduc, Jairo Aponte, Laura Moreno, and Andrian Marcus. On the use of automated text\\nsummarization techniques for summarizing source code. In 2010 17th Working Conference on\\nReverse Engineering , pp. 35–44. IEEE, 2010.\\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In\\nAdvances in neural information processing systems , pp. 1024–1034, 2017.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\\npp. 770–778, 2016.\\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):\\n1735–1780, 1997.\\nXing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. Deep code comment generation. In Proceedings of\\nthe 26th Conference on Program Comprehension , pp. 200–210, 2018a.\\nXing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. Summarizing source code with transferred\\napi knowledge. 2018b.\\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Summarizing source code\\nusing a neural attention model. In Proceedings of the 54th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers) , pp. 2073–2083, 2016.\\nSiyuan Jiang, Ameer Armaly, and Collin McMillan. Automatically generating commit messages\\nfrom diffs using neural machine translation. In 2017 32nd IEEE/ACM International Conference on\\nAutomated Software Engineering (ASE) , pp. 135–146. IEEE, 2017.\\nToshihiro Kamiya, Shinji Kusumoto, and Katsuro Inoue. Ccﬁnder: a multilinguistic token-based code\\nclone detection system for large scale source code. IEEE Transactions on Software Engineering ,\\n28(7):654–670, 2002.\\nThomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.\\narXiv preprint arXiv:1609.02907 , 2016.\\nAlexander LeClair, Sakib Haque, Linfgei Wu, and Collin McMillan. Improved code summarization\\nvia a graph neural network. arXiv preprint arXiv:2004.02843 , 2020.\\nGuohao Li, Matthias Müller, Ali K. Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep\\nas cnns? In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul,\\nKorea (South), October 27 - November 2, 2019 , pp. 9266–9275. IEEE, 2019. doi: 10.1109/ICCV .\\n2019.00936. URL https://doi.org/10.1109/ICCV.2019.00936 .\\nYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural\\nnetworks. arXiv preprint arXiv:1511.05493 , 2015.\\nZhenmin Li, Shan Lu, Suvda Myagmar, and Yuanyuan Zhou. Cp-miner: Finding copy-paste and\\nrelated bugs in large-scale software code. IEEE Transactions on software Engineering , 32(3):\\n176–192, 2006.\\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\\nBranches Out , pp. 74–81. Association for Computational Linguistics, July 2004.\\nShangqing Liu, Cuiyun Gao, Sen Chen, Nie Lun Yiu, and Yang Liu. Atom: Commit message\\ngeneration based on abstract syntax tree and hybrid ranking. IEEE Transactions on Software\\nEngineering , 2020.\\n11Published as a conference paper at ICLR 2021\\nZhongxin Liu, Xin Xia, Ahmed E Hassan, David Lo, Zhenchang Xing, and Xinyu Wang. Neural-\\nmachine-translation-based commit message generation: how far are we? In Proceedings of the\\n33rd ACM/IEEE International Conference on Automated Software Engineering , pp. 373–384,\\n2018.\\nMinh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based\\nneural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\nLei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su,\\nLi Li, Yang Liu, et al. Deepgauge: Multi-granularity testing criteria for deep learning systems. In\\nProceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering ,\\npp. 120–131, 2018.\\nWill Norcliffe-Brown, Stathis Vafeias, and Sarah Parisot. Learning conditioned graph structures for\\ninterpretable visual question answering. In Advances in Neural Information Processing Systems ,\\npp. 8344–8353, 2018.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\\nevaluation of machine translation. In Proceedings of the 40th annual meeting on association for\\ncomputational linguistics , pp. 311–318. Association for Computational Linguistics, 2002.\\nJing Kai Siow, Cuiyun Gao, Lingling Fan, Sen Chen, and Yang Liu. Core: Automating review\\nrecommendation for code changes. In 2020 IEEE 27th International Conference on Software\\nAnalysis, Evolution and Reengineering (SANER) , pp. 284–295. IEEE, 2020.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\\nprocessing systems , pp. 5998–6008, 2017.\\nPetar Velickovic, Guillem Cucurull, A. Casanova, A. Romero, P. Liò, and Yoshua Bengio. Graph\\nattention networks. ArXiv , abs/1710.10903, 2018.\\nYao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S Yu. Improving\\nautomatic source code summarization via deep reinforcement learning. In Proceedings of the 33rd\\nACM/IEEE International Conference on Automated Software Engineering , pp. 397–407, 2018.\\nBolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. Code generation as a dual task of code summariza-\\ntion. In Advances in Neural Information Processing Systems , pp. 6563–6573, 2019.\\nEdmund Wong, Jinqiu Yang, and Lin Tan. Autocomment: Mining question and answer sites for\\nautomatic comment generation. In 2013 28th IEEE/ACM International Conference on Automated\\nSoftware Engineering (ASE) , pp. 562–567. IEEE, 2013.\\nEdmund Wong, Taiyue Liu, and Lin Tan. Clocom: Mining existing source code for automatic\\ncomment generation. In 2015 IEEE 22nd International Conference on Software Analysis, Evolution,\\nand Reengineering (SANER) , pp. 380–389. IEEE, 2015.\\nXiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jianjun Zhao, Bo Li,\\nJianxiong Yin, and Simon See. Deephunter: a coverage-guided fuzz testing framework for deep\\nneural networks. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software\\nTesting and Analysis , pp. 146–157, 2019a.\\nXiaofei Xie, Lei Ma, Haijun Wang, Yuekang Li, Yang Liu, and Xiaohong Li. Diffchaser: Detecting\\ndisagreements for deep neural networks. In IJCAI , pp. 5772–5778, 2019b.\\nKun Xu, Lingfei Wu, Zhiguo Wang, and Vadim Sheinin. Graph2seq: Graph to sequence learning\\nwith attention-based neural networks. arXiv preprint arXiv:1804.00823 , 2018a.\\nKun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Liwei Chen, and Vadim Sheinin. Sql-to-text generation\\nwith graph-to-sequence model. arXiv preprint arXiv:1809.05255 , 2018b.\\nFabian Yamaguchi, Nico Golde, Daniel Arp, and Konrad Rieck. Modeling and discovering vulnera-\\nbilities with code property graphs. In 2014 IEEE Symposium on Security and Privacy , pp. 590–604.\\nIEEE, 2014.\\n12Published as a conference paper at ICLR 2021\\nJian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. Retrieval-based neural\\nsource code summarization. In Proceedings of the 42nd International Conference on Software\\nEngineering. IEEE , 2020.\\nLingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. arXiv preprint\\narXiv:1909.12223 , 2019.\\nYaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. Devign: Effective vulnera-\\nbility identiﬁcation by learning comprehensive program semantics via graph neural networks. In\\nAdvances in Neural Information Processing Systems , pp. 10197–10207, 2019.\\nJie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. Modeling graph\\nstructure in transformer for better amr-to-text generation. arXiv preprint arXiv:1909.00136 , 2019.\\n13Published as a conference paper at ICLR 2021\\nAppendices\\nA D ETAILS ON CODE PROPERTY GRAPH\\nCode Property Graph (CPG) (Yamaguchi et al., 2014), which is constructed on abstract syntax tree\\n(AST), combines different edges (i.e., Flow to, Control) to represent the semantics of the program.\\nWe describe each representation combining with Figure 2 as follows:\\n•Abstract Syntax Tree (AST). AST contains syntactic information for a program and omits irrele-\\nvant details that have no effect on the semantics. Figure 2 shows the completed AST nodes on the\\nleft simple program and each node has a code sequence in the ﬁrst line and type attribute in the\\nsecond line. The black arrows represent the child-parent relations among ASTs.\\n•Control Flow Graph (CFG). Compared with AST highlighting the syntactic structure, CFG\\ndisplays statement execution order, i.e., the possible order in which statements may be executed\\nand the conditions that must be met for this to happen. Each statement in the program is treated as\\nan independent node as well as a designated entry and exit node. Based on the keywords if,for,\\ngoto,break andcontinue , control ﬂow graphs can be easily built and “Flow to” with green dashed\\narrows in Figure 2 represents this ﬂow order.\\n•Program Dependency Graph (PDG). PDG includes data dependencies andcontrol dependen-\\ncies: 1) data dependencies are described as the deﬁnition of a variable in a statement reaches the\\nusage of the same variable at another statement. In Figure 2, the variable “ b” is deﬁned in the\\nstatement “ int b = a++ ” and used in “ call (b) ”. Hence, there is a “Reach” edge with blue arrows\\npoint from “ int b = a++ ” to “ call (b) ”. Furthermore, Deﬁne/Use edge with orange double arrows\\ndenotes the deﬁnition and usage of the variable. 2) different from CFG displaying the execution\\nprocess of the complete program, control dependencies deﬁne the execution of a statement may be\\ndependent on the value of a predicate, which more focus on the statement itself. For instance, the\\nstatements “ int b = a++ ” and “ call(b) ” are only performed “if a is even”. Therefore, a red double\\narrow “Control” points from “ if (a % 2) == 0 ” to “ int b = a++ ” and “ call(b) ”.\\nB D ETAILS ON BASELINE METHODS\\nWe compare our approach with existing baselines. They can be divided into three groups: Retrieval-\\nbased approaches, Sequence-based approaches and Graph-based approaches. For papers that provide\\nthe source code, we directly reproduce their methods on CCSD dataset. Otherwise, we reimplement\\ntheir approaches with reference to the papers.\\nB.1 R ETRIEVAL -BASED APPROACHES\\nTF-IDF (Haiduc et al., 2010) is the abbreviation of Term Frequency-Inverse Document Frequency,\\nwhich is adopted in the early code summarization (Haiduc et al., 2010). It transforms programs\\ninto weight vectors by calculating term frequency and inverse document frequency. We retrieve the\\nsummary of the most similar programs by calculating the cosine similarity on the weighted vectors.\\nNNGen (Liu et al., 2018) is a retrieved-based approach to produce commit messages for code changes.\\nWe reproduce such an algorithm on code summarization. Speciﬁcally, we retrieve the most similar\\ntop-k code snippets on a bag-of-words model and prioritizes the summary in terms of BLEU-4 scores\\nin top-k code snippets.\\nB.2 S EQUENCE -BASED APPROACHES\\nCODE-NN (Iyer et al., 2016; Barone & Sennrich, 2017) adopts an attention-based Seq2Seq model\\nto generate summaries on the source code.\\nTransformer (Ahmad et al., 2020) adopts the transformer architecture (Vaswani et al., 2017) with\\nself-attention to capture long dependency in the code for source code summrization.\\n14Published as a conference paper at ICLR 2021\\nHybrid-DRL (Wan et al., 2018) is a reinforcement learning-based approach, which incorporates AST\\nand sequential code snippets into a deep reinforcement learning framework and employ evaluation\\nmetrics e.g., BLEU as the reward.\\nDual Model (Wei et al., 2019) propose a dual training framework by training code summarization\\nand code generation tasks simultaneously to boost each task performance.\\nRencos (Zhang et al., 2020) is the retrieval-based Seq2Seq model for code summarization. it utilized\\na pretrained Seq2Seq model during the testing phase by computing a joint probability conditioned on\\nboth the original source code and retrieved the most similar source code for the summary generation.\\nCompared with Rencos, we propose a novel retrieval-augmented mechanism for the similar source\\ncode and use it at the model training phase.\\nB.3 G RAPH -BASED APPROACHES\\nWe also compared with some latest GNN-based works, employing graph neural network for source\\ncode summarization.\\nGCN2Seq, GAT2Seq modify Graph Convolution Network (Kipf & Welling, 2016) and Graph\\nAttention Network (Velickovic et al., 2018) to perform convolution operation and attention operation\\non the code property graph for learning and followed by a LSTM to generate summaries. We\\nimplement the related code from scratch.\\nSeqGNN (Fernandes et al., 2018) combines GGNNs and standard sequence encoders for summa-\\nrization. They take the code and relationships between elements of the code as input. Specially, a\\nBiLSTM is employed on the code sequence to learn representations and each source code token is\\nmodelled as a node in the graph, and employed GGNN for graph-level learning. Since our node\\nsequences are sub-sequence of source code rather than individual token, we adjust to slice the output\\nof BiLSTM and sum each token representation in node sequences as node initial representation for\\nsummarization. Furthermore, we implement the related code from scratch.\\nC M ODEL SETTINGS\\nWe embed the most frequent 40,000 words in the training set with 512-dims and set the hidden size\\nof BiLSTM to 256 and the concatenated state size for both directions is 512. The dropout is set to 0.3\\nafter the word embedding layer and BiLSTM. We set GNN hops to 1 for the best performance. The\\noptimizer is selected with Adam with an initial learning rate of 0.001. The batch size is set to 64 and\\nearly stop for 10. The beam search width is set to 5 as usual. All experiments are conducted on the\\nDGX server with four Nvidia Graphics Tesla V100 and each epoch takes 6 minutes averagely. All\\nhyperparameters are tuned with grid search on the validation set.\\nD D ETAILS ON DATA PREPARATION\\nIt is non-trivial to obtain high-quality datasets for code summarization. We noticed that despite some\\nprevious works (Barone & Sennrich, 2017; Hu et al., 2018b) released their datasets, however, they\\nare all based on high-level programming languages i.e. Java, Python. We are the ﬁrst to explore\\nsummarization on Cprogramming language. Speciﬁcally, we crawled from popular Crepositories\\n(e.g., Linux and QEMU) on GitHub, and then extracted separate function-summary pairs from these\\nprojects. Speciﬁcally, we extracted functions and associated comments marked by special characters\\n\"/**\" and \"*/\" over the function declaration. These comments can be considered as explanations of\\nthe functions. We ﬁltered out functions with line exceeding 1000 and any other comments inside\\nthe function, and the ﬁrst sentence was selected as the summary. A similar practice can be found\\nin (Jiang et al., 2017). Totally, we collected 500k+ raw function-summary pairs. Furthermore,\\nfunctions with token size greater than 150 were removed for computational efﬁciency and there\\nwere 130k+ functions left. Since duplication is very common in existing datasets (Fernandes et al.,\\n2018), followed by Allamanis (2019), we performed a de-duplication process and removed functions\\nwith similarity over 80%. Speciﬁcally, we calculated the cosine similarity by encoding the raw\\nfunctions into vectors with sklearn. Finally, we kept 95k+ unique functions. We name this dataset C\\nCode Summarization Dataset (CCSD). To testify model generalization ability, we randomly selected\\n15Published as a conference paper at ICLR 2021\\nTable 5: More Examples of generated summaries on the CCSD test set.\\nExample Example 1 Example 2\\nSource Codestatic void counterMutexFree\\n(sqlite3_mutex *p){\\nassert(g.isInit);\\ng.m.xMutexFree(p->pReal);\\nif( p->eType==SQLITE_MUTEX_FAST\\n|| p->eType==\\nS QLITE_MUTEX_RECURSIVE)\\n{\\nfree(p);\\n}\\n}static void __exit wimax_subsys_exit(void)\\n{\\nwimax_id_table_release();\\ngenl_unregister_family\\n(&wimax_gnl_family);\\n}\\nGround-Truth free a countable mutex shutdown the wimax stack\\nNNGen enter a countable mutex unregisters pmcraid event family return value none\\nTransformer leave a mutex de initialize wimax driver\\nRencos try to enter a mutex unregister the wimax device subsystem\\nSeqGNN free a mutex allocated by sqlite3 mutex this function is called when the driver is not held\\nHGNN release a mutex free the wimax stack\\nRetrieved_codestatic int counterMutexTry\\n(sqlite3_mutex *p){\\nassert( g.isInit );\\nassert( p->eType>=0 );\\nassert( p->eType<MAX_MUTEXES );\\ng.aCounter[p->eType]++;\\nif( g.disableTry )\\nreturn SQLITE_BUSY;\\nreturn g.m.xMutexTry(p->pReal);\\n}static int __init wimax_subsys_init(void){\\nint result; d_fnstart(4, NULL, \"()\\\\n\");\\nd_parse_params(D_LEVEL, D_LEVEL_SIZE,\\nwimax_debug_params, \"wimax.debug\");\\nresult = genl_register_family\\n(&wimax_gnl_family);\\nif (unlikely(result < 0)) {\\npr_err(\"cannot register generic\\nnetlink family: %d\\\\n\", result);\\ngoto error_register_family;}\\nd_fnend(4, NULL, \"() = 0\\\\n\");\\nreturn 0;\\nerror_register_family:\\nd_fnend(4, NULL, \"() = %d\\\\n\", result);\\nreturn result;\\n}\\nRetrieved_summary try to enter a mutex shutdown the wimax stack\\nExample Example 3 Example 4\\nSource Codestatic void udc_dd_free(\\nstruct lpc32xx_udc *udc,\\nstruct lpc32xx_usbd_dd_gad *dd)\\n{\\ndma_pool_free(udc->dd_cache,\\ndd, dd->this_dma);\\n}void ReleaseSockEvent(SOCK_EVENT *event) {\\nif (event == NULL)\\n{\\nreturn;\\n}\\nif (Release(event->ref) == 0)\\n{\\nCleanupSockEvent(event);\\n}\\n}\\nGround-Truth free a dma descriptor release of the socket event\\nNNGen allocate a dma descriptor clean up of the socket event\\nTransformer free the usb device set the event\\nRencos allocate a dma descriptor set of the sock event\\nSeqGNN free dma buffers release of the socket\\nHGNN free a dma descriptor release the sock event\\nRetrieved_codestatic struct lpc32xx_usbd_dd_gad\\n*udc_dd_alloc(struct\\nlpc32xx_udc *udc) {\\ndma_addr_t dma;\\nstruct lpc32xx_usbd_dd_gad *dd;\\ndd = dma_pool_alloc(udc->dd_cache,\\nGFP_ATOMIC | GFP_DMA, &dma);\\nif (dd)\\ndd->this_dma = dma;\\nreturn dd;\\n}void SetL2TPServerSockEvent(\\nL2TP_SERVER *l2tp,SOCK_EVENT *e){\\nif (l2tp == NULL) {\\nreturn;}\\nif (e != NULL){\\nAddRef(e->ref);}\\nif (l2tp->SockEvent != NULL){\\nReleaseSockEvent(l2tp->SockEvent);\\nl2tp->SockEvent = NULL;}\\nl2tp->SockEvent = e;}\\nRetrieved_summary allocate a dma descriptor set a sock event to the l2tp server\\nsome projects as the out-of-domain test set with 2,330 examples and the remaining were randomly\\nsplit into train/validation/test with 84,316/4,432/4,203 examples. The open-source code analysis\\nplatform Joern (Yamaguchi et al., 2014) was applied to construct the code property graph.\\nE M ORE EXAMPLES\\nWe show more examples along with the retrieved code and summary by dynamic programming in\\nTable 5 and we can ﬁnd that HGNN can generate more high-quality summries based on our approach.\\n16',\n",
       " 'T-RAG: End-to-End Table Question Answering via Retrieval-Augmented\\nGeneration\\nFeifei Pan1, Mustafa Canim2, Michael Glass2, Alﬁo Gliozzo2, James Hendler1\\npanf2@rpi.edu ,mustafa@us.ibm.com ,\\nmrglass@us.ibm.com ,gliozzo@us.ibm.com\\nhendler@cs.rpi.edu\\n1Rensselaer Polytechnic Institute\\n2IBM TJ Watson Research Center\\nAbstract\\nMost existing end-to-end Table Question An-\\nswering (Table QA) models consist of a two-\\nstage framework with a retriever to select rel-\\nevant table candidates from a corpus and a\\nreader to locate the correct answers from table\\ncandidates. Even though the accuracy of the\\nreader models is signiﬁcantly improved with\\nthe recent transformer-based approaches, the\\noverall performance of such frameworks still\\nsuffers from the poor accuracy of using tradi-\\ntional information retrieval techniques as re-\\ntrievers. To alleviate this problem, we intro-\\nduce T-RAG, an end-to-end Table QA model,\\nwhere a non-parametric dense vector index is\\nﬁne-tuned jointly with BART, a parametric\\nsequence-to-sequence model to generate an-\\nswer tokens. Given any natural language ques-\\ntion, T-RAG utilizes a uniﬁed pipeline to auto-\\nmatically search through a table corpus to di-\\nrectly locate the correct answer from table cell.\\nWe apply T-RAG on recent open-domain Table\\nQA benchmarks and demonstrate that the ﬁne-\\ntuned T-RAG model is able to achieve state-\\nof-the-art performance in both the end-to-end\\nTable QA and the table retrieval tasks.\\n1 Introduction\\nTabular data is commonly seen in open-domain\\ndocuments (Cafarella et al., 2009; Zhang and Ba-\\nlog, 2018a), such as the Web and Wikipedia, as\\nwell as in domain-speciﬁc papers, journals, manu-\\nals, and reports. Answering questions over these\\ntables requires table retrieval and understanding\\nof the table structure and content. Table QA task\\nis generally more challenging than executing SQL\\nqueries over relational database tables due to the\\nlack of schema information. Most existing studies\\ntackle Table QA as two separate sub-tasks: (1) Ta-\\nble retrieval (Cafarella et al., 2008, 2009; Zhang\\nand Balog, 2018a; Shraga et al., 2020a,b), and (2)\\nQA over tables (Yu et al., 2018; Herzig et al., 2020;\\nYin et al., 2020; Glass et al., 2020). Recently, theDTR (Herzig et al., 2021) and the CLTR (Pan et al.,\\n2021) models have been proposed as end-to-end\\nsolutions for Table QA. Both models consist of a\\ntwo-step pipeline of a retriever to generate a set of\\ncandidate tables and a reader to answer questions\\nover these tables. The two components are trained\\nindividually, causing error propagation from re-\\ntrievers to readers, i.e. with incorrect table can-\\ndidates, it is impossible for the readers to locate\\nthe correct answer despite the design of the mod-\\nels. While dense retrieval and Retrieval Augmented\\nGeneration (RAG) (Karpukhin et al., 2020; Lewis\\net al., 2020b) have achieved great success in open-\\ndomain QA over free text, none of the studies in\\nthe literature leverage a non-parametric memory\\nmodel along with a parametric memory model for\\nthe open-domain Table QA task.\\nIn this paper, we describe a novel end-to-end\\nTable QA model, T-RAG, replacing the two-step\\nframework with a single training process. To\\ntrain T-RAG, we utilize Dense Passage Retrieval\\n(DPR) (Karpukhin et al., 2020) and RAG strate-\\ngies. Speciﬁcally, we jointly train a DPR compo-\\nnent (Glass et al., 2021) together with the BART-\\nbased (Lewis et al., 2020a) sequence-to-sequence\\n(Seq2Seq) model. To the best of our knowledge,\\nT-RAG is the ﬁrst Table QA model where the\\nquery encoder for a non-parametric dense vector\\nindex is ﬁne-tuned along with a parametric gen-\\neration model. We evaluate the performance of\\nT-RAG on NQ-TABLES (Herzig et al., 2021) and\\nE2E_WTQ (Pan et al., 2021), two recent end-to-\\nend Table QA benchmarks. The experimental re-\\nsults indicate that T-RAG outperforms the state-of-\\nthe-art models on the end-to-end Table QA task.\\nThe major contribution of this work is that, we\\npropose the ﬁrst end-to-end Table QA pipeline,\\nleveraging DPR along with the Seq2Seq compo-\\nnent of RAG. T-RAG employs a simple but effec-\\ntive one-step training that reduces error accumu-\\nlations and simpliﬁes model ﬁne-tuning. In thearXiv:2203.16714v1  [cs.CL]  30 Mar 2022experiments, T-RAG achieves state-of-the-art per-\\nformance on two tasks. We ﬁnd T-RAG improves\\nthe results for end-to-end Table QA on two recent\\nbenchmarks. The RAG component of the end-to-\\nend model ﬁne-tuned over Table QA benchmarks\\nalso yields state-of-the-art results on the table re-\\ntrieval task.\\n2 Related Work\\nTable Retrieval Traditional table retrieval mod-\\nels usually concatenate tables into documents\\nwhile disregarding the underlying tabular structure\\n(Pyreddy and Croft, 1997; Wang and Hu, 2002;\\nLiu et al., 2007; Cafarella et al., 2008, 2009). New\\napproaches are proposed to retrieve tables with a\\nset of features of the table, query and table-query\\npair (Zhang and Balog, 2018b; Sun et al., 2019;\\nBhagavatula et al., 2013; Shraga et al., 2020a).\\nZhang and Balog (2018b) uses semantic similari-\\nties to build an ad-hoc table retrieval model with\\nvarious features. A neural ranking model is in-\\ntroduced in Shraga et al. (2020b), where tables are\\ndeﬁned as multi-modal objects and the Gated Multi-\\nmodal Units are used to learn the representation of\\nquery-table pairs. Pan et al. (2021) later follows\\nthis work and improves the table retrieval with a\\n2-step retriever. Kosti ´c et al. (2021) discusses the\\nuse of dense vector embeddings to enhance the per-\\nformance of bi- and tri-encoder in retrieving both\\ntable and text.\\nTable QA Most early Table QA solutions are\\nfully supervised models, focusing on converting\\nnatural language questions into SQL format and\\nusing the SQL-format questions to query the given\\ntables, as seen in Yu et al. (2018); Lin et al. (2019);\\nXu et al. (2018). Open-domain QA over text (Yu\\net al., 2020) usually utilizes multiple knowledge\\nsources. For instance, Oguz et al. (2021) pro-\\nposes a model can convert structured, unstruc-\\ntured and semi-structured knowledge into text for\\nopen-domain QA. Therefore, more recent efforts\\nhave been put into investigating the use of exter-\\nnal knowledge in enhancing the performance of\\nTable QA. Jiménez-Ruiz et al. (2020) ﬁrst proposes\\nthe Semantic Web Challenge on Tabular Data to\\nKnowledge Graph Matching (SemTab) to encour-\\nage such solutions for both table understanding\\nand Table QA. Recently, the transformer-based,\\nweakly supervised solutions have been proposed\\nfor Table QA. These solutions fall into two cat-\\negories: (1) Logic form-based solution, such astheTABERT (Yin et al., 2020) model, which is\\ntrained to capture the representation of natural lan-\\nguage sentences as well as tabular data; (2) Answer\\ncell prediction solutions, such as TAPAS(Herzig\\net al., 2020) and the RCI (Glass et al., 2020) model.\\nThe current state-of-the-art RCI model exploits a\\ntransformer-based framework. Instead of retrieving\\nthe table cells directly for any given question-table\\npairs, the RCI model identiﬁes the most relevant\\ncolumns and rows independently and locates the\\nintersection table cells as the ﬁnal answers.\\nEnd-to-End Table QA Sun et al. (2016) pub-\\nlishes the ﬁrst end-to-end table cell search frame-\\nwork. This work leverages the semantic relations\\nbetween cells and maps queries to table cells with\\nrelational chains. The DTR model (Herzig et al.,\\n2021) addresses the end-to-end Table QA problem\\nwith a table retriever and a TAPAS-based reader\\nmodel. Later, the CLTR model (Pan et al., 2021)\\nintroduces a similar two-step solution, using BM25\\nas the retriever. The model re-ranks the BM25\\nresults and locates the table cells using the RCI\\nscores. Recently, Chen et al. (2021) proposes a\\nnew task for QA over both free text and tables\\nand provides a solution including a retriever with\\nearly fusion techniques and a cross-block reader.\\nIn addition, the open-domain OTT-QA benchmark\\nis released to evaluate models for end-to-end QA\\nover text and table.\\n3 The End-to-End Table QA with T-RAG\\nThe overall architecture of T-RAG is illustrated in\\nFigure 1. In this example, we encode the questions\\n“who was the editor for Ikar?” using the query en-\\ncoder and pre-process the tables, e.g., T 1and T 2,\\nfrom the table corpus for encoding. The encoded ta-\\nbles are later indexed into the Approximate Nearest\\nNeighbors (ANN) data structure for querying. The\\nencoded question is appended to each table before\\ninputting it to the BART-based generator for answer\\nprediction. The DPR and the RAG components are\\ntrained jointly without explicitly considering the\\ntable-level ground truth.\\nSetup We deﬁne the one-step, end-to-end Table\\nQA task as follows. Given a massive corpus C\\nof tables tiand any natural language question qi,\\nwe train a model to directly generate answer to qi\\nfrom the table cell without any intermediate steps.\\nLabeled datasets are available to us with ground\\ntruth samples in the format of fqi; ti; aigwhere aiANNIndexNatural LanguageQuestionWhowastheeditorforIkar?SeriesYears…EditorIvan Zourine1979…Magic-StripSteve Severin1981–2003…Glénat…………Ikar1995–1997…Glénat…………QueryEncoder\\nTableEncoderPierre Makyo at a book fair in Paris, France, in March 2008.BornJuly 16, 1952(age69)NationalityFrench……TableCorpus\\n…Generator!1!2AnswersGlénatCitroënMagic-Strip…DPRRAGSeries[sep]Ivan Zourine[sep]Title[sep]Year……Figure 1: An overview of T-RAG, a model trained end-to-end to directly locate answers from table corpus.\\nstands for the answers.\\nTable pre-processing is implemented before the\\ntraining. We process the tables tiinto a structure-\\npreserving format, where: (1) column headers are\\nappended before cell values, separated by a special\\nsymbol “ j”; and (2) the separator “*” is appended\\nto the end of each row; (3) for the tables with addi-\\ntional information such as titles, we append them\\nin front of the tables. The tables are segmented\\ninto the length of 512 tokens for training. For each\\nquestion, we retrieve hard negatives from the cor-\\npusCand use them as additional negative samples\\nto enhance the T-RAG training.\\nSoft Hard Negatives We implement a BM25-\\nbased hard negative mining for T-RAG. For each\\nquestion, we ﬁrst retrieve a pool of the most rel-\\nevant tables from the corpus using BM25. From\\nthe table pool, we discard the ground truth table.\\nThe top-ranked, non-positive tables are used as the\\nhard negative candidates. In the training process,\\ninstead of using the top 1 negative table, we exploit\\na soft hard negative mechanism, where we select\\nthe hard negative at random from the top knegative\\ntables.\\nRAG For the implementation of RAG, we jointly\\ntrain a DPR-based retriever and a BART-based gen-\\nerator. We index the tables in Cusing a keyword-\\nbased search engine, Anserini1, to harvest the hard\\nnegative training samples using BM25. Later, T-\\nRAG exploits BERT BASE to encode questions\\nalong with the ground truth table and the hard neg-\\native tables. To train RAG, T-RAG employs the\\nanswer-level ground truth and use a Seq2Seq gen-\\n1https://github.com/castorini/anserinierator, the BART LARGE model, for answer predic-\\ntions. The previously encoded tables are indexed\\nwith the open-source FAISS (Johnson et al., 2017)\\nlibrary into the ANN data structure for querying.\\nThe encoded questions are concatenated to each\\nof the top retrieved tables and used as a prompt to\\ngenerate the answer. More concretely, the gener-\\nator predicts probability distributions for possible\\nanswer candidates as the next token. The probabil-\\nity distributions are later marginalized to produce\\na single weighted sequence probability for each\\nanswer candidate. Finally, a standard beam search\\ndecoder (Sutskever et al., 2014) is used to identify\\nthe most relevant candidates as the ﬁnal answers to\\nthe questions at test time. Along with the answers,\\nour model can also return the relevant table ticon-\\ntaining the correct answers from Cfor evaluation\\nand annotation purposes.\\n4 Experiments\\nData We validate T-RAG on two open-domain\\nbenchmarks, NQ-TABLES andE2E_WTQ . NQ-\\nTABLES is the table subset of the Natural Ques-\\ntions dataset (Kwiatkowski et al., 2019), with a\\ntable corpus extracted from the English Wikipedia\\narticles and samples in the fq; T; a gformat, where\\nq,T, and adenote question, ground truth table,\\nand answer, respectively. E2E_WTQ contains the\\nlook-up subset of WikiTableQuestions (Pasupat\\nand Liang, 2015). While a substantial amount of\\ntables in NQ-TABLES are transposed infobox ta-\\nbles, the E2E_WTQ only contains well-formatted\\nbut more complex tables. The data statistics are\\nshown in Table 1.Data Train Dev Test Corpus\\nNQ-TABLES 9,594 1,068 966 169,898\\nE2E_WTQ 851 124 241 2,108\\nTable 1: Data Statistics\\nModel EM F1 Oracle EM Oracle F1\\nDTR+hn 37.69 47.70 48.20 61.50\\nT-RAG 43.06 50.92 50.62 63.18\\n(a) End-to-end Table QA results on the test set of NQ-TABLES.\\nModel MRR Hit@1\\nCLTR 0.5503 0.4675\\nT-RAG 0.5923 0.5065\\n(b) End-to-end Table QA results on the test set of E2E_WTQ.\\nTable 2: Experimental results on End-to-end Table QA.\\nExperimental Settings In the experiments, we\\nﬁrst encode the questions and tables using\\nBERT BASE , and later jointly train the DPR-based\\nretriever and the Seq2Seq generator of RAG. For\\nthe experiments, we set: (1) training batch size =\\n128; (2) number of epochs = 2; (3) learning rate =\\n3e-5; and (4) gradient accumulation steps = 64.\\nEvaluation metrics: Following the evaluation\\nscript in SQUAD (Rajpurkar et al., 2016), we eval-\\nuate end-to-end Table QA using exact match (EM)\\nand token F1 metrics for NQ-TABLES. The ac-\\ncuracy for the top 1 returned answer and mean\\nreciprocal rank (MRR) are used to measure the per-\\nformance on E2E_WTQ. We also evaluate T-RAG\\non the table retrieval task for a fair comparison\\nwith existing work. We utilize the original met-\\nrics in Herzig et al. (2021) and Pan et al. (2021),\\nwith recall (R) for NQ-TABLES, and precision (P),\\nnormalized discounted gain (N), and mean average\\nprecision (MAP) for E2E_WTQ.\\nExperimental Results We compare the end-to-\\nend Table QA performance of T-RAG against the\\nstate-of-the-art DTR and CLTR models in Table 2.\\nWe ﬁnd T-RAG yields better results than the previ-\\nous best models for both datasets with all evalua-\\ntion metrics.\\nTo further validate T-RAG against the existing\\nmodels, we also evaluate the model performance on\\ntable retrieval. The table retrieval results for NQ-\\nTABLES and E2E_WTQ are shown in Table 3a\\nand 3b, respectively. The results indicate that T-\\nRAG outperforms the simple baselines models such\\nas BM25, as well as the strong state-of-the-art mod-\\nels in the experiments.\\nQualitative Analysis We further evaluate the\\ntable retrieval results on NQ-TABLES. We no-Model R@1 R@10 R@50\\nBM25 16.77 40.06 58.39\\nDTR+hn 42.42 81.13 92.56\\nT-RAG 46.07 85.40 95.03\\n(a) Table retrieval results on the test set of NQ-TABLES.\\nModel P@5 P@10 N@5 N@10 MAP\\nBM25 0.5938 0.6587 0.5228 0.5356 0.4704\\nCLTR 0.7437 0.8735 0.6915 0.7119 0.5971\\nT-RAG 0.7806 0.8943 0.7250 0.7467 0.6404\\n(b) Table retrieval results on the test set of E2E_WTQ.\\nTable 3: Experimental results on Table Retrieval.\\ntice that the DPR-based baseline of our approach\\nachieves 43.89 for R@1 and 81.57 for R@10; both\\noutperform the state-of-the-art DTR results. In\\naddition, the retrieval performance is further im-\\nproved with the more effective end-to-end RAG\\ntraining. To validate the effectiveness of our soft\\nhard negative technique, we test the method on the\\nE2E_WTQ dataset. Instead of using the top 1 nega-\\ntive table from the BM25 results, we set k= 3and\\nachieve a 27.17% absolute gain for Hit@1 accuracy\\nin the end-to-end Table QA task.\\nBesides, we perform thorough error analysis on\\non E2E_WTQ and ﬁnd that over 21% of the errors\\ncome from questions that involve numerical values.\\nThe ﬁnding indicates that understanding different\\ntypes of numbers remains a challenge in Table QA.\\n5 Conclusion and Future Work\\nIn this paper, we present a novel Table QA model\\nthat achieves state-of-the-art performance on recent\\nbenchmarks. Instead of training a retriever and\\na reader model independently, T-RAG uniﬁes the\\nprocedure into a single pipeline of only one training\\nstep, which reduces the error accumulations from\\ntwo separate models. In the experiments, T-RAG\\noutperforms the current best models for end-to-\\nend Table QA. We additionally demonstrate the\\nadvantages of T-RAG with the table retrieval task,\\nand T-RAG beats the existing numbers on both\\nbenchmarks.\\nIn the future, we plan to validate T-RAG on\\ndomain-speciﬁc datasets, such as AIT-QA and TAT-\\nQA (Katsis et al., 2021; Zhu et al., 2021) and\\nextend the model to solve multi-modal QA prob-\\nlems, with the corpus containing both tables and\\npassages, as presented in the OTT-QA and Hybrid-\\nQA benchmarks (Chen et al., 2020a,b). To further\\nimprove the model performance, we also plan to\\ninvestigate algorithms to better understand numeric\\nvalues.References\\nChandra Bhagavatula, Thanapon Noraset, and Doug\\nDowney. 2013. Methods for exploring and min-\\ning tables on wikipedia. Proceedings of the ACM\\nSIGKDD Workshop on Interactive Data Exploration\\nand Analytics .\\nMichael J Cafarella, Alon Halevy, and Nodira Khous-\\nsainova. 2009. Data integration for the rela-\\ntional web. Proceedings of the VLDB Endowment ,\\n2(1):1090–1101.\\nMichael J Cafarella, Alon Halevy, Daisy Zhe Wang, Eu-\\ngene Wu, and Yang Zhang. 2008. Webtables: ex-\\nploring the power of tables on the web. Proceedings\\nof the VLDB Endowment , 1(1):538–549.\\nWenhu Chen, Ming-Wei Chang, Eva Schlinger,\\nWilliam Yang Wang, and William W Cohen. 2020a.\\nOpen question answering over tables and text. In\\nInternational Conference on Learning Representa-\\ntions .\\nWenhu Chen, Ming-Wei Chang, Eva Schlinger,\\nWilliam Yang Wang, and William W. Cohen. 2021.\\nOpen question answering over tables and text. In\\nInternational Conference on Learning Representa-\\ntions .\\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan\\nXiong, Hong Wang, and William Yang Wang. 2020b.\\nHybridqa: A dataset of multi-hop question answer-\\ning over tabular and textual data. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing: Findings , pages 1026–\\n1036.\\nMichael Glass, Mustafa Canim, Alﬁo Gliozzo, Saneem\\nChemmengath, Rishav Chakravarti, Avi Sil, Feifei\\nPan, Samarth Bharadwaj, and Nicolas Rodolfo\\nFauceglia. 2020. Capturing row and column seman-\\ntics in transformer based question answering over ta-\\nbles. Proceedings of the Annual Conference of the\\nNorth American Chapter of the Association for Com-\\nputational Linguistics (NAACL-HLT2020) .\\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub\\nChowdhury, and Alﬁo Gliozzo. 2021. Robust re-\\ntrieval augmented generation for zero-shot slot ﬁll-\\ning. In Proceedings of the 2021 Conference on Em-\\npirical Methods in Natural Language Processing ,\\npages 1939–1949, Online and Punta Cana, Domini-\\ncan Republic. Association for Computational Lin-\\nguistics.\\nJonathan Herzig, Thomas Müller, Syrine Krichene, and\\nJulian Martin Eisenschlos. 2021. Open domain ques-\\ntion answering over tables via dense retrieval. arXiv\\npreprint arXiv:2103.12011 .\\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\\nMüller, Francesco Piccinno, and Julian Eisensch-\\nlos. 2020. TaPas: Weakly supervised table pars-\\ning via pre-training. In Proceedings of the 58th An-\\nnual Meeting of the Association for ComputationalLinguistics , pages 4320–4333, Seattle, Washington,\\nUnited States. Association for Computational Lin-\\nguistics.\\nErnesto Jiménez-Ruiz, Oktie Hassanzadeh, Vasilis\\nEfthymiou, Jiaoyan Chen, and Kavitha Srinivas.\\n2020. Semtab 2019: Resources to benchmark tab-\\nular data to knowledge graph matching systems. In\\nESWC , pages 514–530.\\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.\\nBillion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 .\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 6769–\\n6781, Online. Association for Computational Lin-\\nguistics.\\nYannis Katsis, Saneem Chemmengath, Vishwajeet\\nKumar, Samarth Bharadwaj, Mustafa Canim,\\nMichael Glass, Alﬁo Gliozzo, Feifei Pan, Jay-\\ndeep Sen, Karthik Sankaranarayanan, and Soumen\\nChakrabarti. 2021. Ait-qa: Question answering\\ndataset over complex tables in the airline industry.\\nBogdan Kosti ´c, Julian Risch, and Timo Möller. 2021.\\nMulti-modal retrieval of tables and texts using tri-\\nencoder models.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral questions: a benchmark for question answering\\nresearch. Transactions of the Association of Compu-\\ntational Linguistics .\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\\n2020a. BART: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation,\\nand comprehension. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics , pages 7871–7880, Online. Association\\nfor Computational Linguistics.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020b.\\nRetrieval-augmented generation for knowledge-\\nintensive nlp tasks. In Advances in Neural Infor-\\nmation Processing Systems , volume 33, pages 9459–\\n9474. Curran Associates, Inc.\\nKevin Lin, Ben Bogin, Mark Neumann, Jonathan Be-\\nrant, and Matt Gardner. 2019. Grammar-based neu-\\nral text-to-sql generation.Ying Liu, Kun Bai, Prasenjit Mitra, and C Lee Giles.\\n2007. Tableseer: automatic table metadata extrac-\\ntion and searching in digital libraries. In Proceed-\\nings of the 7th ACM/IEEE-CS joint conference on\\nDigital libraries , pages 91–100.\\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin,\\nStan Peshterliev, Dmytro Okhonko, Michael\\nSchlichtkrull, Sonal Gupta, Yashar Mehdad, and\\nScott Yih. 2021. Unik-qa: Uniﬁed representations\\nof structured and unstructured knowledge for\\nopen-domain question answering.\\nFeifei Pan, Mustafa Canim, Michael Glass, Alﬁo\\nGliozzo, and Peter Fox. 2021. CLTR: An end-to-\\nend, transformer-based system for cell-level table re-\\ntrieval and table question answering. In Proceed-\\nings of the 59th Annual Meeting of the Association\\nfor Computational Linguistics and the 11th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing: System Demonstrations , pages 202–209,\\nOnline. Association for Computational Linguistics.\\nPanupong Pasupat and Percy Liang. 2015. Composi-\\ntional semantic parsing on semi-structured tables.\\nPallavi Pyreddy and W Bruce Croft. 1997. Tintin: A\\nsystem for retrieval in text tables. In Proceedings of\\nthe second ACM international conference on Digital\\nlibraries , pages 193–200.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. SQuAD: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Natu-\\nral Language Processing , pages 2383–2392, Austin,\\nTexas. Association for Computational Linguistics.\\nRoee Shraga, Haggai Roitman, Guy Feigenblat, and\\nMustafa Canim. 2020a. Ad hoc table retrieval using\\nintrinsic and extrinsic similarities. In Proceedings of\\nThe Web Conference 2020 , pages 2479–2485.\\nRoee Shraga, Haggai Roitman, Guy Feigenblat, and\\nMustafa Cannim. 2020b. Web table retrieval using\\nmultimodal deep learning. In Proceedings of the\\n43rd International ACM SIGIR Conference on Re-\\nsearch and Development in Information Retrieval ,\\nSIGIR ’20, page 1399–1408, New York, NY , USA.\\nAssociation for Computing Machinery.\\nHuan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su,\\nand Xifeng Yan. 2016. Table cell search for question\\nanswering. In Proceedings of the 25th International\\nConference on World Wide Web , pages 771–782.\\nYibo Sun, Zhao Yan, Duyu Tang, Nan Duan, and Bing\\nQin. 2019. Content-based table retrieval for web\\nqueries. Neurocomputing , 349:183–189.\\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\\nSequence to sequence learning with neural networks.\\nInAdvances in neural information processing sys-\\ntems, pages 3104–3112.Yalin Wang and Jianying Hu. 2002. A machine learn-\\ning based approach for table detection on the web.\\nInProceedings of the 11th International Conference\\non World Wide Web , WWW ’02, page 242–250, New\\nYork, NY , USA. Association for Computing Machin-\\nery.\\nXiaojun Xu, Chang Liu, and Dawn Song. 2018. SQL-\\nNet: Generating structured queries from natural lan-\\nguage without reinforcement learning.\\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\\nbastian Riedel. 2020. TaBERT: Pretraining for joint\\nunderstanding of textual and tabular data. In Pro-\\nceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics , pages 8413–\\n8426, Online. Association for Computational Lin-\\nguistics.\\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\\nDongxu Wang, Zifan Li, James Ma, Irene Li,\\nQingning Yao, Shanelle Roman, Zilin Zhang,\\nand Dragomir Radev. 2018. Spider: A large-\\nscale human-labeled dataset for complex and cross-\\ndomain semantic parsing and text-to-SQL task.\\nWenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,\\nQingyun Wang, Heng Ji, and Meng Jiang. 2020.\\nA survey of knowledge-enhanced text generation.\\narXiv preprint arXiv:2010.04389 .\\nShuo Zhang and K. Balog. 2018a. Ad hoc table re-\\ntrieval using semantic similarity. Proceedings of the\\n2018 World Wide Web Conference .\\nShuo Zhang and Krisztian Balog. 2018b. Ad hoc ta-\\nble retrieval using semantic similarity. In Proceed-\\nings of the 2018 World Wide Web Conference , pages\\n1553–1562.\\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao\\nWang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and\\nTat-Seng Chua. 2021. TAT-QA: A question answer-\\ning benchmark on a hybrid of tabular and textual\\ncontent in ﬁnance. In Proceedings of the 59th An-\\nnual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Confer-\\nence on Natural Language Processing (Volume 1:\\nLong Papers) , pages 3277–3287, Online. Associa-\\ntion for Computational Linguistics.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64002e98-6eb0-4cae-88e5-0dbe3991e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_texts(document_list)\n",
    "bm25_retriever.k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e44c9ac-605f-4e6a-8af7-476a93a5799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87745f39-0dd1-4134-9b75-e1bf985715f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='guistics.\\nDanqi Chen and Wen-tau Yih. 2020. Open-domain\\nquestion answering. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics: Tutorial Abstracts , pages 34–37, On-\\nline. Association for Computational Linguistics.\\nMingda Chen, Qingming Tang, Sam Wiseman, and\\nKevin Gimpel. 2019. Controllable paraphrase gen-\\neration with a syntactic exemplar. In Proceedings of\\nthe 57th Conference of the Association for Compu-\\ntational Linguistics, ACL 2019, Florence, Italy, July\\n28- August 2, 2019, Volume 1: Long Papers , pages\\n5972–5984. Association for Computational Linguis-\\ntics.\\nDavid Chiang. 2007. Hierarchical phrase-based trans-\\nlation. computational linguistics , 33(2):201–228.\\nSarah Dillon and Janet Fraser. 2006. Translators and\\ntm: An investigation of translators’ perceptions of\\ntranslation memory adoption. Machine Translation ,\\n20(2):67–79.\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\\nFan, Michael Auli, and Jason Weston. 2018. Wizard', metadata={'page': 8, 'source': './data\\\\a survey.pdf'}),\n",
       " Document(page_content='Generation-Augmented Retrieval for Open-Domain Question Answering\\nYuning Mao1\\x03, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\\nJianfeng Gao3, Jiawei Han1, Weizhu Chen2\\n1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\\n1fyuningm2, hanjg@illinois.edu\\n2;3fpenhe, xiaodl, yeshe, jfgao,wzchen g@microsoft.com\\nAbstract\\nWe propose Generation-Augmented Retrieval\\n(GAR) for answering open-domain questions,\\nwhich augments a query through text genera-\\ntion of heuristically discovered relevant con-\\ntexts without external resources as supervi-\\nsion. We demonstrate that the generated con-\\ntexts substantially enrich the semantics of the\\nqueries and G ARwith sparse representations\\n(BM25) achieves comparable or better per-\\nformance than state-of-the-art dense retrieval\\nmethods such as DPR (Karpukhin et al., 2020).\\nWe show that generating diverse contexts for a\\nquery is beneﬁcial as fusing their results con-\\nsistently yields better retrieval accuracy. More-\\nover, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it\\nis infeasible to examine every piece of information\\nin the entire document collection ( e.g., millions\\nof Wikipedia passages) and the retrieval accuracy\\nbounds the performance of the (extractive) reader.\\n\\x03Work was done during internship at Microsoft Azure AI.\\n1Our code and retrieval results are available at https:\\n//github.com/morningmoni/GAR .Early OpenQA systems (Chen et al., 2017)\\nuse classic retrieval methods such as TF-IDF and\\nBM25 with sparse representations. Sparse methods\\nare lightweight and efﬁcient, but unable to per-\\nform semantic matching and fail to retrieve rele-\\nvant passages without lexical overlap. More re-\\ncently, methods based on dense representations\\n(Guu et al., 2020; Karpukhin et al., 2020) learn to\\nembed queries and passages into a latent vector\\nspace, in which text similarity beyond lexical over-\\nlap can be measured. Dense retrieval methods can\\nretrieve semantically relevant but lexically differ-\\nent passages and often achieve better performance\\nthan sparse methods. However, the dense mod-\\nels are more computationally expensive and suffer\\nfrom information loss as they condense the entire\\ntext sequence into a ﬁxed-size vector that does not\\nguarantee exact matching (Luan et al., 2020).\\nThere have been some recent studies on query re-\\nformulation with text generation for other retrieval\\ntasks, which, for example, rewrite the queries to\\ncontext-independent (Yu et al., 2020; Lin et al.,\\n2020; Vakulenko et al., 2020) or well-formed (Liu\\net al., 2019) ones. However, these methods re-\\nquire either task-speciﬁc data ( e.g., conversational\\ncontexts, ill-formed queries) or external resources\\nsuch as paraphrase data (Zaiem and Sadat, 2019;\\nWang et al., 2020) that cannot or do not trans-\\nfer well to OpenQA. Also, some rely on time-\\nconsuming training process like reinforcement\\nlearning (RL) (Nogueira and Cho, 2017; Liu et al.,\\n2019; Wang et al., 2020) that is not efﬁcient enough\\nfor OpenQA (more discussions in Sec. 2).\\nIn this paper, we propose Generation-\\nAugmented Retrieval ( GAR), which augments\\na query through text generation of a pre-trained\\nlanguage model (PLM). Different from prior\\nstudies that reformulate queries, GARdoes not\\nrequire external resources or downstream feedback\\nvia RL as supervision, because it does not rewrite\\nthe query but expands it with heuristically discov-arXiv:2009.08553v4  [cs.CL]  6 Aug 2021ered relevant contexts, which are fetched from\\nPLMs and provide richer background information\\n(Table 2). For example, by prompting a PLM\\nto generate the title of a relevant passage given\\na query and appending the generated title to the\\nquery, it becomes easier to retrieve that relevant\\npassage. Intuitively, the generated contexts\\nexplicitly express the search intent not presented\\nin the original query. As a result, GARwith\\nsparse representations achieves comparable or\\neven better performance than state-of-the-art\\napproaches (Karpukhin et al., 2020; Guu et al.,\\n2020) with dense representations of the original\\nqueries, while being more lightweight and efﬁcient\\nin terms of both training and inference (including\\nthe cost of the generation model) (Sec. 6.4).\\nSpeciﬁcally, we expand the query (question) by\\nadding relevant contexts as follows. We conduct\\nseq2seq learning with the question as the input\\nand various freely accessible in-domain contexts as\\nthe output such as the answer, the sentence where\\nthe answer belongs to , and the title of a passage\\nthat contains the answer . We then append the gen-\\nerated contexts to the question as the generation-\\naugmented query for retrieval. We demonstrate\\nthat using multiple contexts from diverse gener-\\nation targets is beneﬁcial as fusing the retrieval\\nresults of different generation-augmented queries\\nconsistently yields better retrieval accuracy.\\nWe conduct extensive experiments on the Nat-\\nural Questions (NQ) (Kwiatkowski et al., 2019)\\nand TriviaQA (Trivia) (Joshi et al., 2017) datasets.\\nThe results reveal four major advantages of GAR:\\n(1)GAR, combined with BM25, achieves signif-\\nicant gains over the same BM25 model that uses\\nthe original queries or existing unsupervised query\\nexpansion (QE) methods. (2) GARwith sparse rep-\\nresentations (BM25) achieves comparable or even\\nbetter performance than the current state-of-the-art\\nretrieval methods, such as DPR (Karpukhin et al.,\\n2020), that use dense representations. (3) Since\\nGARuses sparse representations to measure lexical\\noverlap2, it is complementary to dense representa-\\ntions: by fusing the retrieval results of GARand\\nDPR (denoted as GAR+), we obtain consistently\\nbetter performance than either method used individ-\\nually. (4) GARoutperforms DPR in the end-to-end\\nQA performance (EM) when the same extractive\\nreader is used: EM=41.8 (43.8 for GAR+) on NQ\\n2Strictly speaking, GARwith sparse representations han-\\ndles semantics before retrieval by enriching the queries, while\\nmaintaining the advantage of exact matching.and 62.7 on Trivia, creating new state-of-the-art re-\\nsults for extractive OpenQA. GARalso outperforms\\nother retrieval methods under the generative setup\\nwhen the same generative reader is used: EM=38.1\\n(45.3 for G AR+) on NQ and 62.2 on Trivia.\\nContributions . (1) We propose Generation-\\nAugmented Retrieval ( GAR), which augments\\nqueries with heuristically discovered relevant con-\\ntexts through text generation without external su-\\npervision or time-consuming downstream feedback.\\n(2) We show that using generation-augmented\\nqueries achieves signiﬁcantly better retrieval and\\nQA results than using the original queries or ex-\\nisting unsupervised QE methods. (3) We show\\nthatGAR, combined with a simple BM25 model,\\nachieves new state-of-the-art performance on two\\nbenchmark datasets in extractive OpenQA and com-\\npetitive results in the generative setting.\\n2 Related Work\\nConventional Query Expansion .GARshares\\nsome merits with query expansion (QE) meth-\\nods based on pseudo relevance feedback (Rocchio,\\n1971; Abdul-Jaleel et al., 2004; Lv and Zhai, 2010)\\nin that they both expand the queries with relevant\\ncontexts (terms) without the use of external super-\\nvision. GARis superior as it expands the queries\\nwith knowledge stored in the PLMs rather than\\nthe retrieved passages and its expanded terms are\\nlearned through text generation.\\nRecent Query Reformulation . There are recent\\nor concurrent studies (Nogueira and Cho, 2017;\\nZaiem and Sadat, 2019; Yu et al., 2020; Vaku-\\nlenko et al., 2020; Lin et al., 2020) that reformu-\\nlate queries with generation models for other re-\\ntrieval tasks. However, these studies are not eas-\\nily applicable or efﬁcient enough for OpenQA be-\\ncause: (1) They require external resources such as\\nparaphrase data (Zaiem and Sadat, 2019), search\\nsessions (Yu et al., 2020), or conversational con-\\ntexts (Lin et al., 2020; Vakulenko et al., 2020)\\nto form the reformulated queries, which are not\\navailable or showed inferior domain-transfer per-\\nformance in OpenQA (Zaiem and Sadat, 2019);\\n(2) They involve time-consuming training process\\nsuch as RL. For example, Nogueira and Cho (2017)\\nreported a training time of 8 to 10 days as it uses\\nretrieval performance in the reward function and\\nconducts retrieval at each iteration. In contrast,\\nGARuses freely accessible in-domain contexts like\\npassage titles as the generation targets and standardseq2seq learning, which, despite its simplicity, is\\nnot only more efﬁcient but effective for OpenQA.\\nRetrieval for OpenQA . Existing sparse retrieval\\nmethods for OpenQA (Chen et al., 2017) solely rely\\non the information of the questions. GARextends\\nto contexts relevant to the questions by extracting\\ninformation inside PLMs and helps sparse meth-\\nods achieve comparable or better performance than\\ndense methods (Guu et al., 2020; Karpukhin et al.,\\n2020), while enjoying the simplicity and efﬁciency\\nof sparse representations. GARcan also be used\\nwith dense representations to seek for even better\\nperformance, which we leave as future work.\\nGenerative QA . Generative QA generates answers\\nthrough seq2seq learning instead of extracting an-\\nswer spans. Recent studies on generative OpenQA\\n(Lewis et al., 2020a; Min et al., 2020; Izacard and\\nGrave, 2020) are orthogonal to GARin that they\\nfocus on improving the reading stage and directly\\nreuse DPR (Karpukhin et al., 2020) as the retriever.\\nUnlike generative QA, the goal of GARis not to\\ngenerate perfect answers to the questions but perti-\\nnent contexts that are helpful for retrieval. Another\\nline in generative QA learns to generate answers\\nwithout relevant passages as the evidence but solely\\nthe question itself using PLMs (Roberts et al., 2020;\\nBrown et al., 2020). GARfurther conﬁrms that one\\ncan extract factual knowledge from PLMs, which\\nis not limited to the answers as in prior studies but\\nalso other relevant contexts.\\n3 Generation-Augmented Retrieval\\n3.1 Task Formulation\\nOpenQA aims to answer factoid questions with-\\nout pre-speciﬁed domains. We assume that a large\\ncollection of documents C(i.e., Wikipedia) are\\ngiven as the resource to answer the questions and\\na retriever-reader architecture is used to tackle the\\ntask, where the retriever retrieves a small subset\\nof the documents D\\x1aCand the reader reads the\\ndocuments Dto extract (or generate) an answer.\\nOur goal is to improve the effectiveness and efﬁ-\\nciency of the retriever and consequently improve\\nthe performance of the reader.\\n3.2 Generation of Query Contexts\\nInGAR, queries are augmented with various heuris-\\ntically discovered relevant contexts in order to re-\\ntrieve more relevant passages in terms of both quan-\\ntity and quality. For the task of OpenQA where the\\nquery is a question, we take the following threefreely accessible contexts as the generation targets.\\nWe show in Sec. 6.2 that having multiple gener-\\nation targets is helpful in that fusing their results\\nconsistently brings better retrieval accuracy.\\nContext 1: The default target (answer) . The de-\\nfault target is the label in the task of interest, which\\nis the answer in OpenQA. The answer to the ques-\\ntion is apparently useful for the retrieval of relevant\\npassages that contain the answer itself. As shown\\nin previous work (Roberts et al., 2020; Brown et al.,\\n2020), PLMs are able to answer certain questions\\nsolely by taking the questions as input ( i.e., closed-\\nbook QA). Instead of using the generated answers\\ndirectly as in closed-book QA, GARtreats them\\nas contexts of the question for retrieval. The ad-\\nvantage is that even if the generated answers are\\npartially correct (or even incorrect), they may still\\nbeneﬁt retrieval as long as they are relevant to the\\npassages that contain the correct answers ( e.g., co-\\noccur with the correct answers).\\nContext 2: Sentence containing the default tar-\\nget. The sentence in a passage that contains the\\nanswer is used as another generation target. Sim-\\nilar to using answers as the generation target, the\\ngenerated sentences are still beneﬁcial for retriev-\\ning relevant passages even if they do not contain\\nthe answers, as their semantics is highly related to\\nthe questions/answers (examples in Sec. 6.1). One\\ncan take the relevant sentences in the ground-truth\\npassages (if any) or those in the positive passages\\nof a retriever as the reference, depending on the\\ntrade-off between reference quality and diversity.\\nContext 3: Title of passage containing the de-\\nfault target . One can also use the titles of rele-\\nvant passages as the generation target if available.\\nSpeciﬁcally, we retrieve Wikipedia passages using\\nBM25 with the question as the query, and take the\\npage titles of positive passages that contain the an-\\nswers as the generation target. We observe that\\nthe page titles of positive passages are often entity\\nnames of interest, and sometimes (but not always)\\nthe answers to the questions. Intuitively, if GAR\\nlearns which Wikipedia pages the question is re-\\nlated to, the queries augmented by the generated\\ntitles would naturally have a better chance of re-\\ntrieving those relevant passages.\\nWhile it is likely that some of the generated\\nquery contexts involve unfaithful or nonfactual in-\\nformation due to hallucination in text generation\\n(Mao et al., 2020) and introduce noise during re-\\ntrieval, they are beneﬁcial rather than harmful over-all, as our experiments show that GARimprove\\nboth retrieval and QA performance over BM25 sig-\\nniﬁcantly. Also, since we generate 3 different (com-\\nplementary) query contexts and fuse their retrieval\\nresults, the distraction of hallucinated content is\\nfurther alleviated.\\n3.3 Retrieval with Generation-Augmented\\nQueries\\nAfter generating the contexts of a query, we append\\nthem to the query to form a generation-augmented\\nquery .3We observe that conducting retrieval with\\nthe generated contexts ( e.g., answers) alone as\\nqueries instead of concatenation is ineffective be-\\ncause (1) some of the generated answers are rather\\nirrelevant, and (2) a query consisting of the correct\\nanswer alone (without the question) may retrieve\\nfalse positive passages with unrelated contexts that\\nhappen to contain the answer. Such low-quality\\npassages may lead to potential issues in the follow-\\ning passage reading stage.\\nIf there are multiple query contexts, we conduct\\nretrieval using queries with different generated con-\\ntexts separately and then fuse their results. The per-\\nformance of one-time retrieval with all the contexts\\nappended is slightly but not signiﬁcantly worse.\\nFor simplicity, we fuse the retrieval results in a\\nstraightforward way: an equal number of passages\\nare taken from the top-retrieved passages of each\\nsource. One may also use weighted or more so-\\nphisticated fusion strategies such as reciprocal rank\\nfusion (Cormack et al., 2009), the results of which\\nare slightly better according to our experiments.4\\nNext, one can use any off-the-shelf retriever for\\npassage retrieval. Here, we use a simple BM25\\nmodel to demonstrate that GARwith sparse repre-\\nsentations can already achieve comparable or better\\nperformance than state-of-the-art dense methods\\nwhile being more lightweight and efﬁcient (includ-\\ning the cost of the generation model), closing the\\ngap between sparse and dense retrieval methods.\\n4 OpenQA with G AR\\nTo further verify the effectiveness of GAR, we\\nequip it with both extractive and generative read-\\ners for end-to-end QA evaluation. We follow the\\n3One may create a title ﬁeld during document indexing\\nand conduct multi-ﬁeld retrieval but here we append the titles\\nto the questions as other query contexts for generalizability.\\n4We use the fusion tools at https://github.com/\\njoaopalotti/trectools .reader design of the major baselines for a fair com-\\nparison, while virtually any existing QA reader can\\nbe used with G AR.\\n4.1 Extractive Reader\\nFor the extractive setup, we largely follow the de-\\nsign of the extractive reader in DPR (Karpukhin\\net al., 2020). Let D= [d1; d2; :::; d k]denote the list\\nof retrieved passages with passage relevance scores\\nD. LetSi= [s1; s2; :::; s N]denote the top Ntext\\nspans in passage diranked by span relevance scores\\nSi. Brieﬂy, the DPR reader uses BERT-base (De-\\nvlin et al., 2019) for representation learning, where\\nit estimates the passage relevance score Dkfor\\neach retrieved passage dkbased on the [CLS] to-\\nkens of all retrieved passages D, and assigns span\\nrelevance scores Sifor each candidate span based\\non the representations of its start and end tokens.\\nFinally, the span with the highest span relevance\\nscore from the passage with the highest passage rel-\\nevance score is chosen as the answer. We refer the\\nreaders to Karpukhin et al. (2020) for more details.\\nPassage-level Span Voting . Many extractive QA\\nmethods (Chen et al., 2017; Min et al., 2019b; Guu\\net al., 2020; Karpukhin et al., 2020) measure the\\nprobability of span extraction in different retrieved\\npassages independently, despite that their collec-\\ntive signals may provide more evidence in deter-\\nmining the correct answer. We propose a simple\\nyet effective passage-level span voting mechanism,\\nwhich aggregates the predictions of the spans in\\nthe same surface form from different retrieved pas-\\nsages. Intuitively, if a text span is considered as the\\nanswer multiple times in different passages, it is\\nmore likely to be the correct answer. Speciﬁcally,\\nGARcalculates a normalized score p(Si[j])for the\\nj-th span in passage diduring inference as follows:\\np(Si[j]) = softmax (D)[i]\\x02softmax (Si)[j].GAR\\nthen aggregates the scores of the spans with the\\nsame surface string among all the retrieved pas-\\nsages as the collective passage-level score.5\\n4.2 Generative Reader\\nFor the generative setup, we use a seq2seq frame-\\nwork where the input is the concatenation of the\\nquestion and top-retrieved passages and the target\\noutput is the desired answer. Such generative read-\\ners are adopted in recent methods such as SpanSe-\\n5We ﬁnd that the number of spans used for normalization\\nin each passage does not have signiﬁcant impact on the ﬁnal\\nperformance (we take N= 5) and using the raw or normalized\\nstrings for aggregation also perform similarly.qGen (Min et al., 2020) and Longformer (Belt-\\nagy et al., 2020). Speciﬁcally, we use BART-large\\n(Lewis et al., 2019) as the generative reader, which\\nconcatenates the question and top-retrieved pas-\\nsages up to its length limit (1,024 tokens, 7.8 pas-\\nsages on average). Generative GARis directly com-\\nparable with SpanSeqGen (Min et al., 2020) that\\nuses the retrieval results of DPR but not comparable\\nwith Fusion-in-Decoder (FID) (Izacard and Grave,\\n2020) since it encodes 100 passages rather than\\n1,024 tokens and involves more model parameters.\\n5 Experiment Setup\\n5.1 Datasets\\nWe conduct experiments on the open-domain ver-\\nsion of two popular QA benchmarks: Natural Ques-\\ntions (NQ) (Kwiatkowski et al., 2019) and Trivi-\\naQA (Trivia) (Joshi et al., 2017). The statistics of\\nthe datasets are listed in Table 1.\\nDataset Train / Val / Test Q-len A-len #-A\\nNQ 79,168 / 8,757 / 3,610 12.5 5.2 1.2\\nTrivia 78,785 / 8,837 / 11,313 20.2 5.5 13.7\\nTable 1: Dataset statistics that show the number of sam-\\nples per data split, the average question (answer) length,\\nand the number of answers for each question.\\n5.2 Evaluation Metrics\\nFollowing prior studies (Karpukhin et al., 2020),\\nwe use top-k retrieval accuracy to evaluate the per-\\nformance of the retriever and the Exact Match (EM)\\nscore to measure the performance of the reader.\\nTop-k retrieval accuracy is deﬁned as the pro-\\nportion of questions for which the top-k retrieved\\npassages contain at least one answer span, which\\nis an upper bound of how many questions are “an-\\nswerable” by an extractive reader.\\nExact Match (EM) is the proportion of the pre-\\ndicted answer spans being exactly the same as (one\\nof) the ground-truth answer(s), after string normal-\\nization such as article and punctuation removal.\\n5.3 Compared Methods\\nFor passage retrieval, we mainly compare with\\nBM25 and DPR, which represent the most used\\nstate-of-the-art methods of sparse and dense re-\\ntrieval for OpenQA, respectively. For query ex-\\npansion, we re-emphasize that GARis the ﬁrst QE\\napproach designed for OpenQA and most of the\\nrecent approaches are not applicable or efﬁcientenough for OpenQA since they have task-speciﬁc\\nobjectives, require external supervision that was\\nshown to transfer poorly to OpenQA, or take many\\ndays to train (Sec. 2). We thus compare with a clas-\\nsic unsupervised QE method RM3 (Abdul-Jaleel\\net al., 2004) that does not need external resources\\nfor a fair comparison. For passage reading, we\\ncompare with both extractive (Min et al., 2019a;\\nAsai et al., 2019; Lee et al., 2019; Min et al., 2019b;\\nGuu et al., 2020; Karpukhin et al., 2020) and gen-\\nerative (Brown et al., 2020; Roberts et al., 2020;\\nMin et al., 2020; Lewis et al., 2020a; Izacard and\\nGrave, 2020) methods when equipping GARwith\\nthe corresponding reader.\\n5.4 Implementation Details\\nRetriever . We use Anserini (Yang et al., 2017) for\\ntext retrieval of BM25 and GARwith its default\\nparameters. We conduct grid search for the QE\\nbaseline RM3 (Abdul-Jaleel et al., 2004).\\nGenerator . We use BART-large (Lewis et al.,\\n2019) to generate query contexts in GAR. When\\nthere are multiple desired targets (such as multi-\\nple answers or titles), we concatenate them with\\n[SEP] tokens as the reference and remove the [SEP]\\ntokens in the generation-augmented queries. For\\nTrivia, in particular, we use the value ﬁeld as the\\ngeneration target of answer and observe better per-\\nformance. We take the checkpoint with the best\\nROUGE-1 F1 score on the validation set, while\\nobserving that the retrieval accuracy of GARis rel-\\natively stable to the checkpoint selection since we\\ndo not directly use the generated contexts but treat\\nthem as augmentation of queries for retrieval.\\nReader . Extractive GARuses the reader of DPR\\nwith largely the same hyperparameters, which is\\ninitialized with BERT-base (Devlin et al., 2019)\\nand takes 100 (500) retrieved passages during train-\\ning (inference). Generative GARconcatenates the\\nquestion and top-10 retrieved passages, and takes\\nat most 1,024 tokens as input. Greedy decoding is\\nadopted for all generation models, which appears to\\nperform similarly to (more expensive) beam search.\\n6 Experiment Results\\nWe evaluate the effectiveness of GARin three\\nstages: generation of query contexts (Sec. 6.1),\\nretrieval of relevant passages (Sec. 6.2), and pas-\\nsage reading for OpenQA (Sec. 6.3). Ablation\\nstudies are mostly shown on the NQ dataset to un-\\nderstand the drawbacks of GARsince it achievesQuestion : when did bat out of hell get released?\\nAnswer :September 1977 fSeptember 1977g\\nSentence : Bat Out of Hell is the second studio album and the major - label debut by American rock singer Meat\\nLoaf ... released in September 1977 on Cleveland International / Epic Records.\\nfThe album was released in September 1977 on Cleveland International / Epic Records. g\\nTitle :Bat Out of Hell fBat Out of Hellg\\nQuestion : who sings does he love me with reba?\\nAnswer :Brooks & Dunn fLinda Davisg\\nSentence :Linda Kaye Davis ( born November 26, 1962 ) is an American country music singer.\\nf“ Does He Love You ” is a song written by Sandy Knox and Billy Stritch, and recorded as a duet by American\\ncountry music artists Reba McEntire and Linda Davis. g\\nTitle :Does He Love Me [SEP] Does He Love Me (Reba McEntire song) [SEP] I Do (Reba McEntire album)\\nfLinda Davis [SEP] Greatest Hits V olume Two (Reba McEntire album) [SEP] Does He Love You g\\nQuestion : what is the name of wonder womans mother?\\nAnswer :Mother Magda fQueen Hippolytag\\nSentence : In the Amazonian myths, she is the daughter of the Amazon queen Sifrat and the male dwarf Shuri,\\nand is the mother of Wonder Woman. fWonder Woman’s origin story relates that she was sculpted from clay\\nby her mother Queen Hippolyta and given life by Aphrodite. g\\nTitle :Wonder Woman [SEP] Diana Prince [SEP] Wonder Woman (2011 TV pilot)\\nfWonder Woman [SEP] Orana (comics) [SEP] Wonder Woman (TV series) g\\nTable 2: Examples of generated query contexts .Relevant andirrelevant contexts are shown in green and\\nred. Ground-truth references are shown in the fbracesg. The issue of generating wrong answers is alleviated by\\ngenerating other contexts highly related to the question/answer.\\nbetter performance on Trivia.\\n6.1 Query Context Generation\\nAutomatic Evaluation . To evaluate the quality\\nof the generated query contexts, we ﬁrst measure\\ntheir lexical overlap with the ground-truth query\\ncontexts. As suggested by the nontrivial ROUGE\\nscores in Table 3, GARdoes learn to generate\\nmeaningful query contexts that could help the re-\\ntrieval stage. We next measure the lexical overlap\\nbetween the query and the ground-truth passage.\\nThe ROUGE-1/2/L F1 scores between the original\\nquery and ground-truth passage are 6.00/2.36/5.01,\\nand those for the generation-augmented query are\\n7.05/2.84/5.62 (answer), 13.21/6.99/10.27 (sen-\\ntence), 7.13/2.85/5.76 (title) on NQ, respectively.\\nSuch results further demonstrate that the generated\\nquery contexts signiﬁcantly increase the word over-\\nlap between the queries and the positive passages,\\nand thus are likely to improve retrieval results.6\\nCase Studies . In Table 2, we show several ex-\\namples of the generated query contexts and their\\nground-truth references. In the ﬁrst example, the\\ncorrect album release date appears in both the gen-\\nerated answer and the generated sentence, and the\\ngenerated title is the same as the Wikipedia page\\n6We use F1 instead of recall to avoid the unfair favor of\\n(longer) generation-augmented query.Context ROUGE-1 ROUGE-2 ROUGE-L\\nAnswer 33.51 20.54 33.30\\nSentence 37.14 24.71 33.91\\nTitle 43.20 32.11 39.67\\nTable 3: ROUGE F1 scores of the generated query\\ncontexts on the validation set of the NQ dataset.\\ntitle of the album. In the last two examples, the\\ngenerated answers are wrong but fortunately, the\\ngenerated sentences contain the correct answer and\\n(or) other relevant information and the generated\\ntitles are highly related to the question as well,\\nwhich shows that different query contexts are com-\\nplementary to each other and the noise during query\\ncontext generation is thus reduced.\\n6.2 Generation-Augmented Retrieval\\nComparison w. the state-of-the-art . We next\\nevaluate the effectiveness of GARfor retrieval.\\nIn Table 4, we show the top-k retrieval accuracy\\nof BM25, BM25 with query expansion (+RM3)\\n(Abdul-Jaleel et al., 2004), DPR (Karpukhin et al.,\\n2020), G AR, and G AR+(GAR+DPR).\\nOn the NQ dataset, while BM25 clearly under-\\nperforms DPR regardless of the number of retrieved\\npassages, the gap between GARand DPR is signiﬁ-\\ncantly smaller and negligible when k\\x15100. When\\nk\\x15500,GARis slightly better than DPR despiteMethodNQ Trivia\\nTop-5 Top-20 Top-100 Top-500 Top-1000 Top-5 Top-20 Top-100 Top-500 Top-1000\\nBM25 (ours) 43.6 62.9 78.1 85.5 87.8 67.7 77.3 83.9 87.9 88.9\\nBM25 +RM3 44.6 64.2 79.6 86.8 88.9 67.0 77.1 83.8 87.7 88.9\\nDPR 68.3 80.1 86.1 90.3 91.2 72.7 80.2 84.8 - -\\nGAR 60.9 74.4 85.3 90.3 91.7 73.1 80.4 85.7 88.9 89.7\\nGAR+70.7 81.6 88.9 92.0 93.2 76.0 82.1 86.6 - -\\nTable 4: Top-k retrieval accuracy on the test sets . The baselines are evaluated by ourselves and better than\\nreported in Karpukhin et al. (2020). G ARhelps BM25 to achieve comparable or better performance than DPR.\\nBest and second best methods are bold and underlined , respectively.\\nthat it simply uses BM25 for retrieval. In con-\\ntrast, the classic QE method RM3, while showing\\nmarginal improvement over the vanilla BM25, does\\nnot achieve comparable performance with GARor\\nDPR. By fusing the results of GARand DPR in\\nthe same way as described in Sec. 3.3, we further\\nobtain consistently higher performance than both\\nmethods, with top-100 accuracy 88.9% and top-\\n1000 accuracy 93.2%.\\nOn the Trivia dataset, the results are even more\\nencouraging – GARachieves consistently better\\nretrieval accuracy than DPR when k\\x155. On\\nthe other hand, the difference between BM25 and\\nBM25 +RM3 is negligible, which suggests that\\nnaively considering top-ranked passages as relevant\\n(i.e., pseudo relevance feedback) for QE does not\\nalways work for OpenQA. Results on more cutoffs\\nofkcan be found in App. A.\\nEffectiveness of diverse query contexts . In\\nFig. 1, we show the performance of GARwhen\\ndifferent query contexts are used to augment the\\nqueries. Although the individual performance\\nwhen using each query context is somewhat similar,\\nfusing their retrieved passages consistently leads\\nto better performance, conﬁrming that different\\ngeneration-augmented queries are complementary\\nto each other (recall examples in Table 2).\\nPerformance breakdown by question type . In\\nTable 5, we show the top-100 accuracy of the com-\\npared retrieval methods per question type on the\\nNQ test set. Again, GARoutperforms BM25 on all\\ntypes of questions signiﬁcantly and GAR+achieves\\nthe best performance across the board, which fur-\\nther veriﬁes the effectiveness of G AR.\\n6.3 Passage Reading with G AR\\nComparison w. the state-of-the-art . We show\\nthe comparison of end-to-end QA performance of\\nextractive and generative methods in Table 6. Ex-\\ntractive GARachieves state-of-the-art performance\\n1 5 10 20 50 100 200 300 500 1000\\nk: # of retrieved passages30405060708090Top-k Accuracy (%)\\nAnswer+Sentence+Title\\nAnswer+Sentence\\nAnswer+Title\\nAnswer\\nTitle\\nSentenceFigure 1: Top-k retrieval accuracy on the test\\nset of NQ when fusing retrieval results of different\\ngeneration-augmented queries.\\nType Percentage BM25 DPR G AR GAR+\\nWho 37.5% 82.1 88.0 87.5 90.8\\nWhen 19.0% 73.1 86.9 83.8 88.6\\nWhat 15.0% 76.5 82.6 81.5 86.0\\nWhere 10.9% 77.4 89.1 87.0 90.8\\nOther 9.1% 79.3 78.1 81.8 84.2\\nHow 5.0% 78.2 83.8 83.2 85.5\\nWhich 3.3% 89.0 90.7 94.1 94.9\\nWhy 0.3% 90.0 90.0 90.0 90.0\\nTable 5: Top-100 retrieval accuracy breakdown of\\nquestion type on NQ . Best and second best methods\\nin each category are bold and underlined , respectively.\\namong extractive methods on both NQ and Trivia\\ndatasets, despite that it is more lightweight and\\ncomputationally efﬁcient. Generative GARoutper-\\nforms most of the generative methods on Trivia but\\ndoes not perform as well on NQ, which is some-\\nwhat expected and consistent with the performance\\nat the retrieval stage, as the generative reader only\\ntakes a few passages as input and GARdoes not\\noutperform dense retrieval methods on NQ when k\\nis very small. However, combining GARwith DPR\\nachieves signiﬁcantly better performance than bothMethod NQ TriviaExtractiveHard EM (Min et al., 2019a) 28.1 50.9 -\\nPath Retriever (Asai et al., 2019) 32.6 - -\\nORQA (Lee et al., 2019) 33.3 45.0 -\\nGraph Retriever (Min et al., 2019b) 34.5 56.0 -\\nREALM (Guu et al., 2020) 40.4 - -\\nDPR (Karpukhin et al., 2020) 41.5 57.9 -\\nBM25 (ours) 37.7 60.1 -\\nGAR 41.8 62.7 74.8\\nGAR+43.8 - -GenerativeGPT-3 (Brown et al., 2020) 29.9 - 71.2\\nT5 (Roberts et al., 2020) 36.6 60.5 -\\nSpanSeqGen (Min et al., 2020) 42.2 - -\\nRAG (Lewis et al., 2020a) 44.5 56.1 68.0\\nFID (Izacard and Grave, 2020) 51.4 67.6 80.1\\nBM25 (ours) 35.3 58.6 -\\nGAR 38.1 62.2 -\\nGAR+45.3 - -\\nTable 6: End-to-end comparison with the state-of-\\nthe-art methods in EM . For Trivia, the left column\\ndenotes the open-domain test set and the right is the\\nhidden Wikipedia test set on the public leaderboard.\\nmethods or baselines that use DPR as input such as\\nSpanSeqGen (Min et al., 2020) and RAG (Lewis\\net al., 2020a). Also, GARoutperforms BM25 sig-\\nniﬁcantly under both extractive and generative se-\\ntups, which again shows the effectiveness of the\\ngenerated query contexts, even if they are heuristi-\\ncally discovered without any external supervision.\\nThe best performing generative method FID\\n(Izacard and Grave, 2020) is not directly compara-\\nble as it takes more (100) passages as input. As an\\nindirect comparison, GARperforms better than FID\\nwhen FID encodes 10 passages (cf. Fig. 2 in Izac-\\nard and Grave (2020)). Moreover, since FID relies\\non the retrieval results of DPR as well, we believe\\nthat it is a low-hanging fruit to replace its input\\nwith GARorGAR+and further boost the perfor-\\nmance.7We also observe that, perhaps surprisingly,\\nextractive BM25 performs reasonably well, espe-\\ncially on the Trivia dataset, outperforming many\\nrecent state-of-the-art methods.8Generative BM25\\nalso performs competitively in our experiments.\\nModel Generalizability . Recent studies (Lewis\\net al., 2020b) show that there are signiﬁcant ques-\\ntion and answer overlaps between the training and\\ntest sets of popular OpenQA datasets. Speciﬁcally,\\n60% to 70% test-time answers also appear in the\\n7This claim is later veriﬁed by the best systems in the\\nNeurIPS 2020 EfﬁcientQA competition (Min et al., 2021).\\n8We ﬁnd that taking 500 passages during reader inference\\ninstead of 100 as in Karpukhin et al. (2020) improves the\\nperformance of BM25 but not DPR.training set and roughly 30% test-set questions\\nhave a near-duplicate paraphrase in the training\\nset. Such observations suggest that many questions\\nmight have been answered by simple question or\\nanswer memorization. To further examine model\\ngeneralizability, we study the per-category perfor-\\nmance of different methods using the annotations\\nin Lewis et al. (2020b).\\nMethod TotalQuestion\\nOverlapAnswer\\nOverlap\\nOnlyNo\\nOverlap\\nDPR 41.3 69.4 34.6 19.3\\nGAR+(E) 43.8 66.7 38.1 23.9\\nBART 26.5 67.6 10.2 0.8\\nRAG 44.5 70.7 34.9 24.8\\nGAR+(G) 45.3 67.9 38.1 27.0\\nTable 7: EM scores with question-answer overlap\\ncategory breakdown on NQ. (E) and (G) denote ex-\\ntractive and generative readers, respectively. Results of\\nbaseline methods are taken from Lewis et al. (2020b).\\nThe observations on Trivia are similar and omitted.\\nAs listed in Table 7, for the No Overlap category,\\nGAR+(E) outperforms DPR on the extractive setup\\nandGAR+(G) outperforms RAG on the generative\\nsetup, which indicates that better end-to-end model\\ngeneralizability can be achieved by adding GAR\\nfor retrieval. GAR+also achieves the best EM un-\\nder the Answer Overlap Only category. In addition,\\nwe observe that a closed-book BART model that\\nonly takes the question as input performs much\\nworse than additionally taking top-retrieved pas-\\nsages, i.e.,GAR+(G), especially on the questions\\nthat require generalizability. Notably, all methods\\nperform signiﬁcantly better on the Question Over-\\nlapcategory, which suggests that the high Total\\nEM is mostly contributed by question memoriza-\\ntion. That said, GAR+appears to be less dependent\\non question memorization given its lower EM for\\nthis category.9\\n6.4 Efﬁciency of G AR\\nGARis efﬁcient and scalable since it uses sparse\\nrepresentations for retrieval and does not in-\\nvolve time-consuming training process such as\\nRL (Nogueira and Cho, 2017; Liu et al., 2019).\\nThe only overhead of GARis on the generation of\\nquery contexts and the retrieval with generation-\\n9The same ablation study is also conducted on the retrieval\\nstage and similar results are observed. More detailed discus-\\nsions can be found in App. A.Training Indexing Retrieval\\nDPR 24h w. 8 GPUs 17.3h w. 8 GPUs 30 min w. 1 GPU\\nGAR 3\\x186h w. 1 GPU 0.5h w. 35 CPUs 5 min w. 35 CPUs\\nTable 8: Comparison of computational cost between\\nDPR and G ARat different stages. The training time\\nof G ARis for one generation target but different gener-\\nators can be trained in parallel.\\naugmented (thus longer) queries, whose computa-\\ntional complexity is signiﬁcantly lower than other\\nmethods with comparable retrieval accuracy.\\nWe use Nvidia V100 GPUs and Intel Xeon Plat-\\ninum 8168 CPUs in our experiments. As listed in\\nTable 8, the training time of GARis 3 to 6 hours\\non 1 GPU depending on the generation target. As\\na comparison, REALM (Guu et al., 2020) uses\\n64 TPUs to train for 200k steps during pre-training\\nalone and DPR (Karpukhin et al., 2020) takes about\\n24 hours to train with 8 GPUs. To build the indices\\nof Wikipedia passages, GARonly takes around 30\\nmin with 35 CPUs, while DPR takes 8.8 hours\\non 8 GPUs to generate dense representations and\\nanother 8.5 hours to build the FAISS index (John-\\nson et al., 2017). For retrieval, GARtakes about\\n1 min to generate one query context with 1 GPU,\\n1 min to retrieve 1,000 passages for the NQ test\\nset with answer/title-augmented queries and 2 min\\nwith sentence-augmented queries using 35 CPUs.\\nIn contrast, DPR takes about 30 min on 1 GPU.\\n7 Conclusion\\nIn this work, we propose Generation-Augmented\\nRetrieval and demonstrate that the relevant contexts\\ngenerated by PLMs without external supervision\\ncan signiﬁcantly enrich query semantics and im-\\nprove retrieval accuracy. Remarkably, GARwith\\nsparse representations performs similarly or better\\nthan state-of-the-art methods based on the dense\\nrepresentations of the original queries. GARcan\\nalso be easily combined with dense representa-\\ntions to produce even better results. Furthermore,\\nGARachieves state-of-the-art end-to-end perfor-\\nmance on extractive OpenQA and competitive per-\\nformance under the generative setup.\\n8 Future Extensions\\nPotential improvements . There is still much\\nspace to explore and improve for GARin future\\nwork. For query context generation, one can ex-\\nplore multi-task learning to further reduce computa-\\ntional cost and examine whether different contextscan mutually enhance each other when generated\\nby the same generator. One may also sample multi-\\nple contexts instead of greedy decoding to enrich a\\nquery. For retrieval, one can adopt more advanced\\nfusion techniques based on both the ranking and\\nscore of the passages. As the generator and re-\\ntriever are largely independent now, it is also inter-\\nesting to study how to jointly or iteratively optimize\\ngeneration and retrieval such that the generator is\\naware of the retriever and generates query contexts\\nmore beneﬁcial for the retrieval stage. Last but not\\nleast, it is very likely that better results can be ob-\\ntained by more extensive hyper-parameter tuning.\\nApplicability to other tasks . Beyond OpenQA,\\nGARalso has great potentials for other tasks that\\ninvolve text matching such as conversation utter-\\nance selection (Lowe et al., 2015; Dinan et al.,\\n2020) or information retrieval (Nguyen et al., 2016;\\nCraswell et al., 2020). The default generation tar-\\nget is always available for supervised tasks. For\\nexample, for conversation utterance selection one\\ncan use the reference utterance as the default target\\nand then match the concatenation of the conversa-\\ntion history and the generated utterance with the\\nprovided utterance candidates. For article search,\\nthe default target could be (part of) the ground-truth\\narticle itself. Other generation targets are more task-\\nspeciﬁc and can be designed as long as they can\\nbe fetched from the latent knowledge inside PLMs\\nand are helpful for further text retrieval (matching).\\nNote that by augmenting (expanding) the queries\\nwith heuristically discovered relevant contexts ex-\\ntracted from PLMs instead of reformulating them,\\nGARbypasses the need for external supervision to\\nform the original-reformulated query pairs.\\nAcknowledgments\\nWe thank Vladimir Karpukhin, Sewon Min, Gau-\\ntier Izacard, Wenda Qiu, Revanth Reddy, and Hao\\nCheng for helpful discussions. We thank the anony-\\nmous reviewers for valuable comments.\\nReferences\\nNasreen Abdul-Jaleel, James Allan, W Bruce Croft,\\nFernando Diaz, Leah Larkey, Xiaoyan Li, Mark D\\nSmucker, and Courtney Wade. 2004. Umass at trec\\n2004: Novelty and hard. Computer Science Depart-\\nment Faculty Publication Series , page 189.\\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\\nRichard Socher, and Caiming Xiong. 2019. Learn-\\ning to retrieve reasoning paths over wikipediagraph for question answering. arXiv preprint\\narXiv:1911.10470 .\\nIz Beltagy, Matthew E Peters, and Arman Cohan.\\n2020. Longformer: The long-document transformer.\\narXiv preprint arXiv:2004.05150 .\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. arXiv preprint arXiv:2005.14165 .\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 1870–\\n1879, Vancouver, Canada. Association for Computa-\\ntional Linguistics.\\nGordon V Cormack, Charles LA Clarke, and Stefan\\nBuettcher. 2009. Reciprocal rank fusion outper-\\nforms condorcet and individual rank learning meth-\\nods. In Proceedings of the 32nd international ACM\\nSIGIR conference on Research and development in\\ninformation retrieval , pages 758–759.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\\nCampos, and Ellen M V oorhees. 2020. Overview\\nof the trec 2019 deep learning track. arXiv preprint\\narXiv:2003.07820 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) ,\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\\nAlexander Miller, Kurt Shuster, Jack Urbanek,\\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\\nLowe, et al. 2020. The second conversational in-\\ntelligence challenge (convai2). In The NeurIPS’18\\nCompetition , pages 187–208. Springer.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training. arXiv\\npreprint arXiv:2002.08909 .\\nGautier Izacard and Edouard Grave. 2020. Lever-\\naging passage retrieval with generative models for\\nopen domain question answering. arXiv preprint\\narXiv:2007.01282 .\\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\\nBillion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 .Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale dis-\\ntantly supervised challenge dataset for reading com-\\nprehension. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 1601–1611, Van-\\ncouver, Canada. Association for Computational Lin-\\nguistics.\\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\\nWu, Sergey Edunov, Danqi Chen, and Wen-\\ntau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. arXiv preprint\\narXiv:2004.04906 .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\nNatural questions: A benchmark for question an-\\nswering research. Transactions of the Association\\nfor Computational Linguistics , 7:452–466.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 6086–6096, Florence,\\nItaly. Association for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\\nBart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and\\ncomprehension. arXiv preprint arXiv:1910.13461 .\\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\\nRockt ¨aschel, et al. 2020a. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. arXiv\\npreprint arXiv:2005.11401 .\\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\\n2020b. Question and answer test-train overlap in\\nopen-domain question answering datasets. arXiv\\npreprint arXiv:2008.02637 .\\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\\nNogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\\nJimmy Lin. 2020. Query reformulation using query\\nhistory for passage retrieval in conversational search.\\narXiv preprint arXiv:2005.02230 .\\nYe Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\\nPhilip S Yu. 2019. Generative question reﬁnement\\nwith deep reinforcement learning in retrieval-based\\nqa system. In Proceedings of the 28th ACM Inter-\\nnational Conference on Information and Knowledge\\nManagement , pages 1643–1652.Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle\\nPineau. 2015. The ubuntu dialogue corpus: A large\\ndataset for research in unstructured multi-turn dia-\\nlogue systems. arXiv preprint arXiv:1506.08909 .\\nYi Luan, Jacob Eisenstein, Kristina Toutanova, and\\nMichael Collins. 2020. Sparse, dense, and at-\\ntentional representations for text retrieval. arXiv\\npreprint arXiv:2005.00181 .\\nYuanhua Lv and ChengXiang Zhai. 2010. Positional\\nrelevance model for pseudo-relevance feedback. In\\nProceedings of the 33rd international ACM SIGIR\\nconference on Research and development in infor-\\nmation retrieval , pages 579–586.\\nYuning Mao, Xiang Ren, Heng Ji, and Jiawei Han.\\n2020. Constrained abstractive summarization: Pre-\\nserving factual consistency with constrained genera-\\ntion. arXiv preprint arXiv:2010.12723 .\\nSewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi\\nChen, Eunsol Choi, Michael Collins, Kelvin Guu,\\nHannaneh Hajishirzi, Kenton Lee, Jennimaria Palo-\\nmaki, et al. 2021. Neurips 2020 efﬁcientqa compe-\\ntition: Systems, analyses and lessons learned. arXiv\\npreprint arXiv:2101.00133 .\\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2019a. A discrete hard EM ap-\\nproach for weakly supervised question answering.\\nInProceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the\\n9th International Joint Conference on Natural Lan-\\nguage Processing (EMNLP-IJCNLP) , pages 2851–\\n2864, Hong Kong, China. Association for Computa-\\ntional Linguistics.\\nSewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\\nnaneh Hajishirzi. 2019b. Knowledge guided text re-\\ntrieval and reading for open domain question answer-\\ning. arXiv preprint arXiv:1911.03868 .\\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2020. Ambigqa: Answering\\nambiguous open-domain questions. arXiv preprint\\narXiv:2004.10645 .\\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\\n2016. Ms marco: A human-generated machine read-\\ning comprehension dataset.\\nRodrigo Nogueira and Kyunghyun Cho. 2017. Task-\\noriented query reformulation with reinforcement\\nlearning. In Proceedings of the 2017 Conference on\\nEmpirical Methods in Natural Language Processing ,\\npages 574–583, Copenhagen, Denmark. Association\\nfor Computational Linguistics.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the pa-\\nrameters of a language model? arXiv preprint\\narXiv:2002.08910 .Joseph Rocchio. 1971. Relevance feedback in in-\\nformation retrieval. The Smart retrieval system-\\nexperiments in automatic document processing ,\\npages 313–323.\\nSvitlana Vakulenko, Shayne Longpre, Zhucheng Tu,\\nand Raviteja Anantha. 2020. Question rewriting for\\nconversational question answering. arXiv preprint\\narXiv:2004.14652 .\\nXiao Wang, Craig Macdonald, and Iadh Ounis. 2020.\\nDeep reinforced query reformulation for informa-\\ntion retrieval. arXiv preprint arXiv:2007.07987 .\\nPeilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:\\nEnabling the use of lucene for information retrieval\\nresearch. In Proceedings of the 40th International\\nACM SIGIR Conference on Research and Develop-\\nment in Information Retrieval , pages 1253–1256.\\nShi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong,\\nPaul Bennett, Jianfeng Gao, and Zhiyuan Liu. 2020.\\nFew-shot generative conversational query rewriting.\\narXiv preprint arXiv:2006.05009 .\\nSalah Zaiem and Fatiha Sadat. 2019. Sequence to se-\\nquence learning for query expansion. In Proceed-\\nings of the AAAI Conference on Artiﬁcial Intelli-\\ngence, Student Abstract Track , volume 33, pages\\n10075–10076.A More Analysis of Retrieval\\nPerformance\\nWe show the detailed results of top-k retrieval accu-\\nracy of the compared methods in Figs. 2 and 3.\\nGARperforms comparably or better than DPR\\nwhen k\\x15100on NQ and k\\x155on Trivia.\\n1 5 10 20 50 100 200 300 500 1000\\nk: # of retrieved passages2030405060708090Top-k Accuracy (%)\\nGAR +DPR\\nDPR\\nGAR\\nBM25 +RM3\\nBM25\\nFigure 2: Top-k retrieval accuracy of sparse and\\ndense methods on the test set of NQ. GARimproves\\nBM25 and achieves comparable or better performance\\nthan DPR when k\\x15100.\\n1 5 10 20 50 100\\nk: # of retrieved passages5055606570758085Top-k Accuracy (%)\\nGAR +DPR\\nDPR\\nGAR\\nBM25 +RM3\\nBM25\\nFigure 3: Top-k retrieval accuracy on the Trivia test\\nset.GARachieves better results than DPR when k\\x155.\\nWe show in Table 9 the retrieval accuracy break-\\ndown using the question-answer overlap categories.\\nThe most signiﬁcant gap between BM25 and other\\nmethods is on the Question Overlap category,\\nwhich coincides with the fact that BM25 is un-\\nable to conduct question paraphrasing (semantic\\nmatching). GARhelps BM25 to bridge the gap by\\nproviding the query contexts and even outperform\\nDPR in this category. Moreover, GARconsistently\\nimproves over BM25 on other categories and GAR+\\noutperforms DPR as well.Method TotalQuestion\\nOverlapAnswer\\nOverlap\\nOnlyNo\\nOverlap\\nBM25 78.8 81.2 85.1 70.6\\nDPR 86.1 93.2 89.5 76.8\\nGAR 85.3 94.1 87.9 73.7\\nGAR+88.9 96.3 91.7 79.8\\nTable 9: Top-100 retrieval accuracy by question-\\nanswer overlap categories on the NQ test set.'),\n",
       " Document(page_content='T-RAG: End-to-End Table Question Answering via Retrieval-Augmented\\nGeneration\\nFeifei Pan1, Mustafa Canim2, Michael Glass2, Alﬁo Gliozzo2, James Hendler1\\npanf2@rpi.edu ,mustafa@us.ibm.com ,\\nmrglass@us.ibm.com ,gliozzo@us.ibm.com\\nhendler@cs.rpi.edu\\n1Rensselaer Polytechnic Institute\\n2IBM TJ Watson Research Center\\nAbstract\\nMost existing end-to-end Table Question An-\\nswering (Table QA) models consist of a two-\\nstage framework with a retriever to select rel-\\nevant table candidates from a corpus and a\\nreader to locate the correct answers from table\\ncandidates. Even though the accuracy of the\\nreader models is signiﬁcantly improved with\\nthe recent transformer-based approaches, the\\noverall performance of such frameworks still\\nsuffers from the poor accuracy of using tradi-\\ntional information retrieval techniques as re-\\ntrievers. To alleviate this problem, we intro-\\nduce T-RAG, an end-to-end Table QA model,\\nwhere a non-parametric dense vector index is\\nﬁne-tuned jointly with BART, a parametric\\nsequence-to-sequence model to generate an-\\nswer tokens. Given any natural language ques-\\ntion, T-RAG utilizes a uniﬁed pipeline to auto-\\nmatically search through a table corpus to di-\\nrectly locate the correct answer from table cell.\\nWe apply T-RAG on recent open-domain Table\\nQA benchmarks and demonstrate that the ﬁne-\\ntuned T-RAG model is able to achieve state-\\nof-the-art performance in both the end-to-end\\nTable QA and the table retrieval tasks.\\n1 Introduction\\nTabular data is commonly seen in open-domain\\ndocuments (Cafarella et al., 2009; Zhang and Ba-\\nlog, 2018a), such as the Web and Wikipedia, as\\nwell as in domain-speciﬁc papers, journals, manu-\\nals, and reports. Answering questions over these\\ntables requires table retrieval and understanding\\nof the table structure and content. Table QA task\\nis generally more challenging than executing SQL\\nqueries over relational database tables due to the\\nlack of schema information. Most existing studies\\ntackle Table QA as two separate sub-tasks: (1) Ta-\\nble retrieval (Cafarella et al., 2008, 2009; Zhang\\nand Balog, 2018a; Shraga et al., 2020a,b), and (2)\\nQA over tables (Yu et al., 2018; Herzig et al., 2020;\\nYin et al., 2020; Glass et al., 2020). Recently, theDTR (Herzig et al., 2021) and the CLTR (Pan et al.,\\n2021) models have been proposed as end-to-end\\nsolutions for Table QA. Both models consist of a\\ntwo-step pipeline of a retriever to generate a set of\\ncandidate tables and a reader to answer questions\\nover these tables. The two components are trained\\nindividually, causing error propagation from re-\\ntrievers to readers, i.e. with incorrect table can-\\ndidates, it is impossible for the readers to locate\\nthe correct answer despite the design of the mod-\\nels. While dense retrieval and Retrieval Augmented\\nGeneration (RAG) (Karpukhin et al., 2020; Lewis\\net al., 2020b) have achieved great success in open-\\ndomain QA over free text, none of the studies in\\nthe literature leverage a non-parametric memory\\nmodel along with a parametric memory model for\\nthe open-domain Table QA task.\\nIn this paper, we describe a novel end-to-end\\nTable QA model, T-RAG, replacing the two-step\\nframework with a single training process. To\\ntrain T-RAG, we utilize Dense Passage Retrieval\\n(DPR) (Karpukhin et al., 2020) and RAG strate-\\ngies. Speciﬁcally, we jointly train a DPR compo-\\nnent (Glass et al., 2021) together with the BART-\\nbased (Lewis et al., 2020a) sequence-to-sequence\\n(Seq2Seq) model. To the best of our knowledge,\\nT-RAG is the ﬁrst Table QA model where the\\nquery encoder for a non-parametric dense vector\\nindex is ﬁne-tuned along with a parametric gen-\\neration model. We evaluate the performance of\\nT-RAG on NQ-TABLES (Herzig et al., 2021) and\\nE2E_WTQ (Pan et al., 2021), two recent end-to-\\nend Table QA benchmarks. The experimental re-\\nsults indicate that T-RAG outperforms the state-of-\\nthe-art models on the end-to-end Table QA task.\\nThe major contribution of this work is that, we\\npropose the ﬁrst end-to-end Table QA pipeline,\\nleveraging DPR along with the Seq2Seq compo-\\nnent of RAG. T-RAG employs a simple but effec-\\ntive one-step training that reduces error accumu-\\nlations and simpliﬁes model ﬁne-tuning. In thearXiv:2203.16714v1  [cs.CL]  30 Mar 2022experiments, T-RAG achieves state-of-the-art per-\\nformance on two tasks. We ﬁnd T-RAG improves\\nthe results for end-to-end Table QA on two recent\\nbenchmarks. The RAG component of the end-to-\\nend model ﬁne-tuned over Table QA benchmarks\\nalso yields state-of-the-art results on the table re-\\ntrieval task.\\n2 Related Work\\nTable Retrieval Traditional table retrieval mod-\\nels usually concatenate tables into documents\\nwhile disregarding the underlying tabular structure\\n(Pyreddy and Croft, 1997; Wang and Hu, 2002;\\nLiu et al., 2007; Cafarella et al., 2008, 2009). New\\napproaches are proposed to retrieve tables with a\\nset of features of the table, query and table-query\\npair (Zhang and Balog, 2018b; Sun et al., 2019;\\nBhagavatula et al., 2013; Shraga et al., 2020a).\\nZhang and Balog (2018b) uses semantic similari-\\nties to build an ad-hoc table retrieval model with\\nvarious features. A neural ranking model is in-\\ntroduced in Shraga et al. (2020b), where tables are\\ndeﬁned as multi-modal objects and the Gated Multi-\\nmodal Units are used to learn the representation of\\nquery-table pairs. Pan et al. (2021) later follows\\nthis work and improves the table retrieval with a\\n2-step retriever. Kosti ´c et al. (2021) discusses the\\nuse of dense vector embeddings to enhance the per-\\nformance of bi- and tri-encoder in retrieving both\\ntable and text.\\nTable QA Most early Table QA solutions are\\nfully supervised models, focusing on converting\\nnatural language questions into SQL format and\\nusing the SQL-format questions to query the given\\ntables, as seen in Yu et al. (2018); Lin et al. (2019);\\nXu et al. (2018). Open-domain QA over text (Yu\\net al., 2020) usually utilizes multiple knowledge\\nsources. For instance, Oguz et al. (2021) pro-\\nposes a model can convert structured, unstruc-\\ntured and semi-structured knowledge into text for\\nopen-domain QA. Therefore, more recent efforts\\nhave been put into investigating the use of exter-\\nnal knowledge in enhancing the performance of\\nTable QA. Jiménez-Ruiz et al. (2020) ﬁrst proposes\\nthe Semantic Web Challenge on Tabular Data to\\nKnowledge Graph Matching (SemTab) to encour-\\nage such solutions for both table understanding\\nand Table QA. Recently, the transformer-based,\\nweakly supervised solutions have been proposed\\nfor Table QA. These solutions fall into two cat-\\negories: (1) Logic form-based solution, such astheTABERT (Yin et al., 2020) model, which is\\ntrained to capture the representation of natural lan-\\nguage sentences as well as tabular data; (2) Answer\\ncell prediction solutions, such as TAPAS(Herzig\\net al., 2020) and the RCI (Glass et al., 2020) model.\\nThe current state-of-the-art RCI model exploits a\\ntransformer-based framework. Instead of retrieving\\nthe table cells directly for any given question-table\\npairs, the RCI model identiﬁes the most relevant\\ncolumns and rows independently and locates the\\nintersection table cells as the ﬁnal answers.\\nEnd-to-End Table QA Sun et al. (2016) pub-\\nlishes the ﬁrst end-to-end table cell search frame-\\nwork. This work leverages the semantic relations\\nbetween cells and maps queries to table cells with\\nrelational chains. The DTR model (Herzig et al.,\\n2021) addresses the end-to-end Table QA problem\\nwith a table retriever and a TAPAS-based reader\\nmodel. Later, the CLTR model (Pan et al., 2021)\\nintroduces a similar two-step solution, using BM25\\nas the retriever. The model re-ranks the BM25\\nresults and locates the table cells using the RCI\\nscores. Recently, Chen et al. (2021) proposes a\\nnew task for QA over both free text and tables\\nand provides a solution including a retriever with\\nearly fusion techniques and a cross-block reader.\\nIn addition, the open-domain OTT-QA benchmark\\nis released to evaluate models for end-to-end QA\\nover text and table.\\n3 The End-to-End Table QA with T-RAG\\nThe overall architecture of T-RAG is illustrated in\\nFigure 1. In this example, we encode the questions\\n“who was the editor for Ikar?” using the query en-\\ncoder and pre-process the tables, e.g., T 1and T 2,\\nfrom the table corpus for encoding. The encoded ta-\\nbles are later indexed into the Approximate Nearest\\nNeighbors (ANN) data structure for querying. The\\nencoded question is appended to each table before\\ninputting it to the BART-based generator for answer\\nprediction. The DPR and the RAG components are\\ntrained jointly without explicitly considering the\\ntable-level ground truth.\\nSetup We deﬁne the one-step, end-to-end Table\\nQA task as follows. Given a massive corpus C\\nof tables tiand any natural language question qi,\\nwe train a model to directly generate answer to qi\\nfrom the table cell without any intermediate steps.\\nLabeled datasets are available to us with ground\\ntruth samples in the format of fqi; ti; aigwhere aiANNIndexNatural LanguageQuestionWhowastheeditorforIkar?SeriesYears…EditorIvan Zourine1979…Magic-StripSteve Severin1981–2003…Glénat…………Ikar1995–1997…Glénat…………QueryEncoder\\nTableEncoderPierre Makyo at a book fair in Paris, France, in March 2008.BornJuly 16, 1952(age69)NationalityFrench……TableCorpus\\n…Generator!1!2AnswersGlénatCitroënMagic-Strip…DPRRAGSeries[sep]Ivan Zourine[sep]Title[sep]Year……Figure 1: An overview of T-RAG, a model trained end-to-end to directly locate answers from table corpus.\\nstands for the answers.\\nTable pre-processing is implemented before the\\ntraining. We process the tables tiinto a structure-\\npreserving format, where: (1) column headers are\\nappended before cell values, separated by a special\\nsymbol “ j”; and (2) the separator “*” is appended\\nto the end of each row; (3) for the tables with addi-\\ntional information such as titles, we append them\\nin front of the tables. The tables are segmented\\ninto the length of 512 tokens for training. For each\\nquestion, we retrieve hard negatives from the cor-\\npusCand use them as additional negative samples\\nto enhance the T-RAG training.\\nSoft Hard Negatives We implement a BM25-\\nbased hard negative mining for T-RAG. For each\\nquestion, we ﬁrst retrieve a pool of the most rel-\\nevant tables from the corpus using BM25. From\\nthe table pool, we discard the ground truth table.\\nThe top-ranked, non-positive tables are used as the\\nhard negative candidates. In the training process,\\ninstead of using the top 1 negative table, we exploit\\na soft hard negative mechanism, where we select\\nthe hard negative at random from the top knegative\\ntables.\\nRAG For the implementation of RAG, we jointly\\ntrain a DPR-based retriever and a BART-based gen-\\nerator. We index the tables in Cusing a keyword-\\nbased search engine, Anserini1, to harvest the hard\\nnegative training samples using BM25. Later, T-\\nRAG exploits BERT BASE to encode questions\\nalong with the ground truth table and the hard neg-\\native tables. To train RAG, T-RAG employs the\\nanswer-level ground truth and use a Seq2Seq gen-\\n1https://github.com/castorini/anserinierator, the BART LARGE model, for answer predic-\\ntions. The previously encoded tables are indexed\\nwith the open-source FAISS (Johnson et al., 2017)\\nlibrary into the ANN data structure for querying.\\nThe encoded questions are concatenated to each\\nof the top retrieved tables and used as a prompt to\\ngenerate the answer. More concretely, the gener-\\nator predicts probability distributions for possible\\nanswer candidates as the next token. The probabil-\\nity distributions are later marginalized to produce\\na single weighted sequence probability for each\\nanswer candidate. Finally, a standard beam search\\ndecoder (Sutskever et al., 2014) is used to identify\\nthe most relevant candidates as the ﬁnal answers to\\nthe questions at test time. Along with the answers,\\nour model can also return the relevant table ticon-\\ntaining the correct answers from Cfor evaluation\\nand annotation purposes.\\n4 Experiments\\nData We validate T-RAG on two open-domain\\nbenchmarks, NQ-TABLES andE2E_WTQ . NQ-\\nTABLES is the table subset of the Natural Ques-\\ntions dataset (Kwiatkowski et al., 2019), with a\\ntable corpus extracted from the English Wikipedia\\narticles and samples in the fq; T; a gformat, where\\nq,T, and adenote question, ground truth table,\\nand answer, respectively. E2E_WTQ contains the\\nlook-up subset of WikiTableQuestions (Pasupat\\nand Liang, 2015). While a substantial amount of\\ntables in NQ-TABLES are transposed infobox ta-\\nbles, the E2E_WTQ only contains well-formatted\\nbut more complex tables. The data statistics are\\nshown in Table 1.Data Train Dev Test Corpus\\nNQ-TABLES 9,594 1,068 966 169,898\\nE2E_WTQ 851 124 241 2,108\\nTable 1: Data Statistics\\nModel EM F1 Oracle EM Oracle F1\\nDTR+hn 37.69 47.70 48.20 61.50\\nT-RAG 43.06 50.92 50.62 63.18\\n(a) End-to-end Table QA results on the test set of NQ-TABLES.\\nModel MRR Hit@1\\nCLTR 0.5503 0.4675\\nT-RAG 0.5923 0.5065\\n(b) End-to-end Table QA results on the test set of E2E_WTQ.\\nTable 2: Experimental results on End-to-end Table QA.\\nExperimental Settings In the experiments, we\\nﬁrst encode the questions and tables using\\nBERT BASE , and later jointly train the DPR-based\\nretriever and the Seq2Seq generator of RAG. For\\nthe experiments, we set: (1) training batch size =\\n128; (2) number of epochs = 2; (3) learning rate =\\n3e-5; and (4) gradient accumulation steps = 64.\\nEvaluation metrics: Following the evaluation\\nscript in SQUAD (Rajpurkar et al., 2016), we eval-\\nuate end-to-end Table QA using exact match (EM)\\nand token F1 metrics for NQ-TABLES. The ac-\\ncuracy for the top 1 returned answer and mean\\nreciprocal rank (MRR) are used to measure the per-\\nformance on E2E_WTQ. We also evaluate T-RAG\\non the table retrieval task for a fair comparison\\nwith existing work. We utilize the original met-\\nrics in Herzig et al. (2021) and Pan et al. (2021),\\nwith recall (R) for NQ-TABLES, and precision (P),\\nnormalized discounted gain (N), and mean average\\nprecision (MAP) for E2E_WTQ.\\nExperimental Results We compare the end-to-\\nend Table QA performance of T-RAG against the\\nstate-of-the-art DTR and CLTR models in Table 2.\\nWe ﬁnd T-RAG yields better results than the previ-\\nous best models for both datasets with all evalua-\\ntion metrics.\\nTo further validate T-RAG against the existing\\nmodels, we also evaluate the model performance on\\ntable retrieval. The table retrieval results for NQ-\\nTABLES and E2E_WTQ are shown in Table 3a\\nand 3b, respectively. The results indicate that T-\\nRAG outperforms the simple baselines models such\\nas BM25, as well as the strong state-of-the-art mod-\\nels in the experiments.\\nQualitative Analysis We further evaluate the\\ntable retrieval results on NQ-TABLES. We no-Model R@1 R@10 R@50\\nBM25 16.77 40.06 58.39\\nDTR+hn 42.42 81.13 92.56\\nT-RAG 46.07 85.40 95.03\\n(a) Table retrieval results on the test set of NQ-TABLES.\\nModel P@5 P@10 N@5 N@10 MAP\\nBM25 0.5938 0.6587 0.5228 0.5356 0.4704\\nCLTR 0.7437 0.8735 0.6915 0.7119 0.5971\\nT-RAG 0.7806 0.8943 0.7250 0.7467 0.6404\\n(b) Table retrieval results on the test set of E2E_WTQ.\\nTable 3: Experimental results on Table Retrieval.\\ntice that the DPR-based baseline of our approach\\nachieves 43.89 for R@1 and 81.57 for R@10; both\\noutperform the state-of-the-art DTR results. In\\naddition, the retrieval performance is further im-\\nproved with the more effective end-to-end RAG\\ntraining. To validate the effectiveness of our soft\\nhard negative technique, we test the method on the\\nE2E_WTQ dataset. Instead of using the top 1 nega-\\ntive table from the BM25 results, we set k= 3and\\nachieve a 27.17% absolute gain for Hit@1 accuracy\\nin the end-to-end Table QA task.\\nBesides, we perform thorough error analysis on\\non E2E_WTQ and ﬁnd that over 21% of the errors\\ncome from questions that involve numerical values.\\nThe ﬁnding indicates that understanding different\\ntypes of numbers remains a challenge in Table QA.\\n5 Conclusion and Future Work\\nIn this paper, we present a novel Table QA model\\nthat achieves state-of-the-art performance on recent\\nbenchmarks. Instead of training a retriever and\\na reader model independently, T-RAG uniﬁes the\\nprocedure into a single pipeline of only one training\\nstep, which reduces the error accumulations from\\ntwo separate models. In the experiments, T-RAG\\noutperforms the current best models for end-to-\\nend Table QA. We additionally demonstrate the\\nadvantages of T-RAG with the table retrieval task,\\nand T-RAG beats the existing numbers on both\\nbenchmarks.\\nIn the future, we plan to validate T-RAG on\\ndomain-speciﬁc datasets, such as AIT-QA and TAT-\\nQA (Katsis et al., 2021; Zhu et al., 2021) and\\nextend the model to solve multi-modal QA prob-\\nlems, with the corpus containing both tables and\\npassages, as presented in the OTT-QA and Hybrid-\\nQA benchmarks (Chen et al., 2020a,b). To further\\nimprove the model performance, we also plan to\\ninvestigate algorithms to better understand numeric\\nvalues.References\\nChandra Bhagavatula, Thanapon Noraset, and Doug\\nDowney. 2013. Methods for exploring and min-\\ning tables on wikipedia. Proceedings of the ACM\\nSIGKDD Workshop on Interactive Data Exploration\\nand Analytics .\\nMichael J Cafarella, Alon Halevy, and Nodira Khous-\\nsainova. 2009. Data integration for the rela-\\ntional web. Proceedings of the VLDB Endowment ,\\n2(1):1090–1101.\\nMichael J Cafarella, Alon Halevy, Daisy Zhe Wang, Eu-\\ngene Wu, and Yang Zhang. 2008. Webtables: ex-\\nploring the power of tables on the web. Proceedings\\nof the VLDB Endowment , 1(1):538–549.\\nWenhu Chen, Ming-Wei Chang, Eva Schlinger,\\nWilliam Yang Wang, and William W Cohen. 2020a.\\nOpen question answering over tables and text. In\\nInternational Conference on Learning Representa-\\ntions .\\nWenhu Chen, Ming-Wei Chang, Eva Schlinger,\\nWilliam Yang Wang, and William W. Cohen. 2021.\\nOpen question answering over tables and text. In\\nInternational Conference on Learning Representa-\\ntions .\\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan\\nXiong, Hong Wang, and William Yang Wang. 2020b.\\nHybridqa: A dataset of multi-hop question answer-\\ning over tabular and textual data. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing: Findings , pages 1026–\\n1036.\\nMichael Glass, Mustafa Canim, Alﬁo Gliozzo, Saneem\\nChemmengath, Rishav Chakravarti, Avi Sil, Feifei\\nPan, Samarth Bharadwaj, and Nicolas Rodolfo\\nFauceglia. 2020. Capturing row and column seman-\\ntics in transformer based question answering over ta-\\nbles. Proceedings of the Annual Conference of the\\nNorth American Chapter of the Association for Com-\\nputational Linguistics (NAACL-HLT2020) .\\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub\\nChowdhury, and Alﬁo Gliozzo. 2021. Robust re-\\ntrieval augmented generation for zero-shot slot ﬁll-\\ning. In Proceedings of the 2021 Conference on Em-\\npirical Methods in Natural Language Processing ,\\npages 1939–1949, Online and Punta Cana, Domini-\\ncan Republic. Association for Computational Lin-\\nguistics.\\nJonathan Herzig, Thomas Müller, Syrine Krichene, and\\nJulian Martin Eisenschlos. 2021. Open domain ques-\\ntion answering over tables via dense retrieval. arXiv\\npreprint arXiv:2103.12011 .\\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\\nMüller, Francesco Piccinno, and Julian Eisensch-\\nlos. 2020. TaPas: Weakly supervised table pars-\\ning via pre-training. In Proceedings of the 58th An-\\nnual Meeting of the Association for ComputationalLinguistics , pages 4320–4333, Seattle, Washington,\\nUnited States. Association for Computational Lin-\\nguistics.\\nErnesto Jiménez-Ruiz, Oktie Hassanzadeh, Vasilis\\nEfthymiou, Jiaoyan Chen, and Kavitha Srinivas.\\n2020. Semtab 2019: Resources to benchmark tab-\\nular data to knowledge graph matching systems. In\\nESWC , pages 514–530.\\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.\\nBillion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 .\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 6769–\\n6781, Online. Association for Computational Lin-\\nguistics.\\nYannis Katsis, Saneem Chemmengath, Vishwajeet\\nKumar, Samarth Bharadwaj, Mustafa Canim,\\nMichael Glass, Alﬁo Gliozzo, Feifei Pan, Jay-\\ndeep Sen, Karthik Sankaranarayanan, and Soumen\\nChakrabarti. 2021. Ait-qa: Question answering\\ndataset over complex tables in the airline industry.\\nBogdan Kosti ´c, Julian Risch, and Timo Möller. 2021.\\nMulti-modal retrieval of tables and texts using tri-\\nencoder models.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral questions: a benchmark for question answering\\nresearch. Transactions of the Association of Compu-\\ntational Linguistics .\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\\n2020a. BART: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation,\\nand comprehension. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics , pages 7871–7880, Online. Association\\nfor Computational Linguistics.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020b.\\nRetrieval-augmented generation for knowledge-\\nintensive nlp tasks. In Advances in Neural Infor-\\nmation Processing Systems , volume 33, pages 9459–\\n9474. Curran Associates, Inc.\\nKevin Lin, Ben Bogin, Mark Neumann, Jonathan Be-\\nrant, and Matt Gardner. 2019. Grammar-based neu-\\nral text-to-sql generation.Ying Liu, Kun Bai, Prasenjit Mitra, and C Lee Giles.\\n2007. Tableseer: automatic table metadata extrac-\\ntion and searching in digital libraries. In Proceed-\\nings of the 7th ACM/IEEE-CS joint conference on\\nDigital libraries , pages 91–100.\\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin,\\nStan Peshterliev, Dmytro Okhonko, Michael\\nSchlichtkrull, Sonal Gupta, Yashar Mehdad, and\\nScott Yih. 2021. Unik-qa: Uniﬁed representations\\nof structured and unstructured knowledge for\\nopen-domain question answering.\\nFeifei Pan, Mustafa Canim, Michael Glass, Alﬁo\\nGliozzo, and Peter Fox. 2021. CLTR: An end-to-\\nend, transformer-based system for cell-level table re-\\ntrieval and table question answering. In Proceed-\\nings of the 59th Annual Meeting of the Association\\nfor Computational Linguistics and the 11th Interna-\\ntional Joint Conference on Natural Language Pro-\\ncessing: System Demonstrations , pages 202–209,\\nOnline. Association for Computational Linguistics.\\nPanupong Pasupat and Percy Liang. 2015. Composi-\\ntional semantic parsing on semi-structured tables.\\nPallavi Pyreddy and W Bruce Croft. 1997. Tintin: A\\nsystem for retrieval in text tables. In Proceedings of\\nthe second ACM international conference on Digital\\nlibraries , pages 193–200.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. SQuAD: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Natu-\\nral Language Processing , pages 2383–2392, Austin,\\nTexas. Association for Computational Linguistics.\\nRoee Shraga, Haggai Roitman, Guy Feigenblat, and\\nMustafa Canim. 2020a. Ad hoc table retrieval using\\nintrinsic and extrinsic similarities. In Proceedings of\\nThe Web Conference 2020 , pages 2479–2485.\\nRoee Shraga, Haggai Roitman, Guy Feigenblat, and\\nMustafa Cannim. 2020b. Web table retrieval using\\nmultimodal deep learning. In Proceedings of the\\n43rd International ACM SIGIR Conference on Re-\\nsearch and Development in Information Retrieval ,\\nSIGIR ’20, page 1399–1408, New York, NY , USA.\\nAssociation for Computing Machinery.\\nHuan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su,\\nand Xifeng Yan. 2016. Table cell search for question\\nanswering. In Proceedings of the 25th International\\nConference on World Wide Web , pages 771–782.\\nYibo Sun, Zhao Yan, Duyu Tang, Nan Duan, and Bing\\nQin. 2019. Content-based table retrieval for web\\nqueries. Neurocomputing , 349:183–189.\\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\\nSequence to sequence learning with neural networks.\\nInAdvances in neural information processing sys-\\ntems, pages 3104–3112.Yalin Wang and Jianying Hu. 2002. A machine learn-\\ning based approach for table detection on the web.\\nInProceedings of the 11th International Conference\\non World Wide Web , WWW ’02, page 242–250, New\\nYork, NY , USA. Association for Computing Machin-\\nery.\\nXiaojun Xu, Chang Liu, and Dawn Song. 2018. SQL-\\nNet: Generating structured queries from natural lan-\\nguage without reinforcement learning.\\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\\nbastian Riedel. 2020. TaBERT: Pretraining for joint\\nunderstanding of textual and tabular data. In Pro-\\nceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics , pages 8413–\\n8426, Online. Association for Computational Lin-\\nguistics.\\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\\nDongxu Wang, Zifan Li, James Ma, Irene Li,\\nQingning Yao, Shanelle Roman, Zilin Zhang,\\nand Dragomir Radev. 2018. Spider: A large-\\nscale human-labeled dataset for complex and cross-\\ndomain semantic parsing and text-to-SQL task.\\nWenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,\\nQingyun Wang, Heng Ji, and Meng Jiang. 2020.\\nA survey of knowledge-enhanced text generation.\\narXiv preprint arXiv:2010.04389 .\\nShuo Zhang and K. Balog. 2018a. Ad hoc table re-\\ntrieval using semantic similarity. Proceedings of the\\n2018 World Wide Web Conference .\\nShuo Zhang and Krisztian Balog. 2018b. Ad hoc ta-\\nble retrieval using semantic similarity. In Proceed-\\nings of the 2018 World Wide Web Conference , pages\\n1553–1562.\\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao\\nWang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and\\nTat-Seng Chua. 2021. TAT-QA: A question answer-\\ning benchmark on a hybrid of tabular and textual\\ncontent in ﬁnance. In Proceedings of the 59th An-\\nnual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Confer-\\nence on Natural Language Processing (Volume 1:\\nLong Papers) , pages 3277–3287, Online. Associa-\\ntion for Computational Linguistics.'),\n",
       " Document(page_content='the test set of Natural Questions and TriviaQA mea-\\nsured by exact match. In the open-book setting, we\\ninclude the top two documents returned by DPR.\\n7 In-Context RALM for Open-Domain\\nQuestion Answering\\nSo far, we evaluated our framework on language\\nmodeling benchmarks. To test its efficacy in addi-\\ntional scenarios, and specifically downstream tasks,\\nwe now turn to evaluate In-Context RALM on open-\\ndomain question answering (ODQA; Chen et al.\\n2017). This experiment is intended to verify, in\\na controlled environment, that LMs can leverage\\nretrieved documents without further training and\\nwithout any training examples . Specifically, we\\nuse the LLaMA family (Touvron et al., 2023) with\\nandwithout In-Context RALM (often referred to\\nin ODQA literature as open-book and closed-book\\nsettings, respectively). In contrast to most prior\\nwork on ODQA ( e.g., Izacard and Grave 2021; Fa-\\njcik et al. 2021; Izacard et al. 2022b; Levine et al.\\n2022b), our “reader” ( i.e., the model that gets the', metadata={'page': 9, 'source': './data\\\\In context retrieval.pdf'}),\n",
       " Document(page_content='[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', metadata={'page': 4, 'source': './data\\\\RAG on large language models.pdf'}),\n",
       " Document(page_content='models are few-shot learners. In Advances in\\nNeural Information Processing Systems .\\nDanqi Chen, Adam Fisch, Jason Weston, and An-\\ntoine Bordes. 2017. Reading Wikipedia to an-\\nswer open-domain questions. In Proceedings\\nof the 55th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long', metadata={'page': 10, 'source': './data\\\\In context retrieval.pdf'}),\n",
       " Document(page_content='Generation-Augmented Retrieval for Open-Domain Question Answering\\nYuning Mao1∗, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\\nJianfeng Gao3, Jiawei Han1, Weizhu Chen2\\n1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\\n1{yuningm2, hanj}@illinois.edu\\n2,3{penhe, xiaodl, yeshe, jfgao,wzchen }@microsoft.com\\nAbstract\\nWe propose Generation-Augmented Retrieval\\n(GAR) for answering open-domain questions,\\nwhich augments a query through text genera-\\ntion of heuristically discovered relevant con-\\ntexts without external resources as supervi-\\nsion. We demonstrate that the generated con-\\ntexts substantially enrich the semantics of the\\nqueries and G ARwith sparse representations\\n(BM25) achieves comparable or better per-\\nformance than state-of-the-art dense retrieval\\nmethods such as DPR (Karpukhin et al., 2020).\\nWe show that generating diverse contexts for a\\nquery is beneﬁcial as fusing their results con-\\nsistently yields better retrieval accuracy. More-', metadata={'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='tition: Systems, analyses and lessons learned. arXiv\\npreprint arXiv:2101.00133 .\\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2019a. A discrete hard EM ap-\\nproach for weakly supervised question answering.\\nInProceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the\\n9th International Joint Conference on Natural Lan-\\nguage Processing (EMNLP-IJCNLP) , pages 2851–\\n2864, Hong Kong, China. Association for Computa-\\ntional Linguistics.\\nSewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\\nnaneh Hajishirzi. 2019b. Knowledge guided text re-\\ntrieval and reading for open domain question answer-\\ning. arXiv preprint arXiv:1911.03868 .\\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2020. Ambigqa: Answering\\nambiguous open-domain questions. arXiv preprint\\narXiv:2004.10645 .\\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\\n2016. Ms marco: A human-generated machine read-', metadata={'page': 10, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. In Proceedings of\\nthe 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 6769–\\n6781, Online. Association for Computational Lin-\\nguistics.\\nYannis Katsis, Saneem Chemmengath, Vishwajeet\\nKumar, Samarth Bharadwaj, Mustafa Canim,\\nMichael Glass, Alﬁo Gliozzo, Feifei Pan, Jay-\\ndeep Sen, Karthik Sankaranarayanan, and Soumen\\nChakrabarti. 2021. Ait-qa: Question answering\\ndataset over complex tables in the airline industry.\\nBogdan Kosti ´c, Julian Risch, and Timo Möller. 2021.\\nMulti-modal retrieval of tables and texts using tri-\\nencoder models.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob', metadata={'page': 4, 'source': './data\\\\T-RAG End-to-End Table Question Answering via Retrieval-Augmented.pdf'}),\n",
       " Document(page_content='Unlike generative QA, the goal of GARis not to\\ngenerate perfect answers to the questions but perti-\\nnent contexts that are helpful for retrieval. Another\\nline in generative QA learns to generate answers\\nwithout relevant passages as the evidence but solely\\nthe question itself using PLMs (Roberts et al., 2020;\\nBrown et al., 2020). GARfurther conﬁrms that one\\ncan extract factual knowledge from PLMs, which\\nis not limited to the answers as in prior studies but\\nalso other relevant contexts.\\n3 Generation-Augmented Retrieval\\n3.1 Task Formulation\\nOpenQA aims to answer factoid questions with-\\nout pre-speciﬁed domains. We assume that a large\\ncollection of documents C(i.e., Wikipedia) are\\ngiven as the resource to answer the questions and\\na retriever-reader architecture is used to tackle the\\ntask, where the retriever retrieves a small subset\\nof the documents D⊂Cand the reader reads the\\ndocuments Dto extract (or generate) an answer.\\nOur goal is to improve the effectiveness and efﬁ-', metadata={'page': 2, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='Recent Query Reformulation . There are recent\\nor concurrent studies (Nogueira and Cho, 2017;\\nZaiem and Sadat, 2019; Yu et al., 2020; Vaku-\\nlenko et al., 2020; Lin et al., 2020) that reformu-\\nlate queries with generation models for other re-\\ntrieval tasks. However, these studies are not eas-\\nily applicable or efﬁcient enough for OpenQA be-\\ncause: (1) They require external resources such as\\nparaphrase data (Zaiem and Sadat, 2019), search\\nsessions (Yu et al., 2020), or conversational con-\\ntexts (Lin et al., 2020; Vakulenko et al., 2020)\\nto form the reformulated queries, which are not\\navailable or showed inferior domain-transfer per-\\nformance in OpenQA (Zaiem and Sadat, 2019);\\n(2) They involve time-consuming training process\\nsuch as RL. For example, Nogueira and Cho (2017)\\nreported a training time of 8 to 10 days as it uses\\nretrieval performance in the reward function and\\nconducts retrieval at each iteration. In contrast,\\nGARuses freely accessible in-domain contexts like', metadata={'page': 1, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\nNatural questions: A benchmark for question an-\\nswering research. Transactions of the Association\\nfor Computational Linguistics , 7:452–466.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 6086–6096, Florence,\\nItaly. Association for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\\nBart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and', metadata={'page': 9, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ensemble_retriever.get_relevant_documents(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af172f1f-9790-4076-900b-b26db9347ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
