{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f14beb8b-cf28-4dc2-a9dd-83ecf3812875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import openai\n",
    "from langchain.llms import openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_transformers import (\n",
    "    EmbeddingsRedundantFilter,\n",
    "    EmbeddingsClusteringFilter,\n",
    ")\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader , PdfWriter, PdfMerger\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a5b9d16-f4a9-4829-9146-b546a86c9c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection already exists\n",
      "-0.0006200000007083872\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = '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'\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "def load_chunk_persist_pdf() -> Chroma:\n",
    "    start = timeit.timeit()\n",
    "    pdf_folder_path = \"./data\"\n",
    "    documents = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    client = chromadb.Client()\n",
    "    if client.list_collections():\n",
    "        consent_collection = client.create_collection(\"consent_collection\")\n",
    "    else:\n",
    "        print(\"Collection already exists\")\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunked_documents,\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "        persist_directory=\"store/chroma\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    end = timeit.timeit()\n",
    "    print(start - end)\n",
    "    return vectordb\n",
    "\n",
    "c_docs = load_chunk_persist_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec6bce19-7b05-4199-95b1-0b26e2d92000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "529"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83477cf1-2566-4c0a-b108-154dcdc39eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = c_docs.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":10, \"include_metadata\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88b3eaf1-e54e-4272-a858-f91cc1e9d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Open-domain question answering?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96e2d374-5689-4c14-bf06-88cdf5f18c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', metadata={'page': 4, 'source': './data\\\\RAG on large language models.pdf'}),\n",
       " Document(page_content='[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', metadata={'page': 4, 'source': './data\\\\RAG on large language models.pdf'}),\n",
       " Document(page_content='[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', metadata={'page': 4, 'source': './data\\\\RAG on large language models.pdf'}),\n",
       " Document(page_content='[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', metadata={'page': 4, 'source': './data\\\\RAG on large language models.pdf'}),\n",
       " Document(page_content='[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', metadata={'page': 4, 'source': './data\\\\RAG on large language models.pdf'}),\n",
       " Document(page_content='over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', metadata={'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', metadata={'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', metadata={'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', metadata={'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', metadata={'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'})]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7fea99ce-04a6-4538-949a-105a8fea0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7887da49-1a6f-4648-8e8a-e686ae3050f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 4\n",
      "Source: ./data\\RAG on large language models.pdf\n",
      "Content: [14] G. Izacard and E. Grave, “Leveraging passage retrieval with\n",
      "generative models for open domain question answering,” arXiv\n",
      "preprint arXiv:2007.01282 , 2020.\n",
      "[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\n",
      "question answering research,” Transactions of the Association\n",
      "for Computational Linguistics , vol. 7, pp. 452–466, 2019.\n",
      "[Online]. Available: https://aclanthology.org/Q19-1026\n",
      "[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\n",
      "A large scale distantly supervised challenge dataset for\n",
      "reading comprehension,” in Proceedings of the 55th Annual\n",
      "Meeting of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers) . Vancouver, Canada: Association\n",
      "for Computational Linguistics, Jul. 2017, pp. 1601–1611.\n",
      "[Online]. Available: https://aclanthology.org/P17-1147\n",
      "[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\n",
      "“Constructing a multi-hop QA dataset for comprehensive\n",
      "evaluation of reasoning steps,” in Proceedings of the\n",
      "-----------------------------------------------------------\n",
      "Page: 4\n",
      "Source: ./data\\RAG on large language models.pdf\n",
      "Content: [14] G. Izacard and E. Grave, “Leveraging passage retrieval with\n",
      "generative models for open domain question answering,” arXiv\n",
      "preprint arXiv:2007.01282 , 2020.\n",
      "[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\n",
      "question answering research,” Transactions of the Association\n",
      "for Computational Linguistics , vol. 7, pp. 452–466, 2019.\n",
      "[Online]. Available: https://aclanthology.org/Q19-1026\n",
      "[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\n",
      "A large scale distantly supervised challenge dataset for\n",
      "reading comprehension,” in Proceedings of the 55th Annual\n",
      "Meeting of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers) . Vancouver, Canada: Association\n",
      "for Computational Linguistics, Jul. 2017, pp. 1601–1611.\n",
      "[Online]. Available: https://aclanthology.org/P17-1147\n",
      "[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\n",
      "“Constructing a multi-hop QA dataset for comprehensive\n",
      "evaluation of reasoning steps,” in Proceedings of the\n",
      "-----------------------------------------------------------\n",
      "Page: 4\n",
      "Source: ./data\\RAG on large language models.pdf\n",
      "Content: [14] G. Izacard and E. Grave, “Leveraging passage retrieval with\n",
      "generative models for open domain question answering,” arXiv\n",
      "preprint arXiv:2007.01282 , 2020.\n",
      "[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\n",
      "question answering research,” Transactions of the Association\n",
      "for Computational Linguistics , vol. 7, pp. 452–466, 2019.\n",
      "[Online]. Available: https://aclanthology.org/Q19-1026\n",
      "[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\n",
      "A large scale distantly supervised challenge dataset for\n",
      "reading comprehension,” in Proceedings of the 55th Annual\n",
      "Meeting of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers) . Vancouver, Canada: Association\n",
      "for Computational Linguistics, Jul. 2017, pp. 1601–1611.\n",
      "[Online]. Available: https://aclanthology.org/P17-1147\n",
      "[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\n",
      "“Constructing a multi-hop QA dataset for comprehensive\n",
      "evaluation of reasoning steps,” in Proceedings of the\n",
      "-----------------------------------------------------------\n",
      "Page: 4\n",
      "Source: ./data\\RAG on large language models.pdf\n",
      "Content: [14] G. Izacard and E. Grave, “Leveraging passage retrieval with\n",
      "generative models for open domain question answering,” arXiv\n",
      "preprint arXiv:2007.01282 , 2020.\n",
      "[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\n",
      "question answering research,” Transactions of the Association\n",
      "for Computational Linguistics , vol. 7, pp. 452–466, 2019.\n",
      "[Online]. Available: https://aclanthology.org/Q19-1026\n",
      "[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\n",
      "A large scale distantly supervised challenge dataset for\n",
      "reading comprehension,” in Proceedings of the 55th Annual\n",
      "Meeting of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers) . Vancouver, Canada: Association\n",
      "for Computational Linguistics, Jul. 2017, pp. 1601–1611.\n",
      "[Online]. Available: https://aclanthology.org/P17-1147\n",
      "[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\n",
      "“Constructing a multi-hop QA dataset for comprehensive\n",
      "evaluation of reasoning steps,” in Proceedings of the\n",
      "-----------------------------------------------------------\n",
      "Page: 4\n",
      "Source: ./data\\RAG on large language models.pdf\n",
      "Content: [14] G. Izacard and E. Grave, “Leveraging passage retrieval with\n",
      "generative models for open domain question answering,” arXiv\n",
      "preprint arXiv:2007.01282 , 2020.\n",
      "[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\n",
      "question answering research,” Transactions of the Association\n",
      "for Computational Linguistics , vol. 7, pp. 452–466, 2019.\n",
      "[Online]. Available: https://aclanthology.org/Q19-1026\n",
      "[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\n",
      "A large scale distantly supervised challenge dataset for\n",
      "reading comprehension,” in Proceedings of the 55th Annual\n",
      "Meeting of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers) . Vancouver, Canada: Association\n",
      "for Computational Linguistics, Jul. 2017, pp. 1601–1611.\n",
      "[Online]. Available: https://aclanthology.org/P17-1147\n",
      "[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\n",
      "“Constructing a multi-hop QA dataset for comprehensive\n",
      "evaluation of reasoning steps,” in Proceedings of the\n",
      "-----------------------------------------------------------\n",
      "Page: 0\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: over, as sparse and dense representations are\n",
      "often complementary, G ARcan be easily com-\n",
      "bined with DPR to achieve even better per-\n",
      "formance. G ARachieves state-of-the-art per-\n",
      "formance on Natural Questions and TriviaQA\n",
      "datasets under the extractive QA setup when\n",
      "equipped with an extractive reader, and con-\n",
      "sistently outperforms other retrieval methods\n",
      "when the same generative reader is used.1\n",
      "1 Introduction\n",
      "Open-domain question answering (OpenQA) aims\n",
      "to answer factoid questions without a pre-speciﬁed\n",
      "domain and has numerous real-world applications.\n",
      "In OpenQA, a large collection of documents ( e.g.,\n",
      "Wikipedia) are often used to seek information per-\n",
      "taining to the questions. One of the most com-\n",
      "mon approaches uses a retriever-reader architecture\n",
      "(Chen et al., 2017), which ﬁrst retrieves a small sub-\n",
      "set of documents using the question as the query\n",
      "and then reads the retrieved documents to extract\n",
      "(or generate) an answer. The retriever is crucial as it\n",
      "-----------------------------------------------------------\n",
      "Page: 0\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: over, as sparse and dense representations are\n",
      "often complementary, G ARcan be easily com-\n",
      "bined with DPR to achieve even better per-\n",
      "formance. G ARachieves state-of-the-art per-\n",
      "formance on Natural Questions and TriviaQA\n",
      "datasets under the extractive QA setup when\n",
      "equipped with an extractive reader, and con-\n",
      "sistently outperforms other retrieval methods\n",
      "when the same generative reader is used.1\n",
      "1 Introduction\n",
      "Open-domain question answering (OpenQA) aims\n",
      "to answer factoid questions without a pre-speciﬁed\n",
      "domain and has numerous real-world applications.\n",
      "In OpenQA, a large collection of documents ( e.g.,\n",
      "Wikipedia) are often used to seek information per-\n",
      "taining to the questions. One of the most com-\n",
      "mon approaches uses a retriever-reader architecture\n",
      "(Chen et al., 2017), which ﬁrst retrieves a small sub-\n",
      "set of documents using the question as the query\n",
      "and then reads the retrieved documents to extract\n",
      "(or generate) an answer. The retriever is crucial as it\n",
      "-----------------------------------------------------------\n",
      "Page: 0\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: over, as sparse and dense representations are\n",
      "often complementary, G ARcan be easily com-\n",
      "bined with DPR to achieve even better per-\n",
      "formance. G ARachieves state-of-the-art per-\n",
      "formance on Natural Questions and TriviaQA\n",
      "datasets under the extractive QA setup when\n",
      "equipped with an extractive reader, and con-\n",
      "sistently outperforms other retrieval methods\n",
      "when the same generative reader is used.1\n",
      "1 Introduction\n",
      "Open-domain question answering (OpenQA) aims\n",
      "to answer factoid questions without a pre-speciﬁed\n",
      "domain and has numerous real-world applications.\n",
      "In OpenQA, a large collection of documents ( e.g.,\n",
      "Wikipedia) are often used to seek information per-\n",
      "taining to the questions. One of the most com-\n",
      "mon approaches uses a retriever-reader architecture\n",
      "(Chen et al., 2017), which ﬁrst retrieves a small sub-\n",
      "set of documents using the question as the query\n",
      "and then reads the retrieved documents to extract\n",
      "(or generate) an answer. The retriever is crucial as it\n",
      "-----------------------------------------------------------\n",
      "Page: 0\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: over, as sparse and dense representations are\n",
      "often complementary, G ARcan be easily com-\n",
      "bined with DPR to achieve even better per-\n",
      "formance. G ARachieves state-of-the-art per-\n",
      "formance on Natural Questions and TriviaQA\n",
      "datasets under the extractive QA setup when\n",
      "equipped with an extractive reader, and con-\n",
      "sistently outperforms other retrieval methods\n",
      "when the same generative reader is used.1\n",
      "1 Introduction\n",
      "Open-domain question answering (OpenQA) aims\n",
      "to answer factoid questions without a pre-speciﬁed\n",
      "domain and has numerous real-world applications.\n",
      "In OpenQA, a large collection of documents ( e.g.,\n",
      "Wikipedia) are often used to seek information per-\n",
      "taining to the questions. One of the most com-\n",
      "mon approaches uses a retriever-reader architecture\n",
      "(Chen et al., 2017), which ﬁrst retrieves a small sub-\n",
      "set of documents using the question as the query\n",
      "and then reads the retrieved documents to extract\n",
      "(or generate) an answer. The retriever is crucial as it\n",
      "-----------------------------------------------------------\n",
      "Page: 0\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: over, as sparse and dense representations are\n",
      "often complementary, G ARcan be easily com-\n",
      "bined with DPR to achieve even better per-\n",
      "formance. G ARachieves state-of-the-art per-\n",
      "formance on Natural Questions and TriviaQA\n",
      "datasets under the extractive QA setup when\n",
      "equipped with an extractive reader, and con-\n",
      "sistently outperforms other retrieval methods\n",
      "when the same generative reader is used.1\n",
      "1 Introduction\n",
      "Open-domain question answering (OpenQA) aims\n",
      "to answer factoid questions without a pre-speciﬁed\n",
      "domain and has numerous real-world applications.\n",
      "In OpenQA, a large collection of documents ( e.g.,\n",
      "Wikipedia) are often used to seek information per-\n",
      "taining to the questions. One of the most com-\n",
      "mon approaches uses a retriever-reader architecture\n",
      "(Chen et al., 2017), which ﬁrst retrieves a small sub-\n",
      "set of documents using the question as the query\n",
      "and then reads the retrieved documents to extract\n",
      "(or generate) an answer. The retriever is crucial as it\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def convert_docs_to_dict(docs):\n",
    "    \"\"\"\n",
    "    Convert a list of Document objects to a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "    - docs (list): List of Document objects.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of dictionaries containing 'page_content' and 'metadata'.\n",
    "    \"\"\"\n",
    "    doc_dicts = []\n",
    "    for doc in docs:\n",
    "        doc_dict = {\n",
    "            'page_content': str(doc.page_content),\n",
    "            'metadata': doc.metadata\n",
    "        }\n",
    "        doc_dicts.append(doc_dict)\n",
    "    return doc_dicts\n",
    "\n",
    "# Example usage:\n",
    "docs_dict_list = convert_docs_to_dict(docs)\n",
    "for doc_dict in docs_dict_list:\n",
    "    print(\"Page:\", doc_dict['metadata']['page'])\n",
    "    print(\"Source:\", doc_dict['metadata']['source'])\n",
    "    print(\"Content:\", doc_dict['page_content'])\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "41e00e4a-fc89-41d6-aaa7-767df7014b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_content': '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the',\n",
       " 'metadata': {'page': 4, 'source': './data\\\\RAG on large language models.pdf'}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_dict_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9330861a-4ae2-4315-a71a-f64402db98ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_content': '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the',\n",
       "  'metadata': {'page': 4,\n",
       "   'source': './data\\\\RAG on large language models.pdf'}},\n",
       " {'page_content': '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the',\n",
       "  'metadata': {'page': 4,\n",
       "   'source': './data\\\\RAG on large language models.pdf'}},\n",
       " {'page_content': '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the',\n",
       "  'metadata': {'page': 4,\n",
       "   'source': './data\\\\RAG on large language models.pdf'}},\n",
       " {'page_content': '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the',\n",
       "  'metadata': {'page': 4,\n",
       "   'source': './data\\\\RAG on large language models.pdf'}},\n",
       " {'page_content': '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the',\n",
       "  'metadata': {'page': 4,\n",
       "   'source': './data\\\\RAG on large language models.pdf'}},\n",
       " {'page_content': 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it',\n",
       "  'metadata': {'page': 0,\n",
       "   'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}},\n",
       " {'page_content': 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it',\n",
       "  'metadata': {'page': 0,\n",
       "   'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}},\n",
       " {'page_content': 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it',\n",
       "  'metadata': {'page': 0,\n",
       "   'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}},\n",
       " {'page_content': 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it',\n",
       "  'metadata': {'page': 0,\n",
       "   'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}},\n",
       " {'page_content': 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it',\n",
       "  'metadata': {'page': 0,\n",
       "   'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "70ae24e3-b0bb-437f-ab5b-c87ad5868d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "score  = model.predict([query, docs_dict_list[0]['page_content']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "169bddfa-1761-47d3-908c-1ceac5f26dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2003322"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d57bd61a-90a7-4568-ad34-af1bea97b825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2003322\n",
      "---------------------\n",
      "['[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it']\n",
      "3.2003322\n",
      "---------------------\n",
      "['[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it']\n",
      "3.2003322\n",
      "---------------------\n",
      "['[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it']\n",
      "3.2003322\n",
      "---------------------\n",
      "['[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it']\n",
      "3.2003322\n",
      "---------------------\n",
      "['[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it']\n",
      "6.049387\n",
      "---------------------\n",
      "['[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it']\n",
      "6.049387\n",
      "---------------------\n",
      "['[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it']\n",
      "6.049387\n",
      "---------------------\n",
      "['[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it']\n",
      "6.049387\n",
      "---------------------\n",
      "['[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it']\n",
      "6.049387\n",
      "---------------------\n",
      "['[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it']\n",
      "[{'page_content': 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'metadata': {'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'metadata': {'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'metadata': {'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'metadata': {'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': 'over, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it', 'metadata': {'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'metadata': {'page': 4, 'source': './data\\\\RAG on large language models.pdf'}}, {'page_content': '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'metadata': {'page': 4, 'source': './data\\\\RAG on large language models.pdf'}}, {'page_content': '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'metadata': {'page': 4, 'source': './data\\\\RAG on large language models.pdf'}}, {'page_content': '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'metadata': {'page': 4, 'source': './data\\\\RAG on large language models.pdf'}}, {'page_content': '[14] G. Izacard and E. Grave, “Leveraging passage retrieval with\\ngenerative models for open domain question answering,” arXiv\\npreprint arXiv:2007.01282 , 2020.\\n[15] T. Kwiatkowski et al. , “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association\\nfor Computational Linguistics , vol. 7, pp. 452–466, 2019.\\n[Online]. Available: https://aclanthology.org/Q19-1026\\n[16] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for\\nreading comprehension,” in Proceedings of the 55th Annual\\nMeeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) . Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611.\\n[Online]. Available: https://aclanthology.org/P17-1147\\n[17] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,\\n“Constructing a multi-hop QA dataset for comprehensive\\nevaluation of reasoning steps,” in Proceedings of the', 'metadata': {'page': 4, 'source': './data\\\\RAG on large language models.pdf'}}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming model is your machine learning model\n",
    "# query is the query you want to predict scores for\n",
    "query = \"What is Open-domain question answering?\"\n",
    "\n",
    "# Assuming docs_dict_list is a list of dictionaries with 'page_content' key\n",
    "# containing the content of each document\n",
    "docs_content_list = [doc['page_content'] for doc in docs_dict_list]\n",
    "\n",
    "# Initialize an empty list to store the scores along with document indices\n",
    "scores_list = []\n",
    "\n",
    "# Loop through each document and calculate the score\n",
    "for i, doc_content in enumerate(docs_content_list):\n",
    "    # Assuming model.predict returns a scalar score\n",
    "    score = model.predict([query, doc_content])\n",
    "    print(score)\n",
    "    print(\"---------------------\")\n",
    "    print(docs_content_list)\n",
    "    \n",
    "    # Append the score along with the document index to the scores_list\n",
    "    scores_list.append((i, score))\n",
    "\n",
    "# Sort the scores_list in descending order based on scores\n",
    "scores_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Now, scores_list contains tuples of (document_index, score) sorted in descending order\n",
    "# You can use these indices to access the corresponding documents in docs_dict_list if needed\n",
    "# Example:\n",
    "sorted_documents = [docs_dict_list[i] for i, _ in scores_list]\n",
    "\n",
    "# Print or use the sorted_documents as needed\n",
    "print(sorted_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312096c6-c5e1-46ec-890b-fb2aa72d0645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
