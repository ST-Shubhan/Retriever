{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f8d230e-efe6-4cea-8b06-7504a5911f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import openai\n",
    "from langchain.llms import openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_transformers import (\n",
    "    EmbeddingsRedundantFilter,\n",
    "    EmbeddingsClusteringFilter,\n",
    ")\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader , PdfWriter, PdfMerger\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c0a635c-06eb-4045-80c8-311b254e9eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection already exists\n",
      "-0.00010850000035134144\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = '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'\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "def load_chunk_persist_pdf() -> Chroma:\n",
    "    start = timeit.timeit()\n",
    "    pdf_folder_path = \"./data\"\n",
    "    documents = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "    text_splitter = CharacterTextSplitter(separator = \"\\n\\n\",chunk_size=1000, chunk_overlap=10)\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    client = chromadb.Client()\n",
    "    if client.list_collections():\n",
    "        consent_collection = client.create_collection(\"consent_collection\")\n",
    "    else:\n",
    "        print(\"Collection already exists\")\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunked_documents,\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "        persist_directory=\"store/chroma/charactertext\"\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    end = timeit.timeit()\n",
    "    print(start - end)\n",
    "    return vectordb\n",
    "\n",
    "c_docs = load_chunk_persist_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d564d718-10e9-44e4-ab84-0e978ea39c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = c_docs.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":10, \"include_metadata\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "835dd70e-4ec3-4a9f-9b97-54e7c17a6399",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Open-domain question answering?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02623a24-d1ba-4f1c-8383-a49344645d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Generation-Augmented Retrieval for Open-Domain Question Answering\\nYuning Mao1∗, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\\nJianfeng Gao3, Jiawei Han1, Weizhu Chen2\\n1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\\n1{yuningm2, hanj}@illinois.edu\\n2,3{penhe, xiaodl, yeshe, jfgao,wzchen }@microsoft.com\\nAbstract\\nWe propose Generation-Augmented Retrieval\\n(GAR) for answering open-domain questions,\\nwhich augments a query through text genera-\\ntion of heuristically discovered relevant con-\\ntexts without external resources as supervi-\\nsion. We demonstrate that the generated con-\\ntexts substantially enrich the semantics of the\\nqueries and G ARwith sparse representations\\n(BM25) achieves comparable or better per-\\nformance than state-of-the-art dense retrieval\\nmethods such as DPR (Karpukhin et al., 2020).\\nWe show that generating diverse contexts for a\\nquery is beneﬁcial as fusing their results con-\\nsistently yields better retrieval accuracy. More-\\nover, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it\\nis infeasible to examine every piece of information\\nin the entire document collection ( e.g., millions\\nof Wikipedia passages) and the retrieval accuracy\\nbounds the performance of the (extractive) reader.\\n∗Work was done during internship at Microsoft Azure AI.\\n1Our code and retrieval results are available at https:\\n//github.com/morningmoni/GAR .Early OpenQA systems (Chen et al., 2017)\\nuse classic retrieval methods such as TF-IDF and\\nBM25 with sparse representations. Sparse methods\\nare lightweight and efﬁcient, but unable to per-\\nform semantic matching and fail to retrieve rele-\\nvant passages without lexical overlap. More re-\\ncently, methods based on dense representations\\n(Guu et al., 2020; Karpukhin et al., 2020) learn to\\nembed queries and passages into a latent vector\\nspace, in which text similarity beyond lexical over-\\nlap can be measured. Dense retrieval methods can\\nretrieve semantically relevant but lexically differ-\\nent passages and often achieve better performance\\nthan sparse methods. However, the dense mod-\\nels are more computationally expensive and suffer\\nfrom information loss as they condense the entire\\ntext sequence into a ﬁxed-size vector that does not\\nguarantee exact matching (Luan et al., 2020).\\nThere have been some recent studies on query re-\\nformulation with text generation for other retrieval\\ntasks, which, for example, rewrite the queries to\\ncontext-independent (Yu et al., 2020; Lin et al.,\\n2020; Vakulenko et al., 2020) or well-formed (Liu\\net al., 2019) ones. However, these methods re-\\nquire either task-speciﬁc data ( e.g., conversational\\ncontexts, ill-formed queries) or external resources\\nsuch as paraphrase data (Zaiem and Sadat, 2019;\\nWang et al., 2020) that cannot or do not trans-\\nfer well to OpenQA. Also, some rely on time-\\nconsuming training process like reinforcement\\nlearning (RL) (Nogueira and Cho, 2017; Liu et al.,\\n2019; Wang et al., 2020) that is not efﬁcient enough\\nfor OpenQA (more discussions in Sec. 2).\\nIn this paper, we propose Generation-\\nAugmented Retrieval ( GAR), which augments\\na query through text generation of a pre-trained\\nlanguage model (PLM). Different from prior\\nstudies that reformulate queries, GARdoes not\\nrequire external resources or downstream feedback\\nvia RL as supervision, because it does not rewrite\\nthe query but expands it with heuristically discov-arXiv:2009.08553v4  [cs.CL]  6 Aug 2021', metadata={'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='Generation-Augmented Retrieval for Open-Domain Question Answering\\nYuning Mao1∗, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\\nJianfeng Gao3, Jiawei Han1, Weizhu Chen2\\n1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\\n1{yuningm2, hanj}@illinois.edu\\n2,3{penhe, xiaodl, yeshe, jfgao,wzchen }@microsoft.com\\nAbstract\\nWe propose Generation-Augmented Retrieval\\n(GAR) for answering open-domain questions,\\nwhich augments a query through text genera-\\ntion of heuristically discovered relevant con-\\ntexts without external resources as supervi-\\nsion. We demonstrate that the generated con-\\ntexts substantially enrich the semantics of the\\nqueries and G ARwith sparse representations\\n(BM25) achieves comparable or better per-\\nformance than state-of-the-art dense retrieval\\nmethods such as DPR (Karpukhin et al., 2020).\\nWe show that generating diverse contexts for a\\nquery is beneﬁcial as fusing their results con-\\nsistently yields better retrieval accuracy. More-\\nover, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it\\nis infeasible to examine every piece of information\\nin the entire document collection ( e.g., millions\\nof Wikipedia passages) and the retrieval accuracy\\nbounds the performance of the (extractive) reader.\\n∗Work was done during internship at Microsoft Azure AI.\\n1Our code and retrieval results are available at https:\\n//github.com/morningmoni/GAR .Early OpenQA systems (Chen et al., 2017)\\nuse classic retrieval methods such as TF-IDF and\\nBM25 with sparse representations. Sparse methods\\nare lightweight and efﬁcient, but unable to per-\\nform semantic matching and fail to retrieve rele-\\nvant passages without lexical overlap. More re-\\ncently, methods based on dense representations\\n(Guu et al., 2020; Karpukhin et al., 2020) learn to\\nembed queries and passages into a latent vector\\nspace, in which text similarity beyond lexical over-\\nlap can be measured. Dense retrieval methods can\\nretrieve semantically relevant but lexically differ-\\nent passages and often achieve better performance\\nthan sparse methods. However, the dense mod-\\nels are more computationally expensive and suffer\\nfrom information loss as they condense the entire\\ntext sequence into a ﬁxed-size vector that does not\\nguarantee exact matching (Luan et al., 2020).\\nThere have been some recent studies on query re-\\nformulation with text generation for other retrieval\\ntasks, which, for example, rewrite the queries to\\ncontext-independent (Yu et al., 2020; Lin et al.,\\n2020; Vakulenko et al., 2020) or well-formed (Liu\\net al., 2019) ones. However, these methods re-\\nquire either task-speciﬁc data ( e.g., conversational\\ncontexts, ill-formed queries) or external resources\\nsuch as paraphrase data (Zaiem and Sadat, 2019;\\nWang et al., 2020) that cannot or do not trans-\\nfer well to OpenQA. Also, some rely on time-\\nconsuming training process like reinforcement\\nlearning (RL) (Nogueira and Cho, 2017; Liu et al.,\\n2019; Wang et al., 2020) that is not efﬁcient enough\\nfor OpenQA (more discussions in Sec. 2).\\nIn this paper, we propose Generation-\\nAugmented Retrieval ( GAR), which augments\\na query through text generation of a pre-trained\\nlanguage model (PLM). Different from prior\\nstudies that reformulate queries, GARdoes not\\nrequire external resources or downstream feedback\\nvia RL as supervision, because it does not rewrite\\nthe query but expands it with heuristically discov-arXiv:2009.08553v4  [cs.CL]  6 Aug 2021', metadata={'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='Generation-Augmented Retrieval for Open-Domain Question Answering\\nYuning Mao1∗, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\\nJianfeng Gao3, Jiawei Han1, Weizhu Chen2\\n1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\\n1{yuningm2, hanj}@illinois.edu\\n2,3{penhe, xiaodl, yeshe, jfgao,wzchen }@microsoft.com\\nAbstract\\nWe propose Generation-Augmented Retrieval\\n(GAR) for answering open-domain questions,\\nwhich augments a query through text genera-\\ntion of heuristically discovered relevant con-\\ntexts without external resources as supervi-\\nsion. We demonstrate that the generated con-\\ntexts substantially enrich the semantics of the\\nqueries and G ARwith sparse representations\\n(BM25) achieves comparable or better per-\\nformance than state-of-the-art dense retrieval\\nmethods such as DPR (Karpukhin et al., 2020).\\nWe show that generating diverse contexts for a\\nquery is beneﬁcial as fusing their results con-\\nsistently yields better retrieval accuracy. More-\\nover, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it\\nis infeasible to examine every piece of information\\nin the entire document collection ( e.g., millions\\nof Wikipedia passages) and the retrieval accuracy\\nbounds the performance of the (extractive) reader.\\n∗Work was done during internship at Microsoft Azure AI.\\n1Our code and retrieval results are available at https:\\n//github.com/morningmoni/GAR .Early OpenQA systems (Chen et al., 2017)\\nuse classic retrieval methods such as TF-IDF and\\nBM25 with sparse representations. Sparse methods\\nare lightweight and efﬁcient, but unable to per-\\nform semantic matching and fail to retrieve rele-\\nvant passages without lexical overlap. More re-\\ncently, methods based on dense representations\\n(Guu et al., 2020; Karpukhin et al., 2020) learn to\\nembed queries and passages into a latent vector\\nspace, in which text similarity beyond lexical over-\\nlap can be measured. Dense retrieval methods can\\nretrieve semantically relevant but lexically differ-\\nent passages and often achieve better performance\\nthan sparse methods. However, the dense mod-\\nels are more computationally expensive and suffer\\nfrom information loss as they condense the entire\\ntext sequence into a ﬁxed-size vector that does not\\nguarantee exact matching (Luan et al., 2020).\\nThere have been some recent studies on query re-\\nformulation with text generation for other retrieval\\ntasks, which, for example, rewrite the queries to\\ncontext-independent (Yu et al., 2020; Lin et al.,\\n2020; Vakulenko et al., 2020) or well-formed (Liu\\net al., 2019) ones. However, these methods re-\\nquire either task-speciﬁc data ( e.g., conversational\\ncontexts, ill-formed queries) or external resources\\nsuch as paraphrase data (Zaiem and Sadat, 2019;\\nWang et al., 2020) that cannot or do not trans-\\nfer well to OpenQA. Also, some rely on time-\\nconsuming training process like reinforcement\\nlearning (RL) (Nogueira and Cho, 2017; Liu et al.,\\n2019; Wang et al., 2020) that is not efﬁcient enough\\nfor OpenQA (more discussions in Sec. 2).\\nIn this paper, we propose Generation-\\nAugmented Retrieval ( GAR), which augments\\na query through text generation of a pre-trained\\nlanguage model (PLM). Different from prior\\nstudies that reformulate queries, GARdoes not\\nrequire external resources or downstream feedback\\nvia RL as supervision, because it does not rewrite\\nthe query but expands it with heuristically discov-arXiv:2009.08553v4  [cs.CL]  6 Aug 2021', metadata={'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='graph for question answering. arXiv preprint\\narXiv:1911.10470 .\\nIz Beltagy, Matthew E Peters, and Arman Cohan.\\n2020. Longformer: The long-document transformer.\\narXiv preprint arXiv:2004.05150 .\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. arXiv preprint arXiv:2005.14165 .\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 1870–\\n1879, Vancouver, Canada. Association for Computa-\\ntional Linguistics.\\nGordon V Cormack, Charles LA Clarke, and Stefan\\nBuettcher. 2009. Reciprocal rank fusion outper-\\nforms condorcet and individual rank learning meth-\\nods. In Proceedings of the 32nd international ACM\\nSIGIR conference on Research and development in\\ninformation retrieval , pages 758–759.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\\nCampos, and Ellen M V oorhees. 2020. Overview\\nof the trec 2019 deep learning track. arXiv preprint\\narXiv:2003.07820 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) ,\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\\nAlexander Miller, Kurt Shuster, Jack Urbanek,\\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\\nLowe, et al. 2020. The second conversational in-\\ntelligence challenge (convai2). In The NeurIPS’18\\nCompetition , pages 187–208. Springer.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training. arXiv\\npreprint arXiv:2002.08909 .\\nGautier Izacard and Edouard Grave. 2020. Lever-\\naging passage retrieval with generative models for\\nopen domain question answering. arXiv preprint\\narXiv:2007.01282 .\\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\\nBillion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 .Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale dis-\\ntantly supervised challenge dataset for reading com-\\nprehension. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 1601–1611, Van-\\ncouver, Canada. Association for Computational Lin-\\nguistics.\\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\\nWu, Sergey Edunov, Danqi Chen, and Wen-\\ntau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. arXiv preprint\\narXiv:2004.04906 .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\nNatural questions: A benchmark for question an-\\nswering research. Transactions of the Association\\nfor Computational Linguistics , 7:452–466.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 6086–6096, Florence,\\nItaly. Association for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\\nBart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and\\ncomprehension. arXiv preprint arXiv:1910.13461 .\\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\\nRockt ¨aschel, et al. 2020a. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. arXiv\\npreprint arXiv:2005.11401 .\\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\\n2020b. Question and answer test-train overlap in\\nopen-domain question answering datasets. arXiv\\npreprint arXiv:2008.02637 .\\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\\nNogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\\nJimmy Lin. 2020. Query reformulation using query\\nhistory for passage retrieval in conversational search.\\narXiv preprint arXiv:2005.02230 .\\nYe Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\\nPhilip S Yu. 2019. Generative question reﬁnement\\nwith deep reinforcement learning in retrieval-based\\nqa system. In Proceedings of the 28th ACM Inter-\\nnational Conference on Information and Knowledge\\nManagement , pages 1643–1652.', metadata={'page': 9, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='graph for question answering. arXiv preprint\\narXiv:1911.10470 .\\nIz Beltagy, Matthew E Peters, and Arman Cohan.\\n2020. Longformer: The long-document transformer.\\narXiv preprint arXiv:2004.05150 .\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. arXiv preprint arXiv:2005.14165 .\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 1870–\\n1879, Vancouver, Canada. Association for Computa-\\ntional Linguistics.\\nGordon V Cormack, Charles LA Clarke, and Stefan\\nBuettcher. 2009. Reciprocal rank fusion outper-\\nforms condorcet and individual rank learning meth-\\nods. In Proceedings of the 32nd international ACM\\nSIGIR conference on Research and development in\\ninformation retrieval , pages 758–759.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\\nCampos, and Ellen M V oorhees. 2020. Overview\\nof the trec 2019 deep learning track. arXiv preprint\\narXiv:2003.07820 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) ,\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\\nAlexander Miller, Kurt Shuster, Jack Urbanek,\\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\\nLowe, et al. 2020. The second conversational in-\\ntelligence challenge (convai2). In The NeurIPS’18\\nCompetition , pages 187–208. Springer.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training. arXiv\\npreprint arXiv:2002.08909 .\\nGautier Izacard and Edouard Grave. 2020. Lever-\\naging passage retrieval with generative models for\\nopen domain question answering. arXiv preprint\\narXiv:2007.01282 .\\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\\nBillion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 .Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale dis-\\ntantly supervised challenge dataset for reading com-\\nprehension. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 1601–1611, Van-\\ncouver, Canada. Association for Computational Lin-\\nguistics.\\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\\nWu, Sergey Edunov, Danqi Chen, and Wen-\\ntau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. arXiv preprint\\narXiv:2004.04906 .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\nNatural questions: A benchmark for question an-\\nswering research. Transactions of the Association\\nfor Computational Linguistics , 7:452–466.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 6086–6096, Florence,\\nItaly. Association for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\\nBart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and\\ncomprehension. arXiv preprint arXiv:1910.13461 .\\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\\nRockt ¨aschel, et al. 2020a. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. arXiv\\npreprint arXiv:2005.11401 .\\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\\n2020b. Question and answer test-train overlap in\\nopen-domain question answering datasets. arXiv\\npreprint arXiv:2008.02637 .\\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\\nNogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\\nJimmy Lin. 2020. Query reformulation using query\\nhistory for passage retrieval in conversational search.\\narXiv preprint arXiv:2005.02230 .\\nYe Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\\nPhilip S Yu. 2019. Generative question reﬁnement\\nwith deep reinforcement learning in retrieval-based\\nqa system. In Proceedings of the 28th ACM Inter-\\nnational Conference on Information and Knowledge\\nManagement , pages 1643–1652.', metadata={'page': 9, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='graph for question answering. arXiv preprint\\narXiv:1911.10470 .\\nIz Beltagy, Matthew E Peters, and Arman Cohan.\\n2020. Longformer: The long-document transformer.\\narXiv preprint arXiv:2004.05150 .\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. arXiv preprint arXiv:2005.14165 .\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 1870–\\n1879, Vancouver, Canada. Association for Computa-\\ntional Linguistics.\\nGordon V Cormack, Charles LA Clarke, and Stefan\\nBuettcher. 2009. Reciprocal rank fusion outper-\\nforms condorcet and individual rank learning meth-\\nods. In Proceedings of the 32nd international ACM\\nSIGIR conference on Research and development in\\ninformation retrieval , pages 758–759.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\\nCampos, and Ellen M V oorhees. 2020. Overview\\nof the trec 2019 deep learning track. arXiv preprint\\narXiv:2003.07820 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) ,\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\\nAlexander Miller, Kurt Shuster, Jack Urbanek,\\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\\nLowe, et al. 2020. The second conversational in-\\ntelligence challenge (convai2). In The NeurIPS’18\\nCompetition , pages 187–208. Springer.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training. arXiv\\npreprint arXiv:2002.08909 .\\nGautier Izacard and Edouard Grave. 2020. Lever-\\naging passage retrieval with generative models for\\nopen domain question answering. arXiv preprint\\narXiv:2007.01282 .\\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\\nBillion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 .Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale dis-\\ntantly supervised challenge dataset for reading com-\\nprehension. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 1601–1611, Van-\\ncouver, Canada. Association for Computational Lin-\\nguistics.\\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\\nWu, Sergey Edunov, Danqi Chen, and Wen-\\ntau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. arXiv preprint\\narXiv:2004.04906 .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\nNatural questions: A benchmark for question an-\\nswering research. Transactions of the Association\\nfor Computational Linguistics , 7:452–466.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 6086–6096, Florence,\\nItaly. Association for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\\nBart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and\\ncomprehension. arXiv preprint arXiv:1910.13461 .\\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\\nRockt ¨aschel, et al. 2020a. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. arXiv\\npreprint arXiv:2005.11401 .\\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\\n2020b. Question and answer test-train overlap in\\nopen-domain question answering datasets. arXiv\\npreprint arXiv:2008.02637 .\\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\\nNogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\\nJimmy Lin. 2020. Query reformulation using query\\nhistory for passage retrieval in conversational search.\\narXiv preprint arXiv:2005.02230 .\\nYe Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\\nPhilip S Yu. 2019. Generative question reﬁnement\\nwith deep reinforcement learning in retrieval-based\\nqa system. In Proceedings of the 28th ACM Inter-\\nnational Conference on Information and Knowledge\\nManagement , pages 1643–1652.', metadata={'page': 9, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}),\n",
       " Document(page_content='Slav Petrov. 2019. Natural questions: A bench-\\nmark for question answering research. Trans-\\nactions of the Association for Computational\\nLinguistics , 7:452–466.\\nYoav Levine, Itay Dalmedigos, Ori Ram, Yoel\\nZeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin,\\nOpher Lieber, Barak Lenz, Shai Shalev-Shwartz,\\nAmnon Shashua, Kevin Leyton-Brown, and\\nYoav Shoham. 2022a. Standing on the shoul-\\nders of giant frozen language models.\\nYoav Levine, Ori Ram, Daniel Jannai, Barak Lenz,\\nShai Shalev-Shwartz, Amnon Shashua, Kevin\\nLeyton-Brown, and Yoav Shoham. 2022b. Huge\\nfrozen language models as readers for open-\\ndomain question answering. In ICML 2022\\nWorkshop on Knowledge Retrieval and Lan-\\nguage Models .\\nYoav Levine, Noam Wies, Daniel Jannai, Dan\\nNavon, Yedid Hoshen, and Amnon Shashua.\\n2022c. The inductive bias of in-context learn-\\ning: Rethinking pretraining example design. In\\nInternational Conference on Learning Represen-\\ntations .\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus,\\nFabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau\\nYih, Tim Rocktäschel, Sebastian Riedel, and\\nDouwe Kiela. 2020. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. In\\nAdvances in Neural Information Processing Sys-\\ntems, pages 9459–9474.\\nZonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022.\\nDecoupled context processing for context aug-\\nmented language modeling. In Advances in Neu-\\nral Information Processing Systems .\\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\\nShoham. 2021. Jurassic-1: Technical details and\\nevaluation.\\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin,\\nJheng-Hong Yang, Ronak Pradeep, and Rodrigo\\nNogueira. 2021. Pyserini: A python toolkit for\\nreproducible information retrieval research with\\nsparse and dense representations. In Proceed-\\nings of the 44th International ACM SIGIR Con-\\nference on Research and Development in Infor-\\nmation Retrieval , SIGIR ’21, page 2356–2362,\\nNew York, NY , USA. Association for Comput-\\ning Machinery.Stephanie Lin, Jacob Hilton, and Owain Evans.\\n2022. TruthfulQA: Measuring how models\\nmimic human falsehoods. In Proceedings of the\\n60th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers) ,\\npages 3214–3252, Dublin, Ireland. Association\\nfor Computational Linguistics.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\\nMandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019. RoBERTa: A robustly optimized bert\\npretraining approach.\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet,\\nand Ryan McDonald. 2020. On faithfulness and\\nfactuality in abstractive summarization. In Pro-\\nceedings of the 58th Annual Meeting of the As-\\nsociation for Computational Linguistics , pages\\n1906–1919, Online. Association for Computa-\\ntional Linguistics.\\nStephen Merity, Caiming Xiong, James Bradbury,\\nand Richard Socher. 2016. Pointer sentinel mix-\\nture models.\\nDor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\\nNir Ratner, Yonatan Belinkov, Omri Abend,\\nKevin Leyton-Brown, Amnon Shashua, and\\nYoav Shoham. 2023. Generating benchmarks\\nfor factuality evaluation of language models.\\nFabio Petroni, Aleksandra Piktus, Angela Fan,\\nPatrick Lewis, Majid Yazdani, Nicola De Cao,\\nJames Thorne, Yacine Jernite, Vladimir\\nKarpukhin, Jean Maillard, Vassilis Plachouras,\\nTim Rocktäschel, and Sebastian Riedel. 2021.\\nKILT: a benchmark for knowledge intensive lan-\\nguage tasks. In Proceedings of the 2021 Con-\\nference of the North American Chapter of the\\nAssociation for Computational Linguistics: Hu-\\nman Language Technologies , pages 2523–2544,\\nOnline. Association for Computational Linguis-\\ntics.\\nAlec Radford, Karthik Narasimhan, Tim Salimans,\\nand Ilya Sutskever. 2018. Improving language\\nunderstanding by generative pre-training.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Lan-\\nguage models are unsupervised multitask learn-\\ners.', metadata={'page': 12, 'source': './data\\\\In context retrieval.pdf'}),\n",
       " Document(page_content='Slav Petrov. 2019. Natural questions: A bench-\\nmark for question answering research. Trans-\\nactions of the Association for Computational\\nLinguistics , 7:452–466.\\nYoav Levine, Itay Dalmedigos, Ori Ram, Yoel\\nZeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin,\\nOpher Lieber, Barak Lenz, Shai Shalev-Shwartz,\\nAmnon Shashua, Kevin Leyton-Brown, and\\nYoav Shoham. 2022a. Standing on the shoul-\\nders of giant frozen language models.\\nYoav Levine, Ori Ram, Daniel Jannai, Barak Lenz,\\nShai Shalev-Shwartz, Amnon Shashua, Kevin\\nLeyton-Brown, and Yoav Shoham. 2022b. Huge\\nfrozen language models as readers for open-\\ndomain question answering. In ICML 2022\\nWorkshop on Knowledge Retrieval and Lan-\\nguage Models .\\nYoav Levine, Noam Wies, Daniel Jannai, Dan\\nNavon, Yedid Hoshen, and Amnon Shashua.\\n2022c. The inductive bias of in-context learn-\\ning: Rethinking pretraining example design. In\\nInternational Conference on Learning Represen-\\ntations .\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus,\\nFabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau\\nYih, Tim Rocktäschel, Sebastian Riedel, and\\nDouwe Kiela. 2020. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. In\\nAdvances in Neural Information Processing Sys-\\ntems, pages 9459–9474.\\nZonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022.\\nDecoupled context processing for context aug-\\nmented language modeling. In Advances in Neu-\\nral Information Processing Systems .\\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\\nShoham. 2021. Jurassic-1: Technical details and\\nevaluation.\\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin,\\nJheng-Hong Yang, Ronak Pradeep, and Rodrigo\\nNogueira. 2021. Pyserini: A python toolkit for\\nreproducible information retrieval research with\\nsparse and dense representations. In Proceed-\\nings of the 44th International ACM SIGIR Con-\\nference on Research and Development in Infor-\\nmation Retrieval , SIGIR ’21, page 2356–2362,\\nNew York, NY , USA. Association for Comput-\\ning Machinery.Stephanie Lin, Jacob Hilton, and Owain Evans.\\n2022. TruthfulQA: Measuring how models\\nmimic human falsehoods. In Proceedings of the\\n60th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers) ,\\npages 3214–3252, Dublin, Ireland. Association\\nfor Computational Linguistics.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\\nMandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019. RoBERTa: A robustly optimized bert\\npretraining approach.\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet,\\nand Ryan McDonald. 2020. On faithfulness and\\nfactuality in abstractive summarization. In Pro-\\nceedings of the 58th Annual Meeting of the As-\\nsociation for Computational Linguistics , pages\\n1906–1919, Online. Association for Computa-\\ntional Linguistics.\\nStephen Merity, Caiming Xiong, James Bradbury,\\nand Richard Socher. 2016. Pointer sentinel mix-\\nture models.\\nDor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\\nNir Ratner, Yonatan Belinkov, Omri Abend,\\nKevin Leyton-Brown, Amnon Shashua, and\\nYoav Shoham. 2023. Generating benchmarks\\nfor factuality evaluation of language models.\\nFabio Petroni, Aleksandra Piktus, Angela Fan,\\nPatrick Lewis, Majid Yazdani, Nicola De Cao,\\nJames Thorne, Yacine Jernite, Vladimir\\nKarpukhin, Jean Maillard, Vassilis Plachouras,\\nTim Rocktäschel, and Sebastian Riedel. 2021.\\nKILT: a benchmark for knowledge intensive lan-\\nguage tasks. In Proceedings of the 2021 Con-\\nference of the North American Chapter of the\\nAssociation for Computational Linguistics: Hu-\\nman Language Technologies , pages 2523–2544,\\nOnline. Association for Computational Linguis-\\ntics.\\nAlec Radford, Karthik Narasimhan, Tim Salimans,\\nand Ilya Sutskever. 2018. Improving language\\nunderstanding by generative pre-training.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Lan-\\nguage models are unsupervised multitask learn-\\ners.', metadata={'page': 12, 'source': './data\\\\In context retrieval.pdf'}),\n",
       " Document(page_content='Slav Petrov. 2019. Natural questions: A bench-\\nmark for question answering research. Trans-\\nactions of the Association for Computational\\nLinguistics , 7:452–466.\\nYoav Levine, Itay Dalmedigos, Ori Ram, Yoel\\nZeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin,\\nOpher Lieber, Barak Lenz, Shai Shalev-Shwartz,\\nAmnon Shashua, Kevin Leyton-Brown, and\\nYoav Shoham. 2022a. Standing on the shoul-\\nders of giant frozen language models.\\nYoav Levine, Ori Ram, Daniel Jannai, Barak Lenz,\\nShai Shalev-Shwartz, Amnon Shashua, Kevin\\nLeyton-Brown, and Yoav Shoham. 2022b. Huge\\nfrozen language models as readers for open-\\ndomain question answering. In ICML 2022\\nWorkshop on Knowledge Retrieval and Lan-\\nguage Models .\\nYoav Levine, Noam Wies, Daniel Jannai, Dan\\nNavon, Yedid Hoshen, and Amnon Shashua.\\n2022c. The inductive bias of in-context learn-\\ning: Rethinking pretraining example design. In\\nInternational Conference on Learning Represen-\\ntations .\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus,\\nFabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau\\nYih, Tim Rocktäschel, Sebastian Riedel, and\\nDouwe Kiela. 2020. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. In\\nAdvances in Neural Information Processing Sys-\\ntems, pages 9459–9474.\\nZonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022.\\nDecoupled context processing for context aug-\\nmented language modeling. In Advances in Neu-\\nral Information Processing Systems .\\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\\nShoham. 2021. Jurassic-1: Technical details and\\nevaluation.\\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin,\\nJheng-Hong Yang, Ronak Pradeep, and Rodrigo\\nNogueira. 2021. Pyserini: A python toolkit for\\nreproducible information retrieval research with\\nsparse and dense representations. In Proceed-\\nings of the 44th International ACM SIGIR Con-\\nference on Research and Development in Infor-\\nmation Retrieval , SIGIR ’21, page 2356–2362,\\nNew York, NY , USA. Association for Comput-\\ning Machinery.Stephanie Lin, Jacob Hilton, and Owain Evans.\\n2022. TruthfulQA: Measuring how models\\nmimic human falsehoods. In Proceedings of the\\n60th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers) ,\\npages 3214–3252, Dublin, Ireland. Association\\nfor Computational Linguistics.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\\nMandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019. RoBERTa: A robustly optimized bert\\npretraining approach.\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet,\\nand Ryan McDonald. 2020. On faithfulness and\\nfactuality in abstractive summarization. In Pro-\\nceedings of the 58th Annual Meeting of the As-\\nsociation for Computational Linguistics , pages\\n1906–1919, Online. Association for Computa-\\ntional Linguistics.\\nStephen Merity, Caiming Xiong, James Bradbury,\\nand Richard Socher. 2016. Pointer sentinel mix-\\nture models.\\nDor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\\nNir Ratner, Yonatan Belinkov, Omri Abend,\\nKevin Leyton-Brown, Amnon Shashua, and\\nYoav Shoham. 2023. Generating benchmarks\\nfor factuality evaluation of language models.\\nFabio Petroni, Aleksandra Piktus, Angela Fan,\\nPatrick Lewis, Majid Yazdani, Nicola De Cao,\\nJames Thorne, Yacine Jernite, Vladimir\\nKarpukhin, Jean Maillard, Vassilis Plachouras,\\nTim Rocktäschel, and Sebastian Riedel. 2021.\\nKILT: a benchmark for knowledge intensive lan-\\nguage tasks. In Proceedings of the 2021 Con-\\nference of the North American Chapter of the\\nAssociation for Computational Linguistics: Hu-\\nman Language Technologies , pages 2523–2544,\\nOnline. Association for Computational Linguis-\\ntics.\\nAlec Radford, Karthik Narasimhan, Tim Salimans,\\nand Ilya Sutskever. 2018. Improving language\\nunderstanding by generative pre-training.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Lan-\\nguage models are unsupervised multitask learn-\\ners.', metadata={'page': 12, 'source': './data\\\\In context retrieval.pdf'}),\n",
       " Document(page_content='seq2seq learning, which, despite its simplicity, is\\nnot only more efﬁcient but effective for OpenQA.\\nRetrieval for OpenQA . Existing sparse retrieval\\nmethods for OpenQA (Chen et al., 2017) solely rely\\non the information of the questions. GARextends\\nto contexts relevant to the questions by extracting\\ninformation inside PLMs and helps sparse meth-\\nods achieve comparable or better performance than\\ndense methods (Guu et al., 2020; Karpukhin et al.,\\n2020), while enjoying the simplicity and efﬁciency\\nof sparse representations. GARcan also be used\\nwith dense representations to seek for even better\\nperformance, which we leave as future work.\\nGenerative QA . Generative QA generates answers\\nthrough seq2seq learning instead of extracting an-\\nswer spans. Recent studies on generative OpenQA\\n(Lewis et al., 2020a; Min et al., 2020; Izacard and\\nGrave, 2020) are orthogonal to GARin that they\\nfocus on improving the reading stage and directly\\nreuse DPR (Karpukhin et al., 2020) as the retriever.\\nUnlike generative QA, the goal of GARis not to\\ngenerate perfect answers to the questions but perti-\\nnent contexts that are helpful for retrieval. Another\\nline in generative QA learns to generate answers\\nwithout relevant passages as the evidence but solely\\nthe question itself using PLMs (Roberts et al., 2020;\\nBrown et al., 2020). GARfurther conﬁrms that one\\ncan extract factual knowledge from PLMs, which\\nis not limited to the answers as in prior studies but\\nalso other relevant contexts.\\n3 Generation-Augmented Retrieval\\n3.1 Task Formulation\\nOpenQA aims to answer factoid questions with-\\nout pre-speciﬁed domains. We assume that a large\\ncollection of documents C(i.e., Wikipedia) are\\ngiven as the resource to answer the questions and\\na retriever-reader architecture is used to tackle the\\ntask, where the retriever retrieves a small subset\\nof the documents D⊂Cand the reader reads the\\ndocuments Dto extract (or generate) an answer.\\nOur goal is to improve the effectiveness and efﬁ-\\nciency of the retriever and consequently improve\\nthe performance of the reader.\\n3.2 Generation of Query Contexts\\nInGAR, queries are augmented with various heuris-\\ntically discovered relevant contexts in order to re-\\ntrieve more relevant passages in terms of both quan-\\ntity and quality. For the task of OpenQA where the\\nquery is a question, we take the following threefreely accessible contexts as the generation targets.\\nWe show in Sec. 6.2 that having multiple gener-\\nation targets is helpful in that fusing their results\\nconsistently brings better retrieval accuracy.\\nContext 1: The default target (answer) . The de-\\nfault target is the label in the task of interest, which\\nis the answer in OpenQA. The answer to the ques-\\ntion is apparently useful for the retrieval of relevant\\npassages that contain the answer itself. As shown\\nin previous work (Roberts et al., 2020; Brown et al.,\\n2020), PLMs are able to answer certain questions\\nsolely by taking the questions as input ( i.e., closed-\\nbook QA). Instead of using the generated answers\\ndirectly as in closed-book QA, GARtreats them\\nas contexts of the question for retrieval. The ad-\\nvantage is that even if the generated answers are\\npartially correct (or even incorrect), they may still\\nbeneﬁt retrieval as long as they are relevant to the\\npassages that contain the correct answers ( e.g., co-\\noccur with the correct answers).\\nContext 2: Sentence containing the default tar-\\nget. The sentence in a passage that contains the\\nanswer is used as another generation target. Sim-\\nilar to using answers as the generation target, the\\ngenerated sentences are still beneﬁcial for retriev-\\ning relevant passages even if they do not contain\\nthe answers, as their semantics is highly related to\\nthe questions/answers (examples in Sec. 6.1). One\\ncan take the relevant sentences in the ground-truth\\npassages (if any) or those in the positive passages\\nof a retriever as the reference, depending on the\\ntrade-off between reference quality and diversity.\\nContext 3: Title of passage containing the de-\\nfault target . One can also use the titles of rele-\\nvant passages as the generation target if available.\\nSpeciﬁcally, we retrieve Wikipedia passages using\\nBM25 with the question as the query, and take the\\npage titles of positive passages that contain the an-\\nswers as the generation target. We observe that\\nthe page titles of positive passages are often entity\\nnames of interest, and sometimes (but not always)\\nthe answers to the questions. Intuitively, if GAR\\nlearns which Wikipedia pages the question is re-\\nlated to, the queries augmented by the generated\\ntitles would naturally have a better chance of re-\\ntrieving those relevant passages.\\nWhile it is likely that some of the generated\\nquery contexts involve unfaithful or nonfactual in-\\nformation due to hallucination in text generation\\n(Mao et al., 2020) and introduce noise during re-\\ntrieval, they are beneﬁcial rather than harmful over-', metadata={'page': 2, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6977fc70-c911-4853-a323-2cea30d453d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "571fd4a3-fbb0-4eb7-950f-ea1dc5425eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 0\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: Generation-Augmented Retrieval for Open-Domain Question Answering\n",
      "Yuning Mao1∗, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\n",
      "Jianfeng Gao3, Jiawei Han1, Weizhu Chen2\n",
      "1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\n",
      "1{yuningm2, hanj}@illinois.edu\n",
      "2,3{penhe, xiaodl, yeshe, jfgao,wzchen }@microsoft.com\n",
      "Abstract\n",
      "We propose Generation-Augmented Retrieval\n",
      "(GAR) for answering open-domain questions,\n",
      "which augments a query through text genera-\n",
      "tion of heuristically discovered relevant con-\n",
      "texts without external resources as supervi-\n",
      "sion. We demonstrate that the generated con-\n",
      "texts substantially enrich the semantics of the\n",
      "queries and G ARwith sparse representations\n",
      "(BM25) achieves comparable or better per-\n",
      "formance than state-of-the-art dense retrieval\n",
      "methods such as DPR (Karpukhin et al., 2020).\n",
      "We show that generating diverse contexts for a\n",
      "query is beneﬁcial as fusing their results con-\n",
      "sistently yields better retrieval accuracy. More-\n",
      "over, as sparse and dense representations are\n",
      "often complementary, G ARcan be easily com-\n",
      "bined with DPR to achieve even better per-\n",
      "formance. G ARachieves state-of-the-art per-\n",
      "formance on Natural Questions and TriviaQA\n",
      "datasets under the extractive QA setup when\n",
      "equipped with an extractive reader, and con-\n",
      "sistently outperforms other retrieval methods\n",
      "when the same generative reader is used.1\n",
      "1 Introduction\n",
      "Open-domain question answering (OpenQA) aims\n",
      "to answer factoid questions without a pre-speciﬁed\n",
      "domain and has numerous real-world applications.\n",
      "In OpenQA, a large collection of documents ( e.g.,\n",
      "Wikipedia) are often used to seek information per-\n",
      "taining to the questions. One of the most com-\n",
      "mon approaches uses a retriever-reader architecture\n",
      "(Chen et al., 2017), which ﬁrst retrieves a small sub-\n",
      "set of documents using the question as the query\n",
      "and then reads the retrieved documents to extract\n",
      "(or generate) an answer. The retriever is crucial as it\n",
      "is infeasible to examine every piece of information\n",
      "in the entire document collection ( e.g., millions\n",
      "of Wikipedia passages) and the retrieval accuracy\n",
      "bounds the performance of the (extractive) reader.\n",
      "∗Work was done during internship at Microsoft Azure AI.\n",
      "1Our code and retrieval results are available at https:\n",
      "//github.com/morningmoni/GAR .Early OpenQA systems (Chen et al., 2017)\n",
      "use classic retrieval methods such as TF-IDF and\n",
      "BM25 with sparse representations. Sparse methods\n",
      "are lightweight and efﬁcient, but unable to per-\n",
      "form semantic matching and fail to retrieve rele-\n",
      "vant passages without lexical overlap. More re-\n",
      "cently, methods based on dense representations\n",
      "(Guu et al., 2020; Karpukhin et al., 2020) learn to\n",
      "embed queries and passages into a latent vector\n",
      "space, in which text similarity beyond lexical over-\n",
      "lap can be measured. Dense retrieval methods can\n",
      "retrieve semantically relevant but lexically differ-\n",
      "ent passages and often achieve better performance\n",
      "than sparse methods. However, the dense mod-\n",
      "els are more computationally expensive and suffer\n",
      "from information loss as they condense the entire\n",
      "text sequence into a ﬁxed-size vector that does not\n",
      "guarantee exact matching (Luan et al., 2020).\n",
      "There have been some recent studies on query re-\n",
      "formulation with text generation for other retrieval\n",
      "tasks, which, for example, rewrite the queries to\n",
      "context-independent (Yu et al., 2020; Lin et al.,\n",
      "2020; Vakulenko et al., 2020) or well-formed (Liu\n",
      "et al., 2019) ones. However, these methods re-\n",
      "quire either task-speciﬁc data ( e.g., conversational\n",
      "contexts, ill-formed queries) or external resources\n",
      "such as paraphrase data (Zaiem and Sadat, 2019;\n",
      "Wang et al., 2020) that cannot or do not trans-\n",
      "fer well to OpenQA. Also, some rely on time-\n",
      "consuming training process like reinforcement\n",
      "learning (RL) (Nogueira and Cho, 2017; Liu et al.,\n",
      "2019; Wang et al., 2020) that is not efﬁcient enough\n",
      "for OpenQA (more discussions in Sec. 2).\n",
      "In this paper, we propose Generation-\n",
      "Augmented Retrieval ( GAR), which augments\n",
      "a query through text generation of a pre-trained\n",
      "language model (PLM). Different from prior\n",
      "studies that reformulate queries, GARdoes not\n",
      "require external resources or downstream feedback\n",
      "via RL as supervision, because it does not rewrite\n",
      "the query but expands it with heuristically discov-arXiv:2009.08553v4  [cs.CL]  6 Aug 2021\n",
      "-----------------------------------------------------------\n",
      "Page: 0\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: Generation-Augmented Retrieval for Open-Domain Question Answering\n",
      "Yuning Mao1∗, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\n",
      "Jianfeng Gao3, Jiawei Han1, Weizhu Chen2\n",
      "1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\n",
      "1{yuningm2, hanj}@illinois.edu\n",
      "2,3{penhe, xiaodl, yeshe, jfgao,wzchen }@microsoft.com\n",
      "Abstract\n",
      "We propose Generation-Augmented Retrieval\n",
      "(GAR) for answering open-domain questions,\n",
      "which augments a query through text genera-\n",
      "tion of heuristically discovered relevant con-\n",
      "texts without external resources as supervi-\n",
      "sion. We demonstrate that the generated con-\n",
      "texts substantially enrich the semantics of the\n",
      "queries and G ARwith sparse representations\n",
      "(BM25) achieves comparable or better per-\n",
      "formance than state-of-the-art dense retrieval\n",
      "methods such as DPR (Karpukhin et al., 2020).\n",
      "We show that generating diverse contexts for a\n",
      "query is beneﬁcial as fusing their results con-\n",
      "sistently yields better retrieval accuracy. More-\n",
      "over, as sparse and dense representations are\n",
      "often complementary, G ARcan be easily com-\n",
      "bined with DPR to achieve even better per-\n",
      "formance. G ARachieves state-of-the-art per-\n",
      "formance on Natural Questions and TriviaQA\n",
      "datasets under the extractive QA setup when\n",
      "equipped with an extractive reader, and con-\n",
      "sistently outperforms other retrieval methods\n",
      "when the same generative reader is used.1\n",
      "1 Introduction\n",
      "Open-domain question answering (OpenQA) aims\n",
      "to answer factoid questions without a pre-speciﬁed\n",
      "domain and has numerous real-world applications.\n",
      "In OpenQA, a large collection of documents ( e.g.,\n",
      "Wikipedia) are often used to seek information per-\n",
      "taining to the questions. One of the most com-\n",
      "mon approaches uses a retriever-reader architecture\n",
      "(Chen et al., 2017), which ﬁrst retrieves a small sub-\n",
      "set of documents using the question as the query\n",
      "and then reads the retrieved documents to extract\n",
      "(or generate) an answer. The retriever is crucial as it\n",
      "is infeasible to examine every piece of information\n",
      "in the entire document collection ( e.g., millions\n",
      "of Wikipedia passages) and the retrieval accuracy\n",
      "bounds the performance of the (extractive) reader.\n",
      "∗Work was done during internship at Microsoft Azure AI.\n",
      "1Our code and retrieval results are available at https:\n",
      "//github.com/morningmoni/GAR .Early OpenQA systems (Chen et al., 2017)\n",
      "use classic retrieval methods such as TF-IDF and\n",
      "BM25 with sparse representations. Sparse methods\n",
      "are lightweight and efﬁcient, but unable to per-\n",
      "form semantic matching and fail to retrieve rele-\n",
      "vant passages without lexical overlap. More re-\n",
      "cently, methods based on dense representations\n",
      "(Guu et al., 2020; Karpukhin et al., 2020) learn to\n",
      "embed queries and passages into a latent vector\n",
      "space, in which text similarity beyond lexical over-\n",
      "lap can be measured. Dense retrieval methods can\n",
      "retrieve semantically relevant but lexically differ-\n",
      "ent passages and often achieve better performance\n",
      "than sparse methods. However, the dense mod-\n",
      "els are more computationally expensive and suffer\n",
      "from information loss as they condense the entire\n",
      "text sequence into a ﬁxed-size vector that does not\n",
      "guarantee exact matching (Luan et al., 2020).\n",
      "There have been some recent studies on query re-\n",
      "formulation with text generation for other retrieval\n",
      "tasks, which, for example, rewrite the queries to\n",
      "context-independent (Yu et al., 2020; Lin et al.,\n",
      "2020; Vakulenko et al., 2020) or well-formed (Liu\n",
      "et al., 2019) ones. However, these methods re-\n",
      "quire either task-speciﬁc data ( e.g., conversational\n",
      "contexts, ill-formed queries) or external resources\n",
      "such as paraphrase data (Zaiem and Sadat, 2019;\n",
      "Wang et al., 2020) that cannot or do not trans-\n",
      "fer well to OpenQA. Also, some rely on time-\n",
      "consuming training process like reinforcement\n",
      "learning (RL) (Nogueira and Cho, 2017; Liu et al.,\n",
      "2019; Wang et al., 2020) that is not efﬁcient enough\n",
      "for OpenQA (more discussions in Sec. 2).\n",
      "In this paper, we propose Generation-\n",
      "Augmented Retrieval ( GAR), which augments\n",
      "a query through text generation of a pre-trained\n",
      "language model (PLM). Different from prior\n",
      "studies that reformulate queries, GARdoes not\n",
      "require external resources or downstream feedback\n",
      "via RL as supervision, because it does not rewrite\n",
      "the query but expands it with heuristically discov-arXiv:2009.08553v4  [cs.CL]  6 Aug 2021\n",
      "-----------------------------------------------------------\n",
      "Page: 0\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: Generation-Augmented Retrieval for Open-Domain Question Answering\n",
      "Yuning Mao1∗, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\n",
      "Jianfeng Gao3, Jiawei Han1, Weizhu Chen2\n",
      "1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\n",
      "1{yuningm2, hanj}@illinois.edu\n",
      "2,3{penhe, xiaodl, yeshe, jfgao,wzchen }@microsoft.com\n",
      "Abstract\n",
      "We propose Generation-Augmented Retrieval\n",
      "(GAR) for answering open-domain questions,\n",
      "which augments a query through text genera-\n",
      "tion of heuristically discovered relevant con-\n",
      "texts without external resources as supervi-\n",
      "sion. We demonstrate that the generated con-\n",
      "texts substantially enrich the semantics of the\n",
      "queries and G ARwith sparse representations\n",
      "(BM25) achieves comparable or better per-\n",
      "formance than state-of-the-art dense retrieval\n",
      "methods such as DPR (Karpukhin et al., 2020).\n",
      "We show that generating diverse contexts for a\n",
      "query is beneﬁcial as fusing their results con-\n",
      "sistently yields better retrieval accuracy. More-\n",
      "over, as sparse and dense representations are\n",
      "often complementary, G ARcan be easily com-\n",
      "bined with DPR to achieve even better per-\n",
      "formance. G ARachieves state-of-the-art per-\n",
      "formance on Natural Questions and TriviaQA\n",
      "datasets under the extractive QA setup when\n",
      "equipped with an extractive reader, and con-\n",
      "sistently outperforms other retrieval methods\n",
      "when the same generative reader is used.1\n",
      "1 Introduction\n",
      "Open-domain question answering (OpenQA) aims\n",
      "to answer factoid questions without a pre-speciﬁed\n",
      "domain and has numerous real-world applications.\n",
      "In OpenQA, a large collection of documents ( e.g.,\n",
      "Wikipedia) are often used to seek information per-\n",
      "taining to the questions. One of the most com-\n",
      "mon approaches uses a retriever-reader architecture\n",
      "(Chen et al., 2017), which ﬁrst retrieves a small sub-\n",
      "set of documents using the question as the query\n",
      "and then reads the retrieved documents to extract\n",
      "(or generate) an answer. The retriever is crucial as it\n",
      "is infeasible to examine every piece of information\n",
      "in the entire document collection ( e.g., millions\n",
      "of Wikipedia passages) and the retrieval accuracy\n",
      "bounds the performance of the (extractive) reader.\n",
      "∗Work was done during internship at Microsoft Azure AI.\n",
      "1Our code and retrieval results are available at https:\n",
      "//github.com/morningmoni/GAR .Early OpenQA systems (Chen et al., 2017)\n",
      "use classic retrieval methods such as TF-IDF and\n",
      "BM25 with sparse representations. Sparse methods\n",
      "are lightweight and efﬁcient, but unable to per-\n",
      "form semantic matching and fail to retrieve rele-\n",
      "vant passages without lexical overlap. More re-\n",
      "cently, methods based on dense representations\n",
      "(Guu et al., 2020; Karpukhin et al., 2020) learn to\n",
      "embed queries and passages into a latent vector\n",
      "space, in which text similarity beyond lexical over-\n",
      "lap can be measured. Dense retrieval methods can\n",
      "retrieve semantically relevant but lexically differ-\n",
      "ent passages and often achieve better performance\n",
      "than sparse methods. However, the dense mod-\n",
      "els are more computationally expensive and suffer\n",
      "from information loss as they condense the entire\n",
      "text sequence into a ﬁxed-size vector that does not\n",
      "guarantee exact matching (Luan et al., 2020).\n",
      "There have been some recent studies on query re-\n",
      "formulation with text generation for other retrieval\n",
      "tasks, which, for example, rewrite the queries to\n",
      "context-independent (Yu et al., 2020; Lin et al.,\n",
      "2020; Vakulenko et al., 2020) or well-formed (Liu\n",
      "et al., 2019) ones. However, these methods re-\n",
      "quire either task-speciﬁc data ( e.g., conversational\n",
      "contexts, ill-formed queries) or external resources\n",
      "such as paraphrase data (Zaiem and Sadat, 2019;\n",
      "Wang et al., 2020) that cannot or do not trans-\n",
      "fer well to OpenQA. Also, some rely on time-\n",
      "consuming training process like reinforcement\n",
      "learning (RL) (Nogueira and Cho, 2017; Liu et al.,\n",
      "2019; Wang et al., 2020) that is not efﬁcient enough\n",
      "for OpenQA (more discussions in Sec. 2).\n",
      "In this paper, we propose Generation-\n",
      "Augmented Retrieval ( GAR), which augments\n",
      "a query through text generation of a pre-trained\n",
      "language model (PLM). Different from prior\n",
      "studies that reformulate queries, GARdoes not\n",
      "require external resources or downstream feedback\n",
      "via RL as supervision, because it does not rewrite\n",
      "the query but expands it with heuristically discov-arXiv:2009.08553v4  [cs.CL]  6 Aug 2021\n",
      "-----------------------------------------------------------\n",
      "Page: 9\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: graph for question answering. arXiv preprint\n",
      "arXiv:1911.10470 .\n",
      "Iz Beltagy, Matthew E Peters, and Arman Cohan.\n",
      "2020. Longformer: The long-document transformer.\n",
      "arXiv preprint arXiv:2004.05150 .\n",
      "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, et al. 2020. Language models are few-shot\n",
      "learners. arXiv preprint arXiv:2005.14165 .\n",
      "Danqi Chen, Adam Fisch, Jason Weston, and Antoine\n",
      "Bordes. 2017. Reading Wikipedia to answer open-\n",
      "domain questions. In Proceedings of the 55th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 1870–\n",
      "1879, Vancouver, Canada. Association for Computa-\n",
      "tional Linguistics.\n",
      "Gordon V Cormack, Charles LA Clarke, and Stefan\n",
      "Buettcher. 2009. Reciprocal rank fusion outper-\n",
      "forms condorcet and individual rank learning meth-\n",
      "ods. In Proceedings of the 32nd international ACM\n",
      "SIGIR conference on Research and development in\n",
      "information retrieval , pages 758–759.\n",
      "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\n",
      "Campos, and Ellen M V oorhees. 2020. Overview\n",
      "of the trec 2019 deep learning track. arXiv preprint\n",
      "arXiv:2003.07820 .\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2019. BERT: Pre-training of\n",
      "deep bidirectional transformers for language under-\n",
      "standing. In Proceedings of the 2019 Conference\n",
      "of the North American Chapter of the Association\n",
      "for Computational Linguistics: Human Language\n",
      "Technologies, Volume 1 (Long and Short Papers) ,\n",
      "pages 4171–4186, Minneapolis, Minnesota. Associ-\n",
      "ation for Computational Linguistics.\n",
      "Emily Dinan, Varvara Logacheva, Valentin Malykh,\n",
      "Alexander Miller, Kurt Shuster, Jack Urbanek,\n",
      "Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan\n",
      "Lowe, et al. 2020. The second conversational in-\n",
      "telligence challenge (convai2). In The NeurIPS’18\n",
      "Competition , pages 187–208. Springer.\n",
      "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\n",
      "pat, and Ming-Wei Chang. 2020. Realm: Retrieval-\n",
      "augmented language model pre-training. arXiv\n",
      "preprint arXiv:2002.08909 .\n",
      "Gautier Izacard and Edouard Grave. 2020. Lever-\n",
      "aging passage retrieval with generative models for\n",
      "open domain question answering. arXiv preprint\n",
      "arXiv:2007.01282 .\n",
      "Jeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\n",
      "Billion-scale similarity search with gpus. arXiv\n",
      "preprint arXiv:1702.08734 .Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\n",
      "Zettlemoyer. 2017. TriviaQA: A large scale dis-\n",
      "tantly supervised challenge dataset for reading com-\n",
      "prehension. In Proceedings of the 55th Annual Meet-\n",
      "ing of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers) , pages 1601–1611, Van-\n",
      "couver, Canada. Association for Computational Lin-\n",
      "guistics.\n",
      "Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\n",
      "Wu, Sergey Edunov, Danqi Chen, and Wen-\n",
      "tau Yih. 2020. Dense passage retrieval for\n",
      "open-domain question answering. arXiv preprint\n",
      "arXiv:2004.04906 .\n",
      "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\n",
      "ﬁeld, Michael Collins, Ankur Parikh, Chris Al-\n",
      "berti, Danielle Epstein, Illia Polosukhin, Jacob De-\n",
      "vlin, Kenton Lee, Kristina Toutanova, Llion Jones,\n",
      "Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\n",
      "Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\n",
      "Natural questions: A benchmark for question an-\n",
      "swering research. Transactions of the Association\n",
      "for Computational Linguistics , 7:452–466.\n",
      "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n",
      "2019. Latent retrieval for weakly supervised open\n",
      "domain question answering. In Proceedings of the\n",
      "57th Annual Meeting of the Association for Com-\n",
      "putational Linguistics , pages 6086–6096, Florence,\n",
      "Italy. Association for Computational Linguistics.\n",
      "Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\n",
      "jan Ghazvininejad, Abdelrahman Mohamed, Omer\n",
      "Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\n",
      "Bart: Denoising sequence-to-sequence pre-training\n",
      "for natural language generation, translation, and\n",
      "comprehension. arXiv preprint arXiv:1910.13461 .\n",
      "Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\n",
      "Petroni, Vladimir Karpukhin, Naman Goyal, Hein-\n",
      "rich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\n",
      "Rockt ¨aschel, et al. 2020a. Retrieval-augmented gen-\n",
      "eration for knowledge-intensive nlp tasks. arXiv\n",
      "preprint arXiv:2005.11401 .\n",
      "Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n",
      "2020b. Question and answer test-train overlap in\n",
      "open-domain question answering datasets. arXiv\n",
      "preprint arXiv:2008.02637 .\n",
      "Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\n",
      "Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\n",
      "Jimmy Lin. 2020. Query reformulation using query\n",
      "history for passage retrieval in conversational search.\n",
      "arXiv preprint arXiv:2005.02230 .\n",
      "Ye Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\n",
      "Philip S Yu. 2019. Generative question reﬁnement\n",
      "with deep reinforcement learning in retrieval-based\n",
      "qa system. In Proceedings of the 28th ACM Inter-\n",
      "national Conference on Information and Knowledge\n",
      "Management , pages 1643–1652.\n",
      "-----------------------------------------------------------\n",
      "Page: 9\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: graph for question answering. arXiv preprint\n",
      "arXiv:1911.10470 .\n",
      "Iz Beltagy, Matthew E Peters, and Arman Cohan.\n",
      "2020. Longformer: The long-document transformer.\n",
      "arXiv preprint arXiv:2004.05150 .\n",
      "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, et al. 2020. Language models are few-shot\n",
      "learners. arXiv preprint arXiv:2005.14165 .\n",
      "Danqi Chen, Adam Fisch, Jason Weston, and Antoine\n",
      "Bordes. 2017. Reading Wikipedia to answer open-\n",
      "domain questions. In Proceedings of the 55th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 1870–\n",
      "1879, Vancouver, Canada. Association for Computa-\n",
      "tional Linguistics.\n",
      "Gordon V Cormack, Charles LA Clarke, and Stefan\n",
      "Buettcher. 2009. Reciprocal rank fusion outper-\n",
      "forms condorcet and individual rank learning meth-\n",
      "ods. In Proceedings of the 32nd international ACM\n",
      "SIGIR conference on Research and development in\n",
      "information retrieval , pages 758–759.\n",
      "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\n",
      "Campos, and Ellen M V oorhees. 2020. Overview\n",
      "of the trec 2019 deep learning track. arXiv preprint\n",
      "arXiv:2003.07820 .\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2019. BERT: Pre-training of\n",
      "deep bidirectional transformers for language under-\n",
      "standing. In Proceedings of the 2019 Conference\n",
      "of the North American Chapter of the Association\n",
      "for Computational Linguistics: Human Language\n",
      "Technologies, Volume 1 (Long and Short Papers) ,\n",
      "pages 4171–4186, Minneapolis, Minnesota. Associ-\n",
      "ation for Computational Linguistics.\n",
      "Emily Dinan, Varvara Logacheva, Valentin Malykh,\n",
      "Alexander Miller, Kurt Shuster, Jack Urbanek,\n",
      "Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan\n",
      "Lowe, et al. 2020. The second conversational in-\n",
      "telligence challenge (convai2). In The NeurIPS’18\n",
      "Competition , pages 187–208. Springer.\n",
      "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\n",
      "pat, and Ming-Wei Chang. 2020. Realm: Retrieval-\n",
      "augmented language model pre-training. arXiv\n",
      "preprint arXiv:2002.08909 .\n",
      "Gautier Izacard and Edouard Grave. 2020. Lever-\n",
      "aging passage retrieval with generative models for\n",
      "open domain question answering. arXiv preprint\n",
      "arXiv:2007.01282 .\n",
      "Jeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\n",
      "Billion-scale similarity search with gpus. arXiv\n",
      "preprint arXiv:1702.08734 .Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\n",
      "Zettlemoyer. 2017. TriviaQA: A large scale dis-\n",
      "tantly supervised challenge dataset for reading com-\n",
      "prehension. In Proceedings of the 55th Annual Meet-\n",
      "ing of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers) , pages 1601–1611, Van-\n",
      "couver, Canada. Association for Computational Lin-\n",
      "guistics.\n",
      "Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\n",
      "Wu, Sergey Edunov, Danqi Chen, and Wen-\n",
      "tau Yih. 2020. Dense passage retrieval for\n",
      "open-domain question answering. arXiv preprint\n",
      "arXiv:2004.04906 .\n",
      "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\n",
      "ﬁeld, Michael Collins, Ankur Parikh, Chris Al-\n",
      "berti, Danielle Epstein, Illia Polosukhin, Jacob De-\n",
      "vlin, Kenton Lee, Kristina Toutanova, Llion Jones,\n",
      "Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\n",
      "Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\n",
      "Natural questions: A benchmark for question an-\n",
      "swering research. Transactions of the Association\n",
      "for Computational Linguistics , 7:452–466.\n",
      "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n",
      "2019. Latent retrieval for weakly supervised open\n",
      "domain question answering. In Proceedings of the\n",
      "57th Annual Meeting of the Association for Com-\n",
      "putational Linguistics , pages 6086–6096, Florence,\n",
      "Italy. Association for Computational Linguistics.\n",
      "Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\n",
      "jan Ghazvininejad, Abdelrahman Mohamed, Omer\n",
      "Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\n",
      "Bart: Denoising sequence-to-sequence pre-training\n",
      "for natural language generation, translation, and\n",
      "comprehension. arXiv preprint arXiv:1910.13461 .\n",
      "Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\n",
      "Petroni, Vladimir Karpukhin, Naman Goyal, Hein-\n",
      "rich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\n",
      "Rockt ¨aschel, et al. 2020a. Retrieval-augmented gen-\n",
      "eration for knowledge-intensive nlp tasks. arXiv\n",
      "preprint arXiv:2005.11401 .\n",
      "Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n",
      "2020b. Question and answer test-train overlap in\n",
      "open-domain question answering datasets. arXiv\n",
      "preprint arXiv:2008.02637 .\n",
      "Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\n",
      "Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\n",
      "Jimmy Lin. 2020. Query reformulation using query\n",
      "history for passage retrieval in conversational search.\n",
      "arXiv preprint arXiv:2005.02230 .\n",
      "Ye Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\n",
      "Philip S Yu. 2019. Generative question reﬁnement\n",
      "with deep reinforcement learning in retrieval-based\n",
      "qa system. In Proceedings of the 28th ACM Inter-\n",
      "national Conference on Information and Knowledge\n",
      "Management , pages 1643–1652.\n",
      "-----------------------------------------------------------\n",
      "Page: 9\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: graph for question answering. arXiv preprint\n",
      "arXiv:1911.10470 .\n",
      "Iz Beltagy, Matthew E Peters, and Arman Cohan.\n",
      "2020. Longformer: The long-document transformer.\n",
      "arXiv preprint arXiv:2004.05150 .\n",
      "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, et al. 2020. Language models are few-shot\n",
      "learners. arXiv preprint arXiv:2005.14165 .\n",
      "Danqi Chen, Adam Fisch, Jason Weston, and Antoine\n",
      "Bordes. 2017. Reading Wikipedia to answer open-\n",
      "domain questions. In Proceedings of the 55th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 1870–\n",
      "1879, Vancouver, Canada. Association for Computa-\n",
      "tional Linguistics.\n",
      "Gordon V Cormack, Charles LA Clarke, and Stefan\n",
      "Buettcher. 2009. Reciprocal rank fusion outper-\n",
      "forms condorcet and individual rank learning meth-\n",
      "ods. In Proceedings of the 32nd international ACM\n",
      "SIGIR conference on Research and development in\n",
      "information retrieval , pages 758–759.\n",
      "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\n",
      "Campos, and Ellen M V oorhees. 2020. Overview\n",
      "of the trec 2019 deep learning track. arXiv preprint\n",
      "arXiv:2003.07820 .\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2019. BERT: Pre-training of\n",
      "deep bidirectional transformers for language under-\n",
      "standing. In Proceedings of the 2019 Conference\n",
      "of the North American Chapter of the Association\n",
      "for Computational Linguistics: Human Language\n",
      "Technologies, Volume 1 (Long and Short Papers) ,\n",
      "pages 4171–4186, Minneapolis, Minnesota. Associ-\n",
      "ation for Computational Linguistics.\n",
      "Emily Dinan, Varvara Logacheva, Valentin Malykh,\n",
      "Alexander Miller, Kurt Shuster, Jack Urbanek,\n",
      "Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan\n",
      "Lowe, et al. 2020. The second conversational in-\n",
      "telligence challenge (convai2). In The NeurIPS’18\n",
      "Competition , pages 187–208. Springer.\n",
      "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\n",
      "pat, and Ming-Wei Chang. 2020. Realm: Retrieval-\n",
      "augmented language model pre-training. arXiv\n",
      "preprint arXiv:2002.08909 .\n",
      "Gautier Izacard and Edouard Grave. 2020. Lever-\n",
      "aging passage retrieval with generative models for\n",
      "open domain question answering. arXiv preprint\n",
      "arXiv:2007.01282 .\n",
      "Jeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\n",
      "Billion-scale similarity search with gpus. arXiv\n",
      "preprint arXiv:1702.08734 .Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\n",
      "Zettlemoyer. 2017. TriviaQA: A large scale dis-\n",
      "tantly supervised challenge dataset for reading com-\n",
      "prehension. In Proceedings of the 55th Annual Meet-\n",
      "ing of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers) , pages 1601–1611, Van-\n",
      "couver, Canada. Association for Computational Lin-\n",
      "guistics.\n",
      "Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\n",
      "Wu, Sergey Edunov, Danqi Chen, and Wen-\n",
      "tau Yih. 2020. Dense passage retrieval for\n",
      "open-domain question answering. arXiv preprint\n",
      "arXiv:2004.04906 .\n",
      "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\n",
      "ﬁeld, Michael Collins, Ankur Parikh, Chris Al-\n",
      "berti, Danielle Epstein, Illia Polosukhin, Jacob De-\n",
      "vlin, Kenton Lee, Kristina Toutanova, Llion Jones,\n",
      "Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\n",
      "Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\n",
      "Natural questions: A benchmark for question an-\n",
      "swering research. Transactions of the Association\n",
      "for Computational Linguistics , 7:452–466.\n",
      "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n",
      "2019. Latent retrieval for weakly supervised open\n",
      "domain question answering. In Proceedings of the\n",
      "57th Annual Meeting of the Association for Com-\n",
      "putational Linguistics , pages 6086–6096, Florence,\n",
      "Italy. Association for Computational Linguistics.\n",
      "Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\n",
      "jan Ghazvininejad, Abdelrahman Mohamed, Omer\n",
      "Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\n",
      "Bart: Denoising sequence-to-sequence pre-training\n",
      "for natural language generation, translation, and\n",
      "comprehension. arXiv preprint arXiv:1910.13461 .\n",
      "Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\n",
      "Petroni, Vladimir Karpukhin, Naman Goyal, Hein-\n",
      "rich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\n",
      "Rockt ¨aschel, et al. 2020a. Retrieval-augmented gen-\n",
      "eration for knowledge-intensive nlp tasks. arXiv\n",
      "preprint arXiv:2005.11401 .\n",
      "Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n",
      "2020b. Question and answer test-train overlap in\n",
      "open-domain question answering datasets. arXiv\n",
      "preprint arXiv:2008.02637 .\n",
      "Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\n",
      "Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\n",
      "Jimmy Lin. 2020. Query reformulation using query\n",
      "history for passage retrieval in conversational search.\n",
      "arXiv preprint arXiv:2005.02230 .\n",
      "Ye Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\n",
      "Philip S Yu. 2019. Generative question reﬁnement\n",
      "with deep reinforcement learning in retrieval-based\n",
      "qa system. In Proceedings of the 28th ACM Inter-\n",
      "national Conference on Information and Knowledge\n",
      "Management , pages 1643–1652.\n",
      "-----------------------------------------------------------\n",
      "Page: 12\n",
      "Source: ./data\\In context retrieval.pdf\n",
      "Content: Slav Petrov. 2019. Natural questions: A bench-\n",
      "mark for question answering research. Trans-\n",
      "actions of the Association for Computational\n",
      "Linguistics , 7:452–466.\n",
      "Yoav Levine, Itay Dalmedigos, Ori Ram, Yoel\n",
      "Zeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin,\n",
      "Opher Lieber, Barak Lenz, Shai Shalev-Shwartz,\n",
      "Amnon Shashua, Kevin Leyton-Brown, and\n",
      "Yoav Shoham. 2022a. Standing on the shoul-\n",
      "ders of giant frozen language models.\n",
      "Yoav Levine, Ori Ram, Daniel Jannai, Barak Lenz,\n",
      "Shai Shalev-Shwartz, Amnon Shashua, Kevin\n",
      "Leyton-Brown, and Yoav Shoham. 2022b. Huge\n",
      "frozen language models as readers for open-\n",
      "domain question answering. In ICML 2022\n",
      "Workshop on Knowledge Retrieval and Lan-\n",
      "guage Models .\n",
      "Yoav Levine, Noam Wies, Daniel Jannai, Dan\n",
      "Navon, Yedid Hoshen, and Amnon Shashua.\n",
      "2022c. The inductive bias of in-context learn-\n",
      "ing: Rethinking pretraining example design. In\n",
      "International Conference on Learning Represen-\n",
      "tations .\n",
      "Patrick Lewis, Ethan Perez, Aleksandra Piktus,\n",
      "Fabio Petroni, Vladimir Karpukhin, Naman\n",
      "Goyal, Heinrich Küttler, Mike Lewis, Wen-tau\n",
      "Yih, Tim Rocktäschel, Sebastian Riedel, and\n",
      "Douwe Kiela. 2020. Retrieval-augmented gen-\n",
      "eration for knowledge-intensive nlp tasks. In\n",
      "Advances in Neural Information Processing Sys-\n",
      "tems, pages 9459–9474.\n",
      "Zonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022.\n",
      "Decoupled context processing for context aug-\n",
      "mented language modeling. In Advances in Neu-\n",
      "ral Information Processing Systems .\n",
      "Opher Lieber, Or Sharir, Barak Lenz, and Yoav\n",
      "Shoham. 2021. Jurassic-1: Technical details and\n",
      "evaluation.\n",
      "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin,\n",
      "Jheng-Hong Yang, Ronak Pradeep, and Rodrigo\n",
      "Nogueira. 2021. Pyserini: A python toolkit for\n",
      "reproducible information retrieval research with\n",
      "sparse and dense representations. In Proceed-\n",
      "ings of the 44th International ACM SIGIR Con-\n",
      "ference on Research and Development in Infor-\n",
      "mation Retrieval , SIGIR ’21, page 2356–2362,\n",
      "New York, NY , USA. Association for Comput-\n",
      "ing Machinery.Stephanie Lin, Jacob Hilton, and Owain Evans.\n",
      "2022. TruthfulQA: Measuring how models\n",
      "mimic human falsehoods. In Proceedings of the\n",
      "60th Annual Meeting of the Association for Com-\n",
      "putational Linguistics (Volume 1: Long Papers) ,\n",
      "pages 3214–3252, Dublin, Ireland. Association\n",
      "for Computational Linguistics.\n",
      "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\n",
      "Mandar Joshi, Danqi Chen, Omer Levy, Mike\n",
      "Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n",
      "2019. RoBERTa: A robustly optimized bert\n",
      "pretraining approach.\n",
      "Joshua Maynez, Shashi Narayan, Bernd Bohnet,\n",
      "and Ryan McDonald. 2020. On faithfulness and\n",
      "factuality in abstractive summarization. In Pro-\n",
      "ceedings of the 58th Annual Meeting of the As-\n",
      "sociation for Computational Linguistics , pages\n",
      "1906–1919, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Stephen Merity, Caiming Xiong, James Bradbury,\n",
      "and Richard Socher. 2016. Pointer sentinel mix-\n",
      "ture models.\n",
      "Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\n",
      "Nir Ratner, Yonatan Belinkov, Omri Abend,\n",
      "Kevin Leyton-Brown, Amnon Shashua, and\n",
      "Yoav Shoham. 2023. Generating benchmarks\n",
      "for factuality evaluation of language models.\n",
      "Fabio Petroni, Aleksandra Piktus, Angela Fan,\n",
      "Patrick Lewis, Majid Yazdani, Nicola De Cao,\n",
      "James Thorne, Yacine Jernite, Vladimir\n",
      "Karpukhin, Jean Maillard, Vassilis Plachouras,\n",
      "Tim Rocktäschel, and Sebastian Riedel. 2021.\n",
      "KILT: a benchmark for knowledge intensive lan-\n",
      "guage tasks. In Proceedings of the 2021 Con-\n",
      "ference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Hu-\n",
      "man Language Technologies , pages 2523–2544,\n",
      "Online. Association for Computational Linguis-\n",
      "tics.\n",
      "Alec Radford, Karthik Narasimhan, Tim Salimans,\n",
      "and Ilya Sutskever. 2018. Improving language\n",
      "understanding by generative pre-training.\n",
      "Alec Radford, Jeff Wu, Rewon Child, David Luan,\n",
      "Dario Amodei, and Ilya Sutskever. 2019. Lan-\n",
      "guage models are unsupervised multitask learn-\n",
      "ers.\n",
      "-----------------------------------------------------------\n",
      "Page: 12\n",
      "Source: ./data\\In context retrieval.pdf\n",
      "Content: Slav Petrov. 2019. Natural questions: A bench-\n",
      "mark for question answering research. Trans-\n",
      "actions of the Association for Computational\n",
      "Linguistics , 7:452–466.\n",
      "Yoav Levine, Itay Dalmedigos, Ori Ram, Yoel\n",
      "Zeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin,\n",
      "Opher Lieber, Barak Lenz, Shai Shalev-Shwartz,\n",
      "Amnon Shashua, Kevin Leyton-Brown, and\n",
      "Yoav Shoham. 2022a. Standing on the shoul-\n",
      "ders of giant frozen language models.\n",
      "Yoav Levine, Ori Ram, Daniel Jannai, Barak Lenz,\n",
      "Shai Shalev-Shwartz, Amnon Shashua, Kevin\n",
      "Leyton-Brown, and Yoav Shoham. 2022b. Huge\n",
      "frozen language models as readers for open-\n",
      "domain question answering. In ICML 2022\n",
      "Workshop on Knowledge Retrieval and Lan-\n",
      "guage Models .\n",
      "Yoav Levine, Noam Wies, Daniel Jannai, Dan\n",
      "Navon, Yedid Hoshen, and Amnon Shashua.\n",
      "2022c. The inductive bias of in-context learn-\n",
      "ing: Rethinking pretraining example design. In\n",
      "International Conference on Learning Represen-\n",
      "tations .\n",
      "Patrick Lewis, Ethan Perez, Aleksandra Piktus,\n",
      "Fabio Petroni, Vladimir Karpukhin, Naman\n",
      "Goyal, Heinrich Küttler, Mike Lewis, Wen-tau\n",
      "Yih, Tim Rocktäschel, Sebastian Riedel, and\n",
      "Douwe Kiela. 2020. Retrieval-augmented gen-\n",
      "eration for knowledge-intensive nlp tasks. In\n",
      "Advances in Neural Information Processing Sys-\n",
      "tems, pages 9459–9474.\n",
      "Zonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022.\n",
      "Decoupled context processing for context aug-\n",
      "mented language modeling. In Advances in Neu-\n",
      "ral Information Processing Systems .\n",
      "Opher Lieber, Or Sharir, Barak Lenz, and Yoav\n",
      "Shoham. 2021. Jurassic-1: Technical details and\n",
      "evaluation.\n",
      "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin,\n",
      "Jheng-Hong Yang, Ronak Pradeep, and Rodrigo\n",
      "Nogueira. 2021. Pyserini: A python toolkit for\n",
      "reproducible information retrieval research with\n",
      "sparse and dense representations. In Proceed-\n",
      "ings of the 44th International ACM SIGIR Con-\n",
      "ference on Research and Development in Infor-\n",
      "mation Retrieval , SIGIR ’21, page 2356–2362,\n",
      "New York, NY , USA. Association for Comput-\n",
      "ing Machinery.Stephanie Lin, Jacob Hilton, and Owain Evans.\n",
      "2022. TruthfulQA: Measuring how models\n",
      "mimic human falsehoods. In Proceedings of the\n",
      "60th Annual Meeting of the Association for Com-\n",
      "putational Linguistics (Volume 1: Long Papers) ,\n",
      "pages 3214–3252, Dublin, Ireland. Association\n",
      "for Computational Linguistics.\n",
      "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\n",
      "Mandar Joshi, Danqi Chen, Omer Levy, Mike\n",
      "Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n",
      "2019. RoBERTa: A robustly optimized bert\n",
      "pretraining approach.\n",
      "Joshua Maynez, Shashi Narayan, Bernd Bohnet,\n",
      "and Ryan McDonald. 2020. On faithfulness and\n",
      "factuality in abstractive summarization. In Pro-\n",
      "ceedings of the 58th Annual Meeting of the As-\n",
      "sociation for Computational Linguistics , pages\n",
      "1906–1919, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Stephen Merity, Caiming Xiong, James Bradbury,\n",
      "and Richard Socher. 2016. Pointer sentinel mix-\n",
      "ture models.\n",
      "Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\n",
      "Nir Ratner, Yonatan Belinkov, Omri Abend,\n",
      "Kevin Leyton-Brown, Amnon Shashua, and\n",
      "Yoav Shoham. 2023. Generating benchmarks\n",
      "for factuality evaluation of language models.\n",
      "Fabio Petroni, Aleksandra Piktus, Angela Fan,\n",
      "Patrick Lewis, Majid Yazdani, Nicola De Cao,\n",
      "James Thorne, Yacine Jernite, Vladimir\n",
      "Karpukhin, Jean Maillard, Vassilis Plachouras,\n",
      "Tim Rocktäschel, and Sebastian Riedel. 2021.\n",
      "KILT: a benchmark for knowledge intensive lan-\n",
      "guage tasks. In Proceedings of the 2021 Con-\n",
      "ference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Hu-\n",
      "man Language Technologies , pages 2523–2544,\n",
      "Online. Association for Computational Linguis-\n",
      "tics.\n",
      "Alec Radford, Karthik Narasimhan, Tim Salimans,\n",
      "and Ilya Sutskever. 2018. Improving language\n",
      "understanding by generative pre-training.\n",
      "Alec Radford, Jeff Wu, Rewon Child, David Luan,\n",
      "Dario Amodei, and Ilya Sutskever. 2019. Lan-\n",
      "guage models are unsupervised multitask learn-\n",
      "ers.\n",
      "-----------------------------------------------------------\n",
      "Page: 12\n",
      "Source: ./data\\In context retrieval.pdf\n",
      "Content: Slav Petrov. 2019. Natural questions: A bench-\n",
      "mark for question answering research. Trans-\n",
      "actions of the Association for Computational\n",
      "Linguistics , 7:452–466.\n",
      "Yoav Levine, Itay Dalmedigos, Ori Ram, Yoel\n",
      "Zeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin,\n",
      "Opher Lieber, Barak Lenz, Shai Shalev-Shwartz,\n",
      "Amnon Shashua, Kevin Leyton-Brown, and\n",
      "Yoav Shoham. 2022a. Standing on the shoul-\n",
      "ders of giant frozen language models.\n",
      "Yoav Levine, Ori Ram, Daniel Jannai, Barak Lenz,\n",
      "Shai Shalev-Shwartz, Amnon Shashua, Kevin\n",
      "Leyton-Brown, and Yoav Shoham. 2022b. Huge\n",
      "frozen language models as readers for open-\n",
      "domain question answering. In ICML 2022\n",
      "Workshop on Knowledge Retrieval and Lan-\n",
      "guage Models .\n",
      "Yoav Levine, Noam Wies, Daniel Jannai, Dan\n",
      "Navon, Yedid Hoshen, and Amnon Shashua.\n",
      "2022c. The inductive bias of in-context learn-\n",
      "ing: Rethinking pretraining example design. In\n",
      "International Conference on Learning Represen-\n",
      "tations .\n",
      "Patrick Lewis, Ethan Perez, Aleksandra Piktus,\n",
      "Fabio Petroni, Vladimir Karpukhin, Naman\n",
      "Goyal, Heinrich Küttler, Mike Lewis, Wen-tau\n",
      "Yih, Tim Rocktäschel, Sebastian Riedel, and\n",
      "Douwe Kiela. 2020. Retrieval-augmented gen-\n",
      "eration for knowledge-intensive nlp tasks. In\n",
      "Advances in Neural Information Processing Sys-\n",
      "tems, pages 9459–9474.\n",
      "Zonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022.\n",
      "Decoupled context processing for context aug-\n",
      "mented language modeling. In Advances in Neu-\n",
      "ral Information Processing Systems .\n",
      "Opher Lieber, Or Sharir, Barak Lenz, and Yoav\n",
      "Shoham. 2021. Jurassic-1: Technical details and\n",
      "evaluation.\n",
      "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin,\n",
      "Jheng-Hong Yang, Ronak Pradeep, and Rodrigo\n",
      "Nogueira. 2021. Pyserini: A python toolkit for\n",
      "reproducible information retrieval research with\n",
      "sparse and dense representations. In Proceed-\n",
      "ings of the 44th International ACM SIGIR Con-\n",
      "ference on Research and Development in Infor-\n",
      "mation Retrieval , SIGIR ’21, page 2356–2362,\n",
      "New York, NY , USA. Association for Comput-\n",
      "ing Machinery.Stephanie Lin, Jacob Hilton, and Owain Evans.\n",
      "2022. TruthfulQA: Measuring how models\n",
      "mimic human falsehoods. In Proceedings of the\n",
      "60th Annual Meeting of the Association for Com-\n",
      "putational Linguistics (Volume 1: Long Papers) ,\n",
      "pages 3214–3252, Dublin, Ireland. Association\n",
      "for Computational Linguistics.\n",
      "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\n",
      "Mandar Joshi, Danqi Chen, Omer Levy, Mike\n",
      "Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n",
      "2019. RoBERTa: A robustly optimized bert\n",
      "pretraining approach.\n",
      "Joshua Maynez, Shashi Narayan, Bernd Bohnet,\n",
      "and Ryan McDonald. 2020. On faithfulness and\n",
      "factuality in abstractive summarization. In Pro-\n",
      "ceedings of the 58th Annual Meeting of the As-\n",
      "sociation for Computational Linguistics , pages\n",
      "1906–1919, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Stephen Merity, Caiming Xiong, James Bradbury,\n",
      "and Richard Socher. 2016. Pointer sentinel mix-\n",
      "ture models.\n",
      "Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\n",
      "Nir Ratner, Yonatan Belinkov, Omri Abend,\n",
      "Kevin Leyton-Brown, Amnon Shashua, and\n",
      "Yoav Shoham. 2023. Generating benchmarks\n",
      "for factuality evaluation of language models.\n",
      "Fabio Petroni, Aleksandra Piktus, Angela Fan,\n",
      "Patrick Lewis, Majid Yazdani, Nicola De Cao,\n",
      "James Thorne, Yacine Jernite, Vladimir\n",
      "Karpukhin, Jean Maillard, Vassilis Plachouras,\n",
      "Tim Rocktäschel, and Sebastian Riedel. 2021.\n",
      "KILT: a benchmark for knowledge intensive lan-\n",
      "guage tasks. In Proceedings of the 2021 Con-\n",
      "ference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Hu-\n",
      "man Language Technologies , pages 2523–2544,\n",
      "Online. Association for Computational Linguis-\n",
      "tics.\n",
      "Alec Radford, Karthik Narasimhan, Tim Salimans,\n",
      "and Ilya Sutskever. 2018. Improving language\n",
      "understanding by generative pre-training.\n",
      "Alec Radford, Jeff Wu, Rewon Child, David Luan,\n",
      "Dario Amodei, and Ilya Sutskever. 2019. Lan-\n",
      "guage models are unsupervised multitask learn-\n",
      "ers.\n",
      "-----------------------------------------------------------\n",
      "Page: 2\n",
      "Source: ./data\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf\n",
      "Content: seq2seq learning, which, despite its simplicity, is\n",
      "not only more efﬁcient but effective for OpenQA.\n",
      "Retrieval for OpenQA . Existing sparse retrieval\n",
      "methods for OpenQA (Chen et al., 2017) solely rely\n",
      "on the information of the questions. GARextends\n",
      "to contexts relevant to the questions by extracting\n",
      "information inside PLMs and helps sparse meth-\n",
      "ods achieve comparable or better performance than\n",
      "dense methods (Guu et al., 2020; Karpukhin et al.,\n",
      "2020), while enjoying the simplicity and efﬁciency\n",
      "of sparse representations. GARcan also be used\n",
      "with dense representations to seek for even better\n",
      "performance, which we leave as future work.\n",
      "Generative QA . Generative QA generates answers\n",
      "through seq2seq learning instead of extracting an-\n",
      "swer spans. Recent studies on generative OpenQA\n",
      "(Lewis et al., 2020a; Min et al., 2020; Izacard and\n",
      "Grave, 2020) are orthogonal to GARin that they\n",
      "focus on improving the reading stage and directly\n",
      "reuse DPR (Karpukhin et al., 2020) as the retriever.\n",
      "Unlike generative QA, the goal of GARis not to\n",
      "generate perfect answers to the questions but perti-\n",
      "nent contexts that are helpful for retrieval. Another\n",
      "line in generative QA learns to generate answers\n",
      "without relevant passages as the evidence but solely\n",
      "the question itself using PLMs (Roberts et al., 2020;\n",
      "Brown et al., 2020). GARfurther conﬁrms that one\n",
      "can extract factual knowledge from PLMs, which\n",
      "is not limited to the answers as in prior studies but\n",
      "also other relevant contexts.\n",
      "3 Generation-Augmented Retrieval\n",
      "3.1 Task Formulation\n",
      "OpenQA aims to answer factoid questions with-\n",
      "out pre-speciﬁed domains. We assume that a large\n",
      "collection of documents C(i.e., Wikipedia) are\n",
      "given as the resource to answer the questions and\n",
      "a retriever-reader architecture is used to tackle the\n",
      "task, where the retriever retrieves a small subset\n",
      "of the documents D⊂Cand the reader reads the\n",
      "documents Dto extract (or generate) an answer.\n",
      "Our goal is to improve the effectiveness and efﬁ-\n",
      "ciency of the retriever and consequently improve\n",
      "the performance of the reader.\n",
      "3.2 Generation of Query Contexts\n",
      "InGAR, queries are augmented with various heuris-\n",
      "tically discovered relevant contexts in order to re-\n",
      "trieve more relevant passages in terms of both quan-\n",
      "tity and quality. For the task of OpenQA where the\n",
      "query is a question, we take the following threefreely accessible contexts as the generation targets.\n",
      "We show in Sec. 6.2 that having multiple gener-\n",
      "ation targets is helpful in that fusing their results\n",
      "consistently brings better retrieval accuracy.\n",
      "Context 1: The default target (answer) . The de-\n",
      "fault target is the label in the task of interest, which\n",
      "is the answer in OpenQA. The answer to the ques-\n",
      "tion is apparently useful for the retrieval of relevant\n",
      "passages that contain the answer itself. As shown\n",
      "in previous work (Roberts et al., 2020; Brown et al.,\n",
      "2020), PLMs are able to answer certain questions\n",
      "solely by taking the questions as input ( i.e., closed-\n",
      "book QA). Instead of using the generated answers\n",
      "directly as in closed-book QA, GARtreats them\n",
      "as contexts of the question for retrieval. The ad-\n",
      "vantage is that even if the generated answers are\n",
      "partially correct (or even incorrect), they may still\n",
      "beneﬁt retrieval as long as they are relevant to the\n",
      "passages that contain the correct answers ( e.g., co-\n",
      "occur with the correct answers).\n",
      "Context 2: Sentence containing the default tar-\n",
      "get. The sentence in a passage that contains the\n",
      "answer is used as another generation target. Sim-\n",
      "ilar to using answers as the generation target, the\n",
      "generated sentences are still beneﬁcial for retriev-\n",
      "ing relevant passages even if they do not contain\n",
      "the answers, as their semantics is highly related to\n",
      "the questions/answers (examples in Sec. 6.1). One\n",
      "can take the relevant sentences in the ground-truth\n",
      "passages (if any) or those in the positive passages\n",
      "of a retriever as the reference, depending on the\n",
      "trade-off between reference quality and diversity.\n",
      "Context 3: Title of passage containing the de-\n",
      "fault target . One can also use the titles of rele-\n",
      "vant passages as the generation target if available.\n",
      "Speciﬁcally, we retrieve Wikipedia passages using\n",
      "BM25 with the question as the query, and take the\n",
      "page titles of positive passages that contain the an-\n",
      "swers as the generation target. We observe that\n",
      "the page titles of positive passages are often entity\n",
      "names of interest, and sometimes (but not always)\n",
      "the answers to the questions. Intuitively, if GAR\n",
      "learns which Wikipedia pages the question is re-\n",
      "lated to, the queries augmented by the generated\n",
      "titles would naturally have a better chance of re-\n",
      "trieving those relevant passages.\n",
      "While it is likely that some of the generated\n",
      "query contexts involve unfaithful or nonfactual in-\n",
      "formation due to hallucination in text generation\n",
      "(Mao et al., 2020) and introduce noise during re-\n",
      "trieval, they are beneﬁcial rather than harmful over-\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def convert_docs_to_dict(docs):\n",
    "    \"\"\"\n",
    "    Convert a list of Document objects to a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "    - docs (list): List of Document objects.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of dictionaries containing 'page_content' and 'metadata'.\n",
    "    \"\"\"\n",
    "    doc_dicts = []\n",
    "    for doc in docs:\n",
    "        doc_dict = {\n",
    "            'page_content': str(doc.page_content),\n",
    "            'metadata': doc.metadata\n",
    "        }\n",
    "        doc_dicts.append(doc_dict)\n",
    "    return doc_dicts\n",
    "\n",
    "# Example usage:\n",
    "docs_dict_list = convert_docs_to_dict(docs)\n",
    "for doc_dict in docs_dict_list:\n",
    "    print(\"Page:\", doc_dict['metadata']['page'])\n",
    "    print(\"Source:\", doc_dict['metadata']['source'])\n",
    "    print(\"Content:\", doc_dict['page_content'])\n",
    "    print(\"-----------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da315090-a736-422a-b7de-7cfbf7b74f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.373701\n",
      "4.373701\n",
      "4.373701\n",
      "-2.4895267\n",
      "-2.4895267\n",
      "-2.4895267\n",
      "-0.1403997\n",
      "-0.1403997\n",
      "-0.1403997\n",
      "-0.9551192\n",
      "[{'page_content': 'Generation-Augmented Retrieval for Open-Domain Question Answering\\nYuning Mao1∗, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\\nJianfeng Gao3, Jiawei Han1, Weizhu Chen2\\n1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\\n1{yuningm2, hanj}@illinois.edu\\n2,3{penhe, xiaodl, yeshe, jfgao,wzchen }@microsoft.com\\nAbstract\\nWe propose Generation-Augmented Retrieval\\n(GAR) for answering open-domain questions,\\nwhich augments a query through text genera-\\ntion of heuristically discovered relevant con-\\ntexts without external resources as supervi-\\nsion. We demonstrate that the generated con-\\ntexts substantially enrich the semantics of the\\nqueries and G ARwith sparse representations\\n(BM25) achieves comparable or better per-\\nformance than state-of-the-art dense retrieval\\nmethods such as DPR (Karpukhin et al., 2020).\\nWe show that generating diverse contexts for a\\nquery is beneﬁcial as fusing their results con-\\nsistently yields better retrieval accuracy. More-\\nover, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it\\nis infeasible to examine every piece of information\\nin the entire document collection ( e.g., millions\\nof Wikipedia passages) and the retrieval accuracy\\nbounds the performance of the (extractive) reader.\\n∗Work was done during internship at Microsoft Azure AI.\\n1Our code and retrieval results are available at https:\\n//github.com/morningmoni/GAR .Early OpenQA systems (Chen et al., 2017)\\nuse classic retrieval methods such as TF-IDF and\\nBM25 with sparse representations. Sparse methods\\nare lightweight and efﬁcient, but unable to per-\\nform semantic matching and fail to retrieve rele-\\nvant passages without lexical overlap. More re-\\ncently, methods based on dense representations\\n(Guu et al., 2020; Karpukhin et al., 2020) learn to\\nembed queries and passages into a latent vector\\nspace, in which text similarity beyond lexical over-\\nlap can be measured. Dense retrieval methods can\\nretrieve semantically relevant but lexically differ-\\nent passages and often achieve better performance\\nthan sparse methods. However, the dense mod-\\nels are more computationally expensive and suffer\\nfrom information loss as they condense the entire\\ntext sequence into a ﬁxed-size vector that does not\\nguarantee exact matching (Luan et al., 2020).\\nThere have been some recent studies on query re-\\nformulation with text generation for other retrieval\\ntasks, which, for example, rewrite the queries to\\ncontext-independent (Yu et al., 2020; Lin et al.,\\n2020; Vakulenko et al., 2020) or well-formed (Liu\\net al., 2019) ones. However, these methods re-\\nquire either task-speciﬁc data ( e.g., conversational\\ncontexts, ill-formed queries) or external resources\\nsuch as paraphrase data (Zaiem and Sadat, 2019;\\nWang et al., 2020) that cannot or do not trans-\\nfer well to OpenQA. Also, some rely on time-\\nconsuming training process like reinforcement\\nlearning (RL) (Nogueira and Cho, 2017; Liu et al.,\\n2019; Wang et al., 2020) that is not efﬁcient enough\\nfor OpenQA (more discussions in Sec. 2).\\nIn this paper, we propose Generation-\\nAugmented Retrieval ( GAR), which augments\\na query through text generation of a pre-trained\\nlanguage model (PLM). Different from prior\\nstudies that reformulate queries, GARdoes not\\nrequire external resources or downstream feedback\\nvia RL as supervision, because it does not rewrite\\nthe query but expands it with heuristically discov-arXiv:2009.08553v4  [cs.CL]  6 Aug 2021', 'metadata': {'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': 'Generation-Augmented Retrieval for Open-Domain Question Answering\\nYuning Mao1∗, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\\nJianfeng Gao3, Jiawei Han1, Weizhu Chen2\\n1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\\n1{yuningm2, hanj}@illinois.edu\\n2,3{penhe, xiaodl, yeshe, jfgao,wzchen }@microsoft.com\\nAbstract\\nWe propose Generation-Augmented Retrieval\\n(GAR) for answering open-domain questions,\\nwhich augments a query through text genera-\\ntion of heuristically discovered relevant con-\\ntexts without external resources as supervi-\\nsion. We demonstrate that the generated con-\\ntexts substantially enrich the semantics of the\\nqueries and G ARwith sparse representations\\n(BM25) achieves comparable or better per-\\nformance than state-of-the-art dense retrieval\\nmethods such as DPR (Karpukhin et al., 2020).\\nWe show that generating diverse contexts for a\\nquery is beneﬁcial as fusing their results con-\\nsistently yields better retrieval accuracy. More-\\nover, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it\\nis infeasible to examine every piece of information\\nin the entire document collection ( e.g., millions\\nof Wikipedia passages) and the retrieval accuracy\\nbounds the performance of the (extractive) reader.\\n∗Work was done during internship at Microsoft Azure AI.\\n1Our code and retrieval results are available at https:\\n//github.com/morningmoni/GAR .Early OpenQA systems (Chen et al., 2017)\\nuse classic retrieval methods such as TF-IDF and\\nBM25 with sparse representations. Sparse methods\\nare lightweight and efﬁcient, but unable to per-\\nform semantic matching and fail to retrieve rele-\\nvant passages without lexical overlap. More re-\\ncently, methods based on dense representations\\n(Guu et al., 2020; Karpukhin et al., 2020) learn to\\nembed queries and passages into a latent vector\\nspace, in which text similarity beyond lexical over-\\nlap can be measured. Dense retrieval methods can\\nretrieve semantically relevant but lexically differ-\\nent passages and often achieve better performance\\nthan sparse methods. However, the dense mod-\\nels are more computationally expensive and suffer\\nfrom information loss as they condense the entire\\ntext sequence into a ﬁxed-size vector that does not\\nguarantee exact matching (Luan et al., 2020).\\nThere have been some recent studies on query re-\\nformulation with text generation for other retrieval\\ntasks, which, for example, rewrite the queries to\\ncontext-independent (Yu et al., 2020; Lin et al.,\\n2020; Vakulenko et al., 2020) or well-formed (Liu\\net al., 2019) ones. However, these methods re-\\nquire either task-speciﬁc data ( e.g., conversational\\ncontexts, ill-formed queries) or external resources\\nsuch as paraphrase data (Zaiem and Sadat, 2019;\\nWang et al., 2020) that cannot or do not trans-\\nfer well to OpenQA. Also, some rely on time-\\nconsuming training process like reinforcement\\nlearning (RL) (Nogueira and Cho, 2017; Liu et al.,\\n2019; Wang et al., 2020) that is not efﬁcient enough\\nfor OpenQA (more discussions in Sec. 2).\\nIn this paper, we propose Generation-\\nAugmented Retrieval ( GAR), which augments\\na query through text generation of a pre-trained\\nlanguage model (PLM). Different from prior\\nstudies that reformulate queries, GARdoes not\\nrequire external resources or downstream feedback\\nvia RL as supervision, because it does not rewrite\\nthe query but expands it with heuristically discov-arXiv:2009.08553v4  [cs.CL]  6 Aug 2021', 'metadata': {'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': 'Generation-Augmented Retrieval for Open-Domain Question Answering\\nYuning Mao1∗, Pengcheng He2, Xiaodong Liu3, Yelong Shen2,\\nJianfeng Gao3, Jiawei Han1, Weizhu Chen2\\n1University of Illinois, Urbana-Champaign2Microsoft Azure AI3Microsoft Research\\n1{yuningm2, hanj}@illinois.edu\\n2,3{penhe, xiaodl, yeshe, jfgao,wzchen }@microsoft.com\\nAbstract\\nWe propose Generation-Augmented Retrieval\\n(GAR) for answering open-domain questions,\\nwhich augments a query through text genera-\\ntion of heuristically discovered relevant con-\\ntexts without external resources as supervi-\\nsion. We demonstrate that the generated con-\\ntexts substantially enrich the semantics of the\\nqueries and G ARwith sparse representations\\n(BM25) achieves comparable or better per-\\nformance than state-of-the-art dense retrieval\\nmethods such as DPR (Karpukhin et al., 2020).\\nWe show that generating diverse contexts for a\\nquery is beneﬁcial as fusing their results con-\\nsistently yields better retrieval accuracy. More-\\nover, as sparse and dense representations are\\noften complementary, G ARcan be easily com-\\nbined with DPR to achieve even better per-\\nformance. G ARachieves state-of-the-art per-\\nformance on Natural Questions and TriviaQA\\ndatasets under the extractive QA setup when\\nequipped with an extractive reader, and con-\\nsistently outperforms other retrieval methods\\nwhen the same generative reader is used.1\\n1 Introduction\\nOpen-domain question answering (OpenQA) aims\\nto answer factoid questions without a pre-speciﬁed\\ndomain and has numerous real-world applications.\\nIn OpenQA, a large collection of documents ( e.g.,\\nWikipedia) are often used to seek information per-\\ntaining to the questions. One of the most com-\\nmon approaches uses a retriever-reader architecture\\n(Chen et al., 2017), which ﬁrst retrieves a small sub-\\nset of documents using the question as the query\\nand then reads the retrieved documents to extract\\n(or generate) an answer. The retriever is crucial as it\\nis infeasible to examine every piece of information\\nin the entire document collection ( e.g., millions\\nof Wikipedia passages) and the retrieval accuracy\\nbounds the performance of the (extractive) reader.\\n∗Work was done during internship at Microsoft Azure AI.\\n1Our code and retrieval results are available at https:\\n//github.com/morningmoni/GAR .Early OpenQA systems (Chen et al., 2017)\\nuse classic retrieval methods such as TF-IDF and\\nBM25 with sparse representations. Sparse methods\\nare lightweight and efﬁcient, but unable to per-\\nform semantic matching and fail to retrieve rele-\\nvant passages without lexical overlap. More re-\\ncently, methods based on dense representations\\n(Guu et al., 2020; Karpukhin et al., 2020) learn to\\nembed queries and passages into a latent vector\\nspace, in which text similarity beyond lexical over-\\nlap can be measured. Dense retrieval methods can\\nretrieve semantically relevant but lexically differ-\\nent passages and often achieve better performance\\nthan sparse methods. However, the dense mod-\\nels are more computationally expensive and suffer\\nfrom information loss as they condense the entire\\ntext sequence into a ﬁxed-size vector that does not\\nguarantee exact matching (Luan et al., 2020).\\nThere have been some recent studies on query re-\\nformulation with text generation for other retrieval\\ntasks, which, for example, rewrite the queries to\\ncontext-independent (Yu et al., 2020; Lin et al.,\\n2020; Vakulenko et al., 2020) or well-formed (Liu\\net al., 2019) ones. However, these methods re-\\nquire either task-speciﬁc data ( e.g., conversational\\ncontexts, ill-formed queries) or external resources\\nsuch as paraphrase data (Zaiem and Sadat, 2019;\\nWang et al., 2020) that cannot or do not trans-\\nfer well to OpenQA. Also, some rely on time-\\nconsuming training process like reinforcement\\nlearning (RL) (Nogueira and Cho, 2017; Liu et al.,\\n2019; Wang et al., 2020) that is not efﬁcient enough\\nfor OpenQA (more discussions in Sec. 2).\\nIn this paper, we propose Generation-\\nAugmented Retrieval ( GAR), which augments\\na query through text generation of a pre-trained\\nlanguage model (PLM). Different from prior\\nstudies that reformulate queries, GARdoes not\\nrequire external resources or downstream feedback\\nvia RL as supervision, because it does not rewrite\\nthe query but expands it with heuristically discov-arXiv:2009.08553v4  [cs.CL]  6 Aug 2021', 'metadata': {'page': 0, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': 'Slav Petrov. 2019. Natural questions: A bench-\\nmark for question answering research. Trans-\\nactions of the Association for Computational\\nLinguistics , 7:452–466.\\nYoav Levine, Itay Dalmedigos, Ori Ram, Yoel\\nZeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin,\\nOpher Lieber, Barak Lenz, Shai Shalev-Shwartz,\\nAmnon Shashua, Kevin Leyton-Brown, and\\nYoav Shoham. 2022a. Standing on the shoul-\\nders of giant frozen language models.\\nYoav Levine, Ori Ram, Daniel Jannai, Barak Lenz,\\nShai Shalev-Shwartz, Amnon Shashua, Kevin\\nLeyton-Brown, and Yoav Shoham. 2022b. Huge\\nfrozen language models as readers for open-\\ndomain question answering. In ICML 2022\\nWorkshop on Knowledge Retrieval and Lan-\\nguage Models .\\nYoav Levine, Noam Wies, Daniel Jannai, Dan\\nNavon, Yedid Hoshen, and Amnon Shashua.\\n2022c. The inductive bias of in-context learn-\\ning: Rethinking pretraining example design. In\\nInternational Conference on Learning Represen-\\ntations .\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus,\\nFabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau\\nYih, Tim Rocktäschel, Sebastian Riedel, and\\nDouwe Kiela. 2020. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. In\\nAdvances in Neural Information Processing Sys-\\ntems, pages 9459–9474.\\nZonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022.\\nDecoupled context processing for context aug-\\nmented language modeling. In Advances in Neu-\\nral Information Processing Systems .\\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\\nShoham. 2021. Jurassic-1: Technical details and\\nevaluation.\\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin,\\nJheng-Hong Yang, Ronak Pradeep, and Rodrigo\\nNogueira. 2021. Pyserini: A python toolkit for\\nreproducible information retrieval research with\\nsparse and dense representations. In Proceed-\\nings of the 44th International ACM SIGIR Con-\\nference on Research and Development in Infor-\\nmation Retrieval , SIGIR ’21, page 2356–2362,\\nNew York, NY , USA. Association for Comput-\\ning Machinery.Stephanie Lin, Jacob Hilton, and Owain Evans.\\n2022. TruthfulQA: Measuring how models\\nmimic human falsehoods. In Proceedings of the\\n60th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers) ,\\npages 3214–3252, Dublin, Ireland. Association\\nfor Computational Linguistics.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\\nMandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019. RoBERTa: A robustly optimized bert\\npretraining approach.\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet,\\nand Ryan McDonald. 2020. On faithfulness and\\nfactuality in abstractive summarization. In Pro-\\nceedings of the 58th Annual Meeting of the As-\\nsociation for Computational Linguistics , pages\\n1906–1919, Online. Association for Computa-\\ntional Linguistics.\\nStephen Merity, Caiming Xiong, James Bradbury,\\nand Richard Socher. 2016. Pointer sentinel mix-\\nture models.\\nDor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\\nNir Ratner, Yonatan Belinkov, Omri Abend,\\nKevin Leyton-Brown, Amnon Shashua, and\\nYoav Shoham. 2023. Generating benchmarks\\nfor factuality evaluation of language models.\\nFabio Petroni, Aleksandra Piktus, Angela Fan,\\nPatrick Lewis, Majid Yazdani, Nicola De Cao,\\nJames Thorne, Yacine Jernite, Vladimir\\nKarpukhin, Jean Maillard, Vassilis Plachouras,\\nTim Rocktäschel, and Sebastian Riedel. 2021.\\nKILT: a benchmark for knowledge intensive lan-\\nguage tasks. In Proceedings of the 2021 Con-\\nference of the North American Chapter of the\\nAssociation for Computational Linguistics: Hu-\\nman Language Technologies , pages 2523–2544,\\nOnline. Association for Computational Linguis-\\ntics.\\nAlec Radford, Karthik Narasimhan, Tim Salimans,\\nand Ilya Sutskever. 2018. Improving language\\nunderstanding by generative pre-training.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Lan-\\nguage models are unsupervised multitask learn-\\ners.', 'metadata': {'page': 12, 'source': './data\\\\In context retrieval.pdf'}}, {'page_content': 'Slav Petrov. 2019. Natural questions: A bench-\\nmark for question answering research. Trans-\\nactions of the Association for Computational\\nLinguistics , 7:452–466.\\nYoav Levine, Itay Dalmedigos, Ori Ram, Yoel\\nZeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin,\\nOpher Lieber, Barak Lenz, Shai Shalev-Shwartz,\\nAmnon Shashua, Kevin Leyton-Brown, and\\nYoav Shoham. 2022a. Standing on the shoul-\\nders of giant frozen language models.\\nYoav Levine, Ori Ram, Daniel Jannai, Barak Lenz,\\nShai Shalev-Shwartz, Amnon Shashua, Kevin\\nLeyton-Brown, and Yoav Shoham. 2022b. Huge\\nfrozen language models as readers for open-\\ndomain question answering. In ICML 2022\\nWorkshop on Knowledge Retrieval and Lan-\\nguage Models .\\nYoav Levine, Noam Wies, Daniel Jannai, Dan\\nNavon, Yedid Hoshen, and Amnon Shashua.\\n2022c. The inductive bias of in-context learn-\\ning: Rethinking pretraining example design. In\\nInternational Conference on Learning Represen-\\ntations .\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus,\\nFabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau\\nYih, Tim Rocktäschel, Sebastian Riedel, and\\nDouwe Kiela. 2020. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. In\\nAdvances in Neural Information Processing Sys-\\ntems, pages 9459–9474.\\nZonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022.\\nDecoupled context processing for context aug-\\nmented language modeling. In Advances in Neu-\\nral Information Processing Systems .\\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\\nShoham. 2021. Jurassic-1: Technical details and\\nevaluation.\\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin,\\nJheng-Hong Yang, Ronak Pradeep, and Rodrigo\\nNogueira. 2021. Pyserini: A python toolkit for\\nreproducible information retrieval research with\\nsparse and dense representations. In Proceed-\\nings of the 44th International ACM SIGIR Con-\\nference on Research and Development in Infor-\\nmation Retrieval , SIGIR ’21, page 2356–2362,\\nNew York, NY , USA. Association for Comput-\\ning Machinery.Stephanie Lin, Jacob Hilton, and Owain Evans.\\n2022. TruthfulQA: Measuring how models\\nmimic human falsehoods. In Proceedings of the\\n60th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers) ,\\npages 3214–3252, Dublin, Ireland. Association\\nfor Computational Linguistics.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\\nMandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019. RoBERTa: A robustly optimized bert\\npretraining approach.\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet,\\nand Ryan McDonald. 2020. On faithfulness and\\nfactuality in abstractive summarization. In Pro-\\nceedings of the 58th Annual Meeting of the As-\\nsociation for Computational Linguistics , pages\\n1906–1919, Online. Association for Computa-\\ntional Linguistics.\\nStephen Merity, Caiming Xiong, James Bradbury,\\nand Richard Socher. 2016. Pointer sentinel mix-\\nture models.\\nDor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\\nNir Ratner, Yonatan Belinkov, Omri Abend,\\nKevin Leyton-Brown, Amnon Shashua, and\\nYoav Shoham. 2023. Generating benchmarks\\nfor factuality evaluation of language models.\\nFabio Petroni, Aleksandra Piktus, Angela Fan,\\nPatrick Lewis, Majid Yazdani, Nicola De Cao,\\nJames Thorne, Yacine Jernite, Vladimir\\nKarpukhin, Jean Maillard, Vassilis Plachouras,\\nTim Rocktäschel, and Sebastian Riedel. 2021.\\nKILT: a benchmark for knowledge intensive lan-\\nguage tasks. In Proceedings of the 2021 Con-\\nference of the North American Chapter of the\\nAssociation for Computational Linguistics: Hu-\\nman Language Technologies , pages 2523–2544,\\nOnline. Association for Computational Linguis-\\ntics.\\nAlec Radford, Karthik Narasimhan, Tim Salimans,\\nand Ilya Sutskever. 2018. Improving language\\nunderstanding by generative pre-training.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Lan-\\nguage models are unsupervised multitask learn-\\ners.', 'metadata': {'page': 12, 'source': './data\\\\In context retrieval.pdf'}}, {'page_content': 'Slav Petrov. 2019. Natural questions: A bench-\\nmark for question answering research. Trans-\\nactions of the Association for Computational\\nLinguistics , 7:452–466.\\nYoav Levine, Itay Dalmedigos, Ori Ram, Yoel\\nZeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin,\\nOpher Lieber, Barak Lenz, Shai Shalev-Shwartz,\\nAmnon Shashua, Kevin Leyton-Brown, and\\nYoav Shoham. 2022a. Standing on the shoul-\\nders of giant frozen language models.\\nYoav Levine, Ori Ram, Daniel Jannai, Barak Lenz,\\nShai Shalev-Shwartz, Amnon Shashua, Kevin\\nLeyton-Brown, and Yoav Shoham. 2022b. Huge\\nfrozen language models as readers for open-\\ndomain question answering. In ICML 2022\\nWorkshop on Knowledge Retrieval and Lan-\\nguage Models .\\nYoav Levine, Noam Wies, Daniel Jannai, Dan\\nNavon, Yedid Hoshen, and Amnon Shashua.\\n2022c. The inductive bias of in-context learn-\\ning: Rethinking pretraining example design. In\\nInternational Conference on Learning Represen-\\ntations .\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus,\\nFabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau\\nYih, Tim Rocktäschel, Sebastian Riedel, and\\nDouwe Kiela. 2020. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. In\\nAdvances in Neural Information Processing Sys-\\ntems, pages 9459–9474.\\nZonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022.\\nDecoupled context processing for context aug-\\nmented language modeling. In Advances in Neu-\\nral Information Processing Systems .\\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\\nShoham. 2021. Jurassic-1: Technical details and\\nevaluation.\\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin,\\nJheng-Hong Yang, Ronak Pradeep, and Rodrigo\\nNogueira. 2021. Pyserini: A python toolkit for\\nreproducible information retrieval research with\\nsparse and dense representations. In Proceed-\\nings of the 44th International ACM SIGIR Con-\\nference on Research and Development in Infor-\\nmation Retrieval , SIGIR ’21, page 2356–2362,\\nNew York, NY , USA. Association for Comput-\\ning Machinery.Stephanie Lin, Jacob Hilton, and Owain Evans.\\n2022. TruthfulQA: Measuring how models\\nmimic human falsehoods. In Proceedings of the\\n60th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers) ,\\npages 3214–3252, Dublin, Ireland. Association\\nfor Computational Linguistics.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\\nMandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019. RoBERTa: A robustly optimized bert\\npretraining approach.\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet,\\nand Ryan McDonald. 2020. On faithfulness and\\nfactuality in abstractive summarization. In Pro-\\nceedings of the 58th Annual Meeting of the As-\\nsociation for Computational Linguistics , pages\\n1906–1919, Online. Association for Computa-\\ntional Linguistics.\\nStephen Merity, Caiming Xiong, James Bradbury,\\nand Richard Socher. 2016. Pointer sentinel mix-\\nture models.\\nDor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,\\nNir Ratner, Yonatan Belinkov, Omri Abend,\\nKevin Leyton-Brown, Amnon Shashua, and\\nYoav Shoham. 2023. Generating benchmarks\\nfor factuality evaluation of language models.\\nFabio Petroni, Aleksandra Piktus, Angela Fan,\\nPatrick Lewis, Majid Yazdani, Nicola De Cao,\\nJames Thorne, Yacine Jernite, Vladimir\\nKarpukhin, Jean Maillard, Vassilis Plachouras,\\nTim Rocktäschel, and Sebastian Riedel. 2021.\\nKILT: a benchmark for knowledge intensive lan-\\nguage tasks. In Proceedings of the 2021 Con-\\nference of the North American Chapter of the\\nAssociation for Computational Linguistics: Hu-\\nman Language Technologies , pages 2523–2544,\\nOnline. Association for Computational Linguis-\\ntics.\\nAlec Radford, Karthik Narasimhan, Tim Salimans,\\nand Ilya Sutskever. 2018. Improving language\\nunderstanding by generative pre-training.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Lan-\\nguage models are unsupervised multitask learn-\\ners.', 'metadata': {'page': 12, 'source': './data\\\\In context retrieval.pdf'}}, {'page_content': 'seq2seq learning, which, despite its simplicity, is\\nnot only more efﬁcient but effective for OpenQA.\\nRetrieval for OpenQA . Existing sparse retrieval\\nmethods for OpenQA (Chen et al., 2017) solely rely\\non the information of the questions. GARextends\\nto contexts relevant to the questions by extracting\\ninformation inside PLMs and helps sparse meth-\\nods achieve comparable or better performance than\\ndense methods (Guu et al., 2020; Karpukhin et al.,\\n2020), while enjoying the simplicity and efﬁciency\\nof sparse representations. GARcan also be used\\nwith dense representations to seek for even better\\nperformance, which we leave as future work.\\nGenerative QA . Generative QA generates answers\\nthrough seq2seq learning instead of extracting an-\\nswer spans. Recent studies on generative OpenQA\\n(Lewis et al., 2020a; Min et al., 2020; Izacard and\\nGrave, 2020) are orthogonal to GARin that they\\nfocus on improving the reading stage and directly\\nreuse DPR (Karpukhin et al., 2020) as the retriever.\\nUnlike generative QA, the goal of GARis not to\\ngenerate perfect answers to the questions but perti-\\nnent contexts that are helpful for retrieval. Another\\nline in generative QA learns to generate answers\\nwithout relevant passages as the evidence but solely\\nthe question itself using PLMs (Roberts et al., 2020;\\nBrown et al., 2020). GARfurther conﬁrms that one\\ncan extract factual knowledge from PLMs, which\\nis not limited to the answers as in prior studies but\\nalso other relevant contexts.\\n3 Generation-Augmented Retrieval\\n3.1 Task Formulation\\nOpenQA aims to answer factoid questions with-\\nout pre-speciﬁed domains. We assume that a large\\ncollection of documents C(i.e., Wikipedia) are\\ngiven as the resource to answer the questions and\\na retriever-reader architecture is used to tackle the\\ntask, where the retriever retrieves a small subset\\nof the documents D⊂Cand the reader reads the\\ndocuments Dto extract (or generate) an answer.\\nOur goal is to improve the effectiveness and efﬁ-\\nciency of the retriever and consequently improve\\nthe performance of the reader.\\n3.2 Generation of Query Contexts\\nInGAR, queries are augmented with various heuris-\\ntically discovered relevant contexts in order to re-\\ntrieve more relevant passages in terms of both quan-\\ntity and quality. For the task of OpenQA where the\\nquery is a question, we take the following threefreely accessible contexts as the generation targets.\\nWe show in Sec. 6.2 that having multiple gener-\\nation targets is helpful in that fusing their results\\nconsistently brings better retrieval accuracy.\\nContext 1: The default target (answer) . The de-\\nfault target is the label in the task of interest, which\\nis the answer in OpenQA. The answer to the ques-\\ntion is apparently useful for the retrieval of relevant\\npassages that contain the answer itself. As shown\\nin previous work (Roberts et al., 2020; Brown et al.,\\n2020), PLMs are able to answer certain questions\\nsolely by taking the questions as input ( i.e., closed-\\nbook QA). Instead of using the generated answers\\ndirectly as in closed-book QA, GARtreats them\\nas contexts of the question for retrieval. The ad-\\nvantage is that even if the generated answers are\\npartially correct (or even incorrect), they may still\\nbeneﬁt retrieval as long as they are relevant to the\\npassages that contain the correct answers ( e.g., co-\\noccur with the correct answers).\\nContext 2: Sentence containing the default tar-\\nget. The sentence in a passage that contains the\\nanswer is used as another generation target. Sim-\\nilar to using answers as the generation target, the\\ngenerated sentences are still beneﬁcial for retriev-\\ning relevant passages even if they do not contain\\nthe answers, as their semantics is highly related to\\nthe questions/answers (examples in Sec. 6.1). One\\ncan take the relevant sentences in the ground-truth\\npassages (if any) or those in the positive passages\\nof a retriever as the reference, depending on the\\ntrade-off between reference quality and diversity.\\nContext 3: Title of passage containing the de-\\nfault target . One can also use the titles of rele-\\nvant passages as the generation target if available.\\nSpeciﬁcally, we retrieve Wikipedia passages using\\nBM25 with the question as the query, and take the\\npage titles of positive passages that contain the an-\\nswers as the generation target. We observe that\\nthe page titles of positive passages are often entity\\nnames of interest, and sometimes (but not always)\\nthe answers to the questions. Intuitively, if GAR\\nlearns which Wikipedia pages the question is re-\\nlated to, the queries augmented by the generated\\ntitles would naturally have a better chance of re-\\ntrieving those relevant passages.\\nWhile it is likely that some of the generated\\nquery contexts involve unfaithful or nonfactual in-\\nformation due to hallucination in text generation\\n(Mao et al., 2020) and introduce noise during re-\\ntrieval, they are beneﬁcial rather than harmful over-', 'metadata': {'page': 2, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': 'graph for question answering. arXiv preprint\\narXiv:1911.10470 .\\nIz Beltagy, Matthew E Peters, and Arman Cohan.\\n2020. Longformer: The long-document transformer.\\narXiv preprint arXiv:2004.05150 .\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. arXiv preprint arXiv:2005.14165 .\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 1870–\\n1879, Vancouver, Canada. Association for Computa-\\ntional Linguistics.\\nGordon V Cormack, Charles LA Clarke, and Stefan\\nBuettcher. 2009. Reciprocal rank fusion outper-\\nforms condorcet and individual rank learning meth-\\nods. In Proceedings of the 32nd international ACM\\nSIGIR conference on Research and development in\\ninformation retrieval , pages 758–759.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\\nCampos, and Ellen M V oorhees. 2020. Overview\\nof the trec 2019 deep learning track. arXiv preprint\\narXiv:2003.07820 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) ,\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\\nAlexander Miller, Kurt Shuster, Jack Urbanek,\\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\\nLowe, et al. 2020. The second conversational in-\\ntelligence challenge (convai2). In The NeurIPS’18\\nCompetition , pages 187–208. Springer.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training. arXiv\\npreprint arXiv:2002.08909 .\\nGautier Izacard and Edouard Grave. 2020. Lever-\\naging passage retrieval with generative models for\\nopen domain question answering. arXiv preprint\\narXiv:2007.01282 .\\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\\nBillion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 .Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale dis-\\ntantly supervised challenge dataset for reading com-\\nprehension. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 1601–1611, Van-\\ncouver, Canada. Association for Computational Lin-\\nguistics.\\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\\nWu, Sergey Edunov, Danqi Chen, and Wen-\\ntau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. arXiv preprint\\narXiv:2004.04906 .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\nNatural questions: A benchmark for question an-\\nswering research. Transactions of the Association\\nfor Computational Linguistics , 7:452–466.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 6086–6096, Florence,\\nItaly. Association for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\\nBart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and\\ncomprehension. arXiv preprint arXiv:1910.13461 .\\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\\nRockt ¨aschel, et al. 2020a. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. arXiv\\npreprint arXiv:2005.11401 .\\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\\n2020b. Question and answer test-train overlap in\\nopen-domain question answering datasets. arXiv\\npreprint arXiv:2008.02637 .\\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\\nNogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\\nJimmy Lin. 2020. Query reformulation using query\\nhistory for passage retrieval in conversational search.\\narXiv preprint arXiv:2005.02230 .\\nYe Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\\nPhilip S Yu. 2019. Generative question reﬁnement\\nwith deep reinforcement learning in retrieval-based\\nqa system. In Proceedings of the 28th ACM Inter-\\nnational Conference on Information and Knowledge\\nManagement , pages 1643–1652.', 'metadata': {'page': 9, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': 'graph for question answering. arXiv preprint\\narXiv:1911.10470 .\\nIz Beltagy, Matthew E Peters, and Arman Cohan.\\n2020. Longformer: The long-document transformer.\\narXiv preprint arXiv:2004.05150 .\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. arXiv preprint arXiv:2005.14165 .\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 1870–\\n1879, Vancouver, Canada. Association for Computa-\\ntional Linguistics.\\nGordon V Cormack, Charles LA Clarke, and Stefan\\nBuettcher. 2009. Reciprocal rank fusion outper-\\nforms condorcet and individual rank learning meth-\\nods. In Proceedings of the 32nd international ACM\\nSIGIR conference on Research and development in\\ninformation retrieval , pages 758–759.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\\nCampos, and Ellen M V oorhees. 2020. Overview\\nof the trec 2019 deep learning track. arXiv preprint\\narXiv:2003.07820 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) ,\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\\nAlexander Miller, Kurt Shuster, Jack Urbanek,\\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\\nLowe, et al. 2020. The second conversational in-\\ntelligence challenge (convai2). In The NeurIPS’18\\nCompetition , pages 187–208. Springer.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training. arXiv\\npreprint arXiv:2002.08909 .\\nGautier Izacard and Edouard Grave. 2020. Lever-\\naging passage retrieval with generative models for\\nopen domain question answering. arXiv preprint\\narXiv:2007.01282 .\\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\\nBillion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 .Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale dis-\\ntantly supervised challenge dataset for reading com-\\nprehension. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 1601–1611, Van-\\ncouver, Canada. Association for Computational Lin-\\nguistics.\\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\\nWu, Sergey Edunov, Danqi Chen, and Wen-\\ntau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. arXiv preprint\\narXiv:2004.04906 .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\nNatural questions: A benchmark for question an-\\nswering research. Transactions of the Association\\nfor Computational Linguistics , 7:452–466.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 6086–6096, Florence,\\nItaly. Association for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\\nBart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and\\ncomprehension. arXiv preprint arXiv:1910.13461 .\\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\\nRockt ¨aschel, et al. 2020a. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. arXiv\\npreprint arXiv:2005.11401 .\\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\\n2020b. Question and answer test-train overlap in\\nopen-domain question answering datasets. arXiv\\npreprint arXiv:2008.02637 .\\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\\nNogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\\nJimmy Lin. 2020. Query reformulation using query\\nhistory for passage retrieval in conversational search.\\narXiv preprint arXiv:2005.02230 .\\nYe Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\\nPhilip S Yu. 2019. Generative question reﬁnement\\nwith deep reinforcement learning in retrieval-based\\nqa system. In Proceedings of the 28th ACM Inter-\\nnational Conference on Information and Knowledge\\nManagement , pages 1643–1652.', 'metadata': {'page': 9, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}, {'page_content': 'graph for question answering. arXiv preprint\\narXiv:1911.10470 .\\nIz Beltagy, Matthew E Peters, and Arman Cohan.\\n2020. Longformer: The long-document transformer.\\narXiv preprint arXiv:2004.05150 .\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot\\nlearners. arXiv preprint arXiv:2005.14165 .\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\\nBordes. 2017. Reading Wikipedia to answer open-\\ndomain questions. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 1870–\\n1879, Vancouver, Canada. Association for Computa-\\ntional Linguistics.\\nGordon V Cormack, Charles LA Clarke, and Stefan\\nBuettcher. 2009. Reciprocal rank fusion outper-\\nforms condorcet and individual rank learning meth-\\nods. In Proceedings of the 32nd international ACM\\nSIGIR conference on Research and development in\\ninformation retrieval , pages 758–759.\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\\nCampos, and Ellen M V oorhees. 2020. Overview\\nof the trec 2019 deep learning track. arXiv preprint\\narXiv:2003.07820 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers) ,\\npages 4171–4186, Minneapolis, Minnesota. Associ-\\nation for Computational Linguistics.\\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\\nAlexander Miller, Kurt Shuster, Jack Urbanek,\\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\\nLowe, et al. 2020. The second conversational in-\\ntelligence challenge (convai2). In The NeurIPS’18\\nCompetition , pages 187–208. Springer.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\\naugmented language model pre-training. arXiv\\npreprint arXiv:2002.08909 .\\nGautier Izacard and Edouard Grave. 2020. Lever-\\naging passage retrieval with generative models for\\nopen domain question answering. arXiv preprint\\narXiv:2007.01282 .\\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\\nBillion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 .Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale dis-\\ntantly supervised challenge dataset for reading com-\\nprehension. In Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 1601–1611, Van-\\ncouver, Canada. Association for Computational Lin-\\nguistics.\\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\\nWu, Sergey Edunov, Danqi Chen, and Wen-\\ntau Yih. 2020. Dense passage retrieval for\\nopen-domain question answering. arXiv preprint\\narXiv:2004.04906 .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\\nNatural questions: A benchmark for question an-\\nswering research. Transactions of the Association\\nfor Computational Linguistics , 7:452–466.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proceedings of the\\n57th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 6086–6096, Florence,\\nItaly. Association for Computational Linguistics.\\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\\nBart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and\\ncomprehension. arXiv preprint arXiv:1910.13461 .\\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\\nRockt ¨aschel, et al. 2020a. Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. arXiv\\npreprint arXiv:2005.11401 .\\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\\n2020b. Question and answer test-train overlap in\\nopen-domain question answering datasets. arXiv\\npreprint arXiv:2008.02637 .\\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo\\nNogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\\nJimmy Lin. 2020. Query reformulation using query\\nhistory for passage retrieval in conversational search.\\narXiv preprint arXiv:2005.02230 .\\nYe Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, and\\nPhilip S Yu. 2019. Generative question reﬁnement\\nwith deep reinforcement learning in retrieval-based\\nqa system. In Proceedings of the 28th ACM Inter-\\nnational Conference on Information and Knowledge\\nManagement , pages 1643–1652.', 'metadata': {'page': 9, 'source': './data\\\\Generation-Augmented Retrieval for Open-Domain Question Answering.pdf'}}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming model is your machine learning model\n",
    "# query is the query you want to predict scores for\n",
    "query = \"What is Open-domain question answering?\"\n",
    "\n",
    "# Assuming docs_dict_list is a list of dictionaries with 'page_content' key\n",
    "# containing the content of each document\n",
    "docs_content_list = [doc['page_content'] for doc in docs_dict_list]\n",
    "\n",
    "# Initialize an empty list to store the scores along with document indices\n",
    "scores_list = []\n",
    "\n",
    "# Loop through each document and calculate the score\n",
    "for i, doc_content in enumerate(docs_content_list):\n",
    "    # Assuming model.predict returns a scalar score\n",
    "    score = model.predict([query, doc_content])\n",
    "    print(score)\n",
    "    \n",
    "    # Append the score along with the document index to the scores_list\n",
    "    scores_list.append((i, score))\n",
    "\n",
    "# Sort the scores_list in descending order based on scores\n",
    "scores_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Now, scores_list contains tuples of (document_index, score) sorted in descending order\n",
    "# You can use these indices to access the corresponding documents in docs_dict_list if needed\n",
    "# Example:\n",
    "sorted_documents = [docs_dict_list[i] for i, _ in scores_list]\n",
    "\n",
    "# Print or use the sorted_documents as needed\n",
    "print(sorted_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f67923-cc03-491e-9369-ecbb737eb1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
